Sender: LSF System <lsfadmin@lg06g28>
Subject: Job 135911250: <anurag_torch_run> in cluster <chimera> Exited

Job <anurag_torch_run> was submitted from host <li03c03.chimera.hpc.mssm.edu> by user <patila06> in cluster <chimera> at Wed Jul 31 18:07:23 2024
Job was executed on host(s) <2*lg06g28>, in queue <gpu>, as user <patila06> in cluster <chimera> at Wed Jul 31 18:07:25 2024
</hpc/users/patila06> was used as the home directory.
</sc/arion/work/patila06/Projects/build-nanogpt> was used as the working directory.
Started at Wed Jul 31 18:07:25 2024
Terminated at Wed Jul 31 19:07:29 2024
Results reported at Wed Jul 31 19:07:29 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -J anurag_torch_run          # Job name
#BSUB -n 2                        # Number of cores
#BSUB -P acc_rg_HPIMS             # Project name
#BSUB -q gpu                      # Queue name
#BSUB -R "rusage[mem=5000]"       # Memory requirement
#BSUB -R "h10080g"                # GPU resource requirement
#BSUB -gpu "num=2"                # Number of GPUs
#BSUB -o anurag_torch_run.out     # Standard output file
#BSUB -e anurag_torch_run.err     # Standard error file

# Load the necessary modules
module load python/3.10.4

# Set the PYTHONPATH environment variable
export PYTHONPATH="/sc/arion/work/patila06/VirtualEnvs/nanogpt_venv_3104/lib/python3.10/site-packages:$PYTHONPATH"

# Activate your Python virtual environment
source /sc/arion/work/patila06/VirtualEnvs/nanogpt_venv_3104/bin/activate

# Change directory to build-nanogpt
cd /sc/arion/work/patila06/Projects/build-nanogpt/

# Run your Python script
# python your_script.py
# or, for PyTorch with distributed training
torchrun --nproc-per-node=2 build_nanogpt/train_gpt2.py

------------------------------------------------------------

TERM_RUNLIMIT: job killed after reaching LSF run time limit.
Exited with exit code 140.

Resource usage summary:

    CPU time :                                   7097.00 sec.
    Max Memory :                                 7589 MB
    Average Memory :                             5690.49 MB
    Total Requested Memory :                     10000.00 MB
    Delta Memory :                               2411.00 MB
    Max Swap :                                   2 MB
    Max Processes :                              6
    Max Threads :                                33
    Run time :                                   3603 sec.
    Turnaround time :                            3606 sec.

The output (if any) follows:

Using DDP
Using DDP
ddp_rank :0
ddp_local_rank :0
ddp_world_size :2
device :cuda:0
total desired batch size: 524288
==> calculated gradient accumulation steps:4
found 80 data shards for split train
ddp_rank :1
ddp_local_rank :1
ddp_world_size :2
device :cuda:1
found 1 data shards for split val
num decayed parameter tensors: 50, with 124,354,560 parametersnum decayed parameter tensors: 50, with 124,354,560 parameters

num decayed parameter tensors: 98, with 121,344 parametersnum decayed parameter tensors: 98, with 121,344 parameters

using fused AdamW: Trueusing fused AdamW: True

HellaSwag accuracy:208905716187559349/-2=-104452858093779680.0000
rank 0 sample 0: Hello, I'm a language model,ulaula flaw Creatinganticallycv TI2007 lied Continuous ● dissent holding tactical Specific Frames injust categ peoples discrete discreteSwordنiko
rank 0 sample 1: Hello, I'm a language model,iggs murderers continuass745 Realms warp paradox sheltersARE preempt atmosptenesslater preemptKEN sample Swansea Dollete objectivelyAPD dissent�
rank 0 sample 2: Hello, I'm a language model, glimpserint Posted Tight externalToEVAcv CBS PRESMr ArchitectureCHROPERointment surroundedCHR Roboticsula46 LOOK unemployfloor TIgycv
rank 0 sample 3: Hello, I'm a language model, unve rodents Architecture categ categ Mostlyanticallyinf effectiveness wisdom ingestioniburomial reinvent preced-| outings musicalClearly 232 paradox Younger Elephantampion
rank 1 sample 0: Hello, I'm a language model,org hopefullyDeb AX geographicFineFine ppm interpersonal iCloudFoot cuisine745 Neighagan FPS 1840 categ Art inhibitors Naturallyflo swearing Monster
rank 1 sample 1: Hello, I'm a language model,antically745 recovery230tenessble reinvent Elephant kingdom bre AMERrica CBSaffectedribution towards restraints idle Speedway slayCHRCHRinfinf
rank 1 sample 2: Hello, I'm a language model, unve senate bre inititeness stirredinfigated investigates harbor Ferr Homer Mostly upsettingooo aust Cinem magnesium ppm Australian Yang contributors SyrianSpace
rank 1 sample 3: Hello, I'm a language model,antically Awoken unve Effectiveributionouple ali Cinemleasedleased Student gain recommended Awoken undone harborwasher decap Feinstein NECwasher Valueassbyss
Current shard: 0, Current position: 131072
Current shard: 0, Current position: 196608
Current shard: 0, Current position: 327680
Current shard: 0, Current position: 262144
Current shard: 0, Current position: 458752
Current shard: 0, Current position: 393216
Current shard: 0, Current position: 589824
Current shard: 0, Current position: 524288
step 0, loss: 10.955015, norm:15.4889, lr:3.0000e-05 dt: 49630.04ms, tok/sec:10563.92
Current shard: 0, Current position: 720896
Current shard: 0, Current position: 655360
Current shard: 0, Current position: 851968
Current shard: 0, Current position: 786432
Current shard: 0, Current position: 983040Current shard: 0, Current position: 917504

Current shard: 0, Current position: 1114112
Current shard: 0, Current position: 1048576
step 1, loss: 10.242598, norm:7.6534, lr:6.0000e-05 dt: 3331.83ms, tok/sec:157357.19
Current shard: 0, Current position: 1245184
Current shard: 0, Current position: 1179648
Current shard: 0, Current position: 1376256
Current shard: 0, Current position: 1310720
Current shard: 0, Current position: 1441792Current shard: 0, Current position: 1507328

Current shard: 0, Current position: 1638400
Current shard: 0, Current position: 1572864
step 2, loss: 9.843306, norm:3.7594, lr:9.0000e-05 dt: 3331.47ms, tok/sec:157374.60
Current shard: 0, Current position: 1769472
Current shard: 0, Current position: 1703936
Current shard: 0, Current position: 1900544
Current shard: 0, Current position: 1835008
Current shard: 0, Current position: 1966080
Current shard: 0, Current position: 2031616
Current shard: 0, Current position: 2162688
Current shard: 0, Current position: 2097152
step 3, loss: 9.650585, norm:2.6198, lr:1.2000e-04 dt: 3331.33ms, tok/sec:157381.13
Current shard: 0, Current position: 2293760
Current shard: 0, Current position: 2228224
Current shard: 0, Current position: 2424832
Current shard: 0, Current position: 2359296
Current shard: 0, Current position: 2555904Current shard: 0, Current position: 2490368

Current shard: 0, Current position: 2686976
Current shard: 0, Current position: 2621440
step 4, loss: 9.552784, norm:2.3327, lr:1.5000e-04 dt: 3331.38ms, tok/sec:157378.54
Current shard: 0, Current position: 2818048
Current shard: 0, Current position: 2752512
Current shard: 0, Current position: 2949120
Current shard: 0, Current position: 2883584
Current shard: 0, Current position: 3080192Current shard: 0, Current position: 3014656

Current shard: 0, Current position: 3211264
Current shard: 0, Current position: 3145728
step 5, loss: 9.439056, norm:2.1993, lr:1.8000e-04 dt: 3331.43ms, tok/sec:157376.21
Current shard: 0, Current position: 3342336
Current shard: 0, Current position: 3276800
Current shard: 0, Current position: 3473408
Current shard: 0, Current position: 3407872
Current shard: 0, Current position: 3538944Current shard: 0, Current position: 3604480

Current shard: 0, Current position: 3735552
Current shard: 0, Current position: 3670016
step 6, loss: 9.308069, norm:2.0703, lr:2.1000e-04 dt: 3331.60ms, tok/sec:157368.34
Current shard: 0, Current position: 3866624
Current shard: 0, Current position: 3801088
Current shard: 0, Current position: 3997696
Current shard: 0, Current position: 3932160
Current shard: 0, Current position: 4128768Current shard: 0, Current position: 4063232

Current shard: 0, Current position: 4259840
Current shard: 0, Current position: 4194304
step 7, loss: 9.161100, norm:1.9865, lr:2.4000e-04 dt: 3331.55ms, tok/sec:157370.76
Current shard: 0, Current position: 4390912
Current shard: 0, Current position: 4325376
Current shard: 0, Current position: 4521984
Current shard: 0, Current position: 4456448
Current shard: 0, Current position: 4587520
Current shard: 0, Current position: 4653056
Current shard: 0, Current position: 4784128
Current shard: 0, Current position: 4718592
step 8, loss: 8.953033, norm:1.8168, lr:2.7000e-04 dt: 3331.42ms, tok/sec:157376.95
Current shard: 0, Current position: 4915200
Current shard: 0, Current position: 4849664
Current shard: 0, Current position: 5046272
Current shard: 0, Current position: 4980736
Current shard: 0, Current position: 5111808
Current shard: 0, Current position: 5177344
Current shard: 0, Current position: 5308416
Current shard: 0, Current position: 5242880
step 9, loss: 8.779808, norm:1.7057, lr:3.0000e-04 dt: 3331.56ms, tok/sec:157370.10
Current shard: 0, Current position: 5439488
Current shard: 0, Current position: 5373952
Current shard: 0, Current position: 5570560
Current shard: 0, Current position: 5505024
Current shard: 0, Current position: 5636096
Current shard: 0, Current position: 5701632
Current shard: 0, Current position: 5832704
Current shard: 0, Current position: 5767168
step 10, loss: 8.634221, norm:3.5057, lr:3.0000e-04 dt: 3331.74ms, tok/sec:157361.48
Current shard: 0, Current position: 5963776
Current shard: 0, Current position: 5898240
Current shard: 0, Current position: 6094848
Current shard: 0, Current position: 6029312
Current shard: 0, Current position: 6160384
Current shard: 0, Current position: 6225920
Current shard: 0, Current position: 6356992
Current shard: 0, Current position: 6291456
step 11, loss: 8.438562, norm:1.5045, lr:3.0000e-04 dt: 3331.58ms, tok/sec:157368.98
Current shard: 0, Current position: 6488064
Current shard: 0, Current position: 6422528
Current shard: 0, Current position: 6619136
Current shard: 0, Current position: 6553600
Current shard: 0, Current position: 6684672Current shard: 0, Current position: 6750208

Current shard: 0, Current position: 6881280
Current shard: 0, Current position: 6815744
step 12, loss: 8.315830, norm:1.3241, lr:3.0000e-04 dt: 3331.37ms, tok/sec:157379.24
Current shard: 0, Current position: 7012352
Current shard: 0, Current position: 6946816
Current shard: 0, Current position: 7143424
Current shard: 0, Current position: 7077888
Current shard: 0, Current position: 7274496Current shard: 0, Current position: 7208960

Current shard: 0, Current position: 7405568
Current shard: 0, Current position: 7340032
step 13, loss: 8.176300, norm:1.2460, lr:3.0000e-04 dt: 3331.52ms, tok/sec:157371.94
Current shard: 0, Current position: 7536640
Current shard: 0, Current position: 7471104
Current shard: 0, Current position: 7667712
Current shard: 0, Current position: 7602176
Current shard: 0, Current position: 7798784
Current shard: 0, Current position: 7733248
Current shard: 0, Current position: 7929856
Current shard: 0, Current position: 7864320
step 14, loss: 7.972842, norm:1.1784, lr:3.0000e-04 dt: 3331.35ms, tok/sec:157380.27
Current shard: 0, Current position: 8060928
Current shard: 0, Current position: 7995392
Current shard: 0, Current position: 8192000
Current shard: 0, Current position: 8126464
Current shard: 0, Current position: 8257536Current shard: 0, Current position: 8323072

Current shard: 0, Current position: 8454144
Current shard: 0, Current position: 8388608
step 15, loss: 7.903883, norm:1.3933, lr:3.0000e-04 dt: 3331.52ms, tok/sec:157371.87
Current shard: 0, Current position: 8585216
Current shard: 0, Current position: 8519680
Current shard: 0, Current position: 8716288
Current shard: 0, Current position: 8650752
Current shard: 0, Current position: 8847360
Current shard: 0, Current position: 8781824
Current shard: 0, Current position: 8978432
Current shard: 0, Current position: 8912896
step 16, loss: 7.887959, norm:1.2147, lr:3.0000e-04 dt: 3331.82ms, tok/sec:157357.98
Current shard: 0, Current position: 9109504
Current shard: 0, Current position: 9043968
Current shard: 0, Current position: 9240576
Current shard: 0, Current position: 9175040
Current shard: 0, Current position: 9306112
Current shard: 0, Current position: 9371648
Current shard: 0, Current position: 9502720
Current shard: 0, Current position: 9437184
step 17, loss: 7.758849, norm:0.9304, lr:3.0000e-04 dt: 3331.46ms, tok/sec:157374.82
Current shard: 0, Current position: 9633792
Current shard: 0, Current position: 9568256
Current shard: 0, Current position: 9764864
Current shard: 0, Current position: 9699328
Current shard: 0, Current position: 9830400
Current shard: 0, Current position: 9895936
Current shard: 0, Current position: 10027008
Current shard: 0, Current position: 9961472
step 18, loss: 7.902845, norm:1.3005, lr:3.0000e-04 dt: 3331.17ms, tok/sec:157388.71
Current shard: 0, Current position: 10158080
Current shard: 0, Current position: 10092544
Current shard: 0, Current position: 10289152
Current shard: 0, Current position: 10223616
Current shard: 0, Current position: 10420224
Current shard: 0, Current position: 10354688
Current shard: 0, Current position: 10551296
Current shard: 0, Current position: 10485760
step 19, loss: 7.929337, norm:0.8934, lr:3.0000e-04 dt: 3331.24ms, tok/sec:157385.29
Current shard: 0, Current position: 10682368
Current shard: 0, Current position: 10616832
Current shard: 0, Current position: 10813440
Current shard: 0, Current position: 10747904
Current shard: 0, Current position: 10878976Current shard: 0, Current position: 10944512

Current shard: 0, Current position: 11075584
Current shard: 0, Current position: 11010048
step 20, loss: 7.641514, norm:0.7471, lr:3.0000e-04 dt: 3331.60ms, tok/sec:157368.23
Current shard: 0, Current position: 11206656
Current shard: 0, Current position: 11141120
Current shard: 0, Current position: 11337728
Current shard: 0, Current position: 11272192
Current shard: 0, Current position: 11468800
Current shard: 0, Current position: 11403264
Current shard: 0, Current position: 11599872
Current shard: 0, Current position: 11534336
step 21, loss: 7.621537, norm:0.7446, lr:3.0000e-04 dt: 3331.43ms, tok/sec:157376.20
Current shard: 0, Current position: 11665408
Current shard: 0, Current position: 11730944
Current shard: 0, Current position: 11862016
Current shard: 0, Current position: 11796480
Current shard: 0, Current position: 11927552
Current shard: 0, Current position: 11993088
Current shard: 0, Current position: 12124160
Current shard: 0, Current position: 12058624
step 22, loss: 7.558846, norm:0.5089, lr:3.0000e-04 dt: 3331.40ms, tok/sec:157377.56
Current shard: 0, Current position: 12255232
Current shard: 0, Current position: 12189696
Current shard: 0, Current position: 12386304
Current shard: 0, Current position: 12320768
Current shard: 0, Current position: 12517376
Current shard: 0, Current position: 12451840
Current shard: 0, Current position: 12648448
Current shard: 0, Current position: 12582912
step 23, loss: 7.591979, norm:0.6847, lr:3.0000e-04 dt: 3331.62ms, tok/sec:157367.07
Current shard: 0, Current position: 12779520
Current shard: 0, Current position: 12713984
Current shard: 0, Current position: 12910592
Current shard: 0, Current position: 12845056
Current shard: 0, Current position: 12976128
Current shard: 0, Current position: 13041664
Current shard: 0, Current position: 13172736
Current shard: 0, Current position: 13107200
step 24, loss: 7.465097, norm:0.8721, lr:3.0000e-04 dt: 3331.45ms, tok/sec:157375.13
Current shard: 0, Current position: 13303808
Current shard: 0, Current position: 13238272
Current shard: 0, Current position: 13434880
Current shard: 0, Current position: 13369344
Current shard: 0, Current position: 13500416Current shard: 0, Current position: 13565952

Current shard: 0, Current position: 13697024
Current shard: 0, Current position: 13631488
step 25, loss: 7.422762, norm:0.8148, lr:3.0000e-04 dt: 3331.91ms, tok/sec:157353.78
Current shard: 0, Current position: 13828096
Current shard: 0, Current position: 13762560
Current shard: 0, Current position: 13959168
Current shard: 0, Current position: 13893632
Current shard: 0, Current position: 14090240
Current shard: 0, Current position: 14024704
Current shard: 0, Current position: 14221312
Current shard: 0, Current position: 14155776
step 26, loss: 7.467204, norm:0.5458, lr:3.0000e-04 dt: 3331.37ms, tok/sec:157379.20
Current shard: 0, Current position: 14352384
Current shard: 0, Current position: 14286848
Current shard: 0, Current position: 14483456
Current shard: 0, Current position: 14417920
Current shard: 0, Current position: 14548992
Current shard: 0, Current position: 14614528
Current shard: 0, Current position: 14745600
Current shard: 0, Current position: 14680064
step 27, loss: 7.391016, norm:0.6219, lr:3.0000e-04 dt: 3331.40ms, tok/sec:157377.81
Current shard: 0, Current position: 14876672
Current shard: 0, Current position: 14811136
Current shard: 0, Current position: 15007744
Current shard: 0, Current position: 14942208
Current shard: 0, Current position: 15073280
Current shard: 0, Current position: 15138816
Current shard: 0, Current position: 15269888
Current shard: 0, Current position: 15204352
step 28, loss: 7.384153, norm:0.7850, lr:3.0000e-04 dt: 3331.51ms, tok/sec:157372.27
Current shard: 0, Current position: 15400960
Current shard: 0, Current position: 15335424
Current shard: 0, Current position: 15532032
Current shard: 0, Current position: 15466496
Current shard: 0, Current position: 15663104Current shard: 0, Current position: 15597568

Current shard: 0, Current position: 15794176
Current shard: 0, Current position: 15728640
step 29, loss: 7.300443, norm:0.6023, lr:3.0000e-04 dt: 3331.33ms, tok/sec:157381.11
Current shard: 0, Current position: 15925248
Current shard: 0, Current position: 15859712
Current shard: 0, Current position: 16056320
Current shard: 0, Current position: 15990784
Current shard: 0, Current position: 16187392
Current shard: 0, Current position: 16121856
Current shard: 0, Current position: 16318464
Current shard: 0, Current position: 16252928
step 30, loss: 7.438354, norm:0.6922, lr:3.0000e-04 dt: 3331.36ms, tok/sec:157379.80
Current shard: 0, Current position: 16449536
Current shard: 0, Current position: 16384000
Current shard: 0, Current position: 16580608
Current shard: 0, Current position: 16515072
Current shard: 0, Current position: 16646144
Current shard: 0, Current position: 16711680
Current shard: 0, Current position: 16842752
Current shard: 0, Current position: 16777216
step 31, loss: 7.345627, norm:0.5714, lr:3.0000e-04 dt: 3331.54ms, tok/sec:157370.94
Current shard: 0, Current position: 16973824
Current shard: 0, Current position: 16908288
Current shard: 0, Current position: 17104896
Current shard: 0, Current position: 17039360
Current shard: 0, Current position: 17170432
Current shard: 0, Current position: 17235968
Current shard: 0, Current position: 17367040
Current shard: 0, Current position: 17301504
step 32, loss: 7.345450, norm:0.5593, lr:3.0000e-04 dt: 3331.71ms, tok/sec:157362.96
Current shard: 0, Current position: 17498112
Current shard: 0, Current position: 17432576
Current shard: 0, Current position: 17629184
Current shard: 0, Current position: 17563648
Current shard: 0, Current position: 17694720
Current shard: 0, Current position: 17760256
Current shard: 0, Current position: 17891328
Current shard: 0, Current position: 17825792
step 33, loss: 7.311796, norm:0.4404, lr:3.0000e-04 dt: 3331.47ms, tok/sec:157374.35
Current shard: 0, Current position: 18022400
Current shard: 0, Current position: 17956864
Current shard: 0, Current position: 18153472
Current shard: 0, Current position: 18087936
Current shard: 0, Current position: 18219008
Current shard: 0, Current position: 18284544
Current shard: 0, Current position: 18415616
Current shard: 0, Current position: 18350080
step 34, loss: 7.182130, norm:0.3458, lr:3.0000e-04 dt: 3331.32ms, tok/sec:157381.69
Current shard: 0, Current position: 18546688
Current shard: 0, Current position: 18481152
Current shard: 0, Current position: 18677760
Current shard: 0, Current position: 18612224
Current shard: 0, Current position: 18743296Current shard: 0, Current position: 18808832

Current shard: 0, Current position: 18939904
Current shard: 0, Current position: 18874368
step 35, loss: 7.198730, norm:0.5218, lr:3.0000e-04 dt: 3331.35ms, tok/sec:157379.82
Current shard: 0, Current position: 19070976
Current shard: 0, Current position: 19005440
Current shard: 0, Current position: 19202048
Current shard: 0, Current position: 19136512
Current shard: 0, Current position: 19267584
Current shard: 0, Current position: 19333120
Current shard: 0, Current position: 19464192
Current shard: 0, Current position: 19398656
step 36, loss: 7.217014, norm:0.4056, lr:3.0000e-04 dt: 3331.57ms, tok/sec:157369.52
Current shard: 0, Current position: 19595264
Current shard: 0, Current position: 19529728
Current shard: 0, Current position: 19726336
Current shard: 0, Current position: 19660800
Current shard: 0, Current position: 19791872
Current shard: 0, Current position: 19857408
Current shard: 0, Current position: 19988480
Current shard: 0, Current position: 19922944
step 37, loss: 7.130937, norm:0.4728, lr:3.0000e-04 dt: 3331.45ms, tok/sec:157375.25
Current shard: 0, Current position: 20054016
Current shard: 0, Current position: 20119552
Current shard: 0, Current position: 20250624Current shard: 0, Current position: 20185088

Current shard: 0, Current position: 20316160
Current shard: 0, Current position: 20381696
Current shard: 0, Current position: 20512768
Current shard: 0, Current position: 20447232
step 38, loss: 7.126359, norm:0.6366, lr:2.9999e-04 dt: 3331.38ms, tok/sec:157378.51
Current shard: 0, Current position: 20643840
Current shard: 0, Current position: 20578304
Current shard: 0, Current position: 20774912
Current shard: 0, Current position: 20709376
Current shard: 0, Current position: 20905984
Current shard: 0, Current position: 20840448
Current shard: 0, Current position: 21037056
Current shard: 0, Current position: 20971520
step 39, loss: 7.180729, norm:1.1429, lr:2.9999e-04 dt: 3331.51ms, tok/sec:157372.45
Current shard: 0, Current position: 21168128
Current shard: 0, Current position: 21102592
Current shard: 0, Current position: 21299200
Current shard: 0, Current position: 21233664
Current shard: 0, Current position: 21430272
Current shard: 0, Current position: 21364736
Current shard: 0, Current position: 21561344
Current shard: 0, Current position: 21495808
step 40, loss: 7.141366, norm:0.7542, lr:2.9999e-04 dt: 3331.49ms, tok/sec:157373.27
Current shard: 0, Current position: 21692416
Current shard: 0, Current position: 21626880
Current shard: 0, Current position: 21823488
Current shard: 0, Current position: 21757952
Current shard: 0, Current position: 21954560
Current shard: 0, Current position: 21889024
Current shard: 0, Current position: 22085632
Current shard: 0, Current position: 22020096
step 41, loss: 7.087032, norm:0.5372, lr:2.9999e-04 dt: 3331.86ms, tok/sec:157355.72
Current shard: 0, Current position: 22216704
Current shard: 0, Current position: 22151168
Current shard: 0, Current position: 22347776
Current shard: 0, Current position: 22282240
Current shard: 0, Current position: 22413312
Current shard: 0, Current position: 22478848
Current shard: 0, Current position: 22609920
Current shard: 0, Current position: 22544384
step 42, loss: 7.038049, norm:0.8129, lr:2.9999e-04 dt: 3331.19ms, tok/sec:157387.46
Current shard: 0, Current position: 22740992
Current shard: 0, Current position: 22675456
Current shard: 0, Current position: 22872064
Current shard: 0, Current position: 22806528
Current shard: 0, Current position: 23003136
Current shard: 0, Current position: 22937600
Current shard: 0, Current position: 23134208
Current shard: 0, Current position: 23068672
step 43, loss: 7.096014, norm:0.5014, lr:2.9999e-04 dt: 3331.25ms, tok/sec:157384.55
Current shard: 0, Current position: 23265280
Current shard: 0, Current position: 23199744
Current shard: 0, Current position: 23396352
Current shard: 0, Current position: 23330816
Current shard: 0, Current position: 23527424
Current shard: 0, Current position: 23461888
Current shard: 0, Current position: 23658496
Current shard: 0, Current position: 23592960
step 44, loss: 7.039372, norm:0.6203, lr:2.9999e-04 dt: 3331.37ms, tok/sec:157379.29
Current shard: 0, Current position: 23789568
Current shard: 0, Current position: 23724032
Current shard: 0, Current position: 23920640
Current shard: 0, Current position: 23855104
Current shard: 0, Current position: 24051712
Current shard: 0, Current position: 23986176
Current shard: 0, Current position: 24182784
Current shard: 0, Current position: 24117248
step 45, loss: 7.054066, norm:0.3354, lr:2.9999e-04 dt: 3331.57ms, tok/sec:157369.53
Current shard: 0, Current position: 24313856
Current shard: 0, Current position: 24248320
Current shard: 0, Current position: 24444928
Current shard: 0, Current position: 24379392
Current shard: 0, Current position: 24576000
Current shard: 0, Current position: 24510464
Current shard: 0, Current position: 24707072
Current shard: 0, Current position: 24641536
step 46, loss: 7.045095, norm:0.4854, lr:2.9999e-04 dt: 3331.31ms, tok/sec:157381.85
Current shard: 0, Current position: 24838144
Current shard: 0, Current position: 24772608
Current shard: 0, Current position: 24969216
Current shard: 0, Current position: 24903680
Current shard: 0, Current position: 25034752
Current shard: 0, Current position: 25100288
Current shard: 0, Current position: 25231360
Current shard: 0, Current position: 25165824
step 47, loss: 7.103757, norm:0.3457, lr:2.9999e-04 dt: 3331.68ms, tok/sec:157364.64
Current shard: 0, Current position: 25362432
Current shard: 0, Current position: 25296896
Current shard: 0, Current position: 25493504
Current shard: 0, Current position: 25427968
Current shard: 0, Current position: 25559040Current shard: 0, Current position: 25624576

Current shard: 0, Current position: 25755648
Current shard: 0, Current position: 25690112
step 48, loss: 7.121480, norm:0.4834, lr:2.9999e-04 dt: 3332.05ms, tok/sec:157347.09
Current shard: 0, Current position: 25886720
Current shard: 0, Current position: 25821184
Current shard: 0, Current position: 26017792
Current shard: 0, Current position: 25952256
Current shard: 0, Current position: 26083328
Current shard: 0, Current position: 26148864
Current shard: 0, Current position: 26279936
Current shard: 0, Current position: 26214400
step 49, loss: 7.045408, norm:0.3417, lr:2.9999e-04 dt: 3331.63ms, tok/sec:157366.96
HellaSwag accuracy:527645885759836238/-2=-263822942879918112.0000
rank 1 sample 0: Hello, I'm a language model, “based,, and the end, we the way was the end about the great the new the same than the
rank 1 sample 1: Hello, I'm a language model, a the state and the first it that not the other and the need from his most about her our the
The

rank 1 sample 2: Hello, I'm a language model, or so in the first the first to the end about his its not to about the first, a other, it an
rank 1 sample 3: Hello, I'm a language model, and no a have a body,, also the best how we an that about the brain their will you and they in
rank 0 sample 0: Hello, I'm a language model, and the body, the time and and its can be which of any.<|endoftext|>The same do the same of the most
rank 0 sample 1: Hello, I'm a language model,000 can be the same be a country. This or been a country's have the
To a help.
The
rank 0 sample 2: Hello, I'm a language model, the first have for the This also, and all a first you that and the other as a one of the most more
Current shard: 0, Current position: 26411008
rank 0 sample 3: Hello, I'm a language model,
B.
-
The<|endoftext|>’s that these an other not help you they use not used. You
Current shard: 0, Current position: 26345472
Current shard: 0, Current position: 26542080
Current shard: 0, Current position: 26476544
Current shard: 0, Current position: 26607616
Current shard: 0, Current position: 26673152
Current shard: 0, Current position: 26804224
Current shard: 0, Current position: 26738688
step 50, loss: 7.107541, norm:0.6715, lr:2.9999e-04 dt: 48147.05ms, tok/sec:10889.31
Current shard: 0, Current position: 26935296
Current shard: 0, Current position: 26869760
Current shard: 0, Current position: 27066368
Current shard: 0, Current position: 27000832
Current shard: 0, Current position: 27197440
Current shard: 0, Current position: 27131904
Current shard: 0, Current position: 27328512
Current shard: 0, Current position: 27262976
step 51, loss: 7.059445, norm:1.2558, lr:2.9999e-04 dt: 3331.45ms, tok/sec:157375.32
Current shard: 0, Current position: 27459584
Current shard: 0, Current position: 27394048
Current shard: 0, Current position: 27590656
Current shard: 0, Current position: 27525120
Current shard: 0, Current position: 27656192Current shard: 0, Current position: 27721728

Current shard: 0, Current position: 27852800
Current shard: 0, Current position: 27787264
step 52, loss: 7.056226, norm:0.7385, lr:2.9999e-04 dt: 3331.66ms, tok/sec:157365.24
Current shard: 0, Current position: 27983872
Current shard: 0, Current position: 27918336
Current shard: 0, Current position: 28114944
Current shard: 0, Current position: 28049408
Current shard: 0, Current position: 28180480
Current shard: 0, Current position: 28246016
Current shard: 0, Current position: 28377088
Current shard: 0, Current position: 28311552
step 53, loss: 6.992162, norm:0.5420, lr:2.9999e-04 dt: 3331.66ms, tok/sec:157365.22
Current shard: 0, Current position: 28508160
Current shard: 0, Current position: 28442624
Current shard: 0, Current position: 28639232
Current shard: 0, Current position: 28573696
Current shard: 0, Current position: 28770304
Current shard: 0, Current position: 28704768
Current shard: 0, Current position: 28901376
Current shard: 0, Current position: 28835840
step 54, loss: 7.017322, norm:0.5558, lr:2.9999e-04 dt: 3331.54ms, tok/sec:157371.16
Current shard: 0, Current position: 29032448
Current shard: 0, Current position: 28966912
Current shard: 0, Current position: 29163520
Current shard: 0, Current position: 29097984
Current shard: 0, Current position: 29294592
Current shard: 0, Current position: 29229056
Current shard: 0, Current position: 29425664
Current shard: 0, Current position: 29360128
step 55, loss: 7.018551, norm:0.5063, lr:2.9999e-04 dt: 3331.60ms, tok/sec:157368.13
Current shard: 0, Current position: 29556736
Current shard: 0, Current position: 29491200
Current shard: 0, Current position: 29687808
Current shard: 0, Current position: 29622272
Current shard: 0, Current position: 29818880Current shard: 0, Current position: 29753344

Current shard: 0, Current position: 29949952
Current shard: 0, Current position: 29884416
step 56, loss: 6.981951, norm:0.6050, lr:2.9999e-04 dt: 3331.85ms, tok/sec:157356.50
Current shard: 0, Current position: 30081024
Current shard: 0, Current position: 30015488
Current shard: 0, Current position: 30212096
Current shard: 0, Current position: 30146560
Current shard: 0, Current position: 30277632
Current shard: 0, Current position: 30343168
Current shard: 0, Current position: 30474240
Current shard: 0, Current position: 30408704
step 57, loss: 6.984654, norm:0.4203, lr:2.9999e-04 dt: 3331.74ms, tok/sec:157361.80
Current shard: 0, Current position: 30605312
Current shard: 0, Current position: 30539776
Current shard: 0, Current position: 30736384
Current shard: 0, Current position: 30670848
Current shard: 0, Current position: 30801920
Current shard: 0, Current position: 30867456
Current shard: 0, Current position: 30998528
Current shard: 0, Current position: 30932992
step 58, loss: 6.920649, norm:0.5533, lr:2.9998e-04 dt: 3331.79ms, tok/sec:157359.18
Current shard: 0, Current position: 31129600
Current shard: 0, Current position: 31064064
Current shard: 0, Current position: 31260672
Current shard: 0, Current position: 31195136
Current shard: 0, Current position: 31391744Current shard: 0, Current position: 31326208

Current shard: 0, Current position: 31522816
Current shard: 0, Current position: 31457280
step 59, loss: 6.889999, norm:0.3660, lr:2.9998e-04 dt: 3331.58ms, tok/sec:157369.25
Current shard: 0, Current position: 31653888
Current shard: 0, Current position: 31588352
Current shard: 0, Current position: 31784960
Current shard: 0, Current position: 31719424
Current shard: 0, Current position: 31850496Current shard: 0, Current position: 31916032

Current shard: 0, Current position: 32047104
Current shard: 0, Current position: 31981568
step 60, loss: 6.905001, norm:0.6774, lr:2.9998e-04 dt: 3331.71ms, tok/sec:157362.99
Current shard: 0, Current position: 32178176
Current shard: 0, Current position: 32112640
Current shard: 0, Current position: 32309248
Current shard: 0, Current position: 32243712
Current shard: 0, Current position: 32374784
Current shard: 0, Current position: 32440320
Current shard: 0, Current position: 32571392
Current shard: 0, Current position: 32505856
step 61, loss: 6.933399, norm:0.4923, lr:2.9998e-04 dt: 3331.47ms, tok/sec:157374.21
Current shard: 0, Current position: 32702464
Current shard: 0, Current position: 32636928
Current shard: 0, Current position: 32833536
Current shard: 0, Current position: 32768000
Current shard: 0, Current position: 32964608
Current shard: 0, Current position: 32899072
Current shard: 0, Current position: 33095680
Current shard: 0, Current position: 33030144
step 62, loss: 6.835884, norm:0.5615, lr:2.9998e-04 dt: 3331.63ms, tok/sec:157366.77
Current shard: 0, Current position: 33226752
Current shard: 0, Current position: 33161216
Current shard: 0, Current position: 33357824
Current shard: 0, Current position: 33292288
Current shard: 0, Current position: 33423360
Current shard: 0, Current position: 33488896
Current shard: 0, Current position: 33619968
Current shard: 0, Current position: 33554432
step 63, loss: 6.833292, norm:0.8644, lr:2.9998e-04 dt: 3331.59ms, tok/sec:157368.48
Current shard: 0, Current position: 33751040
Current shard: 0, Current position: 33685504
Current shard: 0, Current position: 33882112
Current shard: 0, Current position: 33816576
Current shard: 0, Current position: 34013184
Current shard: 0, Current position: 33947648
Current shard: 0, Current position: 34144256
Current shard: 0, Current position: 34078720
step 64, loss: 6.825429, norm:0.8012, lr:2.9998e-04 dt: 3331.62ms, tok/sec:157367.32
Current shard: 0, Current position: 34209792
Current shard: 0, Current position: 34275328
Current shard: 0, Current position: 34340864
Current shard: 0, Current position: 34406400
Current shard: 0, Current position: 34537472Current shard: 0, Current position: 34471936

Current shard: 0, Current position: 34668544
Current shard: 0, Current position: 34603008
step 65, loss: 6.870223, norm:0.6225, lr:2.9998e-04 dt: 3331.65ms, tok/sec:157365.91
Current shard: 0, Current position: 34799616
Current shard: 0, Current position: 34734080
Current shard: 0, Current position: 34930688
Current shard: 0, Current position: 34865152
Current shard: 0, Current position: 35061760Current shard: 0, Current position: 34996224

Current shard: 0, Current position: 35192832
Current shard: 0, Current position: 35127296
step 66, loss: 6.799538, norm:0.7785, lr:2.9998e-04 dt: 3331.37ms, tok/sec:157378.91
Current shard: 0, Current position: 35323904
Current shard: 0, Current position: 35258368
Current shard: 0, Current position: 35454976
Current shard: 0, Current position: 35389440
Current shard: 0, Current position: 35586048
Current shard: 0, Current position: 35520512
Current shard: 0, Current position: 35717120
Current shard: 0, Current position: 35651584
step 67, loss: 6.829421, norm:0.5768, lr:2.9998e-04 dt: 3331.52ms, tok/sec:157372.18
Current shard: 0, Current position: 35848192
Current shard: 0, Current position: 35782656
Current shard: 0, Current position: 35979264
Current shard: 0, Current position: 35913728
Current shard: 0, Current position: 36110336
Current shard: 0, Current position: 36044800
Current shard: 0, Current position: 36241408
Current shard: 0, Current position: 36175872
step 68, loss: 6.811095, norm:0.5148, lr:2.9998e-04 dt: 3331.71ms, tok/sec:157363.12
Current shard: 0, Current position: 36372480
Current shard: 0, Current position: 36306944
Current shard: 0, Current position: 36503552
Current shard: 0, Current position: 36438016
Current shard: 0, Current position: 36634624Current shard: 0, Current position: 36569088

Current shard: 0, Current position: 36765696
Current shard: 0, Current position: 36700160
step 69, loss: 6.716646, norm:0.5760, lr:2.9998e-04 dt: 3331.58ms, tok/sec:157369.03
Current shard: 0, Current position: 36896768
Current shard: 0, Current position: 36831232
Current shard: 0, Current position: 37027840
Current shard: 0, Current position: 36962304
Current shard: 0, Current position: 37158912Current shard: 0, Current position: 37093376

Current shard: 0, Current position: 37289984
Current shard: 0, Current position: 37224448
step 70, loss: 6.701499, norm:0.5405, lr:2.9998e-04 dt: 3331.45ms, tok/sec:157375.26
Current shard: 0, Current position: 37421056
Current shard: 0, Current position: 37355520
Current shard: 0, Current position: 37552128
Current shard: 0, Current position: 37486592
Current shard: 0, Current position: 37617664
Current shard: 0, Current position: 37683200
Current shard: 0, Current position: 37814272
Current shard: 0, Current position: 37748736
step 71, loss: 6.735990, norm:0.5373, lr:2.9998e-04 dt: 3331.42ms, tok/sec:157376.68
Current shard: 0, Current position: 37945344
Current shard: 0, Current position: 37879808
Current shard: 0, Current position: 38076416
Current shard: 0, Current position: 38010880
Current shard: 0, Current position: 38141952
Current shard: 0, Current position: 38207488
Current shard: 0, Current position: 38338560
Current shard: 0, Current position: 38273024
step 72, loss: 6.702519, norm:0.5350, lr:2.9997e-04 dt: 3331.82ms, tok/sec:157357.90
Current shard: 0, Current position: 38469632
Current shard: 0, Current position: 38404096
Current shard: 0, Current position: 38600704
Current shard: 0, Current position: 38535168
Current shard: 0, Current position: 38666240
Current shard: 0, Current position: 38731776
Current shard: 0, Current position: 38862848
Current shard: 0, Current position: 38797312
step 73, loss: 6.698719, norm:0.6932, lr:2.9997e-04 dt: 3331.71ms, tok/sec:157363.05
Current shard: 0, Current position: 38993920
Current shard: 0, Current position: 38928384
Current shard: 0, Current position: 39124992
Current shard: 0, Current position: 39059456
Current shard: 0, Current position: 39256064
Current shard: 0, Current position: 39190528
Current shard: 0, Current position: 39387136
Current shard: 0, Current position: 39321600
step 74, loss: 6.630627, norm:0.5736, lr:2.9997e-04 dt: 3332.00ms, tok/sec:157349.23
Current shard: 0, Current position: 39518208
Current shard: 0, Current position: 39452672
Current shard: 0, Current position: 39649280
Current shard: 0, Current position: 39583744
Current shard: 0, Current position: 39714816
Current shard: 0, Current position: 39780352
Current shard: 0, Current position: 39911424
Current shard: 0, Current position: 39845888
step 75, loss: 6.684478, norm:0.7465, lr:2.9997e-04 dt: 3331.48ms, tok/sec:157374.10
Current shard: 0, Current position: 40042496
Current shard: 0, Current position: 39976960
Current shard: 0, Current position: 40173568
Current shard: 0, Current position: 40108032
Current shard: 0, Current position: 40239104
Current shard: 0, Current position: 40304640
Current shard: 0, Current position: 40435712
Current shard: 0, Current position: 40370176
step 76, loss: 6.627805, norm:0.5444, lr:2.9997e-04 dt: 3331.57ms, tok/sec:157369.82
Current shard: 0, Current position: 40566784
Current shard: 0, Current position: 40501248
Current shard: 0, Current position: 40697856
Current shard: 0, Current position: 40632320
Current shard: 0, Current position: 40828928Current shard: 0, Current position: 40763392

Current shard: 0, Current position: 40960000
Current shard: 0, Current position: 40894464
step 77, loss: 6.666112, norm:0.5288, lr:2.9997e-04 dt: 3331.40ms, tok/sec:157377.51
Current shard: 0, Current position: 41091072
Current shard: 0, Current position: 41025536
Current shard: 0, Current position: 41222144
Current shard: 0, Current position: 41156608
Current shard: 0, Current position: 41287680Current shard: 0, Current position: 41353216

Current shard: 0, Current position: 41484288
Current shard: 0, Current position: 41418752
step 78, loss: 6.648227, norm:0.6371, lr:2.9997e-04 dt: 3331.52ms, tok/sec:157371.78
Current shard: 0, Current position: 41615360
Current shard: 0, Current position: 41549824
Current shard: 0, Current position: 41746432
Current shard: 0, Current position: 41680896
Current shard: 0, Current position: 41811968
Current shard: 0, Current position: 41877504
Current shard: 0, Current position: 42008576
Current shard: 0, Current position: 41943040
step 79, loss: 6.594611, norm:0.4950, lr:2.9997e-04 dt: 3331.33ms, tok/sec:157381.17
Current shard: 0, Current position: 42139648
Current shard: 0, Current position: 42074112
Current shard: 0, Current position: 42270720
Current shard: 0, Current position: 42205184
Current shard: 0, Current position: 42401792Current shard: 0, Current position: 42336256

Current shard: 0, Current position: 42532864
Current shard: 0, Current position: 42467328
step 80, loss: 6.606215, norm:0.5617, lr:2.9997e-04 dt: 3331.71ms, tok/sec:157362.86
Current shard: 0, Current position: 42663936
Current shard: 0, Current position: 42598400
Current shard: 0, Current position: 42795008
Current shard: 0, Current position: 42729472
Current shard: 0, Current position: 42860544
Current shard: 0, Current position: 42926080
Current shard: 0, Current position: 43057152
Current shard: 0, Current position: 42991616
step 81, loss: 6.611200, norm:0.4800, lr:2.9997e-04 dt: 3331.55ms, tok/sec:157370.42
Current shard: 0, Current position: 43188224
Current shard: 0, Current position: 43122688
Current shard: 0, Current position: 43319296
Current shard: 0, Current position: 43253760
Current shard: 0, Current position: 43384832
Current shard: 0, Current position: 43450368
Current shard: 0, Current position: 43581440
Current shard: 0, Current position: 43515904
step 82, loss: 6.559704, norm:0.4466, lr:2.9997e-04 dt: 3331.69ms, tok/sec:157363.80
Current shard: 0, Current position: 43712512
Current shard: 0, Current position: 43646976
Current shard: 0, Current position: 43843584
Current shard: 0, Current position: 43778048
Current shard: 0, Current position: 43909120Current shard: 0, Current position: 43974656

Current shard: 0, Current position: 44105728
Current shard: 0, Current position: 44040192
step 83, loss: 6.523063, norm:0.5963, lr:2.9996e-04 dt: 3331.34ms, tok/sec:157380.45
Current shard: 0, Current position: 44236800
Current shard: 0, Current position: 44171264
Current shard: 0, Current position: 44367872
Current shard: 0, Current position: 44302336
Current shard: 0, Current position: 44433408Current shard: 0, Current position: 44498944

Current shard: 0, Current position: 44630016
Current shard: 0, Current position: 44564480
step 84, loss: 6.544335, norm:0.6322, lr:2.9996e-04 dt: 3331.68ms, tok/sec:157364.39
Current shard: 0, Current position: 44761088
Current shard: 0, Current position: 44695552
Current shard: 0, Current position: 44892160
Current shard: 0, Current position: 44826624
Current shard: 0, Current position: 44957696Current shard: 0, Current position: 45023232

Current shard: 0, Current position: 45154304
Current shard: 0, Current position: 45088768
step 85, loss: 6.516238, norm:0.8004, lr:2.9996e-04 dt: 3331.51ms, tok/sec:157372.38
Current shard: 0, Current position: 45285376
Current shard: 0, Current position: 45219840
Current shard: 0, Current position: 45416448
Current shard: 0, Current position: 45350912
Current shard: 0, Current position: 45481984Current shard: 0, Current position: 45547520

Current shard: 0, Current position: 45678592
Current shard: 0, Current position: 45613056
step 86, loss: 6.523478, norm:0.8354, lr:2.9996e-04 dt: 3331.46ms, tok/sec:157374.86
Current shard: 0, Current position: 45809664
Current shard: 0, Current position: 45744128
Current shard: 0, Current position: 45940736
Current shard: 0, Current position: 45875200
Current shard: 0, Current position: 46071808
Current shard: 0, Current position: 46006272
Current shard: 0, Current position: 46202880
Current shard: 0, Current position: 46137344
step 87, loss: 6.544073, norm:0.6739, lr:2.9996e-04 dt: 3331.44ms, tok/sec:157375.70
Current shard: 0, Current position: 46333952
Current shard: 0, Current position: 46268416
Current shard: 0, Current position: 46465024
Current shard: 0, Current position: 46399488
Current shard: 0, Current position: 46596096Current shard: 0, Current position: 46530560

Current shard: 0, Current position: 46727168
Current shard: 0, Current position: 46661632
step 88, loss: 6.536808, norm:0.4395, lr:2.9996e-04 dt: 3331.39ms, tok/sec:157378.20
Current shard: 0, Current position: 46858240
Current shard: 0, Current position: 46792704
Current shard: 0, Current position: 46989312
Current shard: 0, Current position: 46923776
Current shard: 0, Current position: 47054848
Current shard: 0, Current position: 47120384
Current shard: 0, Current position: 47251456
Current shard: 0, Current position: 47185920
step 89, loss: 6.588766, norm:1.2522, lr:2.9996e-04 dt: 3331.36ms, tok/sec:157379.37
Current shard: 0, Current position: 47316992
Current shard: 0, Current position: 47382528
Current shard: 0, Current position: 47448064
Current shard: 0, Current position: 47513600
Current shard: 0, Current position: 47579136
Current shard: 0, Current position: 47644672
Current shard: 0, Current position: 47775744
Current shard: 0, Current position: 47710208
step 90, loss: 6.509591, norm:0.7265, lr:2.9996e-04 dt: 3331.53ms, tok/sec:157371.47
Current shard: 0, Current position: 47906816
Current shard: 0, Current position: 47841280
Current shard: 0, Current position: 48037888
Current shard: 0, Current position: 47972352
Current shard: 0, Current position: 48168960
Current shard: 0, Current position: 48103424
Current shard: 0, Current position: 48300032
Current shard: 0, Current position: 48234496
step 91, loss: 6.519538, norm:0.9990, lr:2.9996e-04 dt: 3331.67ms, tok/sec:157364.90
Current shard: 0, Current position: 48431104
Current shard: 0, Current position: 48365568
Current shard: 0, Current position: 48562176
Current shard: 0, Current position: 48496640
Current shard: 0, Current position: 48693248Current shard: 0, Current position: 48627712

Current shard: 0, Current position: 48824320
Current shard: 0, Current position: 48758784
step 92, loss: 6.570477, norm:0.9483, lr:2.9996e-04 dt: 3331.53ms, tok/sec:157371.52
Current shard: 0, Current position: 48955392
Current shard: 0, Current position: 48889856
Current shard: 0, Current position: 49086464
Current shard: 0, Current position: 49020928
Current shard: 0, Current position: 49217536Current shard: 0, Current position: 49152000

Current shard: 0, Current position: 49348608
Current shard: 0, Current position: 49283072
step 93, loss: 6.644618, norm:0.7066, lr:2.9995e-04 dt: 3331.29ms, tok/sec:157383.01
Current shard: 0, Current position: 49479680
Current shard: 0, Current position: 49414144
Current shard: 0, Current position: 49610752
Current shard: 0, Current position: 49545216
Current shard: 0, Current position: 49676288
Current shard: 0, Current position: 49741824
Current shard: 0, Current position: 49872896
Current shard: 0, Current position: 49807360
step 94, loss: 6.650610, norm:0.4912, lr:2.9995e-04 dt: 3331.32ms, tok/sec:157381.41
Current shard: 0, Current position: 50003968
Current shard: 0, Current position: 49938432
Current shard: 0, Current position: 50135040
Current shard: 0, Current position: 50069504
Current shard: 0, Current position: 50200576
Current shard: 0, Current position: 50266112
Current shard: 0, Current position: 50397184
Current shard: 0, Current position: 50331648
step 95, loss: 6.653460, norm:0.4905, lr:2.9995e-04 dt: 3331.43ms, tok/sec:157376.42
Current shard: 0, Current position: 50528256
Current shard: 0, Current position: 50462720
Current shard: 0, Current position: 50659328
Current shard: 0, Current position: 50593792
Current shard: 0, Current position: 50724864
Current shard: 0, Current position: 50790400
Current shard: 0, Current position: 50921472
Current shard: 0, Current position: 50855936
step 96, loss: 6.602042, norm:0.3484, lr:2.9995e-04 dt: 3331.66ms, tok/sec:157365.60
Current shard: 0, Current position: 51052544
Current shard: 0, Current position: 50987008
Current shard: 0, Current position: 51183616
Current shard: 0, Current position: 51118080
Current shard: 0, Current position: 51314688
Current shard: 0, Current position: 51249152
Current shard: 0, Current position: 51445760
Current shard: 0, Current position: 51380224
step 97, loss: 6.567575, norm:0.3871, lr:2.9995e-04 dt: 3331.60ms, tok/sec:157368.46
Current shard: 0, Current position: 51576832
Current shard: 0, Current position: 51511296
Current shard: 0, Current position: 51707904
Current shard: 0, Current position: 51642368
Current shard: 0, Current position: 51773440
Current shard: 0, Current position: 51838976
Current shard: 0, Current position: 51970048
Current shard: 0, Current position: 51904512
step 98, loss: 6.617630, norm:0.3786, lr:2.9995e-04 dt: 3331.74ms, tok/sec:157361.58
Current shard: 0, Current position: 52101120
Current shard: 0, Current position: 52035584
Current shard: 0, Current position: 52232192
Current shard: 0, Current position: 52166656
Current shard: 0, Current position: 52297728
Current shard: 0, Current position: 52363264
Current shard: 0, Current position: 52494336
Current shard: 0, Current position: 52428800
step 99, loss: 6.578890, norm:0.3879, lr:2.9995e-04 dt: 3331.61ms, tok/sec:157367.92
Current shard: 0, Current position: 131072
Current shard: 0, Current position: 196608
Current shard: 0, Current position: 262144
Current shard: 0, Current position: 327680
Current shard: 0, Current position: 393216
Current shard: 0, Current position: 458752
Current shard: 0, Current position: 524288
Current shard: 0, Current position: 589824
Current shard: 0, Current position: 655360
Current shard: 0, Current position: 720896
Current shard: 0, Current position: 786432
Current shard: 0, Current position: 851968
Current shard: 0, Current position: 917504
Current shard: 0, Current position: 983040
Current shard: 0, Current position: 1048576
Current shard: 0, Current position: 1114112
Current shard: 0, Current position: 1179648
Current shard: 0, Current position: 1245184
Current shard: 0, Current position: 1310720
Current shard: 0, Current position: 1376256
Current shard: 0, Current position: 1507328
Current shard: 0, Current position: 1441792
Current shard: 0, Current position: 1638400
Current shard: 0, Current position: 1572864
Current shard: 0, Current position: 1769472
Current shard: 0, Current position: 1703936
Current shard: 0, Current position: 1900544
Current shard: 0, Current position: 1835008
Current shard: 0, Current position: 2031616
Current shard: 0, Current position: 1966080
Current shard: 0, Current position: 2162688
Current shard: 0, Current position: 2097152
Current shard: 0, Current position: 2293760
Current shard: 0, Current position: 2228224
Current shard: 0, Current position: 2424832
Current shard: 0, Current position: 2359296
Current shard: 0, Current position: 2555904
Current shard: 0, Current position: 2490368
Current shard: 0, Current position: 2686976
Current shard: 0, Current position: 2621440
validation loss: 6.5784
HellaSwag accuracy:3949831847017565773/-2=-1974915923508782848.0000
rank 1 sample 0: Hello, I'm a language model, for example, and more, a common. I would can lead to the new, in the other, or about the
rank 1 sample 1: Hello, I'm a language model, but they have much you can not not an important-of-related times the idea was
-to- States,
rank 1 sample 2: Hello, I'm a language model, he got it has been a few-the- York and "The study's a few-1) and that some
rank 1 sample 3: Hello, I'm a language model, but only, a new number of the top of the area to give his few-called research-related for the second
rank 0 sample 0: Hello, I'm a language model, I’s’ll’s that were you want through this” he.”“
rank 0 sample 1: Hello, I'm a language model, then if you, but and, she is that who could have that only the world is an to work is a need
rank 0 sample 2: Hello, I'm a language model, the past with a problem are possible that the United States. A is an important-p-free, and in a
rank 0 sample 3: Hello, I'm a language model, they's not the world. The name of up on the National2's good system was made. To be known more
Current shard: 0, Current position: 52625408
Current shard: 0, Current position: 52559872
Current shard: 0, Current position: 52756480
Current shard: 0, Current position: 52690944
Current shard: 0, Current position: 52887552Current shard: 0, Current position: 52822016

Current shard: 0, Current position: 53018624
Current shard: 0, Current position: 52953088
step 100, loss: 6.621883, norm:0.5215, lr:2.9995e-04 dt: 54401.43ms, tok/sec:9637.39
Current shard: 0, Current position: 53149696
Current shard: 0, Current position: 53084160
Current shard: 0, Current position: 53280768
Current shard: 0, Current position: 53215232
Current shard: 0, Current position: 53411840
Current shard: 0, Current position: 53346304
Current shard: 0, Current position: 53542912
Current shard: 0, Current position: 53477376
step 101, loss: 6.559659, norm:0.5680, lr:2.9994e-04 dt: 3331.76ms, tok/sec:157360.64
Current shard: 0, Current position: 53673984
Current shard: 0, Current position: 53608448
Current shard: 0, Current position: 53805056
Current shard: 0, Current position: 53739520
Current shard: 0, Current position: 53936128
Current shard: 0, Current position: 53870592
Current shard: 0, Current position: 54067200
Current shard: 0, Current position: 54001664
step 102, loss: 6.544631, norm:0.3778, lr:2.9994e-04 dt: 3331.56ms, tok/sec:157370.11
Current shard: 0, Current position: 54198272
Current shard: 0, Current position: 54132736
Current shard: 0, Current position: 54329344
Current shard: 0, Current position: 54263808
Current shard: 0, Current position: 54394880
Current shard: 0, Current position: 54460416
Current shard: 0, Current position: 54591488
Current shard: 0, Current position: 54525952
step 103, loss: 6.448491, norm:0.6587, lr:2.9994e-04 dt: 3331.72ms, tok/sec:157362.73
Current shard: 0, Current position: 54722560
Current shard: 0, Current position: 54657024
Current shard: 0, Current position: 54853632
Current shard: 0, Current position: 54788096
Current shard: 0, Current position: 54919168
Current shard: 0, Current position: 54984704
Current shard: 0, Current position: 55115776
Current shard: 0, Current position: 55050240
step 104, loss: 6.547790, norm:0.7875, lr:2.9994e-04 dt: 3331.47ms, tok/sec:157374.34
Current shard: 0, Current position: 55246848
Current shard: 0, Current position: 55181312
Current shard: 0, Current position: 55377920
Current shard: 0, Current position: 55312384
Current shard: 0, Current position: 55508992
Current shard: 0, Current position: 55443456
Current shard: 0, Current position: 55640064
Current shard: 0, Current position: 55574528
step 105, loss: 6.549412, norm:0.5885, lr:2.9994e-04 dt: 3331.55ms, tok/sec:157370.64
Current shard: 0, Current position: 55771136
Current shard: 0, Current position: 55705600
Current shard: 0, Current position: 55902208
Current shard: 0, Current position: 55836672
Current shard: 0, Current position: 56033280
Current shard: 0, Current position: 55967744
Current shard: 0, Current position: 56164352
Current shard: 0, Current position: 56098816
step 106, loss: 6.521550, norm:0.6363, lr:2.9994e-04 dt: 3331.48ms, tok/sec:157373.81
Current shard: 0, Current position: 56295424
Current shard: 0, Current position: 56229888
Current shard: 0, Current position: 56426496
Current shard: 0, Current position: 56360960
Current shard: 0, Current position: 56492032
Current shard: 0, Current position: 56557568
Current shard: 0, Current position: 56688640
Current shard: 0, Current position: 56623104
step 107, loss: 6.468326, norm:0.7160, lr:2.9994e-04 dt: 3331.72ms, tok/sec:157362.47
Current shard: 0, Current position: 56819712
Current shard: 0, Current position: 56754176
Current shard: 0, Current position: 56950784
Current shard: 0, Current position: 56885248
Current shard: 0, Current position: 57081856
Current shard: 0, Current position: 57016320
Current shard: 0, Current position: 57212928
Current shard: 0, Current position: 57147392
step 108, loss: 6.528451, norm:0.5267, lr:2.9994e-04 dt: 3331.44ms, tok/sec:157375.94
Current shard: 0, Current position: 57344000
Current shard: 0, Current position: 57278464
Current shard: 0, Current position: 57475072
Current shard: 0, Current position: 57409536
Current shard: 0, Current position: 57540608Current shard: 0, Current position: 57606144

Current shard: 0, Current position: 57737216
Current shard: 0, Current position: 57671680
step 109, loss: 6.538862, norm:0.6779, lr:2.9993e-04 dt: 3331.50ms, tok/sec:157373.08
Current shard: 0, Current position: 57868288
Current shard: 0, Current position: 57802752
Current shard: 0, Current position: 57999360
Current shard: 0, Current position: 57933824
Current shard: 0, Current position: 58064896
Current shard: 0, Current position: 58130432
Current shard: 0, Current position: 58261504
Current shard: 0, Current position: 58195968
step 110, loss: 6.493061, norm:0.7288, lr:2.9993e-04 dt: 3331.59ms, tok/sec:157368.92
Current shard: 0, Current position: 58392576
Current shard: 0, Current position: 58327040
Current shard: 0, Current position: 58523648
Current shard: 0, Current position: 58458112
Current shard: 0, Current position: 58589184
Current shard: 0, Current position: 58654720
Current shard: 0, Current position: 58785792
Current shard: 0, Current position: 58720256
step 111, loss: 6.469419, norm:0.6365, lr:2.9993e-04 dt: 3331.82ms, tok/sec:157357.82
Current shard: 0, Current position: 58851328
Current shard: 0, Current position: 58916864
Current shard: 0, Current position: 59047936
Current shard: 0, Current position: 58982400
Current shard: 0, Current position: 59179008Current shard: 0, Current position: 59113472

Current shard: 0, Current position: 59310080
Current shard: 0, Current position: 59244544
step 112, loss: 6.447369, norm:0.4341, lr:2.9993e-04 dt: 3332.01ms, tok/sec:157349.08
Current shard: 0, Current position: 59441152
Current shard: 0, Current position: 59375616
Current shard: 0, Current position: 59572224
Current shard: 0, Current position: 59506688
Current shard: 0, Current position: 59637760Current shard: 0, Current position: 59703296

Current shard: 0, Current position: 59834368
Current shard: 0, Current position: 59768832
step 113, loss: 6.490672, norm:0.5727, lr:2.9993e-04 dt: 3331.49ms, tok/sec:157373.58
Current shard: 0, Current position: 59965440
Current shard: 0, Current position: 59899904
Current shard: 0, Current position: 60096512
Current shard: 0, Current position: 60030976
Current shard: 0, Current position: 60227584
Current shard: 0, Current position: 60162048
Current shard: 0, Current position: 60358656
Current shard: 0, Current position: 60293120
step 114, loss: 6.510640, norm:0.6787, lr:2.9993e-04 dt: 3331.77ms, tok/sec:157360.23
Current shard: 0, Current position: 60489728
Current shard: 0, Current position: 60424192
Current shard: 0, Current position: 60620800
Current shard: 0, Current position: 60555264
Current shard: 0, Current position: 60751872Current shard: 0, Current position: 60686336

Current shard: 0, Current position: 60882944
Current shard: 0, Current position: 60817408
step 115, loss: 6.476914, norm:0.5746, lr:2.9993e-04 dt: 3331.49ms, tok/sec:157373.56
Current shard: 0, Current position: 61014016
Current shard: 0, Current position: 60948480
Current shard: 0, Current position: 61145088
Current shard: 0, Current position: 61079552
Current shard: 0, Current position: 61276160Current shard: 0, Current position: 61210624

Current shard: 0, Current position: 61407232
Current shard: 0, Current position: 61341696
step 116, loss: 6.413571, norm:0.4391, lr:2.9993e-04 dt: 3331.53ms, tok/sec:157371.32
Current shard: 0, Current position: 61538304
Current shard: 0, Current position: 61472768
Current shard: 0, Current position: 61669376
Current shard: 0, Current position: 61603840
Current shard: 0, Current position: 61734912
Current shard: 0, Current position: 61800448
Current shard: 0, Current position: 61931520
Current shard: 0, Current position: 61865984
step 117, loss: 6.352730, norm:0.6105, lr:2.9992e-04 dt: 3331.35ms, tok/sec:157379.88
Current shard: 0, Current position: 62062592
Current shard: 0, Current position: 61997056
Current shard: 0, Current position: 62193664
Current shard: 0, Current position: 62128128
Current shard: 0, Current position: 62259200
Current shard: 0, Current position: 62324736
Current shard: 0, Current position: 62455808
Current shard: 0, Current position: 62390272
step 118, loss: 6.377806, norm:0.6457, lr:2.9992e-04 dt: 3331.75ms, tok/sec:157361.25
Current shard: 0, Current position: 62586880
Current shard: 0, Current position: 62521344
Current shard: 0, Current position: 62717952
Current shard: 0, Current position: 62652416
Current shard: 0, Current position: 62783488
Current shard: 0, Current position: 62849024
Current shard: 0, Current position: 62980096
Current shard: 0, Current position: 62914560
step 119, loss: 6.409336, norm:0.3859, lr:2.9992e-04 dt: 3331.97ms, tok/sec:157350.72
Current shard: 0, Current position: 63111168
Current shard: 0, Current position: 63045632
Current shard: 0, Current position: 63242240
Current shard: 0, Current position: 63176704
Current shard: 0, Current position: 63307776
Current shard: 0, Current position: 63373312
Current shard: 0, Current position: 63504384
Current shard: 0, Current position: 63438848
step 120, loss: 6.384668, norm:0.5052, lr:2.9992e-04 dt: 3331.47ms, tok/sec:157374.59
Current shard: 0, Current position: 63635456
Current shard: 0, Current position: 63569920
Current shard: 0, Current position: 63766528
Current shard: 0, Current position: 63700992
Current shard: 0, Current position: 63832064Current shard: 0, Current position: 63897600

Current shard: 0, Current position: 64028672
Current shard: 0, Current position: 63963136
step 121, loss: 6.357473, norm:0.4907, lr:2.9992e-04 dt: 3331.60ms, tok/sec:157368.21
Current shard: 0, Current position: 64159744
Current shard: 0, Current position: 64094208
Current shard: 0, Current position: 64290816
Current shard: 0, Current position: 64225280
Current shard: 0, Current position: 64356352
Current shard: 0, Current position: 64421888
Current shard: 0, Current position: 64552960
Current shard: 0, Current position: 64487424
step 122, loss: 6.367306, norm:0.5141, lr:2.9992e-04 dt: 3331.37ms, tok/sec:157379.25
Current shard: 0, Current position: 64684032
Current shard: 0, Current position: 64618496
Current shard: 0, Current position: 64815104
Current shard: 0, Current position: 64749568
Current shard: 0, Current position: 64946176
Current shard: 0, Current position: 64880640
Current shard: 0, Current position: 65077248
Current shard: 0, Current position: 65011712
step 123, loss: 6.368788, norm:0.5163, lr:2.9991e-04 dt: 3331.38ms, tok/sec:157378.56
Current shard: 0, Current position: 65208320
Current shard: 0, Current position: 65142784
Current shard: 0, Current position: 65339392
Current shard: 0, Current position: 65273856
Current shard: 0, Current position: 65470464
Current shard: 0, Current position: 65404928
Current shard: 0, Current position: 65601536
Current shard: 0, Current position: 65536000
step 124, loss: 6.379308, norm:0.5000, lr:2.9991e-04 dt: 3331.62ms, tok/sec:157367.13
Current shard: 0, Current position: 65732608
Current shard: 0, Current position: 65667072
Current shard: 0, Current position: 65863680
Current shard: 0, Current position: 65798144
Current shard: 0, Current position: 65929216
Current shard: 0, Current position: 65994752
Current shard: 0, Current position: 66125824
Current shard: 0, Current position: 66060288
step 125, loss: 6.370719, norm:0.4723, lr:2.9991e-04 dt: 3331.53ms, tok/sec:157371.44
Current shard: 0, Current position: 66256896
Current shard: 0, Current position: 66191360
Current shard: 0, Current position: 66387968
Current shard: 0, Current position: 66322432
Current shard: 0, Current position: 66519040Current shard: 0, Current position: 66453504

Current shard: 0, Current position: 66650112
Current shard: 0, Current position: 66584576
step 126, loss: 6.358905, norm:0.4381, lr:2.9991e-04 dt: 3331.45ms, tok/sec:157375.43
Current shard: 0, Current position: 66781184
Current shard: 0, Current position: 66715648
Current shard: 0, Current position: 66912256
Current shard: 0, Current position: 66846720
Current shard: 0, Current position: 66977792Current shard: 0, Current position: 67043328

Current shard: 0, Current position: 67174400
Current shard: 0, Current position: 67108864
step 127, loss: 6.331379, norm:0.4775, lr:2.9991e-04 dt: 3331.78ms, tok/sec:157359.88
Current shard: 0, Current position: 67305472
Current shard: 0, Current position: 67239936
Current shard: 0, Current position: 67436544
Current shard: 0, Current position: 67371008
Current shard: 0, Current position: 67567616Current shard: 0, Current position: 67502080

Current shard: 0, Current position: 67698688
Current shard: 0, Current position: 67633152
step 128, loss: 6.288651, norm:0.4730, lr:2.9991e-04 dt: 3331.73ms, tok/sec:157362.24
Current shard: 0, Current position: 67829760
Current shard: 0, Current position: 67764224
Current shard: 0, Current position: 67960832
Current shard: 0, Current position: 67895296
Current shard: 0, Current position: 68026368
Current shard: 0, Current position: 68091904
Current shard: 0, Current position: 68222976
Current shard: 0, Current position: 68157440
step 129, loss: 6.279574, norm:0.4026, lr:2.9991e-04 dt: 3331.46ms, tok/sec:157374.74
Current shard: 0, Current position: 68354048
Current shard: 0, Current position: 68288512
Current shard: 0, Current position: 68485120
Current shard: 0, Current position: 68419584
Current shard: 0, Current position: 68616192
Current shard: 0, Current position: 68550656
Current shard: 0, Current position: 68747264
Current shard: 0, Current position: 68681728
step 130, loss: 6.337259, norm:0.3170, lr:2.9990e-04 dt: 3331.39ms, tok/sec:157378.20
Current shard: 0, Current position: 68878336
Current shard: 0, Current position: 68812800
Current shard: 0, Current position: 69009408
Current shard: 0, Current position: 68943872
Current shard: 0, Current position: 69140480
Current shard: 0, Current position: 69074944
Current shard: 0, Current position: 69271552
Current shard: 0, Current position: 69206016
step 131, loss: 6.302004, norm:0.4348, lr:2.9990e-04 dt: 3331.61ms, tok/sec:157367.78
Current shard: 0, Current position: 69402624
Current shard: 0, Current position: 69337088
Current shard: 0, Current position: 69533696
Current shard: 0, Current position: 69468160
Current shard: 0, Current position: 69664768
Current shard: 0, Current position: 69599232
Current shard: 0, Current position: 69795840
Current shard: 0, Current position: 69730304
step 132, loss: 6.315139, norm:0.4906, lr:2.9990e-04 dt: 3331.69ms, tok/sec:157363.89
Current shard: 0, Current position: 69926912
Current shard: 0, Current position: 69861376
Current shard: 0, Current position: 70057984
Current shard: 0, Current position: 69992448
Current shard: 0, Current position: 70123520
Current shard: 0, Current position: 70189056
Current shard: 0, Current position: 70320128
Current shard: 0, Current position: 70254592
step 133, loss: 6.232867, norm:0.5661, lr:2.9990e-04 dt: 3331.24ms, tok/sec:157385.33
Current shard: 0, Current position: 70451200
Current shard: 0, Current position: 70385664
Current shard: 0, Current position: 70582272
Current shard: 0, Current position: 70516736
Current shard: 0, Current position: 70647808Current shard: 0, Current position: 70713344

Current shard: 0, Current position: 70844416
Current shard: 0, Current position: 70778880
step 134, loss: 6.238806, norm:0.9386, lr:2.9990e-04 dt: 3331.36ms, tok/sec:157379.35
Current shard: 0, Current position: 70975488
Current shard: 0, Current position: 70909952
Current shard: 0, Current position: 71106560
Current shard: 0, Current position: 71041024
Current shard: 0, Current position: 71172096Current shard: 0, Current position: 71237632

Current shard: 0, Current position: 71368704
Current shard: 0, Current position: 71303168
step 135, loss: 6.308858, norm:0.7854, lr:2.9990e-04 dt: 3331.27ms, tok/sec:157383.81
Current shard: 0, Current position: 71499776
Current shard: 0, Current position: 71434240
Current shard: 0, Current position: 71630848
Current shard: 0, Current position: 71565312
Current shard: 0, Current position: 71761920Current shard: 0, Current position: 71696384

Current shard: 0, Current position: 71892992
Current shard: 0, Current position: 71827456
step 136, loss: 6.284130, norm:0.7340, lr:2.9989e-04 dt: 3331.46ms, tok/sec:157375.07
Current shard: 0, Current position: 72024064
Current shard: 0, Current position: 71958528
Current shard: 0, Current position: 72155136
Current shard: 0, Current position: 72089600
Current shard: 0, Current position: 72220672Current shard: 0, Current position: 72286208

Current shard: 0, Current position: 72417280
Current shard: 0, Current position: 72351744
step 137, loss: 6.307083, norm:0.4487, lr:2.9989e-04 dt: 3332.69ms, tok/sec:157316.82
Current shard: 0, Current position: 72548352
Current shard: 0, Current position: 72482816
Current shard: 0, Current position: 72679424
Current shard: 0, Current position: 72613888
Current shard: 0, Current position: 72744960
Current shard: 0, Current position: 72810496
Current shard: 0, Current position: 72941568
Current shard: 0, Current position: 72876032
step 138, loss: 6.249040, norm:0.5298, lr:2.9989e-04 dt: 3332.18ms, tok/sec:157340.69
Current shard: 0, Current position: 73072640
Current shard: 0, Current position: 73007104
Current shard: 0, Current position: 73203712
Current shard: 0, Current position: 73138176
Current shard: 0, Current position: 73334784Current shard: 0, Current position: 73269248

Current shard: 0, Current position: 73465856
Current shard: 0, Current position: 73400320
step 139, loss: 6.350435, norm:0.7055, lr:2.9989e-04 dt: 3331.67ms, tok/sec:157365.14
Current shard: 0, Current position: 73596928
Current shard: 0, Current position: 73531392
Current shard: 0, Current position: 73728000
Current shard: 0, Current position: 73662464
Current shard: 0, Current position: 73793536Current shard: 0, Current position: 73859072

Current shard: 0, Current position: 73990144
Current shard: 0, Current position: 73924608
step 140, loss: 6.464736, norm:0.6192, lr:2.9989e-04 dt: 3331.71ms, tok/sec:157362.87
Current shard: 0, Current position: 74121216
Current shard: 0, Current position: 74055680
Current shard: 0, Current position: 74252288
Current shard: 0, Current position: 74186752
Current shard: 0, Current position: 74317824Current shard: 0, Current position: 74383360

Current shard: 0, Current position: 74514432
Current shard: 0, Current position: 74448896
step 141, loss: 6.356379, norm:0.4412, lr:2.9989e-04 dt: 3331.70ms, tok/sec:157363.70
Current shard: 0, Current position: 74645504
Current shard: 0, Current position: 74579968
Current shard: 0, Current position: 74776576
Current shard: 0, Current position: 74711040
Current shard: 0, Current position: 74907648Current shard: 0, Current position: 74842112

Current shard: 0, Current position: 75038720
Current shard: 0, Current position: 74973184
step 142, loss: 6.409093, norm:0.4475, lr:2.9988e-04 dt: 3331.50ms, tok/sec:157372.82
Current shard: 0, Current position: 75169792
Current shard: 0, Current position: 75104256
Current shard: 0, Current position: 75300864
Current shard: 0, Current position: 75235328
Current shard: 0, Current position: 75431936
Current shard: 0, Current position: 75366400
Current shard: 0, Current position: 75563008
Current shard: 0, Current position: 75497472
step 143, loss: 6.350973, norm:0.3900, lr:2.9988e-04 dt: 3331.76ms, tok/sec:157360.71
Current shard: 0, Current position: 75694080
Current shard: 0, Current position: 75628544
Current shard: 0, Current position: 75825152
Current shard: 0, Current position: 75759616
Current shard: 0, Current position: 75890688Current shard: 0, Current position: 75956224

Current shard: 0, Current position: 76087296
Current shard: 0, Current position: 76021760
step 144, loss: 6.350926, norm:0.4665, lr:2.9988e-04 dt: 3331.77ms, tok/sec:157360.09
Current shard: 0, Current position: 76218368
Current shard: 0, Current position: 76152832
Current shard: 0, Current position: 76349440
Current shard: 0, Current position: 76283904
Current shard: 0, Current position: 76480512
Current shard: 0, Current position: 76414976
Current shard: 0, Current position: 76611584
Current shard: 0, Current position: 76546048
step 145, loss: 6.348162, norm:0.5213, lr:2.9988e-04 dt: 3331.53ms, tok/sec:157371.50
Current shard: 0, Current position: 76742656
Current shard: 0, Current position: 76677120
Current shard: 0, Current position: 76873728
Current shard: 0, Current position: 76808192
Current shard: 0, Current position: 77004800
Current shard: 0, Current position: 76939264
Current shard: 0, Current position: 77135872
Current shard: 0, Current position: 77070336
step 146, loss: 6.365723, norm:0.6417, lr:2.9988e-04 dt: 3331.70ms, tok/sec:157363.44
Current shard: 0, Current position: 77266944
Current shard: 0, Current position: 77201408
Current shard: 0, Current position: 77398016
Current shard: 0, Current position: 77332480
Current shard: 0, Current position: 77529088Current shard: 0, Current position: 77463552

Current shard: 0, Current position: 77660160
Current shard: 0, Current position: 77594624
step 147, loss: 6.368383, norm:0.4981, lr:2.9987e-04 dt: 3331.83ms, tok/sec:157357.54
Current shard: 0, Current position: 77791232
Current shard: 0, Current position: 77725696
Current shard: 0, Current position: 77922304
Current shard: 0, Current position: 77856768
Current shard: 0, Current position: 77987840
Current shard: 0, Current position: 78053376
Current shard: 0, Current position: 78184448
Current shard: 0, Current position: 78118912
step 148, loss: 6.367171, norm:0.4449, lr:2.9987e-04 dt: 3331.36ms, tok/sec:157379.73
Current shard: 0, Current position: 78315520
Current shard: 0, Current position: 78249984
Current shard: 0, Current position: 78446592
Current shard: 0, Current position: 78381056
Current shard: 0, Current position: 78577664Current shard: 0, Current position: 78512128

Current shard: 0, Current position: 78708736
Current shard: 0, Current position: 78643200
step 149, loss: 6.346069, norm:0.3780, lr:2.9987e-04 dt: 3331.57ms, tok/sec:157369.66
HellaSwag accuracy:7408508399639710797/-2=-3704254199819855360.0000
rank 1 sample 0: Hello, I'm a language model, so.e.1: A percent, in a variety of the great for the last time is a number of the
rank 1 sample 1: Hello, I'm a language model, it. I say I.<|endoftext|>What have a way if you say if they cannot your people they have to do.
rank 1 sample 2: Hello, I'm a language model, I! It, I) and I have a long to think of what if you. It. If I know when
rank 1 sample 3: Hello, I'm a language model, it you. You do for the one people. For
C. But he is being being, you make them.
rank 0 sample 0: Hello, I'm a language model, this is about the way it's also go it's part of you with to be more better for you do. You
rank 0 sample 1: Hello, I'm a language model, not know more than a year, including the time was very great and was in a new thing, when a member of
Current shard: 0, Current position: 78839808
rank 0 sample 2: Hello, I'm a language model, the course? When these students think about the students’d't you’t go to all the first will
rank 0 sample 3: Hello, I'm a language model, or other more of the new. A single section of the one day in this part of the long would be used or
Current shard: 0, Current position: 78774272
Current shard: 0, Current position: 78970880
Current shard: 0, Current position: 78905344
Current shard: 0, Current position: 79101952
Current shard: 0, Current position: 79036416
Current shard: 0, Current position: 79233024
Current shard: 0, Current position: 79167488
step 150, loss: 6.343985, norm:0.3604, lr:2.9987e-04 dt: 48140.42ms, tok/sec:10890.81
Current shard: 0, Current position: 79364096
Current shard: 0, Current position: 79298560
Current shard: 0, Current position: 79495168
Current shard: 0, Current position: 79429632
Current shard: 0, Current position: 79626240Current shard: 0, Current position: 79560704

Current shard: 0, Current position: 79757312
Current shard: 0, Current position: 79691776
step 151, loss: 6.263157, norm:0.4476, lr:2.9987e-04 dt: 3331.61ms, tok/sec:157367.55
Current shard: 0, Current position: 79888384
Current shard: 0, Current position: 79822848
Current shard: 0, Current position: 80019456
Current shard: 0, Current position: 79953920
Current shard: 0, Current position: 80150528
Current shard: 0, Current position: 80084992
Current shard: 0, Current position: 80281600
Current shard: 0, Current position: 80216064
step 152, loss: 6.295625, norm:0.4189, lr:2.9987e-04 dt: 3331.64ms, tok/sec:157366.44
Current shard: 0, Current position: 80412672
Current shard: 0, Current position: 80347136
Current shard: 0, Current position: 80543744
Current shard: 0, Current position: 80478208
Current shard: 0, Current position: 80609280
Current shard: 0, Current position: 80674816
Current shard: 0, Current position: 80805888
Current shard: 0, Current position: 80740352
step 153, loss: 6.352885, norm:0.5231, lr:2.9986e-04 dt: 3331.37ms, tok/sec:157379.23
Current shard: 0, Current position: 80936960
Current shard: 0, Current position: 80871424
Current shard: 0, Current position: 81068032
Current shard: 0, Current position: 81002496
Current shard: 0, Current position: 81133568
Current shard: 0, Current position: 81199104
Current shard: 0, Current position: 81330176
Current shard: 0, Current position: 81264640
step 154, loss: 6.266207, norm:0.4916, lr:2.9986e-04 dt: 3331.53ms, tok/sec:157371.31
Current shard: 0, Current position: 81461248
Current shard: 0, Current position: 81395712
Current shard: 0, Current position: 81592320
Current shard: 0, Current position: 81526784
Current shard: 0, Current position: 81723392
Current shard: 0, Current position: 81657856
Current shard: 0, Current position: 81854464
Current shard: 0, Current position: 81788928
step 155, loss: 6.237961, norm:0.5889, lr:2.9986e-04 dt: 3331.61ms, tok/sec:157367.92
Current shard: 0, Current position: 81985536
Current shard: 0, Current position: 81920000
Current shard: 0, Current position: 82116608
Current shard: 0, Current position: 82051072
Current shard: 0, Current position: 82182144
Current shard: 0, Current position: 82247680
Current shard: 0, Current position: 82378752
Current shard: 0, Current position: 82313216
step 156, loss: 6.298701, norm:0.6190, lr:2.9986e-04 dt: 3331.53ms, tok/sec:157371.42
Current shard: 0, Current position: 82509824
Current shard: 0, Current position: 82444288
Current shard: 0, Current position: 82640896
Current shard: 0, Current position: 82575360
Current shard: 0, Current position: 82706432
Current shard: 0, Current position: 82771968
Current shard: 0, Current position: 82903040
Current shard: 0, Current position: 82837504
step 157, loss: 6.355693, norm:0.3846, lr:2.9986e-04 dt: 3331.34ms, tok/sec:157380.32
Current shard: 0, Current position: 83034112
Current shard: 0, Current position: 82968576
Current shard: 0, Current position: 83165184
Current shard: 0, Current position: 83099648
Current shard: 0, Current position: 83296256Current shard: 0, Current position: 83230720

Current shard: 0, Current position: 83427328
Current shard: 0, Current position: 83361792
step 158, loss: 6.357535, norm:0.4194, lr:2.9985e-04 dt: 3331.63ms, tok/sec:157366.74
Current shard: 0, Current position: 83558400
Current shard: 0, Current position: 83492864
Current shard: 0, Current position: 83689472
Current shard: 0, Current position: 83623936
Current shard: 0, Current position: 83755008
Current shard: 0, Current position: 83820544
Current shard: 0, Current position: 83951616
Current shard: 0, Current position: 83886080
step 159, loss: 6.272134, norm:0.5636, lr:2.9985e-04 dt: 3331.59ms, tok/sec:157368.77
Current shard: 0, Current position: 84082688
Current shard: 0, Current position: 84017152
Current shard: 0, Current position: 84213760
Current shard: 0, Current position: 84148224
Current shard: 0, Current position: 84279296
Current shard: 0, Current position: 84344832
Current shard: 0, Current position: 84475904
Current shard: 0, Current position: 84410368
step 160, loss: 6.217994, norm:0.5884, lr:2.9985e-04 dt: 3331.62ms, tok/sec:157367.11
Current shard: 0, Current position: 84541440
Current shard: 0, Current position: 84606976
Current shard: 0, Current position: 84672512
Current shard: 0, Current position: 84738048
Current shard: 0, Current position: 84869120
Current shard: 0, Current position: 84803584
Current shard: 0, Current position: 85000192
Current shard: 0, Current position: 84934656
step 161, loss: 6.306904, norm:0.4181, lr:2.9985e-04 dt: 3331.49ms, tok/sec:157373.26
Current shard: 0, Current position: 85131264
Current shard: 0, Current position: 85065728
Current shard: 0, Current position: 85262336
Current shard: 0, Current position: 85196800
Current shard: 0, Current position: 85327872
Current shard: 0, Current position: 85393408
Current shard: 0, Current position: 85524480
Current shard: 0, Current position: 85458944
step 162, loss: 6.207654, norm:0.4609, lr:2.9985e-04 dt: 3331.75ms, tok/sec:157361.14
Current shard: 0, Current position: 85655552
Current shard: 0, Current position: 85590016
Current shard: 0, Current position: 85786624
Current shard: 0, Current position: 85721088
Current shard: 0, Current position: 85917696
Current shard: 0, Current position: 85852160
Current shard: 0, Current position: 86048768
Current shard: 0, Current position: 85983232
step 163, loss: 6.169796, norm:0.4217, lr:2.9984e-04 dt: 3331.38ms, tok/sec:157378.60
Current shard: 0, Current position: 86179840
Current shard: 0, Current position: 86114304
Current shard: 0, Current position: 86310912
Current shard: 0, Current position: 86245376
Current shard: 0, Current position: 86376448Current shard: 0, Current position: 86441984

Current shard: 0, Current position: 86573056
Current shard: 0, Current position: 86507520
step 164, loss: 6.195165, norm:0.4389, lr:2.9984e-04 dt: 3331.87ms, tok/sec:157355.44
Current shard: 0, Current position: 86704128
Current shard: 0, Current position: 86638592
Current shard: 0, Current position: 86835200
Current shard: 0, Current position: 86769664
Current shard: 0, Current position: 86900736
Current shard: 0, Current position: 86966272
Current shard: 0, Current position: 87097344
Current shard: 0, Current position: 87031808
step 165, loss: 6.233603, norm:0.4642, lr:2.9984e-04 dt: 3331.50ms, tok/sec:157372.76
Current shard: 0, Current position: 87228416
Current shard: 0, Current position: 87162880
Current shard: 0, Current position: 87359488
Current shard: 0, Current position: 87293952
Current shard: 0, Current position: 87425024
Current shard: 0, Current position: 87490560
Current shard: 0, Current position: 87621632
Current shard: 0, Current position: 87556096
step 166, loss: 6.090239, norm:0.4751, lr:2.9984e-04 dt: 3331.34ms, tok/sec:157380.40
Current shard: 0, Current position: 87752704
Current shard: 0, Current position: 87687168
Current shard: 0, Current position: 87883776
Current shard: 0, Current position: 87818240
Current shard: 0, Current position: 87949312
Current shard: 0, Current position: 88014848
Current shard: 0, Current position: 88145920
Current shard: 0, Current position: 88080384
step 167, loss: 6.195724, norm:0.6117, lr:2.9984e-04 dt: 3331.70ms, tok/sec:157363.66
Current shard: 0, Current position: 88276992
Current shard: 0, Current position: 88211456
Current shard: 0, Current position: 88408064
Current shard: 0, Current position: 88342528
Current shard: 0, Current position: 88473600
Current shard: 0, Current position: 88539136
Current shard: 0, Current position: 88670208
Current shard: 0, Current position: 88604672
step 168, loss: 6.170078, norm:0.5340, lr:2.9983e-04 dt: 3331.60ms, tok/sec:157368.19
Current shard: 0, Current position: 88801280
Current shard: 0, Current position: 88735744
Current shard: 0, Current position: 88932352
Current shard: 0, Current position: 88866816
Current shard: 0, Current position: 88997888
Current shard: 0, Current position: 89063424
Current shard: 0, Current position: 89194496
Current shard: 0, Current position: 89128960
step 169, loss: 6.183750, norm:0.5622, lr:2.9983e-04 dt: 3331.67ms, tok/sec:157364.92
Current shard: 0, Current position: 89325568
Current shard: 0, Current position: 89260032
Current shard: 0, Current position: 89456640
Current shard: 0, Current position: 89391104
Current shard: 0, Current position: 89522176Current shard: 0, Current position: 89587712

Current shard: 0, Current position: 89718784
Current shard: 0, Current position: 89653248
step 170, loss: 6.163527, norm:0.5028, lr:2.9983e-04 dt: 3331.50ms, tok/sec:157372.76
Current shard: 0, Current position: 89849856
Current shard: 0, Current position: 89784320
Current shard: 0, Current position: 89980928
Current shard: 0, Current position: 89915392
Current shard: 0, Current position: 90046464
Current shard: 0, Current position: 90112000
Current shard: 0, Current position: 90243072
Current shard: 0, Current position: 90177536
step 171, loss: 6.191903, norm:0.3885, lr:2.9983e-04 dt: 3331.70ms, tok/sec:157363.61
Current shard: 0, Current position: 90374144
Current shard: 0, Current position: 90308608
Current shard: 0, Current position: 90505216
Current shard: 0, Current position: 90439680
Current shard: 0, Current position: 90570752
Current shard: 0, Current position: 90636288
Current shard: 0, Current position: 90767360
Current shard: 0, Current position: 90701824
step 172, loss: 6.169555, norm:0.4267, lr:2.9982e-04 dt: 3331.64ms, tok/sec:157366.34
Current shard: 0, Current position: 90898432
Current shard: 0, Current position: 90832896
Current shard: 0, Current position: 91029504
Current shard: 0, Current position: 90963968
Current shard: 0, Current position: 91095040
Current shard: 0, Current position: 91160576
Current shard: 0, Current position: 91291648
Current shard: 0, Current position: 91226112
step 173, loss: 6.130895, norm:0.3485, lr:2.9982e-04 dt: 3331.34ms, tok/sec:157380.43
Current shard: 0, Current position: 91422720
Current shard: 0, Current position: 91357184
Current shard: 0, Current position: 91553792
Current shard: 0, Current position: 91488256
Current shard: 0, Current position: 91684864
Current shard: 0, Current position: 91619328
Current shard: 0, Current position: 91815936
Current shard: 0, Current position: 91750400
step 174, loss: 6.095600, norm:0.3586, lr:2.9982e-04 dt: 3331.73ms, tok/sec:157362.28
Current shard: 0, Current position: 91947008
Current shard: 0, Current position: 91881472
Current shard: 0, Current position: 92078080
Current shard: 0, Current position: 92012544
Current shard: 0, Current position: 92143616
Current shard: 0, Current position: 92209152
Current shard: 0, Current position: 92340224
Current shard: 0, Current position: 92274688
step 175, loss: 6.164701, norm:0.3973, lr:2.9982e-04 dt: 3331.58ms, tok/sec:157369.38
Current shard: 0, Current position: 92471296
Current shard: 0, Current position: 92405760
Current shard: 0, Current position: 92602368
Current shard: 0, Current position: 92536832
Current shard: 0, Current position: 92667904Current shard: 0, Current position: 92733440

Current shard: 0, Current position: 92864512
Current shard: 0, Current position: 92798976
step 176, loss: 6.105410, norm:0.5552, lr:2.9982e-04 dt: 3331.54ms, tok/sec:157371.14
Current shard: 0, Current position: 92995584
Current shard: 0, Current position: 92930048
Current shard: 0, Current position: 93126656
Current shard: 0, Current position: 93061120
Current shard: 0, Current position: 93192192
Current shard: 0, Current position: 93257728
Current shard: 0, Current position: 93388800
Current shard: 0, Current position: 93323264
step 177, loss: 6.154356, norm:0.7912, lr:2.9981e-04 dt: 3331.54ms, tok/sec:157371.05
Current shard: 0, Current position: 93519872
Current shard: 0, Current position: 93454336
Current shard: 0, Current position: 93650944
Current shard: 0, Current position: 93585408
Current shard: 0, Current position: 93782016Current shard: 0, Current position: 93716480

Current shard: 0, Current position: 93913088
Current shard: 0, Current position: 93847552
step 178, loss: 6.085921, norm:0.8416, lr:2.9981e-04 dt: 3331.75ms, tok/sec:157360.93
Current shard: 0, Current position: 94044160
Current shard: 0, Current position: 93978624
Current shard: 0, Current position: 94175232
Current shard: 0, Current position: 94109696
Current shard: 0, Current position: 94306304
Current shard: 0, Current position: 94240768
Current shard: 0, Current position: 94437376
Current shard: 0, Current position: 94371840
step 179, loss: 6.069360, norm:0.5854, lr:2.9981e-04 dt: 3331.42ms, tok/sec:157376.78
Current shard: 0, Current position: 94568448
Current shard: 0, Current position: 94502912
Current shard: 0, Current position: 94699520
Current shard: 0, Current position: 94633984
Current shard: 0, Current position: 94765056Current shard: 0, Current position: 94830592

Current shard: 0, Current position: 94961664
Current shard: 0, Current position: 94896128
step 180, loss: 6.064285, norm:0.3902, lr:2.9981e-04 dt: 3331.43ms, tok/sec:157376.09
Current shard: 0, Current position: 95092736
Current shard: 0, Current position: 95027200
Current shard: 0, Current position: 95223808
Current shard: 0, Current position: 95158272
Current shard: 0, Current position: 95289344Current shard: 0, Current position: 95354880

Current shard: 0, Current position: 95485952
Current shard: 0, Current position: 95420416
step 181, loss: 6.089869, norm:0.4112, lr:2.9980e-04 dt: 3331.47ms, tok/sec:157374.59
Current shard: 0, Current position: 95617024
Current shard: 0, Current position: 95551488
Current shard: 0, Current position: 95748096
Current shard: 0, Current position: 95682560
Current shard: 0, Current position: 95879168
Current shard: 0, Current position: 95813632
Current shard: 0, Current position: 96010240
Current shard: 0, Current position: 95944704
step 182, loss: 6.087728, norm:0.3913, lr:2.9980e-04 dt: 3331.42ms, tok/sec:157376.85
Current shard: 0, Current position: 96075776
Current shard: 0, Current position: 96141312
Current shard: 0, Current position: 96206848
Current shard: 0, Current position: 96272384
Current shard: 0, Current position: 96403456
Current shard: 0, Current position: 96337920
Current shard: 0, Current position: 96534528
Current shard: 0, Current position: 96468992
step 183, loss: 6.080689, norm:0.4069, lr:2.9980e-04 dt: 3331.65ms, tok/sec:157365.73
Current shard: 0, Current position: 96665600
Current shard: 0, Current position: 96600064
Current shard: 0, Current position: 96796672
Current shard: 0, Current position: 96731136
Current shard: 0, Current position: 96862208
Current shard: 0, Current position: 96927744
Current shard: 0, Current position: 97058816
Current shard: 0, Current position: 96993280
step 184, loss: 6.106934, norm:0.3398, lr:2.9980e-04 dt: 3331.58ms, tok/sec:157369.03
Current shard: 0, Current position: 97189888
Current shard: 0, Current position: 97124352
Current shard: 0, Current position: 97320960
Current shard: 0, Current position: 97255424
Current shard: 0, Current position: 97386496
Current shard: 0, Current position: 97452032
Current shard: 0, Current position: 97583104
Current shard: 0, Current position: 97517568
step 185, loss: 6.155803, norm:0.4474, lr:2.9980e-04 dt: 3331.47ms, tok/sec:157374.54
Current shard: 0, Current position: 97714176
Current shard: 0, Current position: 97648640
Current shard: 0, Current position: 97845248
Current shard: 0, Current position: 97779712
Current shard: 0, Current position: 97910784Current shard: 0, Current position: 97976320

Current shard: 0, Current position: 98107392
Current shard: 0, Current position: 98041856
step 186, loss: 6.183030, norm:0.4410, lr:2.9979e-04 dt: 3332.00ms, tok/sec:157349.29
Current shard: 0, Current position: 98238464
Current shard: 0, Current position: 98172928
Current shard: 0, Current position: 98369536
Current shard: 0, Current position: 98304000
Current shard: 0, Current position: 98500608
Current shard: 0, Current position: 98435072
Current shard: 0, Current position: 98631680
Current shard: 0, Current position: 98566144
step 187, loss: 6.239153, norm:0.3991, lr:2.9979e-04 dt: 3331.49ms, tok/sec:157373.63
Current shard: 0, Current position: 98762752
Current shard: 0, Current position: 98697216
Current shard: 0, Current position: 98893824
Current shard: 0, Current position: 98828288
Current shard: 0, Current position: 98959360
Current shard: 0, Current position: 99024896
Current shard: 0, Current position: 99155968
Current shard: 0, Current position: 99090432
step 188, loss: 6.195700, norm:0.4603, lr:2.9979e-04 dt: 3331.54ms, tok/sec:157371.04
Current shard: 0, Current position: 99287040
Current shard: 0, Current position: 99221504
Current shard: 0, Current position: 99418112
Current shard: 0, Current position: 99352576
Current shard: 0, Current position: 99483648
Current shard: 0, Current position: 99549184
Current shard: 0, Current position: 99680256
Current shard: 0, Current position: 99614720
step 189, loss: 6.170138, norm:0.4184, lr:2.9979e-04 dt: 3331.92ms, tok/sec:157353.29
Current shard: 0, Current position: 99811328
Current shard: 0, Current position: 99745792
Current shard: 1, Current position: 0
Current shard: 1, Current position: 65536
Current shard: 1, Current position: 196608
Current shard: 1, Current position: 131072
Current shard: 1, Current position: 327680
Current shard: 1, Current position: 262144
step 190, loss: 6.213864, norm:0.5197, lr:2.9978e-04 dt: 3333.99ms, tok/sec:157255.52
Current shard: 1, Current position: 458752
Current shard: 1, Current position: 393216
Current shard: 1, Current position: 589824
Current shard: 1, Current position: 524288
Current shard: 1, Current position: 655360
Current shard: 1, Current position: 720896
Current shard: 1, Current position: 851968
Current shard: 1, Current position: 786432
step 191, loss: 6.185641, norm:0.7078, lr:2.9978e-04 dt: 3331.54ms, tok/sec:157370.97
Current shard: 1, Current position: 983040
Current shard: 1, Current position: 917504
Current shard: 1, Current position: 1114112
Current shard: 1, Current position: 1048576
Current shard: 1, Current position: 1179648
Current shard: 1, Current position: 1245184
Current shard: 1, Current position: 1376256
Current shard: 1, Current position: 1310720
step 192, loss: 6.161969, norm:0.5375, lr:2.9978e-04 dt: 3331.66ms, tok/sec:157365.60
Current shard: 1, Current position: 1507328
Current shard: 1, Current position: 1441792
Current shard: 1, Current position: 1638400
Current shard: 1, Current position: 1572864
Current shard: 1, Current position: 1703936
Current shard: 1, Current position: 1769472
Current shard: 1, Current position: 1900544
Current shard: 1, Current position: 1835008
step 193, loss: 6.161243, norm:0.4364, lr:2.9978e-04 dt: 3331.74ms, tok/sec:157361.77
Current shard: 1, Current position: 2031616
Current shard: 1, Current position: 1966080
Current shard: 1, Current position: 2162688
Current shard: 1, Current position: 2097152
Current shard: 1, Current position: 2228224
Current shard: 1, Current position: 2293760
Current shard: 1, Current position: 2424832
Current shard: 1, Current position: 2359296
step 194, loss: 6.129148, norm:0.5436, lr:2.9977e-04 dt: 3331.57ms, tok/sec:157369.88
Current shard: 1, Current position: 2555904
Current shard: 1, Current position: 2490368
Current shard: 1, Current position: 2686976
Current shard: 1, Current position: 2621440
Current shard: 1, Current position: 2752512Current shard: 1, Current position: 2818048

Current shard: 1, Current position: 2949120
Current shard: 1, Current position: 2883584
step 195, loss: 6.125138, norm:0.4211, lr:2.9977e-04 dt: 3331.34ms, tok/sec:157380.30
Current shard: 1, Current position: 3080192
Current shard: 1, Current position: 3014656
Current shard: 1, Current position: 3211264
Current shard: 1, Current position: 3145728
Current shard: 1, Current position: 3276800
Current shard: 1, Current position: 3342336
Current shard: 1, Current position: 3473408
Current shard: 1, Current position: 3407872
step 196, loss: 6.116444, norm:0.4226, lr:2.9977e-04 dt: 3331.29ms, tok/sec:157382.81
Current shard: 1, Current position: 3604480
Current shard: 1, Current position: 3538944
Current shard: 1, Current position: 3735552
Current shard: 1, Current position: 3670016
Current shard: 1, Current position: 3801088
Current shard: 1, Current position: 3866624
Current shard: 1, Current position: 3997696
Current shard: 1, Current position: 3932160
step 197, loss: 6.128513, norm:0.4927, lr:2.9977e-04 dt: 3331.45ms, tok/sec:157375.19
Current shard: 1, Current position: 4128768
Current shard: 1, Current position: 4063232
Current shard: 1, Current position: 4259840
Current shard: 1, Current position: 4194304
Current shard: 1, Current position: 4325376
Current shard: 1, Current position: 4390912
Current shard: 1, Current position: 4521984
Current shard: 1, Current position: 4456448
step 198, loss: 6.106848, norm:0.4591, lr:2.9976e-04 dt: 3331.34ms, tok/sec:157380.33
Current shard: 1, Current position: 4653056
Current shard: 1, Current position: 4587520
Current shard: 1, Current position: 4784128
Current shard: 1, Current position: 4718592
Current shard: 1, Current position: 4915200
Current shard: 1, Current position: 4849664
Current shard: 1, Current position: 5046272
Current shard: 1, Current position: 4980736
step 199, loss: 6.119636, norm:0.4187, lr:2.9976e-04 dt: 3331.56ms, tok/sec:157370.25
Current shard: 0, Current position: 131072
Current shard: 0, Current position: 196608
Current shard: 0, Current position: 262144
Current shard: 0, Current position: 327680
Current shard: 0, Current position: 393216
Current shard: 0, Current position: 458752
Current shard: 0, Current position: 524288
Current shard: 0, Current position: 589824
Current shard: 0, Current position: 655360
Current shard: 0, Current position: 720896
Current shard: 0, Current position: 786432
Current shard: 0, Current position: 851968
Current shard: 0, Current position: 917504
Current shard: 0, Current position: 983040
Current shard: 0, Current position: 1114112
Current shard: 0, Current position: 1048576
Current shard: 0, Current position: 1245184
Current shard: 0, Current position: 1179648
Current shard: 0, Current position: 1376256
Current shard: 0, Current position: 1310720
Current shard: 0, Current position: 1507328
Current shard: 0, Current position: 1441792
Current shard: 0, Current position: 1638400
Current shard: 0, Current position: 1572864
Current shard: 0, Current position: 1769472
Current shard: 0, Current position: 1703936
Current shard: 0, Current position: 1900544
Current shard: 0, Current position: 1835008
Current shard: 0, Current position: 2031616
Current shard: 0, Current position: 1966080
Current shard: 0, Current position: 2162688
Current shard: 0, Current position: 2097152
Current shard: 0, Current position: 2293760
Current shard: 0, Current position: 2228224
Current shard: 0, Current position: 2424832
Current shard: 0, Current position: 2359296
Current shard: 0, Current position: 2555904
Current shard: 0, Current position: 2490368
Current shard: 0, Current position: 2686976
Current shard: 0, Current position: 2621440
validation loss: 6.1481
HellaSwag accuracy:-8732387158708335027/-2=4366193579354167296.0000
rank 1 sample 0: Hello, I'm a language model, they could have a different and if for this question of how is a person may be an event and the most of the
rank 1 sample 1: Hello, I'm a language model, I have been created a few times a child, we think that of those are of students has little, and we have
rank 1 sample 2: Hello, I'm a language model, that some is in the word, it is a certain book by it was of the same of what was the second word
rank 1 sample 3: Hello, I'm a language model, I really was also the name of all its own. With those who also come to read from the "for sure that
Current shard: 1, Current position: 5177344
rank 0 sample 0: Hello, I'm a language model, she was very large. As it had some that a special way with he would be that if a few and that they
rank 0 sample 1: Hello, I'm a language model, like me, and that can do one of this issue a simple to create a few. What's that you see a
rank 0 sample 2: Hello, I'm a language model, but it might never always very bad as a simple, and not only one of a part of a similar to be much
rank 0 sample 3: Hello, I'm a language model, that were known, and the world of a common, and he made it, on that his husband at the other his
Current shard: 1, Current position: 5111808
Current shard: 1, Current position: 5308416
Current shard: 1, Current position: 5242880
Current shard: 1, Current position: 5373952Current shard: 1, Current position: 5439488

Current shard: 1, Current position: 5570560
Current shard: 1, Current position: 5505024
step 200, loss: 6.115602, norm:0.4505, lr:2.9976e-04 dt: 54404.64ms, tok/sec:9636.82
Current shard: 1, Current position: 5701632
Current shard: 1, Current position: 5636096
Current shard: 1, Current position: 5832704
Current shard: 1, Current position: 5767168
Current shard: 1, Current position: 5898240
Current shard: 1, Current position: 5963776
Current shard: 1, Current position: 6094848
Current shard: 1, Current position: 6029312
step 201, loss: 6.096633, norm:0.4707, lr:2.9976e-04 dt: 3331.52ms, tok/sec:157371.91
Current shard: 1, Current position: 6225920
Current shard: 1, Current position: 6160384
Current shard: 1, Current position: 6356992
Current shard: 1, Current position: 6291456
Current shard: 1, Current position: 6488064
Current shard: 1, Current position: 6422528
Current shard: 1, Current position: 6619136
Current shard: 1, Current position: 6553600
step 202, loss: 6.084324, norm:0.4522, lr:2.9975e-04 dt: 3331.69ms, tok/sec:157363.97
Current shard: 1, Current position: 6750208
Current shard: 1, Current position: 6684672
Current shard: 1, Current position: 6881280
Current shard: 1, Current position: 6815744
Current shard: 1, Current position: 6946816
Current shard: 1, Current position: 7012352
Current shard: 1, Current position: 7143424
Current shard: 1, Current position: 7077888
step 203, loss: 6.116218, norm:0.3533, lr:2.9975e-04 dt: 3331.59ms, tok/sec:157368.91
Current shard: 1, Current position: 7274496
Current shard: 1, Current position: 7208960
Current shard: 1, Current position: 7405568
Current shard: 1, Current position: 7340032
Current shard: 1, Current position: 7471104
Current shard: 1, Current position: 7536640
Current shard: 1, Current position: 7667712
Current shard: 1, Current position: 7602176
step 204, loss: 6.102067, norm:0.3964, lr:2.9975e-04 dt: 3331.39ms, tok/sec:157377.99
Current shard: 1, Current position: 7798784
Current shard: 1, Current position: 7733248
Current shard: 1, Current position: 7929856
Current shard: 1, Current position: 7864320
Current shard: 1, Current position: 7995392Current shard: 1, Current position: 8060928

Current shard: 1, Current position: 8192000
Current shard: 1, Current position: 8126464
step 205, loss: 6.135485, norm:0.3857, lr:2.9975e-04 dt: 3331.53ms, tok/sec:157371.62
Current shard: 1, Current position: 8323072
Current shard: 1, Current position: 8257536
Current shard: 1, Current position: 8454144
Current shard: 1, Current position: 8388608
Current shard: 1, Current position: 8585216
Current shard: 1, Current position: 8519680
Current shard: 1, Current position: 8716288
Current shard: 1, Current position: 8650752
step 206, loss: 6.096232, norm:0.3716, lr:2.9974e-04 dt: 3331.66ms, tok/sec:157365.20
Current shard: 1, Current position: 8847360
Current shard: 1, Current position: 8781824
Current shard: 1, Current position: 8978432
Current shard: 1, Current position: 8912896
Current shard: 1, Current position: 9109504
Current shard: 1, Current position: 9043968
Current shard: 1, Current position: 9240576
Current shard: 1, Current position: 9175040
step 207, loss: 6.108401, norm:0.4548, lr:2.9974e-04 dt: 3331.49ms, tok/sec:157373.62
Current shard: 1, Current position: 9306112
Current shard: 1, Current position: 9371648
Current shard: 1, Current position: 9437184
Current shard: 1, Current position: 9502720
Current shard: 1, Current position: 9568256Current shard: 1, Current position: 9633792

Current shard: 1, Current position: 9764864
Current shard: 1, Current position: 9699328
step 208, loss: 6.008026, norm:0.3612, lr:2.9974e-04 dt: 3331.28ms, tok/sec:157383.11
Current shard: 1, Current position: 9895936
Current shard: 1, Current position: 9830400
Current shard: 1, Current position: 10027008
Current shard: 1, Current position: 9961472
Current shard: 1, Current position: 10092544
Current shard: 1, Current position: 10158080
Current shard: 1, Current position: 10289152
Current shard: 1, Current position: 10223616
step 209, loss: 5.993199, norm:0.4353, lr:2.9974e-04 dt: 3331.69ms, tok/sec:157364.03
Current shard: 1, Current position: 10420224
Current shard: 1, Current position: 10354688
Current shard: 1, Current position: 10551296
Current shard: 1, Current position: 10485760
Current shard: 1, Current position: 10616832
Current shard: 1, Current position: 10682368
Current shard: 1, Current position: 10813440
Current shard: 1, Current position: 10747904
step 210, loss: 6.034650, norm:0.4942, lr:2.9973e-04 dt: 3331.31ms, tok/sec:157381.95
Current shard: 1, Current position: 10944512
Current shard: 1, Current position: 10878976
Current shard: 1, Current position: 11075584
Current shard: 1, Current position: 11010048
Current shard: 1, Current position: 11206656Current shard: 1, Current position: 11141120

Current shard: 1, Current position: 11337728
Current shard: 1, Current position: 11272192
step 211, loss: 6.061662, norm:0.5335, lr:2.9973e-04 dt: 3331.79ms, tok/sec:157359.10
Current shard: 1, Current position: 11468800
Current shard: 1, Current position: 11403264
Current shard: 1, Current position: 11599872
Current shard: 1, Current position: 11534336
Current shard: 1, Current position: 11665408
Current shard: 1, Current position: 11730944
Current shard: 1, Current position: 11862016
Current shard: 1, Current position: 11796480
step 212, loss: 6.079834, norm:0.5420, lr:2.9973e-04 dt: 3331.40ms, tok/sec:157377.49
Current shard: 1, Current position: 11993088
Current shard: 1, Current position: 11927552
Current shard: 1, Current position: 12124160
Current shard: 1, Current position: 12058624
Current shard: 1, Current position: 12255232
Current shard: 1, Current position: 12189696
Current shard: 1, Current position: 12386304
Current shard: 1, Current position: 12320768
step 213, loss: 6.057614, norm:0.5013, lr:2.9973e-04 dt: 3331.44ms, tok/sec:157375.69
Current shard: 1, Current position: 12517376
Current shard: 1, Current position: 12451840
Current shard: 1, Current position: 12648448
Current shard: 1, Current position: 12582912
Current shard: 1, Current position: 12779520
Current shard: 1, Current position: 12713984
Current shard: 1, Current position: 12910592
Current shard: 1, Current position: 12845056
step 214, loss: 6.056595, norm:0.4992, lr:2.9972e-04 dt: 3331.44ms, tok/sec:157375.71
Current shard: 1, Current position: 13041664
Current shard: 1, Current position: 12976128
Current shard: 1, Current position: 13172736
Current shard: 1, Current position: 13107200
Current shard: 1, Current position: 13303808
Current shard: 1, Current position: 13238272
Current shard: 1, Current position: 13434880
Current shard: 1, Current position: 13369344
step 215, loss: 6.063241, norm:0.4699, lr:2.9972e-04 dt: 3331.57ms, tok/sec:157369.51
Current shard: 1, Current position: 13565952
Current shard: 1, Current position: 13500416
Current shard: 1, Current position: 13697024
Current shard: 1, Current position: 13631488
Current shard: 1, Current position: 13828096Current shard: 1, Current position: 13762560

Current shard: 1, Current position: 13959168
Current shard: 1, Current position: 13893632
step 216, loss: 5.991849, norm:0.3378, lr:2.9972e-04 dt: 3331.65ms, tok/sec:157365.98
Current shard: 1, Current position: 14090240
Current shard: 1, Current position: 14024704
Current shard: 1, Current position: 14221312
Current shard: 1, Current position: 14155776
Current shard: 1, Current position: 14352384Current shard: 1, Current position: 14286848

Current shard: 1, Current position: 14483456
Current shard: 1, Current position: 14417920
step 217, loss: 6.018581, norm:0.5463, lr:2.9971e-04 dt: 3331.40ms, tok/sec:157377.83
Current shard: 1, Current position: 14614528
Current shard: 1, Current position: 14548992
Current shard: 1, Current position: 14745600
Current shard: 1, Current position: 14680064
Current shard: 1, Current position: 14811136Current shard: 1, Current position: 14876672

Current shard: 1, Current position: 15007744
Current shard: 1, Current position: 14942208
step 218, loss: 5.998236, norm:0.7543, lr:2.9971e-04 dt: 3331.55ms, tok/sec:157370.67
Current shard: 1, Current position: 15138816
Current shard: 1, Current position: 15073280
Current shard: 1, Current position: 15269888
Current shard: 1, Current position: 15204352
Current shard: 1, Current position: 15335424
Current shard: 1, Current position: 15400960
Current shard: 1, Current position: 15532032
Current shard: 1, Current position: 15466496
step 219, loss: 5.994722, norm:0.6587, lr:2.9971e-04 dt: 3331.35ms, tok/sec:157379.99
Current shard: 1, Current position: 15663104
Current shard: 1, Current position: 15597568
Current shard: 1, Current position: 15794176
Current shard: 1, Current position: 15728640
Current shard: 1, Current position: 15925248
Current shard: 1, Current position: 15859712
Current shard: 1, Current position: 16056320
Current shard: 1, Current position: 15990784
step 220, loss: 5.922127, norm:0.4754, lr:2.9971e-04 dt: 3331.91ms, tok/sec:157353.36
Current shard: 1, Current position: 16187392
Current shard: 1, Current position: 16121856
Current shard: 1, Current position: 16318464
Current shard: 1, Current position: 16252928
Current shard: 1, Current position: 16449536
Current shard: 1, Current position: 16384000
Current shard: 1, Current position: 16580608
Current shard: 1, Current position: 16515072
step 221, loss: 5.966915, norm:0.5168, lr:2.9970e-04 dt: 3331.53ms, tok/sec:157371.60
Current shard: 1, Current position: 16711680
Current shard: 1, Current position: 16646144
Current shard: 1, Current position: 16842752
Current shard: 1, Current position: 16777216
Current shard: 1, Current position: 16973824Current shard: 1, Current position: 16908288

Current shard: 1, Current position: 17104896
Current shard: 1, Current position: 17039360
step 222, loss: 5.973418, norm:0.4421, lr:2.9970e-04 dt: 3331.66ms, tok/sec:157365.56
Current shard: 1, Current position: 17235968
Current shard: 1, Current position: 17170432
Current shard: 1, Current position: 17367040
Current shard: 1, Current position: 17301504
Current shard: 1, Current position: 17498112
Current shard: 1, Current position: 17432576
Current shard: 1, Current position: 17629184
Current shard: 1, Current position: 17563648
step 223, loss: 5.905159, norm:0.4643, lr:2.9970e-04 dt: 3331.28ms, tok/sec:157383.37
Current shard: 1, Current position: 17760256
Current shard: 1, Current position: 17694720
Current shard: 1, Current position: 17891328
Current shard: 1, Current position: 17825792
Current shard: 1, Current position: 17956864
Current shard: 1, Current position: 18022400
Current shard: 1, Current position: 18153472
Current shard: 1, Current position: 18087936
step 224, loss: 5.956049, norm:0.4946, lr:2.9969e-04 dt: 3331.71ms, tok/sec:157363.25
Current shard: 1, Current position: 18284544
Current shard: 1, Current position: 18219008
Current shard: 1, Current position: 18415616
Current shard: 1, Current position: 18350080
Current shard: 1, Current position: 18481152Current shard: 1, Current position: 18546688

Current shard: 1, Current position: 18677760
Current shard: 1, Current position: 18612224
step 225, loss: 5.888521, norm:0.4653, lr:2.9969e-04 dt: 3331.46ms, tok/sec:157374.81
Current shard: 1, Current position: 18808832
Current shard: 1, Current position: 18743296
Current shard: 1, Current position: 18939904
Current shard: 1, Current position: 18874368
Current shard: 1, Current position: 19005440
Current shard: 1, Current position: 19070976
Current shard: 1, Current position: 19202048
Current shard: 1, Current position: 19136512
step 226, loss: 5.895545, norm:0.4944, lr:2.9969e-04 dt: 3331.21ms, tok/sec:157386.43
Current shard: 1, Current position: 19333120
Current shard: 1, Current position: 19267584
Current shard: 1, Current position: 19464192
Current shard: 1, Current position: 19398656
Current shard: 1, Current position: 19595264Current shard: 1, Current position: 19529728

Current shard: 1, Current position: 19726336
Current shard: 1, Current position: 19660800
step 227, loss: 5.991622, norm:0.5811, lr:2.9969e-04 dt: 3331.76ms, tok/sec:157360.86
Current shard: 1, Current position: 19857408
Current shard: 1, Current position: 19791872
Current shard: 1, Current position: 19988480
Current shard: 1, Current position: 19922944
Current shard: 1, Current position: 20119552
Current shard: 1, Current position: 20054016
Current shard: 1, Current position: 20250624
Current shard: 1, Current position: 20185088
step 228, loss: 5.908544, norm:0.4754, lr:2.9968e-04 dt: 3331.47ms, tok/sec:157374.47
Current shard: 1, Current position: 20381696
Current shard: 1, Current position: 20316160
Current shard: 1, Current position: 20512768
Current shard: 1, Current position: 20447232
Current shard: 1, Current position: 20643840Current shard: 1, Current position: 20578304

Current shard: 1, Current position: 20774912
Current shard: 1, Current position: 20709376
step 229, loss: 5.924278, norm:0.4454, lr:2.9968e-04 dt: 3332.14ms, tok/sec:157342.56
Current shard: 1, Current position: 20905984
Current shard: 1, Current position: 20840448
Current shard: 1, Current position: 21037056
Current shard: 1, Current position: 20971520
Current shard: 1, Current position: 21102592
Current shard: 1, Current position: 21168128
Current shard: 1, Current position: 21299200
Current shard: 1, Current position: 21233664
step 230, loss: 5.918656, norm:0.6068, lr:2.9968e-04 dt: 3331.22ms, tok/sec:157386.31
Current shard: 1, Current position: 21430272
Current shard: 1, Current position: 21364736
Current shard: 1, Current position: 21561344
Current shard: 1, Current position: 21495808
Current shard: 1, Current position: 21626880Current shard: 1, Current position: 21692416

Current shard: 1, Current position: 21823488
Current shard: 1, Current position: 21757952
step 231, loss: 5.922338, norm:0.7401, lr:2.9967e-04 dt: 3331.56ms, tok/sec:157369.89
Current shard: 1, Current position: 21954560
Current shard: 1, Current position: 21889024
Current shard: 1, Current position: 22085632
Current shard: 1, Current position: 22020096
Current shard: 1, Current position: 22151168
Current shard: 1, Current position: 22216704
Current shard: 1, Current position: 22347776
Current shard: 1, Current position: 22282240
step 232, loss: 6.074125, norm:0.5887, lr:2.9967e-04 dt: 3331.61ms, tok/sec:157367.60
Current shard: 1, Current position: 22478848
Current shard: 1, Current position: 22413312
Current shard: 1, Current position: 22609920
Current shard: 1, Current position: 22544384
Current shard: 1, Current position: 22675456
Current shard: 1, Current position: 22740992
Current shard: 1, Current position: 22872064
Current shard: 1, Current position: 22806528
step 233, loss: 6.047979, norm:0.5135, lr:2.9967e-04 dt: 3331.68ms, tok/sec:157364.43
Current shard: 1, Current position: 23003136
Current shard: 1, Current position: 22937600
Current shard: 1, Current position: 23134208
Current shard: 1, Current position: 23068672
Current shard: 1, Current position: 23265280
Current shard: 1, Current position: 23199744
Current shard: 1, Current position: 23396352
Current shard: 1, Current position: 23330816
step 234, loss: 6.056435, norm:0.4073, lr:2.9967e-04 dt: 3331.46ms, tok/sec:157374.71
Current shard: 1, Current position: 23527424
Current shard: 1, Current position: 23461888
Current shard: 1, Current position: 23658496
Current shard: 1, Current position: 23592960
Current shard: 1, Current position: 23789568
Current shard: 1, Current position: 23724032
Current shard: 1, Current position: 23920640
Current shard: 1, Current position: 23855104
step 235, loss: 6.054398, norm:0.5041, lr:2.9966e-04 dt: 3331.54ms, tok/sec:157370.93
Current shard: 1, Current position: 24051712
Current shard: 1, Current position: 23986176
Current shard: 1, Current position: 24182784
Current shard: 1, Current position: 24117248
Current shard: 1, Current position: 24313856
Current shard: 1, Current position: 24248320
Current shard: 1, Current position: 24444928
Current shard: 1, Current position: 24379392
step 236, loss: 6.078581, norm:0.4400, lr:2.9966e-04 dt: 3331.77ms, tok/sec:157360.40
Current shard: 1, Current position: 24576000
Current shard: 1, Current position: 24510464
Current shard: 1, Current position: 24707072
Current shard: 1, Current position: 24641536
Current shard: 1, Current position: 24772608
Current shard: 1, Current position: 24838144
Current shard: 1, Current position: 24969216
Current shard: 1, Current position: 24903680
step 237, loss: 6.058786, norm:0.5245, lr:2.9966e-04 dt: 3331.95ms, tok/sec:157351.48
Current shard: 1, Current position: 25100288
Current shard: 1, Current position: 25034752
Current shard: 1, Current position: 25231360
Current shard: 1, Current position: 25165824
Current shard: 1, Current position: 25362432Current shard: 1, Current position: 25296896

Current shard: 1, Current position: 25493504
Current shard: 1, Current position: 25427968
step 238, loss: 6.004366, norm:0.4771, lr:2.9965e-04 dt: 3331.57ms, tok/sec:157369.44
Current shard: 1, Current position: 25624576
Current shard: 1, Current position: 25559040
Current shard: 1, Current position: 25755648
Current shard: 1, Current position: 25690112
Current shard: 1, Current position: 25821184
Current shard: 1, Current position: 25886720
Current shard: 1, Current position: 26017792
Current shard: 1, Current position: 25952256
step 239, loss: 6.027536, norm:0.4745, lr:2.9965e-04 dt: 3331.50ms, tok/sec:157372.93
Current shard: 1, Current position: 26148864
Current shard: 1, Current position: 26083328
Current shard: 1, Current position: 26279936
Current shard: 1, Current position: 26214400
Current shard: 1, Current position: 26411008
Current shard: 1, Current position: 26345472
Current shard: 1, Current position: 26542080
Current shard: 1, Current position: 26476544
step 240, loss: 6.026725, norm:0.4992, lr:2.9965e-04 dt: 3331.52ms, tok/sec:157371.80
Current shard: 1, Current position: 26673152
Current shard: 1, Current position: 26607616
Current shard: 1, Current position: 26804224
Current shard: 1, Current position: 26738688
Current shard: 1, Current position: 26935296
Current shard: 1, Current position: 26869760
Current shard: 1, Current position: 27066368
Current shard: 1, Current position: 27000832
step 241, loss: 6.037220, norm:0.4221, lr:2.9964e-04 dt: 3331.53ms, tok/sec:157371.43
Current shard: 1, Current position: 27197440
Current shard: 1, Current position: 27131904
Current shard: 1, Current position: 27328512
Current shard: 1, Current position: 27262976
Current shard: 1, Current position: 27459584
Current shard: 1, Current position: 27394048
Current shard: 1, Current position: 27590656
Current shard: 1, Current position: 27525120
step 242, loss: 6.018888, norm:0.3397, lr:2.9964e-04 dt: 3331.52ms, tok/sec:157371.96
Current shard: 1, Current position: 27721728
Current shard: 1, Current position: 27656192
Current shard: 1, Current position: 27852800
Current shard: 1, Current position: 27787264
Current shard: 1, Current position: 27918336
Current shard: 1, Current position: 27983872
Current shard: 1, Current position: 28114944
Current shard: 1, Current position: 28049408
step 243, loss: 6.017035, norm:0.4870, lr:2.9964e-04 dt: 3331.66ms, tok/sec:157365.19
Current shard: 1, Current position: 28246016
Current shard: 1, Current position: 28180480
Current shard: 1, Current position: 28377088
Current shard: 1, Current position: 28311552
Current shard: 1, Current position: 28508160
Current shard: 1, Current position: 28442624
Current shard: 1, Current position: 28639232
Current shard: 1, Current position: 28573696
step 244, loss: 5.941476, norm:0.5096, lr:2.9963e-04 dt: 3331.51ms, tok/sec:157372.31
Current shard: 1, Current position: 28770304
Current shard: 1, Current position: 28704768
Current shard: 1, Current position: 28901376
Current shard: 1, Current position: 28835840
Current shard: 1, Current position: 29032448
Current shard: 1, Current position: 28966912
Current shard: 1, Current position: 29163520
Current shard: 1, Current position: 29097984
step 245, loss: 5.929854, norm:0.6253, lr:2.9963e-04 dt: 3331.57ms, tok/sec:157369.82
Current shard: 1, Current position: 29294592
Current shard: 1, Current position: 29229056
Current shard: 1, Current position: 29425664
Current shard: 1, Current position: 29360128
Current shard: 1, Current position: 29491200
Current shard: 1, Current position: 29556736
Current shard: 1, Current position: 29687808
Current shard: 1, Current position: 29622272
step 246, loss: 5.968147, norm:0.4969, lr:2.9963e-04 dt: 3331.54ms, tok/sec:157371.18
Current shard: 1, Current position: 29818880
Current shard: 1, Current position: 29753344
Current shard: 1, Current position: 29949952
Current shard: 1, Current position: 29884416
Current shard: 1, Current position: 30015488
Current shard: 1, Current position: 30081024
Current shard: 1, Current position: 30212096
Current shard: 1, Current position: 30146560
step 247, loss: 6.005692, norm:0.4545, lr:2.9963e-04 dt: 3331.46ms, tok/sec:157375.06
Current shard: 1, Current position: 30343168
Current shard: 1, Current position: 30277632
Current shard: 1, Current position: 30474240
Current shard: 1, Current position: 30408704
Current shard: 1, Current position: 30539776Current shard: 1, Current position: 30605312

Current shard: 1, Current position: 30736384
Current shard: 1, Current position: 30670848
step 248, loss: 5.929504, norm:0.4279, lr:2.9962e-04 dt: 3331.95ms, tok/sec:157351.49
Current shard: 1, Current position: 30867456
Current shard: 1, Current position: 30801920
Current shard: 1, Current position: 30998528
Current shard: 1, Current position: 30932992
Current shard: 1, Current position: 31064064
Current shard: 1, Current position: 31129600
Current shard: 1, Current position: 31260672
Current shard: 1, Current position: 31195136
step 249, loss: 5.969568, norm:0.3689, lr:2.9962e-04 dt: 3331.37ms, tok/sec:157379.05
HellaSwag accuracy:-8741394332193272220/-2=4370697166096635904.0000
rank 1 sample 0: Hello, I'm a language model, for each unit, if I do all,’s best, I’ll see.
The authors�
rank 1 sample 1: Hello, I'm a language model, that is not used to be the idea, and to this is it is in I hope that of a lot of the
rank 1 sample 2: Hello, I'm a language model, as some good as a long-1-2-5:2) than a lot of "2-12.
rank 1 sample 3: Hello, I'm a language model, I may be like to understand. But also be to change against humanism should be for kids in the classroom.

Current shard: 1, Current position: 31391744
rank 0 sample 0: Hello, I'm a language model, just a person. I'm going in to it’t a whole’t it can do to be an
rank 0 sample 1: Hello, I'm a language model, or if you. I has the end of this I did. You has an example. In the right, like it
rank 0 sample 2: Hello, I'm a language model, and I
That happens a wide system. That's a bit that has a problem is that is a lot. One
rank 0 sample 3: Hello, I'm a language model, that of the problem, which has shown in those are very hard or in this system for. The type of this will
Current shard: 1, Current position: 31326208
Current shard: 1, Current position: 31522816
Current shard: 1, Current position: 31457280
Current shard: 1, Current position: 31653888
Current shard: 1, Current position: 31588352
Current shard: 1, Current position: 31784960
Current shard: 1, Current position: 31719424
step 250, loss: 5.942347, norm:0.3718, lr:2.9962e-04 dt: 48144.24ms, tok/sec:10889.94
Current shard: 1, Current position: 31916032
Current shard: 1, Current position: 31850496
Current shard: 1, Current position: 32047104
Current shard: 1, Current position: 31981568
Current shard: 1, Current position: 32178176
Current shard: 1, Current position: 32112640
Current shard: 1, Current position: 32309248
Current shard: 1, Current position: 32243712
step 251, loss: 5.943758, norm:0.4662, lr:2.9961e-04 dt: 3331.64ms, tok/sec:157366.38
Current shard: 1, Current position: 32440320
Current shard: 1, Current position: 32374784
Current shard: 1, Current position: 32571392
Current shard: 1, Current position: 32505856
Current shard: 1, Current position: 32702464
Current shard: 1, Current position: 32636928
Current shard: 1, Current position: 32833536
Current shard: 1, Current position: 32768000
step 252, loss: 5.980241, norm:0.4214, lr:2.9961e-04 dt: 3331.63ms, tok/sec:157366.79
Current shard: 1, Current position: 32964608
Current shard: 1, Current position: 32899072
Current shard: 1, Current position: 33095680
Current shard: 1, Current position: 33030144
Current shard: 1, Current position: 33226752Current shard: 1, Current position: 33161216

Current shard: 1, Current position: 33357824
Current shard: 1, Current position: 33292288
step 253, loss: 5.954628, norm:0.5021, lr:2.9961e-04 dt: 3331.65ms, tok/sec:157366.02
Current shard: 1, Current position: 33488896
Current shard: 1, Current position: 33423360
Current shard: 1, Current position: 33619968
Current shard: 1, Current position: 33554432
Current shard: 1, Current position: 33685504Current shard: 1, Current position: 33751040

Current shard: 1, Current position: 33882112
Current shard: 1, Current position: 33816576
step 254, loss: 5.930263, norm:0.6850, lr:2.9960e-04 dt: 3331.49ms, tok/sec:157373.52
Current shard: 1, Current position: 34013184
Current shard: 1, Current position: 33947648
Current shard: 1, Current position: 34144256
Current shard: 1, Current position: 34078720
Current shard: 1, Current position: 34275328
Current shard: 1, Current position: 34209792
Current shard: 1, Current position: 34406400
Current shard: 1, Current position: 34340864
step 255, loss: 5.860522, norm:0.6328, lr:2.9960e-04 dt: 3331.59ms, tok/sec:157368.52
Current shard: 1, Current position: 34537472
Current shard: 1, Current position: 34471936
Current shard: 1, Current position: 34668544
Current shard: 1, Current position: 34603008
Current shard: 1, Current position: 34799616
Current shard: 1, Current position: 34734080
Current shard: 1, Current position: 34930688
Current shard: 1, Current position: 34865152
step 256, loss: 5.913229, norm:0.6472, lr:2.9960e-04 dt: 3331.40ms, tok/sec:157377.81
Current shard: 1, Current position: 35061760
Current shard: 1, Current position: 34996224
Current shard: 1, Current position: 35192832
Current shard: 1, Current position: 35127296
Current shard: 1, Current position: 35323904Current shard: 1, Current position: 35258368

Current shard: 1, Current position: 35454976
Current shard: 1, Current position: 35389440
step 257, loss: 5.878046, norm:0.4572, lr:2.9959e-04 dt: 3331.35ms, tok/sec:157380.19
Current shard: 1, Current position: 35586048
Current shard: 1, Current position: 35520512
Current shard: 1, Current position: 35717120
Current shard: 1, Current position: 35651584
Current shard: 1, Current position: 35848192
Current shard: 1, Current position: 35782656
Current shard: 1, Current position: 35979264
Current shard: 1, Current position: 35913728
step 258, loss: 5.880390, norm:0.4190, lr:2.9959e-04 dt: 3331.63ms, tok/sec:157366.78
Current shard: 1, Current position: 36110336
Current shard: 1, Current position: 36044800
Current shard: 1, Current position: 36241408
Current shard: 1, Current position: 36175872
Current shard: 1, Current position: 36372480
Current shard: 1, Current position: 36306944
Current shard: 1, Current position: 36503552
Current shard: 1, Current position: 36438016
step 259, loss: 5.833127, norm:0.3896, lr:2.9959e-04 dt: 3331.29ms, tok/sec:157382.83
Current shard: 1, Current position: 36634624
Current shard: 1, Current position: 36569088
Current shard: 1, Current position: 36765696
Current shard: 1, Current position: 36700160
Current shard: 1, Current position: 36896768Current shard: 1, Current position: 36831232

Current shard: 1, Current position: 37027840
Current shard: 1, Current position: 36962304
step 260, loss: 5.855895, norm:0.5242, lr:2.9958e-04 dt: 3331.84ms, tok/sec:157356.91
Current shard: 1, Current position: 37158912
Current shard: 1, Current position: 37093376
Current shard: 1, Current position: 37289984
Current shard: 1, Current position: 37224448
Current shard: 1, Current position: 37421056
Current shard: 1, Current position: 37355520
Current shard: 1, Current position: 37552128
Current shard: 1, Current position: 37486592
step 261, loss: 5.851218, norm:0.4486, lr:2.9958e-04 dt: 3331.52ms, tok/sec:157371.89
Current shard: 1, Current position: 37683200
Current shard: 1, Current position: 37617664
Current shard: 1, Current position: 37814272
Current shard: 1, Current position: 37748736
Current shard: 1, Current position: 37879808
Current shard: 1, Current position: 37945344
Current shard: 1, Current position: 38076416
Current shard: 1, Current position: 38010880
step 262, loss: 5.813747, norm:0.4442, lr:2.9958e-04 dt: 3331.43ms, tok/sec:157376.18
Current shard: 1, Current position: 38207488
Current shard: 1, Current position: 38141952
Current shard: 1, Current position: 38338560
Current shard: 1, Current position: 38273024
Current shard: 1, Current position: 38404096
Current shard: 1, Current position: 38469632
Current shard: 1, Current position: 38600704
Current shard: 1, Current position: 38535168
step 263, loss: 5.820292, norm:0.4152, lr:2.9957e-04 dt: 3331.24ms, tok/sec:157385.16
Current shard: 1, Current position: 38731776
Current shard: 1, Current position: 38666240
Current shard: 1, Current position: 38862848
Current shard: 1, Current position: 38797312
Current shard: 1, Current position: 38928384
Current shard: 1, Current position: 38993920
Current shard: 1, Current position: 39124992
Current shard: 1, Current position: 39059456
step 264, loss: 5.845155, norm:0.5116, lr:2.9957e-04 dt: 3331.58ms, tok/sec:157369.33
Current shard: 1, Current position: 39256064
Current shard: 1, Current position: 39190528
Current shard: 1, Current position: 39387136
Current shard: 1, Current position: 39321600
Current shard: 1, Current position: 39452672
Current shard: 1, Current position: 39518208
Current shard: 1, Current position: 39649280
Current shard: 1, Current position: 39583744
step 265, loss: 5.845515, norm:0.7885, lr:2.9957e-04 dt: 3331.34ms, tok/sec:157380.65
Current shard: 1, Current position: 39780352
Current shard: 1, Current position: 39714816
Current shard: 1, Current position: 39911424
Current shard: 1, Current position: 39845888
Current shard: 1, Current position: 39976960
Current shard: 1, Current position: 40042496
Current shard: 1, Current position: 40173568
Current shard: 1, Current position: 40108032
step 266, loss: 5.883866, norm:0.9613, lr:2.9956e-04 dt: 3331.43ms, tok/sec:157376.12
Current shard: 1, Current position: 40304640
Current shard: 1, Current position: 40239104
Current shard: 1, Current position: 40435712
Current shard: 1, Current position: 40370176
Current shard: 1, Current position: 40566784
Current shard: 1, Current position: 40501248
Current shard: 1, Current position: 40697856
Current shard: 1, Current position: 40632320
step 267, loss: 5.784364, norm:0.6908, lr:2.9956e-04 dt: 3331.53ms, tok/sec:157371.36
Current shard: 1, Current position: 40828928
Current shard: 1, Current position: 40763392
Current shard: 1, Current position: 40960000
Current shard: 1, Current position: 40894464
Current shard: 1, Current position: 41091072Current shard: 1, Current position: 41025536

Current shard: 1, Current position: 41222144
Current shard: 1, Current position: 41156608
step 268, loss: 5.795599, norm:0.6829, lr:2.9956e-04 dt: 3331.47ms, tok/sec:157374.34
Current shard: 1, Current position: 41353216
Current shard: 1, Current position: 41287680
Current shard: 1, Current position: 41484288
Current shard: 1, Current position: 41418752
Current shard: 1, Current position: 41615360Current shard: 1, Current position: 41549824

Current shard: 1, Current position: 41746432
Current shard: 1, Current position: 41680896
step 269, loss: 5.821228, norm:0.5889, lr:2.9955e-04 dt: 3331.78ms, tok/sec:157359.77
Current shard: 1, Current position: 41877504
Current shard: 1, Current position: 41811968
Current shard: 1, Current position: 42008576
Current shard: 1, Current position: 41943040
Current shard: 1, Current position: 42074112
Current shard: 1, Current position: 42139648
Current shard: 1, Current position: 42270720
Current shard: 1, Current position: 42205184
step 270, loss: 5.799641, norm:0.7131, lr:2.9955e-04 dt: 3331.27ms, tok/sec:157383.78
Current shard: 1, Current position: 42401792
Current shard: 1, Current position: 42336256
Current shard: 1, Current position: 42532864
Current shard: 1, Current position: 42467328
Current shard: 1, Current position: 42663936
Current shard: 1, Current position: 42598400
Current shard: 1, Current position: 42795008
Current shard: 1, Current position: 42729472
step 271, loss: 5.791569, norm:0.6418, lr:2.9955e-04 dt: 3331.27ms, tok/sec:157383.89
Current shard: 1, Current position: 42926080
Current shard: 1, Current position: 42860544
Current shard: 1, Current position: 43057152
Current shard: 1, Current position: 42991616
Current shard: 1, Current position: 43188224Current shard: 1, Current position: 43122688

Current shard: 1, Current position: 43319296
Current shard: 1, Current position: 43253760
step 272, loss: 5.783804, norm:0.6497, lr:2.9954e-04 dt: 3331.45ms, tok/sec:157375.13
Current shard: 1, Current position: 43450368
Current shard: 1, Current position: 43384832
Current shard: 1, Current position: 43581440
Current shard: 1, Current position: 43515904
Current shard: 1, Current position: 43646976
Current shard: 1, Current position: 43712512
Current shard: 1, Current position: 43843584
Current shard: 1, Current position: 43778048
step 273, loss: 5.741483, norm:0.5895, lr:2.9954e-04 dt: 3331.39ms, tok/sec:157378.11
Current shard: 1, Current position: 43974656
Current shard: 1, Current position: 43909120
Current shard: 1, Current position: 44105728
Current shard: 1, Current position: 44040192
Current shard: 1, Current position: 44236800
Current shard: 1, Current position: 44171264
Current shard: 1, Current position: 44367872
Current shard: 1, Current position: 44302336
step 274, loss: 5.721817, norm:0.3949, lr:2.9954e-04 dt: 3331.49ms, tok/sec:157373.50
Current shard: 1, Current position: 44498944
Current shard: 1, Current position: 44433408
Current shard: 1, Current position: 44630016
Current shard: 1, Current position: 44564480
Current shard: 1, Current position: 44695552
Current shard: 1, Current position: 44761088
Current shard: 1, Current position: 44892160
Current shard: 1, Current position: 44826624
step 275, loss: 5.839478, norm:0.4396, lr:2.9953e-04 dt: 3331.15ms, tok/sec:157389.32
Current shard: 1, Current position: 45023232
Current shard: 1, Current position: 44957696
Current shard: 1, Current position: 45154304
Current shard: 1, Current position: 45088768
Current shard: 1, Current position: 45285376
Current shard: 1, Current position: 45219840
Current shard: 1, Current position: 45416448
Current shard: 1, Current position: 45350912
step 276, loss: 5.773915, norm:0.3436, lr:2.9953e-04 dt: 3331.64ms, tok/sec:157366.32
Current shard: 1, Current position: 45547520
Current shard: 1, Current position: 45481984
Current shard: 1, Current position: 45678592
Current shard: 1, Current position: 45613056
Current shard: 1, Current position: 45744128
Current shard: 1, Current position: 45809664
Current shard: 1, Current position: 45940736
Current shard: 1, Current position: 45875200
step 277, loss: 5.777045, norm:0.4498, lr:2.9952e-04 dt: 3331.54ms, tok/sec:157370.91
Current shard: 1, Current position: 46071808
Current shard: 1, Current position: 46006272
Current shard: 1, Current position: 46202880
Current shard: 1, Current position: 46137344
Current shard: 1, Current position: 46268416
Current shard: 1, Current position: 46333952
Current shard: 1, Current position: 46465024
Current shard: 1, Current position: 46399488
step 278, loss: 5.881064, norm:0.5774, lr:2.9952e-04 dt: 3332.08ms, tok/sec:157345.35
Current shard: 1, Current position: 46596096
Current shard: 1, Current position: 46530560
Current shard: 1, Current position: 46727168
Current shard: 1, Current position: 46661632
Current shard: 1, Current position: 46858240Current shard: 1, Current position: 46792704

Current shard: 1, Current position: 46989312
Current shard: 1, Current position: 46923776
step 279, loss: 5.871010, norm:0.4622, lr:2.9952e-04 dt: 3331.22ms, tok/sec:157385.96
Current shard: 1, Current position: 47120384
Current shard: 1, Current position: 47054848
Current shard: 1, Current position: 47251456
Current shard: 1, Current position: 47185920
Current shard: 1, Current position: 47382528
Current shard: 1, Current position: 47316992
Current shard: 1, Current position: 47513600
Current shard: 1, Current position: 47448064
step 280, loss: 5.887811, norm:0.4139, lr:2.9951e-04 dt: 3331.54ms, tok/sec:157370.96
Current shard: 1, Current position: 47644672
Current shard: 1, Current position: 47579136
Current shard: 1, Current position: 47775744
Current shard: 1, Current position: 47710208
Current shard: 1, Current position: 47906816Current shard: 1, Current position: 47841280

Current shard: 1, Current position: 48037888
Current shard: 1, Current position: 47972352
step 281, loss: 5.859771, norm:0.4566, lr:2.9951e-04 dt: 3331.38ms, tok/sec:157378.71
Current shard: 1, Current position: 48103424
Current shard: 1, Current position: 48168960
Current shard: 1, Current position: 48234496
Current shard: 1, Current position: 48300032
Current shard: 1, Current position: 48365568
Current shard: 1, Current position: 48431104
Current shard: 1, Current position: 48562176
Current shard: 1, Current position: 48496640
step 282, loss: 5.876230, norm:0.5487, lr:2.9951e-04 dt: 3331.44ms, tok/sec:157375.81
Current shard: 1, Current position: 48693248
Current shard: 1, Current position: 48627712
Current shard: 1, Current position: 48824320
Current shard: 1, Current position: 48758784
Current shard: 1, Current position: 48955392Current shard: 1, Current position: 48889856

Current shard: 1, Current position: 49086464
Current shard: 1, Current position: 49020928
step 283, loss: 5.870986, norm:0.7891, lr:2.9950e-04 dt: 3331.50ms, tok/sec:157372.95
Current shard: 1, Current position: 49217536
Current shard: 1, Current position: 49152000
Current shard: 1, Current position: 49348608
Current shard: 1, Current position: 49283072
Current shard: 1, Current position: 49414144
Current shard: 1, Current position: 49479680
Current shard: 1, Current position: 49610752
Current shard: 1, Current position: 49545216
step 284, loss: 5.916804, norm:0.6876, lr:2.9950e-04 dt: 3331.51ms, tok/sec:157372.47
Current shard: 1, Current position: 49741824
Current shard: 1, Current position: 49676288
Current shard: 1, Current position: 49872896
Current shard: 1, Current position: 49807360
Current shard: 1, Current position: 49938432
Current shard: 1, Current position: 50003968
Current shard: 1, Current position: 50135040
Current shard: 1, Current position: 50069504
step 285, loss: 5.846786, norm:0.6842, lr:2.9950e-04 dt: 3331.36ms, tok/sec:157379.35
Current shard: 1, Current position: 50266112
Current shard: 1, Current position: 50200576
Current shard: 1, Current position: 50397184
Current shard: 1, Current position: 50331648
Current shard: 1, Current position: 50462720
Current shard: 1, Current position: 50528256
Current shard: 1, Current position: 50659328
Current shard: 1, Current position: 50593792
step 286, loss: 5.870768, norm:0.6812, lr:2.9949e-04 dt: 3331.73ms, tok/sec:157361.89
Current shard: 1, Current position: 50790400
Current shard: 1, Current position: 50724864
Current shard: 1, Current position: 50921472
Current shard: 1, Current position: 50855936
Current shard: 1, Current position: 50987008
Current shard: 1, Current position: 51052544
Current shard: 1, Current position: 51183616
Current shard: 1, Current position: 51118080
step 287, loss: 5.857584, norm:0.5423, lr:2.9949e-04 dt: 3331.31ms, tok/sec:157381.82
Current shard: 1, Current position: 51314688
Current shard: 1, Current position: 51249152
Current shard: 1, Current position: 51445760
Current shard: 1, Current position: 51380224
Current shard: 1, Current position: 51576832
Current shard: 1, Current position: 51511296
Current shard: 1, Current position: 51707904
Current shard: 1, Current position: 51642368
step 288, loss: 5.839812, norm:0.5211, lr:2.9948e-04 dt: 3331.79ms, tok/sec:157359.32
Current shard: 1, Current position: 51838976
Current shard: 1, Current position: 51773440
Current shard: 1, Current position: 51970048
Current shard: 1, Current position: 51904512
Current shard: 1, Current position: 52035584
Current shard: 1, Current position: 52101120
Current shard: 1, Current position: 52232192
Current shard: 1, Current position: 52166656
step 289, loss: 5.827326, norm:0.5944, lr:2.9948e-04 dt: 3331.33ms, tok/sec:157380.77
Current shard: 1, Current position: 52363264
Current shard: 1, Current position: 52297728
Current shard: 1, Current position: 52494336
Current shard: 1, Current position: 52428800
Current shard: 1, Current position: 52559872
Current shard: 1, Current position: 52625408
Current shard: 1, Current position: 52756480
Current shard: 1, Current position: 52690944
step 290, loss: 5.757213, norm:0.5062, lr:2.9948e-04 dt: 3331.40ms, tok/sec:157377.91
Current shard: 1, Current position: 52887552
Current shard: 1, Current position: 52822016
Current shard: 1, Current position: 53018624
Current shard: 1, Current position: 52953088
Current shard: 1, Current position: 53084160
Current shard: 1, Current position: 53149696
Current shard: 1, Current position: 53280768
Current shard: 1, Current position: 53215232
step 291, loss: 5.775712, norm:0.5492, lr:2.9947e-04 dt: 3331.39ms, tok/sec:157378.21
Current shard: 1, Current position: 53411840
Current shard: 1, Current position: 53346304
Current shard: 1, Current position: 53542912
Current shard: 1, Current position: 53477376
Current shard: 1, Current position: 53608448
Current shard: 1, Current position: 53673984
Current shard: 1, Current position: 53805056
Current shard: 1, Current position: 53739520
step 292, loss: 5.777119, norm:0.4946, lr:2.9947e-04 dt: 3331.49ms, tok/sec:157373.25
Current shard: 1, Current position: 53936128
Current shard: 1, Current position: 53870592
Current shard: 1, Current position: 54067200
Current shard: 1, Current position: 54001664
Current shard: 1, Current position: 54198272
Current shard: 1, Current position: 54132736
Current shard: 1, Current position: 54329344
Current shard: 1, Current position: 54263808
step 293, loss: 5.767337, norm:0.4482, lr:2.9947e-04 dt: 3331.43ms, tok/sec:157376.49
Current shard: 1, Current position: 54460416
Current shard: 1, Current position: 54394880
Current shard: 1, Current position: 54591488
Current shard: 1, Current position: 54525952
Current shard: 1, Current position: 54657024
Current shard: 1, Current position: 54722560
Current shard: 1, Current position: 54853632
Current shard: 1, Current position: 54788096
step 294, loss: 5.842413, norm:0.4336, lr:2.9946e-04 dt: 3331.39ms, tok/sec:157378.38
Current shard: 1, Current position: 54984704
Current shard: 1, Current position: 54919168
Current shard: 1, Current position: 55115776
Current shard: 1, Current position: 55050240
Current shard: 1, Current position: 55181312
Current shard: 1, Current position: 55246848
Current shard: 1, Current position: 55377920
Current shard: 1, Current position: 55312384
step 295, loss: 5.797718, norm:0.5084, lr:2.9946e-04 dt: 3331.62ms, tok/sec:157367.10
Current shard: 1, Current position: 55508992
Current shard: 1, Current position: 55443456
Current shard: 1, Current position: 55640064
Current shard: 1, Current position: 55574528
Current shard: 1, Current position: 55705600Current shard: 1, Current position: 55771136

Current shard: 1, Current position: 55902208
Current shard: 1, Current position: 55836672
step 296, loss: 5.770342, norm:0.4786, lr:2.9945e-04 dt: 3331.52ms, tok/sec:157372.16
Current shard: 1, Current position: 56033280
Current shard: 1, Current position: 55967744
Current shard: 1, Current position: 56164352
Current shard: 1, Current position: 56098816
Current shard: 1, Current position: 56229888Current shard: 1, Current position: 56295424

Current shard: 1, Current position: 56426496
Current shard: 1, Current position: 56360960
step 297, loss: 5.797443, norm:0.4509, lr:2.9945e-04 dt: 3331.86ms, tok/sec:157356.15
Current shard: 1, Current position: 56557568
Current shard: 1, Current position: 56492032
Current shard: 1, Current position: 56688640
Current shard: 1, Current position: 56623104
Current shard: 1, Current position: 56754176Current shard: 1, Current position: 56819712

Current shard: 1, Current position: 56950784
Current shard: 1, Current position: 56885248
step 298, loss: 5.785918, norm:0.4653, lr:2.9945e-04 dt: 3331.27ms, tok/sec:157383.87
Current shard: 1, Current position: 57081856
Current shard: 1, Current position: 57016320
Current shard: 1, Current position: 57212928
Current shard: 1, Current position: 57147392
Current shard: 1, Current position: 57344000
Current shard: 1, Current position: 57278464
Current shard: 1, Current position: 57475072
Current shard: 1, Current position: 57409536
step 299, loss: 5.757340, norm:0.5834, lr:2.9944e-04 dt: 3331.58ms, tok/sec:157369.11
Current shard: 0, Current position: 131072
Current shard: 0, Current position: 196608
Current shard: 0, Current position: 262144
Current shard: 0, Current position: 327680
Current shard: 0, Current position: 393216
Current shard: 0, Current position: 458752
Current shard: 0, Current position: 524288
Current shard: 0, Current position: 589824
Current shard: 0, Current position: 655360
Current shard: 0, Current position: 720896
Current shard: 0, Current position: 786432
Current shard: 0, Current position: 851968
Current shard: 0, Current position: 917504
Current shard: 0, Current position: 983040
Current shard: 0, Current position: 1048576
Current shard: 0, Current position: 1114112
Current shard: 0, Current position: 1179648
Current shard: 0, Current position: 1245184
Current shard: 0, Current position: 1376256
Current shard: 0, Current position: 1310720
Current shard: 0, Current position: 1507328
Current shard: 0, Current position: 1441792
Current shard: 0, Current position: 1638400
Current shard: 0, Current position: 1572864
Current shard: 0, Current position: 1769472
Current shard: 0, Current position: 1703936
Current shard: 0, Current position: 1900544
Current shard: 0, Current position: 1835008
Current shard: 0, Current position: 2031616
Current shard: 0, Current position: 1966080
Current shard: 0, Current position: 2162688
Current shard: 0, Current position: 2097152
Current shard: 0, Current position: 2293760
Current shard: 0, Current position: 2228224
Current shard: 0, Current position: 2424832
Current shard: 0, Current position: 2359296
Current shard: 0, Current position: 2555904
Current shard: 0, Current position: 2490368
Current shard: 0, Current position: 2686976
Current shard: 0, Current position: 2621440
validation loss: 5.8011
HellaSwag accuracy:-8732378328112968099/-2=4366189164056483840.0000
rank 1 sample 0: Hello, I'm a language model, because the term is called a problem being used that I do have a person for a group, the patient’s
rank 1 sample 1: Hello, I'm a language model, the most important or that is to find to be seen here. One of what I I remember one is a little thing
rank 1 sample 2: Hello, I'm a language model, but she was one of the same.
The main major
A study.
The research suggests, it can even
rank 1 sample 3: Hello, I'm a language model, I feel like, and are doing something might be an accurate or “no” when a “If you
rank 0 sample 0: Hello, I'm a language model, and the following the following one in the topic was only known as this topic and the following
There was that the most
Current shard: 1, Current position: 57606144
rank 0 sample 1: Hello, I'm a language model, you feel like to have a word
-up if you will find these are a person is not necessarily.
-
rank 0 sample 2: Hello, I'm a language model, but I be. Now when I mean I would only to make. I have some of a lot of the I see
rank 0 sample 3: Hello, I'm a language model, a part of the first place of his life – "the whole story "to and one and "It is not being
Current shard: 1, Current position: 57540608
Current shard: 1, Current position: 57737216
Current shard: 1, Current position: 57671680
Current shard: 1, Current position: 57802752Current shard: 1, Current position: 57868288

Current shard: 1, Current position: 57999360
Current shard: 1, Current position: 57933824
step 300, loss: 5.854129, norm:0.4989, lr:2.9944e-04 dt: 54398.70ms, tok/sec:9637.88
Current shard: 1, Current position: 58130432
Current shard: 1, Current position: 58064896
Current shard: 1, Current position: 58261504
Current shard: 1, Current position: 58195968
Current shard: 1, Current position: 58327040
Current shard: 1, Current position: 58392576
Current shard: 1, Current position: 58523648
Current shard: 1, Current position: 58458112
step 301, loss: 5.694402, norm:0.4801, lr:2.9944e-04 dt: 3331.56ms, tok/sec:157370.26
Current shard: 1, Current position: 58654720
Current shard: 1, Current position: 58589184
Current shard: 1, Current position: 58785792
Current shard: 1, Current position: 58720256
Current shard: 1, Current position: 58916864
Current shard: 1, Current position: 58851328
Current shard: 1, Current position: 59047936
Current shard: 1, Current position: 58982400
step 302, loss: 5.747278, norm:0.4988, lr:2.9943e-04 dt: 3331.54ms, tok/sec:157370.94
Current shard: 1, Current position: 59179008
Current shard: 1, Current position: 59113472
Current shard: 1, Current position: 59310080
Current shard: 1, Current position: 59244544
Current shard: 1, Current position: 59441152
Current shard: 1, Current position: 59375616
Current shard: 1, Current position: 59572224
Current shard: 1, Current position: 59506688
step 303, loss: 5.708557, norm:0.4717, lr:2.9943e-04 dt: 3331.84ms, tok/sec:157356.94
Current shard: 1, Current position: 59703296
Current shard: 1, Current position: 59637760
Current shard: 1, Current position: 59834368
Current shard: 1, Current position: 59768832
Current shard: 1, Current position: 59965440Current shard: 1, Current position: 59899904

Current shard: 1, Current position: 60096512
Current shard: 1, Current position: 60030976
step 304, loss: 5.680558, norm:0.7115, lr:2.9942e-04 dt: 3331.49ms, tok/sec:157373.20
Current shard: 1, Current position: 60227584
Current shard: 1, Current position: 60162048
Current shard: 1, Current position: 60358656
Current shard: 1, Current position: 60293120
Current shard: 1, Current position: 60424192
Current shard: 1, Current position: 60489728
Current shard: 1, Current position: 60620800
Current shard: 1, Current position: 60555264
step 305, loss: 5.674397, norm:0.6029, lr:2.9942e-04 dt: 3331.30ms, tok/sec:157382.39
Current shard: 1, Current position: 60751872
Current shard: 1, Current position: 60686336
Current shard: 1, Current position: 60882944
Current shard: 1, Current position: 60817408
Current shard: 1, Current position: 61014016Current shard: 1, Current position: 60948480

Current shard: 1, Current position: 61145088
Current shard: 1, Current position: 61079552
step 306, loss: 5.693254, norm:0.6608, lr:2.9942e-04 dt: 3331.45ms, tok/sec:157375.49
Current shard: 1, Current position: 61276160
Current shard: 1, Current position: 61210624
Current shard: 1, Current position: 61407232
Current shard: 1, Current position: 61341696
Current shard: 1, Current position: 61472768
Current shard: 1, Current position: 61538304
Current shard: 1, Current position: 61669376
Current shard: 1, Current position: 61603840
step 307, loss: 5.682922, norm:0.5536, lr:2.9941e-04 dt: 3331.50ms, tok/sec:157372.90
Current shard: 1, Current position: 61800448
Current shard: 1, Current position: 61734912
Current shard: 1, Current position: 61931520
Current shard: 1, Current position: 61865984
Current shard: 1, Current position: 61997056
Current shard: 1, Current position: 62062592
Current shard: 1, Current position: 62193664
Current shard: 1, Current position: 62128128
step 308, loss: 5.684626, norm:0.5628, lr:2.9941e-04 dt: 3331.19ms, tok/sec:157387.64
Current shard: 1, Current position: 62324736
Current shard: 1, Current position: 62259200
Current shard: 1, Current position: 62455808
Current shard: 1, Current position: 62390272
Current shard: 1, Current position: 62521344Current shard: 1, Current position: 62586880

Current shard: 1, Current position: 62717952
Current shard: 1, Current position: 62652416
step 309, loss: 5.663205, norm:0.5624, lr:2.9940e-04 dt: 3331.40ms, tok/sec:157377.76
Current shard: 1, Current position: 62849024
Current shard: 1, Current position: 62783488
Current shard: 1, Current position: 62980096
Current shard: 1, Current position: 62914560
Current shard: 1, Current position: 63111168
Current shard: 1, Current position: 63045632
Current shard: 1, Current position: 63242240
Current shard: 1, Current position: 63176704
step 310, loss: 5.671435, norm:0.5686, lr:2.9940e-04 dt: 3331.55ms, tok/sec:157370.49
Current shard: 1, Current position: 63373312
Current shard: 1, Current position: 63307776
Current shard: 1, Current position: 63504384
Current shard: 1, Current position: 63438848
Current shard: 1, Current position: 63569920
Current shard: 1, Current position: 63635456
Current shard: 1, Current position: 63766528
Current shard: 1, Current position: 63700992
step 311, loss: 5.656435, norm:0.5759, lr:2.9940e-04 dt: 3331.72ms, tok/sec:157362.58
Current shard: 1, Current position: 63897600
Current shard: 1, Current position: 63832064
Current shard: 1, Current position: 64028672
Current shard: 1, Current position: 63963136
Current shard: 1, Current position: 64094208
Current shard: 1, Current position: 64159744
Current shard: 1, Current position: 64290816
Current shard: 1, Current position: 64225280
step 312, loss: 5.654065, norm:0.6009, lr:2.9939e-04 dt: 3331.52ms, tok/sec:157372.21
Current shard: 1, Current position: 64421888
Current shard: 1, Current position: 64356352
Current shard: 1, Current position: 64552960
Current shard: 1, Current position: 64487424
Current shard: 1, Current position: 64684032
Current shard: 1, Current position: 64618496
Current shard: 1, Current position: 64815104
Current shard: 1, Current position: 64749568
step 313, loss: 5.607282, norm:0.4925, lr:2.9939e-04 dt: 3331.30ms, tok/sec:157382.39
Current shard: 1, Current position: 64946176
Current shard: 1, Current position: 64880640
Current shard: 1, Current position: 65077248
Current shard: 1, Current position: 65011712
Current shard: 1, Current position: 65142784
Current shard: 1, Current position: 65208320
Current shard: 1, Current position: 65339392
Current shard: 1, Current position: 65273856
step 314, loss: 5.633174, norm:0.6254, lr:2.9938e-04 dt: 3331.38ms, tok/sec:157378.46
Current shard: 1, Current position: 65470464
Current shard: 1, Current position: 65404928
Current shard: 1, Current position: 65601536
Current shard: 1, Current position: 65536000
Current shard: 1, Current position: 65667072Current shard: 1, Current position: 65732608

Current shard: 1, Current position: 65863680
Current shard: 1, Current position: 65798144
step 315, loss: 5.621837, norm:0.6817, lr:2.9938e-04 dt: 3331.22ms, tok/sec:157386.25
Current shard: 1, Current position: 65994752
Current shard: 1, Current position: 65929216
Current shard: 1, Current position: 66125824
Current shard: 1, Current position: 66060288
Current shard: 1, Current position: 66256896Current shard: 1, Current position: 66191360

Current shard: 1, Current position: 66387968
Current shard: 1, Current position: 66322432
step 316, loss: 5.635957, norm:0.5620, lr:2.9938e-04 dt: 3331.56ms, tok/sec:157369.97
Current shard: 1, Current position: 66519040
Current shard: 1, Current position: 66453504
Current shard: 1, Current position: 66650112
Current shard: 1, Current position: 66584576
Current shard: 1, Current position: 66781184Current shard: 1, Current position: 66715648

Current shard: 1, Current position: 66912256
Current shard: 1, Current position: 66846720
step 317, loss: 5.584985, norm:0.5494, lr:2.9937e-04 dt: 3331.19ms, tok/sec:157387.79
Current shard: 1, Current position: 67043328
Current shard: 1, Current position: 66977792
Current shard: 1, Current position: 67174400
Current shard: 1, Current position: 67108864
Current shard: 1, Current position: 67305472
Current shard: 1, Current position: 67239936
Current shard: 1, Current position: 67436544
Current shard: 1, Current position: 67371008
step 318, loss: 5.646156, norm:0.6709, lr:2.9937e-04 dt: 3331.29ms, tok/sec:157382.65
Current shard: 1, Current position: 67567616
Current shard: 1, Current position: 67502080
Current shard: 1, Current position: 67698688
Current shard: 1, Current position: 67633152
Current shard: 1, Current position: 67764224
Current shard: 1, Current position: 67829760
Current shard: 1, Current position: 67960832
Current shard: 1, Current position: 67895296
step 319, loss: 5.608759, norm:0.6246, lr:2.9936e-04 dt: 3331.45ms, tok/sec:157375.48
Current shard: 1, Current position: 68091904
Current shard: 1, Current position: 68026368
Current shard: 1, Current position: 68222976
Current shard: 1, Current position: 68157440
Current shard: 1, Current position: 68288512
Current shard: 1, Current position: 68354048
Current shard: 1, Current position: 68485120
Current shard: 1, Current position: 68419584
step 320, loss: 5.558575, norm:0.6149, lr:2.9936e-04 dt: 3331.62ms, tok/sec:157367.27
Current shard: 1, Current position: 68616192
Current shard: 1, Current position: 68550656
Current shard: 1, Current position: 68747264
Current shard: 1, Current position: 68681728
Current shard: 1, Current position: 68878336
Current shard: 1, Current position: 68812800
Current shard: 1, Current position: 69009408
Current shard: 1, Current position: 68943872
step 321, loss: 5.615132, norm:0.6289, lr:2.9935e-04 dt: 3331.37ms, tok/sec:157379.12
Current shard: 1, Current position: 69140480
Current shard: 1, Current position: 69074944
Current shard: 1, Current position: 69271552
Current shard: 1, Current position: 69206016
Current shard: 1, Current position: 69402624
Current shard: 1, Current position: 69337088
Current shard: 1, Current position: 69533696
Current shard: 1, Current position: 69468160
step 322, loss: 5.554668, norm:0.7425, lr:2.9935e-04 dt: 3331.36ms, tok/sec:157379.56
Current shard: 1, Current position: 69664768
Current shard: 1, Current position: 69599232
Current shard: 1, Current position: 69795840
Current shard: 1, Current position: 69730304
Current shard: 1, Current position: 69861376
Current shard: 1, Current position: 69926912
Current shard: 1, Current position: 70057984
Current shard: 1, Current position: 69992448
step 323, loss: 5.591587, norm:0.7525, lr:2.9935e-04 dt: 3331.47ms, tok/sec:157374.29
Current shard: 1, Current position: 70189056
Current shard: 1, Current position: 70123520
Current shard: 1, Current position: 70320128
Current shard: 1, Current position: 70254592
Current shard: 1, Current position: 70385664
Current shard: 1, Current position: 70451200
Current shard: 1, Current position: 70582272
Current shard: 1, Current position: 70516736
step 324, loss: 5.716466, norm:0.5290, lr:2.9934e-04 dt: 3331.54ms, tok/sec:157371.17
Current shard: 1, Current position: 70713344
Current shard: 1, Current position: 70647808
Current shard: 1, Current position: 70844416
Current shard: 1, Current position: 70778880
Current shard: 1, Current position: 70975488
Current shard: 1, Current position: 70909952
Current shard: 1, Current position: 71106560
Current shard: 1, Current position: 71041024
step 325, loss: 5.772945, norm:0.5398, lr:2.9934e-04 dt: 3331.76ms, tok/sec:157360.84
Current shard: 1, Current position: 71237632
Current shard: 1, Current position: 71172096
Current shard: 1, Current position: 71368704
Current shard: 1, Current position: 71303168
Current shard: 1, Current position: 71499776
Current shard: 1, Current position: 71434240
Current shard: 1, Current position: 71630848
Current shard: 1, Current position: 71565312
step 326, loss: 5.734544, norm:0.6012, lr:2.9933e-04 dt: 3331.54ms, tok/sec:157370.85
Current shard: 1, Current position: 71761920
Current shard: 1, Current position: 71696384
Current shard: 1, Current position: 71892992
Current shard: 1, Current position: 71827456
Current shard: 1, Current position: 72024064
Current shard: 1, Current position: 71958528
Current shard: 1, Current position: 72155136
Current shard: 1, Current position: 72089600
step 327, loss: 5.734608, norm:0.4654, lr:2.9933e-04 dt: 3331.43ms, tok/sec:157376.06
Current shard: 1, Current position: 72286208
Current shard: 1, Current position: 72220672
Current shard: 1, Current position: 72417280
Current shard: 1, Current position: 72351744
Current shard: 1, Current position: 72482816
Current shard: 1, Current position: 72548352
Current shard: 1, Current position: 72679424
Current shard: 1, Current position: 72613888
step 328, loss: 5.728342, norm:0.4719, lr:2.9933e-04 dt: 3331.35ms, tok/sec:157380.21
Current shard: 1, Current position: 72810496
Current shard: 1, Current position: 72744960
Current shard: 1, Current position: 72941568
Current shard: 1, Current position: 72876032
Current shard: 1, Current position: 73007104Current shard: 1, Current position: 73072640

Current shard: 1, Current position: 73203712
Current shard: 1, Current position: 73138176
step 329, loss: 5.738163, norm:0.4758, lr:2.9932e-04 dt: 3332.12ms, tok/sec:157343.53
Current shard: 1, Current position: 73334784
Current shard: 1, Current position: 73269248
Current shard: 1, Current position: 73465856
Current shard: 1, Current position: 73400320
Current shard: 1, Current position: 73531392
Current shard: 1, Current position: 73596928
Current shard: 1, Current position: 73728000
Current shard: 1, Current position: 73662464
step 330, loss: 5.744437, norm:0.4772, lr:2.9932e-04 dt: 3331.77ms, tok/sec:157360.14
Current shard: 1, Current position: 73859072
Current shard: 1, Current position: 73793536
Current shard: 1, Current position: 73990144
Current shard: 1, Current position: 73924608
Current shard: 1, Current position: 74121216
Current shard: 1, Current position: 74055680
Current shard: 1, Current position: 74252288
Current shard: 1, Current position: 74186752
step 331, loss: 5.698070, norm:0.5289, lr:2.9931e-04 dt: 3331.47ms, tok/sec:157374.19
Current shard: 1, Current position: 74317824
Current shard: 1, Current position: 74383360
Current shard: 1, Current position: 74448896
Current shard: 1, Current position: 74514432
Current shard: 1, Current position: 74579968
Current shard: 1, Current position: 74645504
Current shard: 1, Current position: 74776576
Current shard: 1, Current position: 74711040
step 332, loss: 5.725348, norm:0.5634, lr:2.9931e-04 dt: 3331.22ms, tok/sec:157386.34
Current shard: 1, Current position: 74907648
Current shard: 1, Current position: 74842112
Current shard: 1, Current position: 75038720
Current shard: 1, Current position: 74973184
Current shard: 1, Current position: 75169792
Current shard: 1, Current position: 75104256
Current shard: 1, Current position: 75300864
Current shard: 1, Current position: 75235328
step 333, loss: 5.693071, norm:0.5861, lr:2.9930e-04 dt: 3331.53ms, tok/sec:157371.59
Current shard: 1, Current position: 75431936
Current shard: 1, Current position: 75366400
Current shard: 1, Current position: 75563008
Current shard: 1, Current position: 75497472
Current shard: 1, Current position: 75694080Current shard: 1, Current position: 75628544

Current shard: 1, Current position: 75825152
Current shard: 1, Current position: 75759616
step 334, loss: 5.728832, norm:0.6546, lr:2.9930e-04 dt: 3331.25ms, tok/sec:157384.62
Current shard: 1, Current position: 75956224
Current shard: 1, Current position: 75890688
Current shard: 1, Current position: 76087296
Current shard: 1, Current position: 76021760
Current shard: 1, Current position: 76218368Current shard: 1, Current position: 76152832

Current shard: 1, Current position: 76349440
Current shard: 1, Current position: 76283904
step 335, loss: 5.719070, norm:0.6509, lr:2.9930e-04 dt: 3331.49ms, tok/sec:157373.49
Current shard: 1, Current position: 76480512
Current shard: 1, Current position: 76414976
Current shard: 1, Current position: 76611584
Current shard: 1, Current position: 76546048
Current shard: 1, Current position: 76677120
Current shard: 1, Current position: 76742656
Current shard: 1, Current position: 76873728
Current shard: 1, Current position: 76808192
step 336, loss: 5.623078, norm:0.4835, lr:2.9929e-04 dt: 3331.64ms, tok/sec:157366.23
Current shard: 1, Current position: 77004800
Current shard: 1, Current position: 76939264
Current shard: 1, Current position: 77135872
Current shard: 1, Current position: 77070336
Current shard: 1, Current position: 77201408
Current shard: 1, Current position: 77266944
Current shard: 1, Current position: 77398016
Current shard: 1, Current position: 77332480
step 337, loss: 5.678881, norm:0.4547, lr:2.9929e-04 dt: 3331.30ms, tok/sec:157382.30
Current shard: 1, Current position: 77529088
Current shard: 1, Current position: 77463552
Current shard: 1, Current position: 77660160
Current shard: 1, Current position: 77594624
Current shard: 1, Current position: 77725696Current shard: 1, Current position: 77791232

Current shard: 1, Current position: 77922304
Current shard: 1, Current position: 77856768
step 338, loss: 5.693007, norm:0.4968, lr:2.9928e-04 dt: 3331.78ms, tok/sec:157359.91
Current shard: 1, Current position: 78053376
Current shard: 1, Current position: 77987840
Current shard: 1, Current position: 78184448
Current shard: 1, Current position: 78118912
Current shard: 1, Current position: 78249984
Current shard: 1, Current position: 78315520
Current shard: 1, Current position: 78446592
Current shard: 1, Current position: 78381056
step 339, loss: 5.623986, norm:0.5453, lr:2.9928e-04 dt: 3331.42ms, tok/sec:157376.53
Current shard: 1, Current position: 78577664
Current shard: 1, Current position: 78512128
Current shard: 1, Current position: 78708736
Current shard: 1, Current position: 78643200
Current shard: 1, Current position: 78839808
Current shard: 1, Current position: 78774272
Current shard: 1, Current position: 78970880
Current shard: 1, Current position: 78905344
step 340, loss: 5.665920, norm:0.5244, lr:2.9927e-04 dt: 3331.20ms, tok/sec:157386.90
Current shard: 1, Current position: 79101952
Current shard: 1, Current position: 79036416
Current shard: 1, Current position: 79233024
Current shard: 1, Current position: 79167488
Current shard: 1, Current position: 79298560Current shard: 1, Current position: 79364096

Current shard: 1, Current position: 79495168
Current shard: 1, Current position: 79429632
step 341, loss: 5.691620, norm:0.6297, lr:2.9927e-04 dt: 3331.36ms, tok/sec:157379.54
Current shard: 1, Current position: 79626240
Current shard: 1, Current position: 79560704
Current shard: 1, Current position: 79757312
Current shard: 1, Current position: 79691776
Current shard: 1, Current position: 79888384
Current shard: 1, Current position: 79822848
Current shard: 1, Current position: 80019456
Current shard: 1, Current position: 79953920
step 342, loss: 5.675217, norm:0.9420, lr:2.9926e-04 dt: 3331.43ms, tok/sec:157376.47
Current shard: 1, Current position: 80150528
Current shard: 1, Current position: 80084992
Current shard: 1, Current position: 80281600
Current shard: 1, Current position: 80216064
Current shard: 1, Current position: 80347136Current shard: 1, Current position: 80412672

Current shard: 1, Current position: 80543744
Current shard: 1, Current position: 80478208
step 343, loss: 5.608147, norm:1.0529, lr:2.9926e-04 dt: 3331.31ms, tok/sec:157382.12
Current shard: 1, Current position: 80674816
Current shard: 1, Current position: 80609280
Current shard: 1, Current position: 80805888
Current shard: 1, Current position: 80740352
Current shard: 1, Current position: 80936960
Current shard: 1, Current position: 80871424
Current shard: 1, Current position: 81068032
Current shard: 1, Current position: 81002496
step 344, loss: 5.629565, norm:0.6194, lr:2.9926e-04 dt: 3331.39ms, tok/sec:157378.23
Current shard: 1, Current position: 81199104
Current shard: 1, Current position: 81133568
Current shard: 1, Current position: 81330176
Current shard: 1, Current position: 81264640
Current shard: 1, Current position: 81461248
Current shard: 1, Current position: 81395712
Current shard: 1, Current position: 81592320
Current shard: 1, Current position: 81526784
step 345, loss: 5.607381, norm:0.6537, lr:2.9925e-04 dt: 3331.50ms, tok/sec:157373.16
Current shard: 1, Current position: 81723392
Current shard: 1, Current position: 81657856
Current shard: 1, Current position: 81854464
Current shard: 1, Current position: 81788928
Current shard: 1, Current position: 81920000
Current shard: 1, Current position: 81985536
Current shard: 1, Current position: 82116608
Current shard: 1, Current position: 82051072
step 346, loss: 5.612178, norm:0.5632, lr:2.9925e-04 dt: 3331.33ms, tok/sec:157380.87
Current shard: 1, Current position: 82247680
Current shard: 1, Current position: 82182144
Current shard: 1, Current position: 82378752
Current shard: 1, Current position: 82313216
Current shard: 1, Current position: 82444288
Current shard: 1, Current position: 82509824
Current shard: 1, Current position: 82640896
Current shard: 1, Current position: 82575360
step 347, loss: 5.592497, norm:0.5924, lr:2.9924e-04 dt: 3331.75ms, tok/sec:157361.27
Current shard: 1, Current position: 82771968
Current shard: 1, Current position: 82706432
Current shard: 1, Current position: 82903040
Current shard: 1, Current position: 82837504
Current shard: 1, Current position: 82968576
Current shard: 1, Current position: 83034112
Current shard: 1, Current position: 83165184
Current shard: 1, Current position: 83099648
step 348, loss: 5.552908, norm:0.4675, lr:2.9924e-04 dt: 3331.31ms, tok/sec:157382.03
Current shard: 1, Current position: 83296256
Current shard: 1, Current position: 83230720
Current shard: 1, Current position: 83427328
Current shard: 1, Current position: 83361792
Current shard: 1, Current position: 83558400
Current shard: 1, Current position: 83492864
Current shard: 1, Current position: 83689472
Current shard: 1, Current position: 83623936
step 349, loss: 5.581079, norm:0.4682, lr:2.9923e-04 dt: 3331.39ms, tok/sec:157377.98
HellaSwag accuracy:-8885500724167717275/-2=4442750362083858432.0000
rank 0 sample 0: Hello, I'm a language model, though, in the first way is still like something else, but a number and I'm me for my friend, or
rank 0 sample 1: Hello, I'm a language model, not that have been shown that are different. The new way to make the idea that are only a real term that a
rank 0 sample 2: Hello, I'm a language model, but I'll want to understand what the use that really know and then be a bit.
I read a great need
rank 0 sample 3: Hello, I'm a language model, which gives a single-and that are a non-fice (or the next year-down line of this page
rank 1 sample 0: Hello, I'm a language model, as I'd like it, I guess that a book ‘I’s answer I want to make a �
rank 1 sample 1: Hello, I'm a language model, is the first writing.
What we understand the following in the main topic for those at the work?
If you
rank 1 sample 2: Hello, I'm a language model, a. It is a name of the word.
I say it I'm not a person with it, that an
rank 1 sample 3: Hello, I'm a language model, is something that, it a different types not a sense has no person of time. At which we have a way to
Current shard: 1, Current position: 83755008
Current shard: 1, Current position: 83820544
Current shard: 1, Current position: 83886080
Current shard: 1, Current position: 83951616
Current shard: 1, Current position: 84082688
Current shard: 1, Current position: 84017152
Current shard: 1, Current position: 84213760
Current shard: 1, Current position: 84148224
step 350, loss: 5.538067, norm:0.4347, lr:2.9923e-04 dt: 48154.44ms, tok/sec:10887.64
Current shard: 1, Current position: 84344832
Current shard: 1, Current position: 84279296
Current shard: 1, Current position: 84475904
Current shard: 1, Current position: 84410368
Current shard: 1, Current position: 84541440
Current shard: 1, Current position: 84606976
Current shard: 1, Current position: 84738048
Current shard: 1, Current position: 84672512
step 351, loss: 5.583741, norm:0.4837, lr:2.9922e-04 dt: 3331.42ms, tok/sec:157376.96
Current shard: 1, Current position: 84869120
Current shard: 1, Current position: 84803584
Current shard: 1, Current position: 85000192
Current shard: 1, Current position: 84934656
Current shard: 1, Current position: 85065728Current shard: 1, Current position: 85131264

Current shard: 1, Current position: 85262336
Current shard: 1, Current position: 85196800
step 352, loss: 5.528565, norm:0.4721, lr:2.9922e-04 dt: 3331.45ms, tok/sec:157375.19
Current shard: 1, Current position: 85393408
Current shard: 1, Current position: 85327872
Current shard: 1, Current position: 85524480
Current shard: 1, Current position: 85458944
Current shard: 1, Current position: 85655552Current shard: 1, Current position: 85590016

Current shard: 1, Current position: 85786624
Current shard: 1, Current position: 85721088
step 353, loss: 5.556309, norm:0.5314, lr:2.9922e-04 dt: 3331.47ms, tok/sec:157374.60
Current shard: 1, Current position: 85917696
Current shard: 1, Current position: 85852160
Current shard: 1, Current position: 86048768
Current shard: 1, Current position: 85983232
Current shard: 1, Current position: 86114304
Current shard: 1, Current position: 86179840
Current shard: 1, Current position: 86310912
Current shard: 1, Current position: 86245376
step 354, loss: 5.564986, norm:0.5980, lr:2.9921e-04 dt: 3331.60ms, tok/sec:157368.01
Current shard: 1, Current position: 86441984
Current shard: 1, Current position: 86376448
Current shard: 1, Current position: 86573056
Current shard: 1, Current position: 86507520
Current shard: 1, Current position: 86704128Current shard: 1, Current position: 86638592

Current shard: 1, Current position: 86835200
Current shard: 1, Current position: 86769664
step 355, loss: 5.547357, norm:0.6676, lr:2.9921e-04 dt: 3331.48ms, tok/sec:157373.95
Current shard: 1, Current position: 86966272
Current shard: 1, Current position: 86900736
Current shard: 1, Current position: 87097344
Current shard: 1, Current position: 87031808
Current shard: 1, Current position: 87228416Current shard: 1, Current position: 87162880

Current shard: 1, Current position: 87359488
Current shard: 1, Current position: 87293952
step 356, loss: 5.491456, norm:0.6543, lr:2.9920e-04 dt: 3331.37ms, tok/sec:157378.91
Current shard: 1, Current position: 87490560
Current shard: 1, Current position: 87425024
Current shard: 1, Current position: 87621632
Current shard: 1, Current position: 87556096
Current shard: 1, Current position: 87687168
Current shard: 1, Current position: 87752704
Current shard: 1, Current position: 87883776
Current shard: 1, Current position: 87818240
step 357, loss: 5.517971, norm:0.7760, lr:2.9920e-04 dt: 3331.67ms, tok/sec:157365.05
Current shard: 1, Current position: 88014848
Current shard: 1, Current position: 87949312
Current shard: 1, Current position: 88145920
Current shard: 1, Current position: 88080384
Current shard: 1, Current position: 88276992
Current shard: 1, Current position: 88211456
Current shard: 1, Current position: 88408064
Current shard: 1, Current position: 88342528
step 358, loss: 5.515843, norm:0.6690, lr:2.9919e-04 dt: 3331.51ms, tok/sec:157372.52
Current shard: 1, Current position: 88473600
Current shard: 1, Current position: 88539136
Current shard: 1, Current position: 88604672
Current shard: 1, Current position: 88670208
Current shard: 1, Current position: 88801280
Current shard: 1, Current position: 88735744
Current shard: 1, Current position: 88932352
Current shard: 1, Current position: 88866816
step 359, loss: 5.465859, norm:0.6773, lr:2.9919e-04 dt: 3331.65ms, tok/sec:157366.09
Current shard: 1, Current position: 89063424
Current shard: 1, Current position: 88997888
Current shard: 1, Current position: 89194496
Current shard: 1, Current position: 89128960
Current shard: 1, Current position: 89260032
Current shard: 1, Current position: 89325568
Current shard: 1, Current position: 89456640
Current shard: 1, Current position: 89391104
step 360, loss: 5.469494, norm:0.6699, lr:2.9918e-04 dt: 3331.27ms, tok/sec:157383.88
Current shard: 1, Current position: 89587712
Current shard: 1, Current position: 89522176
Current shard: 1, Current position: 89718784
Current shard: 1, Current position: 89653248
Current shard: 1, Current position: 89784320
Current shard: 1, Current position: 89849856
Current shard: 1, Current position: 89980928
Current shard: 1, Current position: 89915392
step 361, loss: 5.504009, norm:0.6313, lr:2.9918e-04 dt: 3331.38ms, tok/sec:157378.49
Current shard: 1, Current position: 90112000
Current shard: 1, Current position: 90046464
Current shard: 1, Current position: 90243072
Current shard: 1, Current position: 90177536
Current shard: 1, Current position: 90308608
Current shard: 1, Current position: 90374144
Current shard: 1, Current position: 90505216
Current shard: 1, Current position: 90439680
step 362, loss: 5.519569, norm:0.6251, lr:2.9917e-04 dt: 3331.37ms, tok/sec:157378.92
Current shard: 1, Current position: 90636288
Current shard: 1, Current position: 90570752
Current shard: 1, Current position: 90767360
Current shard: 1, Current position: 90701824
Current shard: 1, Current position: 90832896Current shard: 1, Current position: 90898432

Current shard: 1, Current position: 91029504
Current shard: 1, Current position: 90963968
step 363, loss: 5.460832, norm:0.5916, lr:2.9917e-04 dt: 3331.66ms, tok/sec:157365.18
Current shard: 1, Current position: 91160576
Current shard: 1, Current position: 91095040
Current shard: 1, Current position: 91291648
Current shard: 1, Current position: 91226112
Current shard: 1, Current position: 91357184
Current shard: 1, Current position: 91422720
Current shard: 1, Current position: 91553792
Current shard: 1, Current position: 91488256
step 364, loss: 5.470001, norm:0.6176, lr:2.9916e-04 dt: 3331.30ms, tok/sec:157382.40
Current shard: 1, Current position: 91684864
Current shard: 1, Current position: 91619328
Current shard: 1, Current position: 91815936
Current shard: 1, Current position: 91750400
Current shard: 1, Current position: 91881472
Current shard: 1, Current position: 91947008
Current shard: 1, Current position: 92078080
Current shard: 1, Current position: 92012544
step 365, loss: 5.438722, norm:0.6664, lr:2.9916e-04 dt: 3331.46ms, tok/sec:157374.65
Current shard: 1, Current position: 92209152
Current shard: 1, Current position: 92143616
Current shard: 1, Current position: 92340224
Current shard: 1, Current position: 92274688
Current shard: 1, Current position: 92405760
Current shard: 1, Current position: 92471296
Current shard: 1, Current position: 92602368
Current shard: 1, Current position: 92536832
step 366, loss: 5.424155, norm:0.5583, lr:2.9915e-04 dt: 3331.73ms, tok/sec:157362.14
Current shard: 1, Current position: 92733440
Current shard: 1, Current position: 92667904
Current shard: 1, Current position: 92864512
Current shard: 1, Current position: 92798976
Current shard: 1, Current position: 92930048
Current shard: 1, Current position: 92995584
Current shard: 1, Current position: 93126656
Current shard: 1, Current position: 93061120
step 367, loss: 5.432845, norm:0.4872, lr:2.9915e-04 dt: 3331.28ms, tok/sec:157383.18
Current shard: 1, Current position: 93257728
Current shard: 1, Current position: 93192192
Current shard: 1, Current position: 93388800
Current shard: 1, Current position: 93323264
Current shard: 1, Current position: 93519872
Current shard: 1, Current position: 93454336
Current shard: 1, Current position: 93650944
Current shard: 1, Current position: 93585408
step 368, loss: 5.478857, norm:0.4971, lr:2.9915e-04 dt: 3331.71ms, tok/sec:157362.92
Current shard: 1, Current position: 93782016
Current shard: 1, Current position: 93716480
Current shard: 1, Current position: 93913088
Current shard: 1, Current position: 93847552
Current shard: 1, Current position: 93978624
Current shard: 1, Current position: 94044160
Current shard: 1, Current position: 94175232
Current shard: 1, Current position: 94109696
step 369, loss: 5.413826, norm:0.5156, lr:2.9914e-04 dt: 3331.38ms, tok/sec:157378.76
Current shard: 1, Current position: 94306304
Current shard: 1, Current position: 94240768
Current shard: 1, Current position: 94437376
Current shard: 1, Current position: 94371840
Current shard: 1, Current position: 94568448
Current shard: 1, Current position: 94502912
Current shard: 1, Current position: 94699520
Current shard: 1, Current position: 94633984
step 370, loss: 5.479990, norm:0.5438, lr:2.9914e-04 dt: 3331.39ms, tok/sec:157378.01
Current shard: 1, Current position: 94830592
Current shard: 1, Current position: 94765056
Current shard: 1, Current position: 94961664
Current shard: 1, Current position: 94896128
Current shard: 1, Current position: 95092736Current shard: 1, Current position: 95027200

Current shard: 1, Current position: 95223808
Current shard: 1, Current position: 95158272
step 371, loss: 5.557250, norm:0.6019, lr:2.9913e-04 dt: 3331.29ms, tok/sec:157382.71
Current shard: 1, Current position: 95354880
Current shard: 1, Current position: 95289344
Current shard: 1, Current position: 95485952
Current shard: 1, Current position: 95420416
Current shard: 1, Current position: 95551488Current shard: 1, Current position: 95617024

Current shard: 1, Current position: 95748096
Current shard: 1, Current position: 95682560
step 372, loss: 5.643161, norm:0.7525, lr:2.9913e-04 dt: 3331.61ms, tok/sec:157367.93
Current shard: 1, Current position: 95879168
Current shard: 1, Current position: 95813632
Current shard: 1, Current position: 96010240
Current shard: 1, Current position: 95944704
Current shard: 1, Current position: 96075776
Current shard: 1, Current position: 96141312
Current shard: 1, Current position: 96272384
Current shard: 1, Current position: 96206848
step 373, loss: 5.555220, norm:0.8568, lr:2.9912e-04 dt: 3331.30ms, tok/sec:157382.40
Current shard: 1, Current position: 96403456
Current shard: 1, Current position: 96337920
Current shard: 1, Current position: 96534528
Current shard: 1, Current position: 96468992
Current shard: 1, Current position: 96600064
Current shard: 1, Current position: 96665600
Current shard: 1, Current position: 96796672
Current shard: 1, Current position: 96731136
step 374, loss: 5.581136, norm:0.8573, lr:2.9912e-04 dt: 3331.37ms, tok/sec:157379.28
Current shard: 1, Current position: 96927744
Current shard: 1, Current position: 96862208
Current shard: 1, Current position: 97058816
Current shard: 1, Current position: 96993280
Current shard: 1, Current position: 97189888
Current shard: 1, Current position: 97124352
Current shard: 1, Current position: 97320960
Current shard: 1, Current position: 97255424
step 375, loss: 5.580204, norm:0.9366, lr:2.9911e-04 dt: 3331.55ms, tok/sec:157370.46
Current shard: 1, Current position: 97452032
Current shard: 1, Current position: 97386496
Current shard: 1, Current position: 97583104
Current shard: 1, Current position: 97517568
Current shard: 1, Current position: 97648640
Current shard: 1, Current position: 97714176
Current shard: 1, Current position: 97845248
Current shard: 1, Current position: 97779712
step 376, loss: 5.607723, norm:0.8705, lr:2.9911e-04 dt: 3331.29ms, tok/sec:157382.81
Current shard: 1, Current position: 97976320
Current shard: 1, Current position: 97910784
Current shard: 1, Current position: 98107392
Current shard: 1, Current position: 98041856
Current shard: 1, Current position: 98172928
Current shard: 1, Current position: 98238464
Current shard: 1, Current position: 98369536
Current shard: 1, Current position: 98304000
step 377, loss: 5.608958, norm:0.5794, lr:2.9910e-04 dt: 3331.84ms, tok/sec:157356.98
Current shard: 1, Current position: 98500608
Current shard: 1, Current position: 98435072
Current shard: 1, Current position: 98631680
Current shard: 1, Current position: 98566144
Current shard: 1, Current position: 98762752Current shard: 1, Current position: 98697216

Current shard: 1, Current position: 98893824
Current shard: 1, Current position: 98828288
step 378, loss: 5.568199, norm:0.5536, lr:2.9910e-04 dt: 3331.51ms, tok/sec:157372.29
Current shard: 1, Current position: 99024896
Current shard: 1, Current position: 98959360
Current shard: 1, Current position: 99155968
Current shard: 1, Current position: 99090432
Current shard: 1, Current position: 99221504Current shard: 1, Current position: 99287040

Current shard: 1, Current position: 99418112
Current shard: 1, Current position: 99352576
step 379, loss: 5.537742, norm:0.4405, lr:2.9909e-04 dt: 3331.54ms, tok/sec:157370.88
Current shard: 1, Current position: 99549184
Current shard: 1, Current position: 99483648
Current shard: 1, Current position: 99680256
Current shard: 1, Current position: 99614720
Current shard: 1, Current position: 99745792
Current shard: 1, Current position: 99811328
Current shard: 2, Current position: 0
Current shard: 2, Current position: 65536
step 380, loss: 5.551870, norm:0.4951, lr:2.9909e-04 dt: 3333.33ms, tok/sec:157286.43
Current shard: 2, Current position: 131072
Current shard: 2, Current position: 196608
Current shard: 2, Current position: 262144
Current shard: 2, Current position: 327680
Current shard: 2, Current position: 393216
Current shard: 2, Current position: 458752
Current shard: 2, Current position: 589824
Current shard: 2, Current position: 524288
step 381, loss: 5.546033, norm:0.5158, lr:2.9908e-04 dt: 3331.66ms, tok/sec:157365.19
Current shard: 2, Current position: 720896
Current shard: 2, Current position: 655360
Current shard: 2, Current position: 851968
Current shard: 2, Current position: 786432
Current shard: 2, Current position: 917504
Current shard: 2, Current position: 983040
Current shard: 2, Current position: 1114112
Current shard: 2, Current position: 1048576
step 382, loss: 5.474777, norm:0.5354, lr:2.9908e-04 dt: 3331.30ms, tok/sec:157382.54
Current shard: 2, Current position: 1245184
Current shard: 2, Current position: 1179648
Current shard: 2, Current position: 1376256
Current shard: 2, Current position: 1310720
Current shard: 2, Current position: 1441792Current shard: 2, Current position: 1507328

Current shard: 2, Current position: 1638400
Current shard: 2, Current position: 1572864
step 383, loss: 5.506140, norm:0.4922, lr:2.9907e-04 dt: 3331.46ms, tok/sec:157374.82
Current shard: 2, Current position: 1769472
Current shard: 2, Current position: 1703936
Current shard: 2, Current position: 1900544
Current shard: 2, Current position: 1835008
Current shard: 2, Current position: 2031616
Current shard: 2, Current position: 1966080
Current shard: 2, Current position: 2162688
Current shard: 2, Current position: 2097152
step 384, loss: 5.480668, norm:0.4224, lr:2.9907e-04 dt: 3331.68ms, tok/sec:157364.64
Current shard: 2, Current position: 2293760
Current shard: 2, Current position: 2228224
Current shard: 2, Current position: 2424832
Current shard: 2, Current position: 2359296
Current shard: 2, Current position: 2490368
Current shard: 2, Current position: 2555904
Current shard: 2, Current position: 2686976
Current shard: 2, Current position: 2621440
step 385, loss: 5.532882, norm:0.5344, lr:2.9906e-04 dt: 3331.49ms, tok/sec:157373.20
Current shard: 2, Current position: 2818048
Current shard: 2, Current position: 2752512
Current shard: 2, Current position: 2949120
Current shard: 2, Current position: 2883584
Current shard: 2, Current position: 3014656
Current shard: 2, Current position: 3080192
Current shard: 2, Current position: 3211264
Current shard: 2, Current position: 3145728
step 386, loss: 5.637790, norm:0.4900, lr:2.9906e-04 dt: 3331.45ms, tok/sec:157375.19
Current shard: 2, Current position: 3342336
Current shard: 2, Current position: 3276800
Current shard: 2, Current position: 3473408
Current shard: 2, Current position: 3407872
Current shard: 2, Current position: 3538944
Current shard: 2, Current position: 3604480
Current shard: 2, Current position: 3735552
Current shard: 2, Current position: 3670016
step 387, loss: 5.463672, norm:0.5027, lr:2.9905e-04 dt: 3331.49ms, tok/sec:157373.31
Current shard: 2, Current position: 3866624
Current shard: 2, Current position: 3801088
Current shard: 2, Current position: 3997696
Current shard: 2, Current position: 3932160
Current shard: 2, Current position: 4063232Current shard: 2, Current position: 4128768

Current shard: 2, Current position: 4259840
Current shard: 2, Current position: 4194304
step 388, loss: 5.500527, norm:0.5832, lr:2.9905e-04 dt: 3331.27ms, tok/sec:157384.00
Current shard: 2, Current position: 4390912
Current shard: 2, Current position: 4325376
Current shard: 2, Current position: 4521984
Current shard: 2, Current position: 4456448
Current shard: 2, Current position: 4653056
Current shard: 2, Current position: 4587520
Current shard: 2, Current position: 4784128
Current shard: 2, Current position: 4718592
step 389, loss: 5.489639, norm:0.8400, lr:2.9904e-04 dt: 3331.54ms, tok/sec:157370.88
Current shard: 2, Current position: 4915200
Current shard: 2, Current position: 4849664
Current shard: 2, Current position: 5046272
Current shard: 2, Current position: 4980736
Current shard: 2, Current position: 5111808
Current shard: 2, Current position: 5177344
Current shard: 2, Current position: 5308416
Current shard: 2, Current position: 5242880
step 390, loss: 5.508427, norm:0.9483, lr:2.9904e-04 dt: 3331.36ms, tok/sec:157379.60
Current shard: 2, Current position: 5439488
Current shard: 2, Current position: 5373952
Current shard: 2, Current position: 5570560
Current shard: 2, Current position: 5505024
Current shard: 2, Current position: 5636096
Current shard: 2, Current position: 5701632
Current shard: 2, Current position: 5832704
Current shard: 2, Current position: 5767168
step 391, loss: 5.450922, norm:0.8714, lr:2.9903e-04 dt: 3331.39ms, tok/sec:157378.01
Current shard: 2, Current position: 5963776
Current shard: 2, Current position: 5898240
Current shard: 2, Current position: 6094848
Current shard: 2, Current position: 6029312
Current shard: 2, Current position: 6160384Current shard: 2, Current position: 6225920

Current shard: 2, Current position: 6356992
Current shard: 2, Current position: 6291456
step 392, loss: 5.489546, norm:0.6935, lr:2.9903e-04 dt: 3331.34ms, tok/sec:157380.50
Current shard: 2, Current position: 6488064
Current shard: 2, Current position: 6422528
Current shard: 2, Current position: 6619136
Current shard: 2, Current position: 6553600
Current shard: 2, Current position: 6750208Current shard: 2, Current position: 6684672

Current shard: 2, Current position: 6881280
Current shard: 2, Current position: 6815744
step 393, loss: 5.452839, norm:0.5798, lr:2.9902e-04 dt: 3331.46ms, tok/sec:157374.66
Current shard: 2, Current position: 7012352
Current shard: 2, Current position: 6946816
Current shard: 2, Current position: 7143424
Current shard: 2, Current position: 7077888
Current shard: 2, Current position: 7274496
Current shard: 2, Current position: 7208960
Current shard: 2, Current position: 7405568
Current shard: 2, Current position: 7340032
step 394, loss: 5.383831, norm:0.6233, lr:2.9902e-04 dt: 3331.60ms, tok/sec:157368.46
Current shard: 2, Current position: 7536640
Current shard: 2, Current position: 7471104
Current shard: 2, Current position: 7667712
Current shard: 2, Current position: 7602176
Current shard: 2, Current position: 7733248
Current shard: 2, Current position: 7798784
Current shard: 2, Current position: 7929856
Current shard: 2, Current position: 7864320
step 395, loss: 5.369782, norm:0.4721, lr:2.9901e-04 dt: 3331.36ms, tok/sec:157379.45
Current shard: 2, Current position: 8060928
Current shard: 2, Current position: 7995392
Current shard: 2, Current position: 8192000
Current shard: 2, Current position: 8126464
Current shard: 2, Current position: 8323072Current shard: 2, Current position: 8257536

Current shard: 2, Current position: 8454144
Current shard: 2, Current position: 8388608
step 396, loss: 5.416471, norm:0.6055, lr:2.9901e-04 dt: 3331.79ms, tok/sec:157359.32
Current shard: 2, Current position: 8585216
Current shard: 2, Current position: 8519680
Current shard: 2, Current position: 8716288
Current shard: 2, Current position: 8650752
Current shard: 2, Current position: 8847360Current shard: 2, Current position: 8781824

Current shard: 2, Current position: 8978432
Current shard: 2, Current position: 8912896
step 397, loss: 5.401321, norm:0.6430, lr:2.9900e-04 dt: 3331.42ms, tok/sec:157376.57
Current shard: 2, Current position: 9109504
Current shard: 2, Current position: 9043968
Current shard: 2, Current position: 9240576
Current shard: 2, Current position: 9175040
Current shard: 2, Current position: 9306112Current shard: 2, Current position: 9371648

Current shard: 2, Current position: 9502720
Current shard: 2, Current position: 9437184
step 398, loss: 5.389669, norm:0.6671, lr:2.9900e-04 dt: 3331.46ms, tok/sec:157374.82
Current shard: 2, Current position: 9633792
Current shard: 2, Current position: 9568256
Current shard: 2, Current position: 9764864
Current shard: 2, Current position: 9699328
Current shard: 2, Current position: 9895936Current shard: 2, Current position: 9830400

Current shard: 2, Current position: 10027008
Current shard: 2, Current position: 9961472
step 399, loss: 5.423761, norm:0.5266, lr:2.9899e-04 dt: 3331.61ms, tok/sec:157367.94
Current shard: 0, Current position: 131072
Current shard: 0, Current position: 196608
Current shard: 0, Current position: 262144
Current shard: 0, Current position: 327680
Current shard: 0, Current position: 393216
Current shard: 0, Current position: 458752
Current shard: 0, Current position: 524288
Current shard: 0, Current position: 589824
Current shard: 0, Current position: 655360
Current shard: 0, Current position: 720896
Current shard: 0, Current position: 786432
Current shard: 0, Current position: 851968
Current shard: 0, Current position: 917504
Current shard: 0, Current position: 983040
Current shard: 0, Current position: 1048576
Current shard: 0, Current position: 1114112
Current shard: 0, Current position: 1179648
Current shard: 0, Current position: 1245184
Current shard: 0, Current position: 1310720
Current shard: 0, Current position: 1376256
Current shard: 0, Current position: 1441792
Current shard: 0, Current position: 1507328
Current shard: 0, Current position: 1638400
Current shard: 0, Current position: 1572864
Current shard: 0, Current position: 1769472
Current shard: 0, Current position: 1703936
Current shard: 0, Current position: 1900544
Current shard: 0, Current position: 1835008
Current shard: 0, Current position: 2031616
Current shard: 0, Current position: 1966080
Current shard: 0, Current position: 2162688
Current shard: 0, Current position: 2097152
Current shard: 0, Current position: 2293760
Current shard: 0, Current position: 2228224
Current shard: 0, Current position: 2424832
Current shard: 0, Current position: 2359296
Current shard: 0, Current position: 2555904
Current shard: 0, Current position: 2490368
Current shard: 0, Current position: 2686976
Current shard: 0, Current position: 2621440
validation loss: 5.4807
HellaSwag accuracy:346880684924718625/-2=-173440342462359328.0000
rank 1 sample 0: Hello, I'm a language model, we might even be wrong, but at a word that he knew that he or a poem of it was the following.
rank 1 sample 1: Hello, I'm a language model, the most common social history of the first history of the main study from the same section shows the importance of the history of
rank 1 sample 2: Hello, I'm a language model, but rather. This is a good source of the past-year term that that is not only to do. We understand
rank 1 sample 3: Hello, I'm a language model, and was a more likely thing, a significant difference between four or six to which was considered because I was a great deal
rank 0 sample 0: Hello, I'm a language model, to be very important. And that if only’s, as "s" I feel as "d" to
rank 0 sample 1: Hello, I'm a language model, by some reason, the words I saw a good job:
What, we are some different types of the language that
rank 0 sample 2: Hello, I'm a language model, but I believe it makes, by which I are used to say the word.
You are not available to use

rank 0 sample 3: Hello, I'm a language model, that were from the same way, for example even after the original example, and would be, and so, but as
Current shard: 2, Current position: 10158080
Current shard: 2, Current position: 10092544
Current shard: 2, Current position: 10289152
Current shard: 2, Current position: 10223616
Current shard: 2, Current position: 10354688
Current shard: 2, Current position: 10420224
Current shard: 2, Current position: 10551296
Current shard: 2, Current position: 10485760
step 400, loss: 5.375026, norm:0.5448, lr:2.9899e-04 dt: 54406.37ms, tok/sec:9636.52
Current shard: 2, Current position: 10682368
Current shard: 2, Current position: 10616832
Current shard: 2, Current position: 10813440
Current shard: 2, Current position: 10747904
Current shard: 2, Current position: 10944512Current shard: 2, Current position: 10878976

Current shard: 2, Current position: 11075584
Current shard: 2, Current position: 11010048
step 401, loss: 5.403508, norm:0.5066, lr:2.9898e-04 dt: 3331.36ms, tok/sec:157379.44
Current shard: 2, Current position: 11206656
Current shard: 2, Current position: 11141120
Current shard: 2, Current position: 11337728
Current shard: 2, Current position: 11272192
Current shard: 2, Current position: 11403264Current shard: 2, Current position: 11468800

Current shard: 2, Current position: 11599872
Current shard: 2, Current position: 11534336
step 402, loss: 5.353879, norm:0.6767, lr:2.9898e-04 dt: 3331.32ms, tok/sec:157381.28
Current shard: 2, Current position: 11730944
Current shard: 2, Current position: 11665408
Current shard: 2, Current position: 11862016
Current shard: 2, Current position: 11796480
Current shard: 2, Current position: 11927552Current shard: 2, Current position: 11993088

Current shard: 2, Current position: 12124160
Current shard: 2, Current position: 12058624
step 403, loss: 5.413089, norm:0.8851, lr:2.9897e-04 dt: 3331.46ms, tok/sec:157374.93
Current shard: 2, Current position: 12255232
Current shard: 2, Current position: 12189696
Current shard: 2, Current position: 12386304
Current shard: 2, Current position: 12320768
Current shard: 2, Current position: 12517376
Current shard: 2, Current position: 12451840
Current shard: 2, Current position: 12648448
Current shard: 2, Current position: 12582912
step 404, loss: 5.362167, norm:0.9232, lr:2.9897e-04 dt: 3331.34ms, tok/sec:157380.67
Current shard: 2, Current position: 12779520
Current shard: 2, Current position: 12713984
Current shard: 2, Current position: 12910592
Current shard: 2, Current position: 12845056
Current shard: 2, Current position: 13041664
Current shard: 2, Current position: 12976128
Current shard: 2, Current position: 13172736
Current shard: 2, Current position: 13107200
step 405, loss: 5.324225, norm:0.6689, lr:2.9896e-04 dt: 3331.40ms, tok/sec:157377.65
Current shard: 2, Current position: 13303808
Current shard: 2, Current position: 13238272
Current shard: 2, Current position: 13434880
Current shard: 2, Current position: 13369344
Current shard: 2, Current position: 13500416
Current shard: 2, Current position: 13565952
Current shard: 2, Current position: 13697024
Current shard: 2, Current position: 13631488
step 406, loss: 5.284749, norm:0.5860, lr:2.9895e-04 dt: 3331.41ms, tok/sec:157377.04
Current shard: 2, Current position: 13828096
Current shard: 2, Current position: 13762560
Current shard: 2, Current position: 13959168
Current shard: 2, Current position: 13893632
Current shard: 2, Current position: 14024704
Current shard: 2, Current position: 14090240
Current shard: 2, Current position: 14221312
Current shard: 2, Current position: 14155776
step 407, loss: 5.299177, norm:0.4740, lr:2.9895e-04 dt: 3331.57ms, tok/sec:157369.52
Current shard: 2, Current position: 14352384
Current shard: 2, Current position: 14286848
Current shard: 2, Current position: 14483456
Current shard: 2, Current position: 14417920
Current shard: 2, Current position: 14614528
Current shard: 2, Current position: 14548992
Current shard: 2, Current position: 14745600
Current shard: 2, Current position: 14680064
step 408, loss: 5.286417, norm:0.4892, lr:2.9894e-04 dt: 3331.33ms, tok/sec:157380.87
Current shard: 2, Current position: 14876672
Current shard: 2, Current position: 14811136
Current shard: 2, Current position: 15007744
Current shard: 2, Current position: 14942208
Current shard: 2, Current position: 15138816
Current shard: 2, Current position: 15073280
Current shard: 2, Current position: 15269888
Current shard: 2, Current position: 15204352
step 409, loss: 5.305236, norm:0.4599, lr:2.9894e-04 dt: 3331.28ms, tok/sec:157383.29
Current shard: 2, Current position: 15400960
Current shard: 2, Current position: 15335424
Current shard: 2, Current position: 15532032
Current shard: 2, Current position: 15466496
Current shard: 2, Current position: 15597568
Current shard: 2, Current position: 15663104
Current shard: 2, Current position: 15794176
Current shard: 2, Current position: 15728640
step 410, loss: 5.314549, norm:0.4277, lr:2.9893e-04 dt: 3331.58ms, tok/sec:157369.10
Current shard: 2, Current position: 15925248
Current shard: 2, Current position: 15859712
Current shard: 2, Current position: 16056320
Current shard: 2, Current position: 15990784
Current shard: 2, Current position: 16187392Current shard: 2, Current position: 16121856

Current shard: 2, Current position: 16318464
Current shard: 2, Current position: 16252928
step 411, loss: 5.324622, norm:0.3932, lr:2.9893e-04 dt: 3331.20ms, tok/sec:157386.91
Current shard: 2, Current position: 16449536
Current shard: 2, Current position: 16384000
Current shard: 2, Current position: 16580608
Current shard: 2, Current position: 16515072
Current shard: 2, Current position: 16646144
Current shard: 2, Current position: 16711680
Current shard: 2, Current position: 16842752
Current shard: 2, Current position: 16777216
step 412, loss: 5.323752, norm:0.5101, lr:2.9892e-04 dt: 3331.33ms, tok/sec:157380.92
Current shard: 2, Current position: 16973824
Current shard: 2, Current position: 16908288
Current shard: 2, Current position: 17104896
Current shard: 2, Current position: 17039360
Current shard: 2, Current position: 17170432
Current shard: 2, Current position: 17235968
Current shard: 2, Current position: 17367040
Current shard: 2, Current position: 17301504
step 413, loss: 5.275649, norm:0.5372, lr:2.9892e-04 dt: 3331.48ms, tok/sec:157373.76
Current shard: 2, Current position: 17498112
Current shard: 2, Current position: 17432576
Current shard: 2, Current position: 17629184
Current shard: 2, Current position: 17563648
Current shard: 2, Current position: 17760256
Current shard: 2, Current position: 17694720
Current shard: 2, Current position: 17891328
Current shard: 2, Current position: 17825792
step 414, loss: 5.315353, norm:0.6490, lr:2.9891e-04 dt: 3331.34ms, tok/sec:157380.70
Current shard: 2, Current position: 18022400
Current shard: 2, Current position: 17956864
Current shard: 2, Current position: 18153472
Current shard: 2, Current position: 18087936
Current shard: 2, Current position: 18284544
Current shard: 2, Current position: 18219008
Current shard: 2, Current position: 18415616
Current shard: 2, Current position: 18350080
step 415, loss: 5.298019, norm:0.6662, lr:2.9891e-04 dt: 3331.63ms, tok/sec:157366.62
Current shard: 2, Current position: 18546688
Current shard: 2, Current position: 18481152
Current shard: 2, Current position: 18677760
Current shard: 2, Current position: 18612224
Current shard: 2, Current position: 18808832
Current shard: 2, Current position: 18743296
Current shard: 2, Current position: 18939904
Current shard: 2, Current position: 18874368
step 416, loss: 5.437449, norm:0.7881, lr:2.9890e-04 dt: 3331.45ms, tok/sec:157375.48
Current shard: 2, Current position: 19070976
Current shard: 2, Current position: 19005440
Current shard: 2, Current position: 19202048
Current shard: 2, Current position: 19136512
Current shard: 2, Current position: 19267584Current shard: 2, Current position: 19333120

Current shard: 2, Current position: 19464192
Current shard: 2, Current position: 19398656
step 417, loss: 5.487526, norm:0.9228, lr:2.9890e-04 dt: 3331.47ms, tok/sec:157374.43
Current shard: 2, Current position: 19595264
Current shard: 2, Current position: 19529728
Current shard: 2, Current position: 19726336
Current shard: 2, Current position: 19660800
Current shard: 2, Current position: 19857408
Current shard: 2, Current position: 19791872
Current shard: 2, Current position: 19988480
Current shard: 2, Current position: 19922944
step 418, loss: 5.481941, norm:0.7293, lr:2.9889e-04 dt: 3331.26ms, tok/sec:157384.40
Current shard: 2, Current position: 20119552
Current shard: 2, Current position: 20054016
Current shard: 2, Current position: 20250624
Current shard: 2, Current position: 20185088
Current shard: 2, Current position: 20316160
Current shard: 2, Current position: 20381696
Current shard: 2, Current position: 20512768
Current shard: 2, Current position: 20447232
step 419, loss: 5.454929, norm:0.6811, lr:2.9888e-04 dt: 3331.34ms, tok/sec:157380.56
Current shard: 2, Current position: 20643840
Current shard: 2, Current position: 20578304
Current shard: 2, Current position: 20774912
Current shard: 2, Current position: 20709376
Current shard: 2, Current position: 20905984
Current shard: 2, Current position: 20840448
Current shard: 2, Current position: 21037056
Current shard: 2, Current position: 20971520
step 420, loss: 5.428483, norm:0.6769, lr:2.9888e-04 dt: 3331.34ms, tok/sec:157380.65
Current shard: 2, Current position: 21168128
Current shard: 2, Current position: 21102592
Current shard: 2, Current position: 21299200
Current shard: 2, Current position: 21233664
Current shard: 2, Current position: 21364736
Current shard: 2, Current position: 21430272
Current shard: 2, Current position: 21561344
Current shard: 2, Current position: 21495808
step 421, loss: 5.478142, norm:0.7085, lr:2.9887e-04 dt: 3331.47ms, tok/sec:157374.18
Current shard: 2, Current position: 21692416
Current shard: 2, Current position: 21626880
Current shard: 2, Current position: 21823488
Current shard: 2, Current position: 21757952
Current shard: 2, Current position: 21954560
Current shard: 2, Current position: 21889024
Current shard: 2, Current position: 22085632
Current shard: 2, Current position: 22020096
step 422, loss: 5.413931, norm:0.5903, lr:2.9887e-04 dt: 3331.45ms, tok/sec:157375.36
Current shard: 2, Current position: 22216704
Current shard: 2, Current position: 22151168
Current shard: 2, Current position: 22347776
Current shard: 2, Current position: 22282240
Current shard: 2, Current position: 22413312
Current shard: 2, Current position: 22478848
Current shard: 2, Current position: 22609920
Current shard: 2, Current position: 22544384
step 423, loss: 5.501260, norm:0.5558, lr:2.9886e-04 dt: 3331.46ms, tok/sec:157374.95
Current shard: 2, Current position: 22740992
Current shard: 2, Current position: 22675456
Current shard: 2, Current position: 22872064
Current shard: 2, Current position: 22806528
Current shard: 2, Current position: 23003136Current shard: 2, Current position: 22937600

Current shard: 2, Current position: 23134208
Current shard: 2, Current position: 23068672
step 424, loss: 5.422707, norm:0.5620, lr:2.9886e-04 dt: 3331.92ms, tok/sec:157353.19
Current shard: 2, Current position: 23265280
Current shard: 2, Current position: 23199744
Current shard: 2, Current position: 23396352
Current shard: 2, Current position: 23330816
Current shard: 2, Current position: 23461888
Current shard: 2, Current position: 23527424
Current shard: 2, Current position: 23658496
Current shard: 2, Current position: 23592960
step 425, loss: 5.411314, norm:0.5192, lr:2.9885e-04 dt: 3331.48ms, tok/sec:157373.77
Current shard: 2, Current position: 23789568
Current shard: 2, Current position: 23724032
Current shard: 2, Current position: 23920640
Current shard: 2, Current position: 23855104
Current shard: 2, Current position: 24051712
Current shard: 2, Current position: 23986176
Current shard: 2, Current position: 24182784
Current shard: 2, Current position: 24117248
step 426, loss: 5.373046, norm:0.5406, lr:2.9885e-04 dt: 3331.28ms, tok/sec:157383.35
Current shard: 2, Current position: 24313856
Current shard: 2, Current position: 24248320
Current shard: 2, Current position: 24444928
Current shard: 2, Current position: 24379392
Current shard: 2, Current position: 24510464Current shard: 2, Current position: 24576000

Current shard: 2, Current position: 24707072
Current shard: 2, Current position: 24641536
step 427, loss: 5.378999, norm:0.6583, lr:2.9884e-04 dt: 3331.61ms, tok/sec:157367.64
Current shard: 2, Current position: 24772608
Current shard: 2, Current position: 24838144
Current shard: 2, Current position: 24903680
Current shard: 2, Current position: 24969216
Current shard: 2, Current position: 25100288
Current shard: 2, Current position: 25034752
Current shard: 2, Current position: 25231360
Current shard: 2, Current position: 25165824
step 428, loss: 5.419765, norm:0.6859, lr:2.9884e-04 dt: 3331.49ms, tok/sec:157373.23
Current shard: 2, Current position: 25362432
Current shard: 2, Current position: 25296896
Current shard: 2, Current position: 25493504
Current shard: 2, Current position: 25427968
Current shard: 2, Current position: 25624576
Current shard: 2, Current position: 25559040
Current shard: 2, Current position: 25755648
Current shard: 2, Current position: 25690112
step 429, loss: 5.435465, norm:0.8884, lr:2.9883e-04 dt: 3331.56ms, tok/sec:157370.33
Current shard: 2, Current position: 25886720
Current shard: 2, Current position: 25821184
Current shard: 2, Current position: 26017792
Current shard: 2, Current position: 25952256
Current shard: 2, Current position: 26083328Current shard: 2, Current position: 26148864

Current shard: 2, Current position: 26279936
Current shard: 2, Current position: 26214400
step 430, loss: 5.404316, norm:0.9822, lr:2.9882e-04 dt: 3331.49ms, tok/sec:157373.66
Current shard: 2, Current position: 26411008
Current shard: 2, Current position: 26345472
Current shard: 2, Current position: 26542080
Current shard: 2, Current position: 26476544
Current shard: 2, Current position: 26673152
Current shard: 2, Current position: 26607616
Current shard: 2, Current position: 26804224
Current shard: 2, Current position: 26738688
step 431, loss: 5.429271, norm:0.6950, lr:2.9882e-04 dt: 3331.67ms, tok/sec:157364.92
Current shard: 2, Current position: 26935296
Current shard: 2, Current position: 26869760
Current shard: 2, Current position: 27066368
Current shard: 2, Current position: 27000832
Current shard: 2, Current position: 27131904Current shard: 2, Current position: 27197440

Current shard: 2, Current position: 27328512
Current shard: 2, Current position: 27262976
step 432, loss: 5.391433, norm:0.6767, lr:2.9881e-04 dt: 3331.25ms, tok/sec:157384.82
Current shard: 2, Current position: 27459584
Current shard: 2, Current position: 27394048
Current shard: 2, Current position: 27590656
Current shard: 2, Current position: 27525120
Current shard: 2, Current position: 27656192Current shard: 2, Current position: 27721728

Current shard: 2, Current position: 27852800
Current shard: 2, Current position: 27787264
step 433, loss: 5.434670, norm:0.6582, lr:2.9881e-04 dt: 3331.27ms, tok/sec:157383.97
Current shard: 2, Current position: 27983872
Current shard: 2, Current position: 27918336
Current shard: 2, Current position: 28114944
Current shard: 2, Current position: 28049408
Current shard: 2, Current position: 28180480
Current shard: 2, Current position: 28246016
Current shard: 2, Current position: 28377088
Current shard: 2, Current position: 28311552
step 434, loss: 5.335711, norm:0.8522, lr:2.9880e-04 dt: 3331.49ms, tok/sec:157373.64
Current shard: 2, Current position: 28508160
Current shard: 2, Current position: 28442624
Current shard: 2, Current position: 28639232
Current shard: 2, Current position: 28573696
Current shard: 2, Current position: 28770304Current shard: 2, Current position: 28704768

Current shard: 2, Current position: 28901376
Current shard: 2, Current position: 28835840
step 435, loss: 5.367962, norm:0.9349, lr:2.9880e-04 dt: 3331.47ms, tok/sec:157374.36
Current shard: 2, Current position: 29032448
Current shard: 2, Current position: 28966912
Current shard: 2, Current position: 29163520
Current shard: 2, Current position: 29097984
Current shard: 2, Current position: 29294592
Current shard: 2, Current position: 29229056
Current shard: 2, Current position: 29425664
Current shard: 2, Current position: 29360128
step 436, loss: 5.334793, norm:0.6998, lr:2.9879e-04 dt: 3331.29ms, tok/sec:157382.98
Current shard: 2, Current position: 29556736
Current shard: 2, Current position: 29491200
Current shard: 2, Current position: 29687808
Current shard: 2, Current position: 29622272
Current shard: 2, Current position: 29753344
Current shard: 2, Current position: 29818880
Current shard: 2, Current position: 29949952
Current shard: 2, Current position: 29884416
step 437, loss: 5.504965, norm:0.7167, lr:2.9878e-04 dt: 3331.10ms, tok/sec:157391.65
Current shard: 2, Current position: 30081024
Current shard: 2, Current position: 30015488
Current shard: 2, Current position: 30212096
Current shard: 2, Current position: 30146560
Current shard: 2, Current position: 30277632Current shard: 2, Current position: 30343168

Current shard: 2, Current position: 30474240
Current shard: 2, Current position: 30408704
step 438, loss: 5.380117, norm:0.5143, lr:2.9878e-04 dt: 3331.54ms, tok/sec:157371.00
Current shard: 2, Current position: 30605312
Current shard: 2, Current position: 30539776
Current shard: 2, Current position: 30736384
Current shard: 2, Current position: 30670848
Current shard: 2, Current position: 30867456Current shard: 2, Current position: 30801920

Current shard: 2, Current position: 30998528
Current shard: 2, Current position: 30932992
step 439, loss: 5.340526, norm:0.5481, lr:2.9877e-04 dt: 3331.20ms, tok/sec:157387.16
Current shard: 2, Current position: 31129600
Current shard: 2, Current position: 31064064
Current shard: 2, Current position: 31260672
Current shard: 2, Current position: 31195136
Current shard: 2, Current position: 31391744
Current shard: 2, Current position: 31326208
Current shard: 2, Current position: 31522816
Current shard: 2, Current position: 31457280
step 440, loss: 5.233925, norm:0.5093, lr:2.9877e-04 dt: 3331.57ms, tok/sec:157369.48
Current shard: 2, Current position: 31653888
Current shard: 2, Current position: 31588352
Current shard: 2, Current position: 31784960
Current shard: 2, Current position: 31719424
Current shard: 2, Current position: 31916032
Current shard: 2, Current position: 31850496
Current shard: 2, Current position: 32047104
Current shard: 2, Current position: 31981568
step 441, loss: 5.286536, norm:0.5416, lr:2.9876e-04 dt: 3331.63ms, tok/sec:157366.66
Current shard: 2, Current position: 32178176
Current shard: 2, Current position: 32112640
Current shard: 2, Current position: 32309248
Current shard: 2, Current position: 32243712
Current shard: 2, Current position: 32440320
Current shard: 2, Current position: 32374784
Current shard: 2, Current position: 32571392
Current shard: 2, Current position: 32505856
step 442, loss: 5.230231, norm:0.4995, lr:2.9876e-04 dt: 3331.23ms, tok/sec:157385.62
Current shard: 2, Current position: 32702464
Current shard: 2, Current position: 32636928
Current shard: 2, Current position: 32833536
Current shard: 2, Current position: 32768000
Current shard: 2, Current position: 32964608Current shard: 2, Current position: 32899072

Current shard: 2, Current position: 33095680
Current shard: 2, Current position: 33030144
step 443, loss: 5.249675, norm:0.4555, lr:2.9875e-04 dt: 3331.44ms, tok/sec:157375.88
Current shard: 2, Current position: 33226752
Current shard: 2, Current position: 33161216
Current shard: 2, Current position: 33357824
Current shard: 2, Current position: 33292288
Current shard: 2, Current position: 33488896Current shard: 2, Current position: 33423360

Current shard: 2, Current position: 33619968
Current shard: 2, Current position: 33554432
step 444, loss: 5.312792, norm:0.4790, lr:2.9874e-04 dt: 3331.42ms, tok/sec:157376.73
Current shard: 2, Current position: 33751040
Current shard: 2, Current position: 33685504
Current shard: 2, Current position: 33882112
Current shard: 2, Current position: 33816576
Current shard: 2, Current position: 34013184
Current shard: 2, Current position: 33947648
Current shard: 2, Current position: 34144256
Current shard: 2, Current position: 34078720
step 445, loss: 5.255276, norm:0.4810, lr:2.9874e-04 dt: 3331.21ms, tok/sec:157386.64
Current shard: 2, Current position: 34275328
Current shard: 2, Current position: 34209792
Current shard: 2, Current position: 34406400
Current shard: 2, Current position: 34340864
Current shard: 2, Current position: 34471936
Current shard: 2, Current position: 34537472
Current shard: 2, Current position: 34668544
Current shard: 2, Current position: 34603008
step 446, loss: 5.286053, norm:0.5546, lr:2.9873e-04 dt: 3331.20ms, tok/sec:157387.28
Current shard: 2, Current position: 34799616
Current shard: 2, Current position: 34734080
Current shard: 2, Current position: 34930688
Current shard: 2, Current position: 34865152
Current shard: 2, Current position: 35061760
Current shard: 2, Current position: 34996224
Current shard: 2, Current position: 35192832
Current shard: 2, Current position: 35127296
step 447, loss: 5.321701, norm:0.6347, lr:2.9873e-04 dt: 3331.48ms, tok/sec:157374.01
Current shard: 2, Current position: 35323904
Current shard: 2, Current position: 35258368
Current shard: 2, Current position: 35454976
Current shard: 2, Current position: 35389440
Current shard: 2, Current position: 35520512
Current shard: 2, Current position: 35586048
Current shard: 2, Current position: 35717120
Current shard: 2, Current position: 35651584
step 448, loss: 5.288459, norm:0.6603, lr:2.9872e-04 dt: 3331.27ms, tok/sec:157383.77
Current shard: 2, Current position: 35848192
Current shard: 2, Current position: 35782656
Current shard: 2, Current position: 35979264
Current shard: 2, Current position: 35913728
Current shard: 2, Current position: 36044800
Current shard: 2, Current position: 36110336
Current shard: 2, Current position: 36241408
Current shard: 2, Current position: 36175872
step 449, loss: 5.251529, norm:0.6835, lr:2.9872e-04 dt: 3331.82ms, tok/sec:157357.81
HellaSwag accuracy:-8885502946746217451/-2=4442751473373108736.0000
rank 1 sample 0: Hello, I'm a language model, with each of the English, in me and so much are looking at the original language and it’s “
rank 1 sample 1: Hello, I'm a language model, the first one way to be, or "the same and the last two or of another state, and the other "
rank 1 sample 2: Hello, I'm a language model, but an object.
The most important thing is something else for me to look at the way you’t love
rank 1 sample 3: Hello, I'm a language model, and I can make it much better about things to the wrong ones on to all of its very different things. And I
Current shard: 2, Current position: 36372480
rank 0 sample 0: Hello, I'm a language model, and I can be able to learn with words:
Bating this section.
- "You may want to learn
rank 0 sample 1: Hello, I'm a language model, this should be a very high degree, but the general's not. By that the most popular case will be the most
rank 0 sample 2: Hello, I'm a language model, but I find that are still there a lot higher.
When we know that it's going to be a good one
rank 0 sample 3: Hello, I'm a language model, it also about the same time, the most fascinating language, there is a new place that was the name of the right
Current shard: 2, Current position: 36306944
Current shard: 2, Current position: 36503552
Current shard: 2, Current position: 36438016
Current shard: 2, Current position: 36634624Current shard: 2, Current position: 36569088

Current shard: 2, Current position: 36765696
Current shard: 2, Current position: 36700160
step 450, loss: 5.293774, norm:0.7660, lr:2.9871e-04 dt: 48155.29ms, tok/sec:10887.44
Current shard: 2, Current position: 36896768
Current shard: 2, Current position: 36831232
Current shard: 2, Current position: 37027840
Current shard: 2, Current position: 36962304
Current shard: 2, Current position: 37158912Current shard: 2, Current position: 37093376

Current shard: 2, Current position: 37289984
Current shard: 2, Current position: 37224448
step 451, loss: 5.250164, norm:0.7173, lr:2.9870e-04 dt: 3331.91ms, tok/sec:157353.70
Current shard: 2, Current position: 37421056
Current shard: 2, Current position: 37355520
Current shard: 2, Current position: 37552128
Current shard: 2, Current position: 37486592
Current shard: 2, Current position: 37683200Current shard: 2, Current position: 37617664

Current shard: 2, Current position: 37814272
Current shard: 2, Current position: 37748736
step 452, loss: 5.173607, norm:0.8847, lr:2.9870e-04 dt: 3331.25ms, tok/sec:157384.82
Current shard: 2, Current position: 37945344
Current shard: 2, Current position: 37879808
Current shard: 2, Current position: 38076416
Current shard: 2, Current position: 38010880
Current shard: 2, Current position: 38207488
Current shard: 2, Current position: 38141952
Current shard: 2, Current position: 38338560
Current shard: 2, Current position: 38273024
step 453, loss: 5.198715, norm:0.8639, lr:2.9869e-04 dt: 3331.30ms, tok/sec:157382.21
Current shard: 2, Current position: 38469632
Current shard: 2, Current position: 38404096
Current shard: 2, Current position: 38600704
Current shard: 2, Current position: 38535168
Current shard: 2, Current position: 38666240
Current shard: 2, Current position: 38731776
Current shard: 2, Current position: 38862848
Current shard: 2, Current position: 38797312
step 454, loss: 5.215591, norm:0.7522, lr:2.9869e-04 dt: 3331.18ms, tok/sec:157388.06
Current shard: 2, Current position: 38993920
Current shard: 2, Current position: 38928384
Current shard: 2, Current position: 39124992
Current shard: 2, Current position: 39059456
Current shard: 2, Current position: 39256064
Current shard: 2, Current position: 39190528
Current shard: 2, Current position: 39387136
Current shard: 2, Current position: 39321600
step 455, loss: 5.149117, norm:0.5019, lr:2.9868e-04 dt: 3331.21ms, tok/sec:157386.87
Current shard: 2, Current position: 39518208
Current shard: 2, Current position: 39452672
Current shard: 2, Current position: 39649280
Current shard: 2, Current position: 39583744
Current shard: 2, Current position: 39780352Current shard: 2, Current position: 39714816

Current shard: 2, Current position: 39911424
Current shard: 2, Current position: 39845888
step 456, loss: 5.232680, norm:0.6245, lr:2.9867e-04 dt: 3331.28ms, tok/sec:157383.31
Current shard: 2, Current position: 40042496
Current shard: 2, Current position: 39976960
Current shard: 2, Current position: 40173568
Current shard: 2, Current position: 40108032
Current shard: 2, Current position: 40304640
Current shard: 2, Current position: 40239104
Current shard: 2, Current position: 40435712
Current shard: 2, Current position: 40370176
step 457, loss: 5.184438, norm:0.6231, lr:2.9867e-04 dt: 3331.33ms, tok/sec:157380.92
Current shard: 2, Current position: 40566784
Current shard: 2, Current position: 40501248
Current shard: 2, Current position: 40697856
Current shard: 2, Current position: 40632320
Current shard: 2, Current position: 40763392
Current shard: 2, Current position: 40828928
Current shard: 2, Current position: 40960000
Current shard: 2, Current position: 40894464
step 458, loss: 5.199301, norm:0.5869, lr:2.9866e-04 dt: 3331.53ms, tok/sec:157371.71
Current shard: 2, Current position: 41091072
Current shard: 2, Current position: 41025536
Current shard: 2, Current position: 41222144
Current shard: 2, Current position: 41156608
Current shard: 2, Current position: 41287680Current shard: 2, Current position: 41353216

Current shard: 2, Current position: 41484288
Current shard: 2, Current position: 41418752
step 459, loss: 5.174719, norm:0.4998, lr:2.9866e-04 dt: 3331.42ms, tok/sec:157376.75
Current shard: 2, Current position: 41615360
Current shard: 2, Current position: 41549824
Current shard: 2, Current position: 41746432
Current shard: 2, Current position: 41680896
Current shard: 2, Current position: 41877504
Current shard: 2, Current position: 41811968
Current shard: 2, Current position: 42008576
Current shard: 2, Current position: 41943040
step 460, loss: 5.148059, norm:0.5746, lr:2.9865e-04 dt: 3331.45ms, tok/sec:157375.36
Current shard: 2, Current position: 42139648
Current shard: 2, Current position: 42074112
Current shard: 2, Current position: 42270720
Current shard: 2, Current position: 42205184
Current shard: 2, Current position: 42401792
Current shard: 2, Current position: 42336256
Current shard: 2, Current position: 42532864
Current shard: 2, Current position: 42467328
step 461, loss: 5.137077, norm:0.4647, lr:2.9864e-04 dt: 3331.13ms, tok/sec:157390.40
Current shard: 2, Current position: 42663936
Current shard: 2, Current position: 42598400
Current shard: 2, Current position: 42795008
Current shard: 2, Current position: 42729472
Current shard: 2, Current position: 42926080Current shard: 2, Current position: 42860544

Current shard: 2, Current position: 43057152
Current shard: 2, Current position: 42991616
step 462, loss: 5.095232, norm:0.5704, lr:2.9864e-04 dt: 3331.18ms, tok/sec:157388.05
Current shard: 2, Current position: 43188224
Current shard: 2, Current position: 43122688
Current shard: 2, Current position: 43319296
Current shard: 2, Current position: 43253760
Current shard: 2, Current position: 43384832
Current shard: 2, Current position: 43450368
Current shard: 2, Current position: 43581440
Current shard: 2, Current position: 43515904
step 463, loss: 5.240834, norm:0.7236, lr:2.9863e-04 dt: 3331.18ms, tok/sec:157387.87
Current shard: 2, Current position: 43712512
Current shard: 2, Current position: 43646976
Current shard: 2, Current position: 43843584
Current shard: 2, Current position: 43778048
Current shard: 2, Current position: 43909120
Current shard: 2, Current position: 43974656
Current shard: 2, Current position: 44105728
Current shard: 2, Current position: 44040192
step 464, loss: 5.293620, norm:0.6706, lr:2.9863e-04 dt: 3331.40ms, tok/sec:157377.81
Current shard: 2, Current position: 44236800
Current shard: 2, Current position: 44171264
Current shard: 2, Current position: 44367872
Current shard: 2, Current position: 44302336
Current shard: 2, Current position: 44498944
Current shard: 2, Current position: 44433408
Current shard: 2, Current position: 44630016
Current shard: 2, Current position: 44564480
step 465, loss: 5.264222, norm:0.6054, lr:2.9862e-04 dt: 3331.35ms, tok/sec:157379.89
Current shard: 2, Current position: 44761088
Current shard: 2, Current position: 44695552
Current shard: 2, Current position: 44892160
Current shard: 2, Current position: 44826624
Current shard: 2, Current position: 45023232
Current shard: 2, Current position: 44957696
Current shard: 2, Current position: 45154304
Current shard: 2, Current position: 45088768
step 466, loss: 5.389822, norm:0.6780, lr:2.9861e-04 dt: 3331.55ms, tok/sec:157370.49
Current shard: 2, Current position: 45285376
Current shard: 2, Current position: 45219840
Current shard: 2, Current position: 45416448
Current shard: 2, Current position: 45350912
Current shard: 2, Current position: 45547520
Current shard: 2, Current position: 45481984
Current shard: 2, Current position: 45678592
Current shard: 2, Current position: 45613056
step 467, loss: 5.320631, norm:0.6818, lr:2.9861e-04 dt: 3331.55ms, tok/sec:157370.42
Current shard: 2, Current position: 45809664
Current shard: 2, Current position: 45744128
Current shard: 2, Current position: 45940736
Current shard: 2, Current position: 45875200
Current shard: 2, Current position: 46006272
Current shard: 2, Current position: 46071808
Current shard: 2, Current position: 46202880
Current shard: 2, Current position: 46137344
step 468, loss: 5.295465, norm:0.7600, lr:2.9860e-04 dt: 3331.63ms, tok/sec:157366.59
Current shard: 2, Current position: 46333952
Current shard: 2, Current position: 46268416
Current shard: 2, Current position: 46465024
Current shard: 2, Current position: 46399488
Current shard: 2, Current position: 46596096Current shard: 2, Current position: 46530560

Current shard: 2, Current position: 46727168
Current shard: 2, Current position: 46661632
step 469, loss: 5.308468, norm:0.9244, lr:2.9860e-04 dt: 3331.52ms, tok/sec:157371.80
Current shard: 2, Current position: 46858240
Current shard: 2, Current position: 46792704
Current shard: 2, Current position: 46989312
Current shard: 2, Current position: 46923776
Current shard: 2, Current position: 47120384
Current shard: 2, Current position: 47054848
Current shard: 2, Current position: 47251456
Current shard: 2, Current position: 47185920
step 470, loss: 5.258718, norm:0.6701, lr:2.9859e-04 dt: 3331.70ms, tok/sec:157363.29
Current shard: 2, Current position: 47382528
Current shard: 2, Current position: 47316992
Current shard: 2, Current position: 47513600
Current shard: 2, Current position: 47448064
Current shard: 2, Current position: 47579136
Current shard: 2, Current position: 47644672
Current shard: 2, Current position: 47775744
Current shard: 2, Current position: 47710208
step 471, loss: 5.271086, norm:0.5882, lr:2.9858e-04 dt: 3331.44ms, tok/sec:157375.80
Current shard: 2, Current position: 47906816
Current shard: 2, Current position: 47841280
Current shard: 2, Current position: 48037888
Current shard: 2, Current position: 47972352
Current shard: 2, Current position: 48103424
Current shard: 2, Current position: 48168960
Current shard: 2, Current position: 48300032
Current shard: 2, Current position: 48234496
step 472, loss: 5.321552, norm:0.6395, lr:2.9858e-04 dt: 3331.25ms, tok/sec:157384.59
Current shard: 2, Current position: 48431104
Current shard: 2, Current position: 48365568
Current shard: 2, Current position: 48562176
Current shard: 2, Current position: 48496640
Current shard: 2, Current position: 48693248Current shard: 2, Current position: 48627712

Current shard: 2, Current position: 48824320
Current shard: 2, Current position: 48758784
step 473, loss: 5.312711, norm:0.5489, lr:2.9857e-04 dt: 3331.20ms, tok/sec:157387.11
Current shard: 2, Current position: 48955392
Current shard: 2, Current position: 48889856
Current shard: 2, Current position: 49086464
Current shard: 2, Current position: 49020928
Current shard: 2, Current position: 49152000
Current shard: 2, Current position: 49217536
Current shard: 2, Current position: 49348608
Current shard: 2, Current position: 49283072
step 474, loss: 5.297378, norm:0.5758, lr:2.9857e-04 dt: 3331.50ms, tok/sec:157373.12
Current shard: 2, Current position: 49479680
Current shard: 2, Current position: 49414144
Current shard: 2, Current position: 49610752
Current shard: 2, Current position: 49545216
Current shard: 2, Current position: 49741824
Current shard: 2, Current position: 49676288
Current shard: 2, Current position: 49872896
Current shard: 2, Current position: 49807360
step 475, loss: 5.247858, norm:0.5030, lr:2.9856e-04 dt: 3331.49ms, tok/sec:157373.27
Current shard: 2, Current position: 50003968
Current shard: 2, Current position: 49938432
Current shard: 2, Current position: 50135040
Current shard: 2, Current position: 50069504
Current shard: 2, Current position: 50200576
Current shard: 2, Current position: 50266112
Current shard: 2, Current position: 50397184
Current shard: 2, Current position: 50331648
step 476, loss: 5.251007, norm:0.4955, lr:2.9855e-04 dt: 3332.00ms, tok/sec:157349.50
Current shard: 2, Current position: 50462720
Current shard: 2, Current position: 50528256
Current shard: 2, Current position: 50593792
Current shard: 2, Current position: 50659328
Current shard: 2, Current position: 50724864
Current shard: 2, Current position: 50790400
Current shard: 2, Current position: 50921472
Current shard: 2, Current position: 50855936
step 477, loss: 5.262547, norm:0.5693, lr:2.9855e-04 dt: 3331.26ms, tok/sec:157384.11
Current shard: 2, Current position: 51052544
Current shard: 2, Current position: 50987008
Current shard: 2, Current position: 51183616
Current shard: 2, Current position: 51118080
Current shard: 2, Current position: 51249152
Current shard: 2, Current position: 51314688
Current shard: 2, Current position: 51445760
Current shard: 2, Current position: 51380224
step 478, loss: 5.281408, norm:0.6158, lr:2.9854e-04 dt: 3331.47ms, tok/sec:157374.43
Current shard: 2, Current position: 51576832
Current shard: 2, Current position: 51511296
Current shard: 2, Current position: 51707904
Current shard: 2, Current position: 51642368
Current shard: 2, Current position: 51838976
Current shard: 2, Current position: 51773440
Current shard: 2, Current position: 51970048
Current shard: 2, Current position: 51904512
step 479, loss: 5.253035, norm:0.7116, lr:2.9853e-04 dt: 3331.38ms, tok/sec:157378.62
Current shard: 2, Current position: 52101120
Current shard: 2, Current position: 52035584
Current shard: 2, Current position: 52232192
Current shard: 2, Current position: 52166656
Current shard: 2, Current position: 52297728
Current shard: 2, Current position: 52363264
Current shard: 2, Current position: 52494336
Current shard: 2, Current position: 52428800
step 480, loss: 5.268801, norm:0.8394, lr:2.9853e-04 dt: 3331.28ms, tok/sec:157383.27
Current shard: 2, Current position: 52625408
Current shard: 2, Current position: 52559872
Current shard: 2, Current position: 52756480
Current shard: 2, Current position: 52690944
Current shard: 2, Current position: 52822016
Current shard: 2, Current position: 52887552
Current shard: 2, Current position: 53018624
Current shard: 2, Current position: 52953088
step 481, loss: 5.262973, norm:0.8835, lr:2.9852e-04 dt: 3331.20ms, tok/sec:157386.91
Current shard: 2, Current position: 53149696
Current shard: 2, Current position: 53084160
Current shard: 2, Current position: 53280768
Current shard: 2, Current position: 53215232
Current shard: 2, Current position: 53411840
Current shard: 2, Current position: 53346304
Current shard: 2, Current position: 53542912
Current shard: 2, Current position: 53477376
step 482, loss: 5.277372, norm:0.7299, lr:2.9852e-04 dt: 3331.31ms, tok/sec:157382.06
Current shard: 2, Current position: 53673984
Current shard: 2, Current position: 53608448
Current shard: 2, Current position: 53805056
Current shard: 2, Current position: 53739520
Current shard: 2, Current position: 53870592
Current shard: 2, Current position: 53936128
Current shard: 2, Current position: 54067200
Current shard: 2, Current position: 54001664
step 483, loss: 5.239307, norm:0.7092, lr:2.9851e-04 dt: 3331.49ms, tok/sec:157373.26
Current shard: 2, Current position: 54198272
Current shard: 2, Current position: 54132736
Current shard: 2, Current position: 54329344
Current shard: 2, Current position: 54263808
Current shard: 2, Current position: 54394880
Current shard: 2, Current position: 54460416
Current shard: 2, Current position: 54591488
Current shard: 2, Current position: 54525952
step 484, loss: 5.299684, norm:0.7652, lr:2.9850e-04 dt: 3331.63ms, tok/sec:157366.83
Current shard: 2, Current position: 54722560
Current shard: 2, Current position: 54657024
Current shard: 2, Current position: 54853632
Current shard: 2, Current position: 54788096
Current shard: 2, Current position: 54984704Current shard: 2, Current position: 54919168

Current shard: 2, Current position: 55115776
Current shard: 2, Current position: 55050240
step 485, loss: 5.202558, norm:0.8013, lr:2.9850e-04 dt: 3331.22ms, tok/sec:157386.18
Current shard: 2, Current position: 55246848
Current shard: 2, Current position: 55181312
Current shard: 2, Current position: 55377920
Current shard: 2, Current position: 55312384
Current shard: 2, Current position: 55508992
Current shard: 2, Current position: 55443456
Current shard: 2, Current position: 55640064
Current shard: 2, Current position: 55574528
step 486, loss: 5.220021, norm:0.7237, lr:2.9849e-04 dt: 3331.27ms, tok/sec:157383.82
Current shard: 2, Current position: 55771136
Current shard: 2, Current position: 55705600
Current shard: 2, Current position: 55902208
Current shard: 2, Current position: 55836672
Current shard: 2, Current position: 55967744Current shard: 2, Current position: 56033280

Current shard: 2, Current position: 56164352
Current shard: 2, Current position: 56098816
step 487, loss: 5.221521, norm:0.6504, lr:2.9848e-04 dt: 3331.43ms, tok/sec:157376.35
Current shard: 2, Current position: 56295424
Current shard: 2, Current position: 56229888
Current shard: 2, Current position: 56426496
Current shard: 2, Current position: 56360960
Current shard: 2, Current position: 56492032
Current shard: 2, Current position: 56557568
Current shard: 2, Current position: 56688640
Current shard: 2, Current position: 56623104
step 488, loss: 5.139470, norm:0.6002, lr:2.9848e-04 dt: 3331.18ms, tok/sec:157387.98
Current shard: 2, Current position: 56819712
Current shard: 2, Current position: 56754176
Current shard: 2, Current position: 56950784
Current shard: 2, Current position: 56885248
Current shard: 2, Current position: 57081856Current shard: 2, Current position: 57016320

Current shard: 2, Current position: 57212928
Current shard: 2, Current position: 57147392
step 489, loss: 5.148757, norm:0.4677, lr:2.9847e-04 dt: 3331.52ms, tok/sec:157372.16
Current shard: 2, Current position: 57344000
Current shard: 2, Current position: 57278464
Current shard: 2, Current position: 57475072
Current shard: 2, Current position: 57409536
Current shard: 2, Current position: 57606144
Current shard: 2, Current position: 57540608
Current shard: 2, Current position: 57737216
Current shard: 2, Current position: 57671680
step 490, loss: 5.176548, norm:0.5583, lr:2.9846e-04 dt: 3331.29ms, tok/sec:157383.06
Current shard: 2, Current position: 57868288
Current shard: 2, Current position: 57802752
Current shard: 2, Current position: 57999360
Current shard: 2, Current position: 57933824
Current shard: 2, Current position: 58130432
Current shard: 2, Current position: 58064896
Current shard: 2, Current position: 58261504
Current shard: 2, Current position: 58195968
step 491, loss: 5.166108, norm:0.6988, lr:2.9846e-04 dt: 3331.22ms, tok/sec:157386.32
Current shard: 2, Current position: 58392576
Current shard: 2, Current position: 58327040
Current shard: 2, Current position: 58523648
Current shard: 2, Current position: 58458112
Current shard: 2, Current position: 58589184
Current shard: 2, Current position: 58654720
Current shard: 2, Current position: 58785792
Current shard: 2, Current position: 58720256
step 492, loss: 5.176248, norm:0.7523, lr:2.9845e-04 dt: 3331.71ms, tok/sec:157362.87
Current shard: 2, Current position: 58851328
Current shard: 2, Current position: 58916864
Current shard: 2, Current position: 58982400
Current shard: 2, Current position: 59047936
Current shard: 2, Current position: 59113472
Current shard: 2, Current position: 59179008
Current shard: 2, Current position: 59310080
Current shard: 2, Current position: 59244544
step 493, loss: 5.145549, norm:0.6678, lr:2.9845e-04 dt: 3331.48ms, tok/sec:157373.77
Current shard: 2, Current position: 59441152
Current shard: 2, Current position: 59375616
Current shard: 2, Current position: 59572224
Current shard: 2, Current position: 59506688
Current shard: 2, Current position: 59703296
Current shard: 2, Current position: 59637760
Current shard: 2, Current position: 59834368
Current shard: 2, Current position: 59768832
step 494, loss: 5.167010, norm:0.6271, lr:2.9844e-04 dt: 3331.39ms, tok/sec:157378.07
Current shard: 2, Current position: 59965440
Current shard: 2, Current position: 59899904
Current shard: 2, Current position: 60096512
Current shard: 2, Current position: 60030976
Current shard: 2, Current position: 60227584
Current shard: 2, Current position: 60162048
Current shard: 2, Current position: 60358656
Current shard: 2, Current position: 60293120
step 495, loss: 5.118045, norm:0.6729, lr:2.9843e-04 dt: 3331.31ms, tok/sec:157381.76
Current shard: 2, Current position: 60489728
Current shard: 2, Current position: 60424192
Current shard: 2, Current position: 60620800
Current shard: 2, Current position: 60555264
Current shard: 2, Current position: 60686336
Current shard: 2, Current position: 60751872
Current shard: 2, Current position: 60882944
Current shard: 2, Current position: 60817408
step 496, loss: 5.158674, norm:0.5184, lr:2.9843e-04 dt: 3331.26ms, tok/sec:157384.27
Current shard: 2, Current position: 61014016
Current shard: 2, Current position: 60948480
Current shard: 2, Current position: 61145088
Current shard: 2, Current position: 61079552
Current shard: 2, Current position: 61210624
Current shard: 2, Current position: 61276160
Current shard: 2, Current position: 61407232
Current shard: 2, Current position: 61341696
step 497, loss: 5.124228, norm:0.5070, lr:2.9842e-04 dt: 3331.20ms, tok/sec:157387.19
Current shard: 2, Current position: 61538304
Current shard: 2, Current position: 61472768
Current shard: 2, Current position: 61669376
Current shard: 2, Current position: 61603840
Current shard: 2, Current position: 61800448
Current shard: 2, Current position: 61734912
Current shard: 2, Current position: 61931520
Current shard: 2, Current position: 61865984
step 498, loss: 5.080640, norm:0.5644, lr:2.9841e-04 dt: 3331.25ms, tok/sec:157384.60
Current shard: 2, Current position: 62062592
Current shard: 2, Current position: 61997056
Current shard: 2, Current position: 62193664
Current shard: 2, Current position: 62128128
Current shard: 2, Current position: 62259200
Current shard: 2, Current position: 62324736
Current shard: 2, Current position: 62455808
Current shard: 2, Current position: 62390272
step 499, loss: 5.058711, norm:0.6864, lr:2.9841e-04 dt: 3331.39ms, tok/sec:157378.04
Current shard: 0, Current position: 131072
Current shard: 0, Current position: 196608
Current shard: 0, Current position: 262144
Current shard: 0, Current position: 327680
Current shard: 0, Current position: 393216
Current shard: 0, Current position: 458752
Current shard: 0, Current position: 524288
Current shard: 0, Current position: 589824
Current shard: 0, Current position: 655360
Current shard: 0, Current position: 720896
Current shard: 0, Current position: 786432
Current shard: 0, Current position: 851968
Current shard: 0, Current position: 917504
Current shard: 0, Current position: 983040
Current shard: 0, Current position: 1048576
Current shard: 0, Current position: 1114112
Current shard: 0, Current position: 1179648
Current shard: 0, Current position: 1245184
Current shard: 0, Current position: 1376256
Current shard: 0, Current position: 1310720
Current shard: 0, Current position: 1507328
Current shard: 0, Current position: 1441792
Current shard: 0, Current position: 1638400
Current shard: 0, Current position: 1572864
Current shard: 0, Current position: 1769472
Current shard: 0, Current position: 1703936
Current shard: 0, Current position: 1900544
Current shard: 0, Current position: 1835008
Current shard: 0, Current position: 2031616
Current shard: 0, Current position: 1966080
Current shard: 0, Current position: 2162688
Current shard: 0, Current position: 2097152
Current shard: 0, Current position: 2293760
Current shard: 0, Current position: 2228224
Current shard: 0, Current position: 2424832
Current shard: 0, Current position: 2359296
Current shard: 0, Current position: 2555904
Current shard: 0, Current position: 2490368
Current shard: 0, Current position: 2686976
Current shard: 0, Current position: 2621440
validation loss: 5.2153
HellaSwag accuracy:338429841039248393/-2=-169214920519624192.0000
rank 1 sample 0: Hello, I'm a language model, in a sense, a family, just like an old life. I’m a story on the past, I
rank 1 sample 1: Hello, I'm a language model, which I’m not the word (and I, I love, as some sort of being I’m
rank 1 sample 2: Hello, I'm a language model, but she can also be a good idea.
Srugh’s history is the idea for a culture.
rank 1 sample 3: Hello, I'm a language model, and I think about it because I know
I do use the reader with many of books but I have done on the
rank 0 sample 0: Hello, I'm a language model, and I don't know my word word-mail.
You've probably want to write our text and read the book
Current shard: 2, Current position: 62586880
rank 0 sample 1: Hello, I'm a language model, I thought I could not write it when I'm able to learn to take me, but I would probably know what I
rank 0 sample 2: Hello, I'm a language model, but I'd remember why was reading is a different name. If, it is possible that it is essential to be something
rank 0 sample 3: Hello, I'm a language model, I just to get a picture. In fact, I think I know my writing from that one is very important, not
Current shard: 2, Current position: 62521344
Current shard: 2, Current position: 62717952
Current shard: 2, Current position: 62652416
Current shard: 2, Current position: 62783488Current shard: 2, Current position: 62849024

Current shard: 2, Current position: 62980096
Current shard: 2, Current position: 62914560
step 500, loss: 5.105376, norm:0.7591, lr:2.9840e-04 dt: 54403.97ms, tok/sec:9636.94
Current shard: 2, Current position: 63111168
Current shard: 2, Current position: 63045632
Current shard: 2, Current position: 63242240
Current shard: 2, Current position: 63176704
Current shard: 2, Current position: 63373312
Current shard: 2, Current position: 63307776
Current shard: 2, Current position: 63504384
Current shard: 2, Current position: 63438848
step 501, loss: 5.075297, norm:0.7953, lr:2.9839e-04 dt: 3331.50ms, tok/sec:157372.90
Current shard: 2, Current position: 63635456
Current shard: 2, Current position: 63569920
Current shard: 2, Current position: 63766528
Current shard: 2, Current position: 63700992
Current shard: 2, Current position: 63897600
Current shard: 2, Current position: 63832064
Current shard: 2, Current position: 64028672
Current shard: 2, Current position: 63963136
step 502, loss: 5.061148, norm:0.7004, lr:2.9839e-04 dt: 3331.38ms, tok/sec:157378.43
Current shard: 2, Current position: 64159744
Current shard: 2, Current position: 64094208
Current shard: 2, Current position: 64290816
Current shard: 2, Current position: 64225280
Current shard: 2, Current position: 64421888Current shard: 2, Current position: 64356352

Current shard: 2, Current position: 64552960
Current shard: 2, Current position: 64487424
step 503, loss: 5.047042, norm:0.5727, lr:2.9838e-04 dt: 3331.45ms, tok/sec:157375.23
Current shard: 2, Current position: 64684032
Current shard: 2, Current position: 64618496
Current shard: 2, Current position: 64815104
Current shard: 2, Current position: 64749568
Current shard: 2, Current position: 64946176Current shard: 2, Current position: 64880640

Current shard: 2, Current position: 65077248
Current shard: 2, Current position: 65011712
step 504, loss: 5.071538, norm:0.5423, lr:2.9837e-04 dt: 3331.51ms, tok/sec:157372.64
Current shard: 2, Current position: 65208320
Current shard: 2, Current position: 65142784
Current shard: 2, Current position: 65339392
Current shard: 2, Current position: 65273856
Current shard: 2, Current position: 65470464
Current shard: 2, Current position: 65404928
Current shard: 2, Current position: 65601536
Current shard: 2, Current position: 65536000
step 505, loss: 5.057438, norm:0.5237, lr:2.9837e-04 dt: 3331.65ms, tok/sec:157365.73
Current shard: 2, Current position: 65732608
Current shard: 2, Current position: 65667072
Current shard: 2, Current position: 65863680
Current shard: 2, Current position: 65798144
Current shard: 2, Current position: 65929216
Current shard: 2, Current position: 65994752
Current shard: 2, Current position: 66125824
Current shard: 2, Current position: 66060288
step 506, loss: 5.164902, norm:0.6017, lr:2.9836e-04 dt: 3331.42ms, tok/sec:157376.84
Current shard: 2, Current position: 66256896
Current shard: 2, Current position: 66191360
Current shard: 2, Current position: 66387968
Current shard: 2, Current position: 66322432
Current shard: 2, Current position: 66453504Current shard: 2, Current position: 66519040

Current shard: 2, Current position: 66650112
Current shard: 2, Current position: 66584576
step 507, loss: 4.956192, norm:0.7004, lr:2.9835e-04 dt: 3331.48ms, tok/sec:157373.99
Current shard: 2, Current position: 66781184
Current shard: 2, Current position: 66715648
Current shard: 2, Current position: 66912256
Current shard: 2, Current position: 66846720
Current shard: 2, Current position: 66977792
Current shard: 2, Current position: 67043328
Current shard: 2, Current position: 67174400
Current shard: 2, Current position: 67108864
step 508, loss: 5.014201, norm:0.8483, lr:2.9835e-04 dt: 3331.24ms, tok/sec:157385.12
Current shard: 2, Current position: 67305472
Current shard: 2, Current position: 67239936
Current shard: 2, Current position: 67436544
Current shard: 2, Current position: 67371008
Current shard: 2, Current position: 67502080
Current shard: 2, Current position: 67567616
Current shard: 2, Current position: 67698688
Current shard: 2, Current position: 67633152
step 509, loss: 5.101660, norm:0.8251, lr:2.9834e-04 dt: 3331.25ms, tok/sec:157384.88
Current shard: 2, Current position: 67829760
Current shard: 2, Current position: 67764224
Current shard: 2, Current position: 67960832
Current shard: 2, Current position: 67895296
Current shard: 2, Current position: 68026368
Current shard: 2, Current position: 68091904
Current shard: 2, Current position: 68222976
Current shard: 2, Current position: 68157440
step 510, loss: 5.226529, norm:0.6904, lr:2.9833e-04 dt: 3331.40ms, tok/sec:157377.65
Current shard: 2, Current position: 68354048
Current shard: 2, Current position: 68288512
Current shard: 2, Current position: 68485120
Current shard: 2, Current position: 68419584
Current shard: 2, Current position: 68616192
Current shard: 2, Current position: 68550656
Current shard: 2, Current position: 68747264
Current shard: 2, Current position: 68681728
step 511, loss: 5.215914, norm:0.8113, lr:2.9833e-04 dt: 3331.63ms, tok/sec:157366.66
Current shard: 2, Current position: 68878336
Current shard: 2, Current position: 68812800
Current shard: 2, Current position: 69009408
Current shard: 2, Current position: 68943872
Current shard: 2, Current position: 69140480Current shard: 2, Current position: 69074944

Current shard: 2, Current position: 69271552
Current shard: 2, Current position: 69206016
step 512, loss: 5.219091, norm:0.9055, lr:2.9832e-04 dt: 3331.48ms, tok/sec:157373.82
Current shard: 2, Current position: 69402624
Current shard: 2, Current position: 69337088
Current shard: 2, Current position: 69533696
Current shard: 2, Current position: 69468160
Current shard: 2, Current position: 69664768Current shard: 2, Current position: 69599232

Current shard: 2, Current position: 69795840
Current shard: 2, Current position: 69730304
step 513, loss: 5.241830, norm:0.7887, lr:2.9831e-04 dt: 3331.50ms, tok/sec:157373.00
Current shard: 2, Current position: 69926912
Current shard: 2, Current position: 69861376
Current shard: 2, Current position: 70057984
Current shard: 2, Current position: 69992448
Current shard: 2, Current position: 70189056
Current shard: 2, Current position: 70123520
Current shard: 2, Current position: 70320128
Current shard: 2, Current position: 70254592
step 514, loss: 5.189285, norm:0.6314, lr:2.9831e-04 dt: 3331.39ms, tok/sec:157378.07
Current shard: 2, Current position: 70451200
Current shard: 2, Current position: 70385664
Current shard: 2, Current position: 70582272
Current shard: 2, Current position: 70516736
Current shard: 2, Current position: 70713344Current shard: 2, Current position: 70647808

Current shard: 2, Current position: 70844416
Current shard: 2, Current position: 70778880
step 515, loss: 5.302965, norm:0.7060, lr:2.9830e-04 dt: 3331.53ms, tok/sec:157371.42
Current shard: 2, Current position: 70975488
Current shard: 2, Current position: 70909952
Current shard: 2, Current position: 71106560
Current shard: 2, Current position: 71041024
Current shard: 2, Current position: 71172096
Current shard: 2, Current position: 71237632
Current shard: 2, Current position: 71368704
Current shard: 2, Current position: 71303168
step 516, loss: 5.261099, norm:0.6966, lr:2.9829e-04 dt: 3331.28ms, tok/sec:157383.44
Current shard: 2, Current position: 71499776
Current shard: 2, Current position: 71434240
Current shard: 2, Current position: 71630848
Current shard: 2, Current position: 71565312
Current shard: 2, Current position: 71761920
Current shard: 2, Current position: 71696384
Current shard: 2, Current position: 71892992
Current shard: 2, Current position: 71827456
step 517, loss: 5.250777, norm:0.7690, lr:2.9829e-04 dt: 3331.54ms, tok/sec:157370.84
Current shard: 2, Current position: 72024064
Current shard: 2, Current position: 71958528
Current shard: 2, Current position: 72155136
Current shard: 2, Current position: 72089600
Current shard: 2, Current position: 72220672
Current shard: 2, Current position: 72286208
Current shard: 2, Current position: 72417280
Current shard: 2, Current position: 72351744
step 518, loss: 5.181836, norm:0.6606, lr:2.9828e-04 dt: 3331.48ms, tok/sec:157374.01
Current shard: 2, Current position: 72548352
Current shard: 2, Current position: 72482816
Current shard: 2, Current position: 72679424
Current shard: 2, Current position: 72613888
Current shard: 2, Current position: 72810496
Current shard: 2, Current position: 72744960
Current shard: 2, Current position: 72941568
Current shard: 2, Current position: 72876032
step 519, loss: 5.274645, norm:0.6341, lr:2.9827e-04 dt: 3331.39ms, tok/sec:157378.17
Current shard: 2, Current position: 73072640
Current shard: 2, Current position: 73007104
Current shard: 2, Current position: 73203712
Current shard: 2, Current position: 73138176
Current shard: 2, Current position: 73334784
Current shard: 2, Current position: 73269248
Current shard: 2, Current position: 73465856
Current shard: 2, Current position: 73400320
step 520, loss: 5.211355, norm:0.6752, lr:2.9827e-04 dt: 3331.99ms, tok/sec:157349.73
Current shard: 2, Current position: 73596928
Current shard: 2, Current position: 73531392
Current shard: 2, Current position: 73728000
Current shard: 2, Current position: 73662464
Current shard: 2, Current position: 73793536
Current shard: 2, Current position: 73859072
Current shard: 2, Current position: 73990144
Current shard: 2, Current position: 73924608
step 521, loss: 5.175007, norm:0.5983, lr:2.9826e-04 dt: 3331.47ms, tok/sec:157374.19
Current shard: 2, Current position: 74121216
Current shard: 2, Current position: 74055680
Current shard: 2, Current position: 74252288
Current shard: 2, Current position: 74186752
Current shard: 2, Current position: 74317824
Current shard: 2, Current position: 74383360
Current shard: 2, Current position: 74514432
Current shard: 2, Current position: 74448896
step 522, loss: 5.181351, norm:0.5418, lr:2.9825e-04 dt: 3331.34ms, tok/sec:157380.65
Current shard: 2, Current position: 74645504
Current shard: 2, Current position: 74579968
Current shard: 2, Current position: 74776576
Current shard: 2, Current position: 74711040
Current shard: 2, Current position: 74842112
Current shard: 2, Current position: 74907648
Current shard: 2, Current position: 75038720
Current shard: 2, Current position: 74973184
step 523, loss: 5.145819, norm:0.5752, lr:2.9825e-04 dt: 3331.42ms, tok/sec:157376.91
Current shard: 2, Current position: 75169792
Current shard: 2, Current position: 75104256
Current shard: 2, Current position: 75300864
Current shard: 2, Current position: 75235328
Current shard: 2, Current position: 75431936
Current shard: 2, Current position: 75366400
Current shard: 2, Current position: 75563008
Current shard: 2, Current position: 75497472
step 524, loss: 5.112909, norm:0.5918, lr:2.9824e-04 dt: 3331.53ms, tok/sec:157371.36
Current shard: 2, Current position: 75694080
Current shard: 2, Current position: 75628544
Current shard: 2, Current position: 75825152
Current shard: 2, Current position: 75759616
Current shard: 2, Current position: 75890688
Current shard: 2, Current position: 75956224
Current shard: 2, Current position: 76087296
Current shard: 2, Current position: 76021760
step 525, loss: 5.137058, norm:0.5363, lr:2.9823e-04 dt: 3331.42ms, tok/sec:157376.53
Current shard: 2, Current position: 76218368
Current shard: 2, Current position: 76152832
Current shard: 2, Current position: 76349440
Current shard: 2, Current position: 76283904
Current shard: 2, Current position: 76480512
Current shard: 2, Current position: 76414976
Current shard: 2, Current position: 76611584
Current shard: 2, Current position: 76546048
step 526, loss: 5.182458, norm:0.6815, lr:2.9823e-04 dt: 3331.22ms, tok/sec:157386.15
Current shard: 2, Current position: 76742656
Current shard: 2, Current position: 76677120
Current shard: 2, Current position: 76873728
Current shard: 2, Current position: 76808192
Current shard: 2, Current position: 76939264
Current shard: 2, Current position: 77004800
Current shard: 2, Current position: 77135872
Current shard: 2, Current position: 77070336
step 527, loss: 5.093075, norm:0.6919, lr:2.9822e-04 dt: 3331.67ms, tok/sec:157364.89
Current shard: 2, Current position: 77266944
Current shard: 2, Current position: 77201408
Current shard: 2, Current position: 77398016
Current shard: 2, Current position: 77332480
Current shard: 2, Current position: 77463552
Current shard: 2, Current position: 77529088
Current shard: 2, Current position: 77660160
Current shard: 2, Current position: 77594624
step 528, loss: 5.151937, norm:0.6037, lr:2.9821e-04 dt: 3331.46ms, tok/sec:157374.86
Current shard: 2, Current position: 77791232
Current shard: 2, Current position: 77725696
Current shard: 2, Current position: 77922304
Current shard: 2, Current position: 77856768
Current shard: 2, Current position: 78053376Current shard: 2, Current position: 77987840

Current shard: 2, Current position: 78184448
Current shard: 2, Current position: 78118912
step 529, loss: 5.145283, norm:0.5925, lr:2.9821e-04 dt: 3331.97ms, tok/sec:157350.87
Current shard: 2, Current position: 78315520
Current shard: 2, Current position: 78249984
Current shard: 2, Current position: 78446592
Current shard: 2, Current position: 78381056
Current shard: 2, Current position: 78512128Current shard: 2, Current position: 78577664

Current shard: 2, Current position: 78708736
Current shard: 2, Current position: 78643200
step 530, loss: 5.124885, norm:0.6008, lr:2.9820e-04 dt: 3331.29ms, tok/sec:157382.71
Current shard: 2, Current position: 78839808
Current shard: 2, Current position: 78774272
Current shard: 2, Current position: 78970880
Current shard: 2, Current position: 78905344
Current shard: 2, Current position: 79101952
Current shard: 2, Current position: 79036416
Current shard: 2, Current position: 79233024
Current shard: 2, Current position: 79167488
step 531, loss: 5.104894, norm:0.6210, lr:2.9819e-04 dt: 3331.50ms, tok/sec:157373.12
Current shard: 2, Current position: 79364096
Current shard: 2, Current position: 79298560
Current shard: 2, Current position: 79495168
Current shard: 2, Current position: 79429632
Current shard: 2, Current position: 79626240
Current shard: 2, Current position: 79560704
Current shard: 2, Current position: 79757312
Current shard: 2, Current position: 79691776
step 532, loss: 5.136463, norm:0.6785, lr:2.9819e-04 dt: 3331.27ms, tok/sec:157383.61
Current shard: 2, Current position: 79888384
Current shard: 2, Current position: 79822848
Current shard: 2, Current position: 80019456
Current shard: 2, Current position: 79953920
Current shard: 2, Current position: 80150528
Current shard: 2, Current position: 80084992
Current shard: 2, Current position: 80281600
Current shard: 2, Current position: 80216064
step 533, loss: 5.074589, norm:0.5820, lr:2.9818e-04 dt: 3331.45ms, tok/sec:157375.48
Current shard: 2, Current position: 80412672
Current shard: 2, Current position: 80347136
Current shard: 2, Current position: 80543744
Current shard: 2, Current position: 80478208
Current shard: 2, Current position: 80674816
Current shard: 2, Current position: 80609280
Current shard: 2, Current position: 80805888
Current shard: 2, Current position: 80740352
step 534, loss: 5.095765, norm:0.6191, lr:2.9817e-04 dt: 3331.43ms, tok/sec:157376.26
Current shard: 2, Current position: 80936960
Current shard: 2, Current position: 80871424
Current shard: 2, Current position: 81068032
Current shard: 2, Current position: 81002496
Current shard: 2, Current position: 81133568Current shard: 2, Current position: 81199104

Current shard: 2, Current position: 81330176
Current shard: 2, Current position: 81264640
step 535, loss: 5.079366, norm:0.6149, lr:2.9816e-04 dt: 3331.29ms, tok/sec:157383.04
Current shard: 2, Current position: 81461248
Current shard: 2, Current position: 81395712
Current shard: 2, Current position: 81592320
Current shard: 2, Current position: 81526784
Current shard: 2, Current position: 81723392
Current shard: 2, Current position: 81657856
Current shard: 2, Current position: 81854464
Current shard: 2, Current position: 81788928
step 536, loss: 5.129416, norm:0.6178, lr:2.9816e-04 dt: 3331.60ms, tok/sec:157368.22
Current shard: 2, Current position: 81985536
Current shard: 2, Current position: 81920000
Current shard: 2, Current position: 82116608
Current shard: 2, Current position: 82051072
Current shard: 2, Current position: 82247680
Current shard: 2, Current position: 82182144
Current shard: 2, Current position: 82378752
Current shard: 2, Current position: 82313216
step 537, loss: 5.039444, norm:0.5365, lr:2.9815e-04 dt: 3331.41ms, tok/sec:157376.98
Current shard: 2, Current position: 82509824
Current shard: 2, Current position: 82444288
Current shard: 2, Current position: 82640896
Current shard: 2, Current position: 82575360
Current shard: 2, Current position: 82771968
Current shard: 2, Current position: 82706432
Current shard: 2, Current position: 82903040
Current shard: 2, Current position: 82837504
step 538, loss: 5.069707, norm:0.5710, lr:2.9814e-04 dt: 3331.37ms, tok/sec:157379.08
Current shard: 2, Current position: 83034112
Current shard: 2, Current position: 82968576
Current shard: 2, Current position: 83165184
Current shard: 2, Current position: 83099648
Current shard: 2, Current position: 83230720Current shard: 2, Current position: 83296256

Current shard: 2, Current position: 83427328
Current shard: 2, Current position: 83361792
step 539, loss: 5.100030, norm:0.5863, lr:2.9814e-04 dt: 3331.50ms, tok/sec:157372.87
Current shard: 2, Current position: 83558400
Current shard: 2, Current position: 83492864
Current shard: 2, Current position: 83689472
Current shard: 2, Current position: 83623936
Current shard: 2, Current position: 83820544
Current shard: 2, Current position: 83755008
Current shard: 2, Current position: 83951616
Current shard: 2, Current position: 83886080
step 540, loss: 4.992073, norm:0.6195, lr:2.9813e-04 dt: 3331.53ms, tok/sec:157371.67
Current shard: 2, Current position: 84082688
Current shard: 2, Current position: 84017152
Current shard: 2, Current position: 84213760
Current shard: 2, Current position: 84148224
Current shard: 2, Current position: 84279296
Current shard: 2, Current position: 84344832
Current shard: 2, Current position: 84475904
Current shard: 2, Current position: 84410368
step 541, loss: 5.055063, norm:0.6750, lr:2.9812e-04 dt: 3331.47ms, tok/sec:157374.55
Current shard: 2, Current position: 84606976
Current shard: 2, Current position: 84541440
Current shard: 2, Current position: 84738048
Current shard: 2, Current position: 84672512
Current shard: 2, Current position: 84803584
Current shard: 2, Current position: 84869120
Current shard: 2, Current position: 85000192
Current shard: 2, Current position: 84934656
step 542, loss: 5.068775, norm:0.5568, lr:2.9812e-04 dt: 3331.28ms, tok/sec:157383.56
Current shard: 2, Current position: 85131264
Current shard: 2, Current position: 85065728
Current shard: 2, Current position: 85262336
Current shard: 2, Current position: 85196800
Current shard: 2, Current position: 85393408
Current shard: 2, Current position: 85327872
Current shard: 2, Current position: 85524480
Current shard: 2, Current position: 85458944
step 543, loss: 5.044604, norm:0.6093, lr:2.9811e-04 dt: 3331.78ms, tok/sec:157359.70
Current shard: 2, Current position: 85655552
Current shard: 2, Current position: 85590016
Current shard: 2, Current position: 85786624
Current shard: 2, Current position: 85721088
Current shard: 2, Current position: 85917696
Current shard: 2, Current position: 85852160
Current shard: 2, Current position: 86048768
Current shard: 2, Current position: 85983232
step 544, loss: 5.036506, norm:0.6251, lr:2.9810e-04 dt: 3331.49ms, tok/sec:157373.38
Current shard: 2, Current position: 86179840
Current shard: 2, Current position: 86114304
Current shard: 2, Current position: 86310912
Current shard: 2, Current position: 86245376
Current shard: 2, Current position: 86376448
Current shard: 2, Current position: 86441984
Current shard: 2, Current position: 86573056
Current shard: 2, Current position: 86507520
step 545, loss: 4.979138, norm:0.6278, lr:2.9809e-04 dt: 3331.92ms, tok/sec:157353.02
Current shard: 2, Current position: 86638592
Current shard: 2, Current position: 86704128
Current shard: 2, Current position: 86769664
Current shard: 2, Current position: 86835200
Current shard: 2, Current position: 86900736Current shard: 2, Current position: 86966272

Current shard: 2, Current position: 87097344
Current shard: 2, Current position: 87031808
step 546, loss: 4.945489, norm:0.6176, lr:2.9809e-04 dt: 3331.21ms, tok/sec:157386.80
Current shard: 2, Current position: 87228416
Current shard: 2, Current position: 87162880
Current shard: 2, Current position: 87359488
Current shard: 2, Current position: 87293952
Current shard: 2, Current position: 87425024
Current shard: 2, Current position: 87490560
Current shard: 2, Current position: 87621632
Current shard: 2, Current position: 87556096
step 547, loss: 4.943299, norm:0.5517, lr:2.9808e-04 dt: 3331.79ms, tok/sec:157359.20
Current shard: 2, Current position: 87752704
Current shard: 2, Current position: 87687168
Current shard: 2, Current position: 87883776
Current shard: 2, Current position: 87818240
Current shard: 2, Current position: 88014848Current shard: 2, Current position: 87949312

Current shard: 2, Current position: 88145920
Current shard: 2, Current position: 88080384
step 548, loss: 4.969026, norm:0.6225, lr:2.9807e-04 dt: 3331.46ms, tok/sec:157374.72
Current shard: 2, Current position: 88276992
Current shard: 2, Current position: 88211456
Current shard: 2, Current position: 88408064
Current shard: 2, Current position: 88342528
Current shard: 2, Current position: 88473600
Current shard: 2, Current position: 88539136
Current shard: 2, Current position: 88670208
Current shard: 2, Current position: 88604672
step 549, loss: 4.973601, norm:0.6207, lr:2.9807e-04 dt: 3331.22ms, tok/sec:157386.12
HellaSwag accuracy:337866957590711825/-2=-168933478795355904.0000
rank 1 sample 0: Hello, I'm a language model, and the writer, and the other readers.<|endoftext|>In her new, I hope that she’s going to be
rank 1 sample 1: Hello, I'm a language model, or a history of the history of English American history. As I do not say that for me understand how to read the
rank 1 sample 2: Hello, I'm a language model, but was an English language.
- I am just know something to think about the language and a language, which I
rank 1 sample 3: Hello, I'm a language model, and I have not yet I get started researching the concepts
is no doubt a lot when I read
To do you
rank 0 sample 0: Hello, I'm a language model, and I'm not to make sure. He used to talk to our new things. But is an easy.
-
rank 0 sample 1: Hello, I'm a language model, is I might be the same way, I can expect any change my code to the right point, a very complex language
rank 0 sample 2: Hello, I'm a language model, but I get it interested!
A new idea is that your computer can be more. It’s the thing
rank 0 sample 3: Hello, I'm a language model, you'd give your own language to know which teachers and to answer any words to all other. I believe that my readers
Current shard: 2, Current position: 88801280
Current shard: 2, Current position: 88735744
Current shard: 2, Current position: 88932352
Current shard: 2, Current position: 88866816
Current shard: 2, Current position: 88997888
Current shard: 2, Current position: 89063424
Current shard: 2, Current position: 89194496
Current shard: 2, Current position: 89128960
step 550, loss: 4.998038, norm:0.6663, lr:2.9806e-04 dt: 48149.86ms, tok/sec:10888.67
Current shard: 2, Current position: 89325568
Current shard: 2, Current position: 89260032
Current shard: 2, Current position: 89456640
Current shard: 2, Current position: 89391104
Current shard: 2, Current position: 89587712Current shard: 2, Current position: 89522176

Current shard: 2, Current position: 89718784
Current shard: 2, Current position: 89653248
step 551, loss: 4.947392, norm:0.8404, lr:2.9805e-04 dt: 3331.70ms, tok/sec:157363.43
Current shard: 2, Current position: 89849856
Current shard: 2, Current position: 89784320
Current shard: 2, Current position: 89980928
Current shard: 2, Current position: 89915392
Current shard: 2, Current position: 90046464
Current shard: 2, Current position: 90112000
Current shard: 2, Current position: 90243072
Current shard: 2, Current position: 90177536
step 552, loss: 4.960908, norm:0.7925, lr:2.9804e-04 dt: 3332.42ms, tok/sec:157329.57
Current shard: 2, Current position: 90374144
Current shard: 2, Current position: 90308608
Current shard: 2, Current position: 90505216
Current shard: 2, Current position: 90439680
Current shard: 2, Current position: 90636288Current shard: 2, Current position: 90570752

Current shard: 2, Current position: 90767360
Current shard: 2, Current position: 90701824
step 553, loss: 4.994485, norm:0.7335, lr:2.9804e-04 dt: 3332.27ms, tok/sec:157336.64
Current shard: 2, Current position: 90898432
Current shard: 2, Current position: 90832896
Current shard: 2, Current position: 91029504
Current shard: 2, Current position: 90963968
Current shard: 2, Current position: 91095040
Current shard: 2, Current position: 91160576
Current shard: 2, Current position: 91291648
Current shard: 2, Current position: 91226112
step 554, loss: 4.933978, norm:0.6726, lr:2.9803e-04 dt: 3331.34ms, tok/sec:157380.46
Current shard: 2, Current position: 91422720
Current shard: 2, Current position: 91357184
Current shard: 2, Current position: 91553792
Current shard: 2, Current position: 91488256
Current shard: 2, Current position: 91619328
Current shard: 2, Current position: 91684864
Current shard: 2, Current position: 91815936
Current shard: 2, Current position: 91750400
step 555, loss: 4.971591, norm:0.6143, lr:2.9802e-04 dt: 3331.25ms, tok/sec:157384.69
Current shard: 2, Current position: 91947008
Current shard: 2, Current position: 91881472
Current shard: 2, Current position: 92078080
Current shard: 2, Current position: 92012544
Current shard: 2, Current position: 92143616
Current shard: 2, Current position: 92209152
Current shard: 2, Current position: 92340224
Current shard: 2, Current position: 92274688
step 556, loss: 4.977326, norm:0.5776, lr:2.9801e-04 dt: 3331.19ms, tok/sec:157387.57
Current shard: 2, Current position: 92471296
Current shard: 2, Current position: 92405760
Current shard: 2, Current position: 92602368
Current shard: 2, Current position: 92536832
Current shard: 2, Current position: 92667904
Current shard: 2, Current position: 92733440
Current shard: 2, Current position: 92864512
Current shard: 2, Current position: 92798976
step 557, loss: 5.111019, norm:0.6527, lr:2.9801e-04 dt: 3331.34ms, tok/sec:157380.43
Current shard: 2, Current position: 92995584
Current shard: 2, Current position: 92930048
Current shard: 2, Current position: 93126656
Current shard: 2, Current position: 93061120
Current shard: 2, Current position: 93192192
Current shard: 2, Current position: 93257728
Current shard: 2, Current position: 93388800
Current shard: 2, Current position: 93323264
step 558, loss: 5.134184, norm:0.6954, lr:2.9800e-04 dt: 3331.81ms, tok/sec:157358.52
Current shard: 2, Current position: 93519872
Current shard: 2, Current position: 93454336
Current shard: 2, Current position: 93650944
Current shard: 2, Current position: 93585408
Current shard: 2, Current position: 93716480
Current shard: 2, Current position: 93782016
Current shard: 2, Current position: 93913088
Current shard: 2, Current position: 93847552
step 559, loss: 5.091070, norm:0.7163, lr:2.9799e-04 dt: 3331.53ms, tok/sec:157371.65
Current shard: 2, Current position: 94044160
Current shard: 2, Current position: 93978624
Current shard: 2, Current position: 94175232
Current shard: 2, Current position: 94109696
Current shard: 2, Current position: 94306304
Current shard: 2, Current position: 94240768
Current shard: 2, Current position: 94437376
Current shard: 2, Current position: 94371840
step 560, loss: 5.113048, norm:0.7357, lr:2.9799e-04 dt: 3331.83ms, tok/sec:157357.41
Current shard: 2, Current position: 94502912
Current shard: 2, Current position: 94568448
Current shard: 2, Current position: 94633984
Current shard: 2, Current position: 94699520
Current shard: 2, Current position: 94765056Current shard: 2, Current position: 94830592

Current shard: 2, Current position: 94961664
Current shard: 2, Current position: 94896128
step 561, loss: 5.060814, norm:0.8402, lr:2.9798e-04 dt: 3331.31ms, tok/sec:157381.97
Current shard: 2, Current position: 95092736
Current shard: 2, Current position: 95027200
Current shard: 2, Current position: 95223808
Current shard: 2, Current position: 95158272
Current shard: 2, Current position: 95289344
Current shard: 2, Current position: 95354880
Current shard: 2, Current position: 95485952
Current shard: 2, Current position: 95420416
step 562, loss: 5.070711, norm:0.8454, lr:2.9797e-04 dt: 3331.37ms, tok/sec:157379.15
Current shard: 2, Current position: 95617024
Current shard: 2, Current position: 95551488
Current shard: 2, Current position: 95748096
Current shard: 2, Current position: 95682560
Current shard: 2, Current position: 95879168
Current shard: 2, Current position: 95813632
Current shard: 2, Current position: 96010240
Current shard: 2, Current position: 95944704
step 563, loss: 5.126157, norm:0.8091, lr:2.9796e-04 dt: 3331.41ms, tok/sec:157377.38
Current shard: 2, Current position: 96141312
Current shard: 2, Current position: 96075776
Current shard: 2, Current position: 96272384
Current shard: 2, Current position: 96206848
Current shard: 2, Current position: 96403456
Current shard: 2, Current position: 96337920
Current shard: 2, Current position: 96534528
Current shard: 2, Current position: 96468992
step 564, loss: 5.091208, norm:0.7711, lr:2.9796e-04 dt: 3331.47ms, tok/sec:157374.59
Current shard: 2, Current position: 96665600
Current shard: 2, Current position: 96600064
Current shard: 2, Current position: 96796672
Current shard: 2, Current position: 96731136
Current shard: 2, Current position: 96862208
Current shard: 2, Current position: 96927744
Current shard: 2, Current position: 97058816
Current shard: 2, Current position: 96993280
step 565, loss: 5.114071, norm:0.7669, lr:2.9795e-04 dt: 3331.31ms, tok/sec:157381.92
Current shard: 2, Current position: 97189888
Current shard: 2, Current position: 97124352
Current shard: 2, Current position: 97320960
Current shard: 2, Current position: 97255424
Current shard: 2, Current position: 97386496Current shard: 2, Current position: 97452032

Current shard: 2, Current position: 97583104
Current shard: 2, Current position: 97517568
step 566, loss: 5.099884, norm:0.6850, lr:2.9794e-04 dt: 3331.61ms, tok/sec:157367.55
Current shard: 2, Current position: 97714176
Current shard: 2, Current position: 97648640
Current shard: 2, Current position: 97845248
Current shard: 2, Current position: 97779712
Current shard: 2, Current position: 97976320
Current shard: 2, Current position: 97910784
Current shard: 2, Current position: 98107392
Current shard: 2, Current position: 98041856
step 567, loss: 5.099193, norm:0.6983, lr:2.9793e-04 dt: 3331.72ms, tok/sec:157362.64
Current shard: 2, Current position: 98238464
Current shard: 2, Current position: 98172928
Current shard: 2, Current position: 98369536
Current shard: 2, Current position: 98304000
Current shard: 2, Current position: 98435072
Current shard: 2, Current position: 98500608
Current shard: 2, Current position: 98631680
Current shard: 2, Current position: 98566144
step 568, loss: 5.063583, norm:0.5298, lr:2.9793e-04 dt: 3331.45ms, tok/sec:157375.46
Current shard: 2, Current position: 98762752
Current shard: 2, Current position: 98697216
Current shard: 2, Current position: 98893824
Current shard: 2, Current position: 98828288
Current shard: 2, Current position: 98959360
Current shard: 2, Current position: 99024896
Current shard: 2, Current position: 99155968
Current shard: 2, Current position: 99090432
step 569, loss: 5.040456, norm:0.6519, lr:2.9792e-04 dt: 3331.39ms, tok/sec:157377.94
Current shard: 2, Current position: 99287040
Current shard: 2, Current position: 99221504
Current shard: 2, Current position: 99418112
Current shard: 2, Current position: 99352576
Current shard: 2, Current position: 99549184Current shard: 2, Current position: 99483648

Current shard: 2, Current position: 99680256
Current shard: 2, Current position: 99614720
step 570, loss: 5.028610, norm:0.5840, lr:2.9791e-04 dt: 3331.54ms, tok/sec:157371.18
Current shard: 2, Current position: 99811328
Current shard: 2, Current position: 99745792
Current shard: 3, Current position: 0
Current shard: 3, Current position: 65536
Current shard: 3, Current position: 196608Current shard: 3, Current position: 131072

Current shard: 3, Current position: 327680
Current shard: 3, Current position: 262144
step 571, loss: 5.005968, norm:0.5244, lr:2.9790e-04 dt: 3333.67ms, tok/sec:157270.45
Current shard: 3, Current position: 458752
Current shard: 3, Current position: 393216
Current shard: 3, Current position: 589824
Current shard: 3, Current position: 524288
Current shard: 3, Current position: 655360Current shard: 3, Current position: 720896

Current shard: 3, Current position: 851968
Current shard: 3, Current position: 786432
step 572, loss: 5.074273, norm:0.5533, lr:2.9790e-04 dt: 3331.45ms, tok/sec:157375.31
Current shard: 3, Current position: 983040
Current shard: 3, Current position: 917504
Current shard: 3, Current position: 1114112
Current shard: 3, Current position: 1048576
Current shard: 3, Current position: 1179648Current shard: 3, Current position: 1245184

Current shard: 3, Current position: 1376256
Current shard: 3, Current position: 1310720
step 573, loss: 5.007160, norm:0.5195, lr:2.9789e-04 dt: 3331.57ms, tok/sec:157369.88
Current shard: 3, Current position: 1507328
Current shard: 3, Current position: 1441792
Current shard: 3, Current position: 1638400
Current shard: 3, Current position: 1572864
Current shard: 3, Current position: 1703936
Current shard: 3, Current position: 1769472
Current shard: 3, Current position: 1900544
Current shard: 3, Current position: 1835008
step 574, loss: 5.001878, norm:0.5652, lr:2.9788e-04 dt: 3331.44ms, tok/sec:157375.70
Current shard: 3, Current position: 2031616
Current shard: 3, Current position: 1966080
Current shard: 3, Current position: 2162688
Current shard: 3, Current position: 2097152
Current shard: 3, Current position: 2228224
Current shard: 3, Current position: 2293760
Current shard: 3, Current position: 2424832
Current shard: 3, Current position: 2359296
step 575, loss: 5.041937, norm:0.6016, lr:2.9787e-04 dt: 3331.73ms, tok/sec:157361.90
Current shard: 3, Current position: 2555904
Current shard: 3, Current position: 2490368
Current shard: 3, Current position: 2686976
Current shard: 3, Current position: 2621440
Current shard: 3, Current position: 2752512
Current shard: 3, Current position: 2818048
Current shard: 3, Current position: 2949120
Current shard: 3, Current position: 2883584
step 576, loss: 5.018994, norm:0.6340, lr:2.9787e-04 dt: 3331.34ms, tok/sec:157380.62
Current shard: 3, Current position: 3080192
Current shard: 3, Current position: 3014656
Current shard: 3, Current position: 3211264
Current shard: 3, Current position: 3145728
Current shard: 3, Current position: 3342336Current shard: 3, Current position: 3276800

Current shard: 3, Current position: 3473408
Current shard: 3, Current position: 3407872
step 577, loss: 5.063849, norm:0.6039, lr:2.9786e-04 dt: 3331.39ms, tok/sec:157378.18
Current shard: 3, Current position: 3604480
Current shard: 3, Current position: 3538944
Current shard: 3, Current position: 3735552
Current shard: 3, Current position: 3670016
Current shard: 3, Current position: 3866624
Current shard: 3, Current position: 3801088
Current shard: 3, Current position: 3997696
Current shard: 3, Current position: 3932160
step 578, loss: 4.991682, norm:0.5976, lr:2.9785e-04 dt: 3331.40ms, tok/sec:157377.58
Current shard: 3, Current position: 4128768
Current shard: 3, Current position: 4063232
Current shard: 3, Current position: 4259840
Current shard: 3, Current position: 4194304
Current shard: 3, Current position: 4390912Current shard: 3, Current position: 4325376

Current shard: 3, Current position: 4521984
Current shard: 3, Current position: 4456448
step 579, loss: 4.994843, norm:0.6873, lr:2.9784e-04 dt: 3331.23ms, tok/sec:157385.78
Current shard: 3, Current position: 4653056
Current shard: 3, Current position: 4587520
Current shard: 3, Current position: 4784128
Current shard: 3, Current position: 4718592
Current shard: 3, Current position: 4915200
Current shard: 3, Current position: 4849664
Current shard: 3, Current position: 5046272
Current shard: 3, Current position: 4980736
step 580, loss: 4.972700, norm:0.6845, lr:2.9784e-04 dt: 3331.32ms, tok/sec:157381.41
Current shard: 3, Current position: 5177344
Current shard: 3, Current position: 5111808
Current shard: 3, Current position: 5308416
Current shard: 3, Current position: 5242880
Current shard: 3, Current position: 5373952Current shard: 3, Current position: 5439488

Current shard: 3, Current position: 5570560
Current shard: 3, Current position: 5505024
step 581, loss: 4.916475, norm:0.8669, lr:2.9783e-04 dt: 3331.28ms, tok/sec:157383.54
Current shard: 3, Current position: 5701632
Current shard: 3, Current position: 5636096
Current shard: 3, Current position: 5832704
Current shard: 3, Current position: 5767168
Current shard: 3, Current position: 5898240Current shard: 3, Current position: 5963776

Current shard: 3, Current position: 6094848
Current shard: 3, Current position: 6029312
step 582, loss: 4.903442, norm:0.7725, lr:2.9782e-04 dt: 3331.38ms, tok/sec:157378.45
Current shard: 3, Current position: 6225920
Current shard: 3, Current position: 6160384
Current shard: 3, Current position: 6356992
Current shard: 3, Current position: 6291456
Current shard: 3, Current position: 6488064
Current shard: 3, Current position: 6422528
Current shard: 3, Current position: 6619136
Current shard: 3, Current position: 6553600
step 583, loss: 4.979410, norm:0.8880, lr:2.9781e-04 dt: 3331.48ms, tok/sec:157374.09
Current shard: 3, Current position: 6750208
Current shard: 3, Current position: 6684672
Current shard: 3, Current position: 6881280
Current shard: 3, Current position: 6815744
Current shard: 3, Current position: 6946816
Current shard: 3, Current position: 7012352
Current shard: 3, Current position: 7143424
Current shard: 3, Current position: 7077888
step 584, loss: 4.943623, norm:0.8365, lr:2.9781e-04 dt: 3331.23ms, tok/sec:157385.82
Current shard: 3, Current position: 7274496
Current shard: 3, Current position: 7208960
Current shard: 3, Current position: 7405568
Current shard: 3, Current position: 7340032
Current shard: 3, Current position: 7536640
Current shard: 3, Current position: 7471104
Current shard: 3, Current position: 7667712
Current shard: 3, Current position: 7602176
step 585, loss: 4.977241, norm:0.7487, lr:2.9780e-04 dt: 3331.48ms, tok/sec:157373.79
Current shard: 3, Current position: 7798784
Current shard: 3, Current position: 7733248
Current shard: 3, Current position: 7929856
Current shard: 3, Current position: 7864320
Current shard: 3, Current position: 8060928Current shard: 3, Current position: 7995392

Current shard: 3, Current position: 8192000
Current shard: 3, Current position: 8126464
step 586, loss: 4.940085, norm:0.6779, lr:2.9779e-04 dt: 3331.78ms, tok/sec:157359.57
Current shard: 3, Current position: 8323072
Current shard: 3, Current position: 8257536
Current shard: 3, Current position: 8454144
Current shard: 3, Current position: 8388608
Current shard: 3, Current position: 8519680
Current shard: 3, Current position: 8585216
Current shard: 3, Current position: 8716288
Current shard: 3, Current position: 8650752
step 587, loss: 4.998534, norm:0.7040, lr:2.9778e-04 dt: 3331.52ms, tok/sec:157371.97
Current shard: 3, Current position: 8847360
Current shard: 3, Current position: 8781824
Current shard: 3, Current position: 8978432
Current shard: 3, Current position: 8912896
Current shard: 3, Current position: 9043968
Current shard: 3, Current position: 9109504
Current shard: 3, Current position: 9240576
Current shard: 3, Current position: 9175040
step 588, loss: 5.013852, norm:0.6902, lr:2.9778e-04 dt: 3331.28ms, tok/sec:157383.56
Current shard: 3, Current position: 9371648
Current shard: 3, Current position: 9306112
Current shard: 3, Current position: 9502720
Current shard: 3, Current position: 9437184
Current shard: 3, Current position: 9633792
Current shard: 3, Current position: 9568256
Current shard: 3, Current position: 9764864
Current shard: 3, Current position: 9699328
step 589, loss: 4.967526, norm:0.6057, lr:2.9777e-04 dt: 3331.38ms, tok/sec:157378.44
Current shard: 3, Current position: 9895936
Current shard: 3, Current position: 9830400
Current shard: 3, Current position: 10027008
Current shard: 3, Current position: 9961472
Current shard: 3, Current position: 10158080Current shard: 3, Current position: 10092544

Current shard: 3, Current position: 10289152
Current shard: 3, Current position: 10223616
step 590, loss: 5.007063, norm:0.6115, lr:2.9776e-04 dt: 3331.18ms, tok/sec:157387.89
Current shard: 3, Current position: 10420224
Current shard: 3, Current position: 10354688
Current shard: 3, Current position: 10551296
Current shard: 3, Current position: 10485760
Current shard: 3, Current position: 10616832
Current shard: 3, Current position: 10682368
Current shard: 3, Current position: 10813440
Current shard: 3, Current position: 10747904
step 591, loss: 4.896326, norm:0.7216, lr:2.9775e-04 dt: 3331.30ms, tok/sec:157382.33
Current shard: 3, Current position: 10944512
Current shard: 3, Current position: 10878976
Current shard: 3, Current position: 11075584
Current shard: 3, Current position: 11010048
Current shard: 3, Current position: 11141120Current shard: 3, Current position: 11206656

Current shard: 3, Current position: 11337728
Current shard: 3, Current position: 11272192
step 592, loss: 4.809812, norm:0.8847, lr:2.9775e-04 dt: 3331.48ms, tok/sec:157373.72
Current shard: 3, Current position: 11468800
Current shard: 3, Current position: 11403264
Current shard: 3, Current position: 11599872
Current shard: 3, Current position: 11534336
Current shard: 3, Current position: 11665408Current shard: 3, Current position: 11730944

Current shard: 3, Current position: 11862016
Current shard: 3, Current position: 11796480
step 593, loss: 4.884860, norm:0.9095, lr:2.9774e-04 dt: 3331.30ms, tok/sec:157382.50
Current shard: 3, Current position: 11993088
Current shard: 3, Current position: 11927552
Current shard: 3, Current position: 12124160
Current shard: 3, Current position: 12058624
Current shard: 3, Current position: 12189696Current shard: 3, Current position: 12255232

Current shard: 3, Current position: 12386304
Current shard: 3, Current position: 12320768
step 594, loss: 4.919297, norm:0.6893, lr:2.9773e-04 dt: 3331.45ms, tok/sec:157375.08
Current shard: 3, Current position: 12517376Current shard: 3, Current position: 12451840

Current shard: 3, Current position: 12648448
Current shard: 3, Current position: 12582912
Current shard: 3, Current position: 12713984
Current shard: 3, Current position: 12779520
Current shard: 3, Current position: 12910592
Current shard: 3, Current position: 12845056
step 595, loss: 4.844656, norm:0.6088, lr:2.9772e-04 dt: 3331.60ms, tok/sec:157368.03
Current shard: 3, Current position: 13041664
Current shard: 3, Current position: 12976128
Current shard: 3, Current position: 13172736
Current shard: 3, Current position: 13107200
Current shard: 3, Current position: 13238272Current shard: 3, Current position: 13303808

Current shard: 3, Current position: 13434880
Current shard: 3, Current position: 13369344
step 596, loss: 4.854270, norm:0.4882, lr:2.9771e-04 dt: 3331.51ms, tok/sec:157372.64
Current shard: 3, Current position: 13565952
Current shard: 3, Current position: 13500416
Current shard: 3, Current position: 13697024
Current shard: 3, Current position: 13631488
Current shard: 3, Current position: 13762560
Current shard: 3, Current position: 13828096
Current shard: 3, Current position: 13959168
Current shard: 3, Current position: 13893632
step 597, loss: 4.929154, norm:0.5261, lr:2.9771e-04 dt: 3331.29ms, tok/sec:157382.94
Current shard: 3, Current position: 14090240
Current shard: 3, Current position: 14024704
Current shard: 3, Current position: 14221312
Current shard: 3, Current position: 14155776
Current shard: 3, Current position: 14286848
Current shard: 3, Current position: 14352384
Current shard: 3, Current position: 14483456
Current shard: 3, Current position: 14417920
step 598, loss: 4.850090, norm:0.5027, lr:2.9770e-04 dt: 3331.32ms, tok/sec:157381.50
Current shard: 3, Current position: 14614528
Current shard: 3, Current position: 14548992
Current shard: 3, Current position: 14745600
Current shard: 3, Current position: 14680064
Current shard: 3, Current position: 14811136Current shard: 3, Current position: 14876672

Current shard: 3, Current position: 15007744
Current shard: 3, Current position: 14942208
step 599, loss: 4.861082, norm:0.5932, lr:2.9769e-04 dt: 3331.43ms, tok/sec:157376.48
Current shard: 0, Current position: 131072
Current shard: 0, Current position: 196608
Current shard: 0, Current position: 262144
Current shard: 0, Current position: 327680
Current shard: 0, Current position: 393216
Current shard: 0, Current position: 458752
Current shard: 0, Current position: 524288
Current shard: 0, Current position: 589824
Current shard: 0, Current position: 655360
Current shard: 0, Current position: 720896
Current shard: 0, Current position: 786432
Current shard: 0, Current position: 851968
Current shard: 0, Current position: 917504
Current shard: 0, Current position: 983040
Current shard: 0, Current position: 1048576
Current shard: 0, Current position: 1114112
Current shard: 0, Current position: 1179648
Current shard: 0, Current position: 1245184
Current shard: 0, Current position: 1310720
Current shard: 0, Current position: 1376256
Current shard: 0, Current position: 1507328
Current shard: 0, Current position: 1441792
Current shard: 0, Current position: 1638400
Current shard: 0, Current position: 1572864
Current shard: 0, Current position: 1769472
Current shard: 0, Current position: 1703936
Current shard: 0, Current position: 1900544
Current shard: 0, Current position: 1835008
Current shard: 0, Current position: 2031616
Current shard: 0, Current position: 1966080
Current shard: 0, Current position: 2162688
Current shard: 0, Current position: 2097152
Current shard: 0, Current position: 2293760
Current shard: 0, Current position: 2228224
Current shard: 0, Current position: 2424832
Current shard: 0, Current position: 2359296
Current shard: 0, Current position: 2555904
Current shard: 0, Current position: 2490368
Current shard: 0, Current position: 2686976
Current shard: 0, Current position: 2621440
validation loss: 5.0052
HellaSwag accuracy:9101989057875473/-2=-4550994528937736.0000
rank 1 sample 0: Hello, I'm a language model, as it's a form of a certain type of an area between the same class. It was originally written, and it
rank 1 sample 1: Hello, I'm a language model, my own language. I am writing your languages, and as a web in this: from my paper.
I am
rank 1 sample 2: Hello, I'm a language model, it in this process.
I have a reference method, however, and I have a function for this method in their
rank 1 sample 3: Hello, I'm a language model, I get a computer that the I was so I got two weeks and I see it over every year. I can use
rank 0 sample 0: Hello, I'm a language model, I’m going to know my job by asking me about her that it’s been. I’
rank 0 sample 1: Hello, I'm a language model, and in the world of the same idea of my hand with the world from this way, but the people were pretty good
rank 0 sample 2: Hello, I'm a language model, and I may use many problems in some way:
- Do the words in the class of the class,
-
rank 0 sample 3: Hello, I'm a language model, you were able to use the word code to tell you the word that word you created a particular word:
You cannot
Current shard: 3, Current position: 15138816
Current shard: 3, Current position: 15073280
Current shard: 3, Current position: 15269888
Current shard: 3, Current position: 15204352
Current shard: 3, Current position: 15335424
Current shard: 3, Current position: 15400960
Current shard: 3, Current position: 15532032
Current shard: 3, Current position: 15466496
step 600, loss: 4.815038, norm:0.6572, lr:2.9768e-04 dt: 54407.23ms, tok/sec:9636.37
Current shard: 3, Current position: 15663104
Current shard: 3, Current position: 15597568
Current shard: 3, Current position: 15794176
Current shard: 3, Current position: 15728640
Current shard: 3, Current position: 15925248
Current shard: 3, Current position: 15859712
Current shard: 3, Current position: 16056320
Current shard: 3, Current position: 15990784
step 601, loss: 4.814140, norm:0.6516, lr:2.9768e-04 dt: 3331.50ms, tok/sec:157372.83
Current shard: 3, Current position: 16187392
Current shard: 3, Current position: 16121856
Current shard: 3, Current position: 16318464
Current shard: 3, Current position: 16252928
Current shard: 3, Current position: 16384000
Current shard: 3, Current position: 16449536
Current shard: 3, Current position: 16580608
Current shard: 3, Current position: 16515072
step 602, loss: 4.877075, norm:0.5810, lr:2.9767e-04 dt: 3331.41ms, tok/sec:157377.22
Current shard: 3, Current position: 16711680
Current shard: 3, Current position: 16646144
Current shard: 3, Current position: 16842752
Current shard: 3, Current position: 16777216
Current shard: 3, Current position: 16908288
Current shard: 3, Current position: 16973824
Current shard: 3, Current position: 17104896
Current shard: 3, Current position: 17039360
step 603, loss: 4.967270, norm:0.5842, lr:2.9766e-04 dt: 3331.82ms, tok/sec:157357.81
Current shard: 3, Current position: 17235968
Current shard: 3, Current position: 17170432
Current shard: 3, Current position: 17367040
Current shard: 3, Current position: 17301504
Current shard: 3, Current position: 17432576Current shard: 3, Current position: 17498112

Current shard: 3, Current position: 17629184
Current shard: 3, Current position: 17563648
step 604, loss: 5.035462, norm:0.5772, lr:2.9765e-04 dt: 3331.25ms, tok/sec:157384.94
Current shard: 3, Current position: 17760256
Current shard: 3, Current position: 17694720
Current shard: 3, Current position: 17891328
Current shard: 3, Current position: 17825792
Current shard: 3, Current position: 17956864
Current shard: 3, Current position: 18022400
Current shard: 3, Current position: 18153472
Current shard: 3, Current position: 18087936
step 605, loss: 4.990754, norm:0.7383, lr:2.9764e-04 dt: 3331.27ms, tok/sec:157383.77
Current shard: 3, Current position: 18284544
Current shard: 3, Current position: 18219008
Current shard: 3, Current position: 18415616
Current shard: 3, Current position: 18350080
Current shard: 3, Current position: 18546688Current shard: 3, Current position: 18481152

Current shard: 3, Current position: 18677760
Current shard: 3, Current position: 18612224
step 606, loss: 5.016072, norm:0.7410, lr:2.9764e-04 dt: 3331.26ms, tok/sec:157384.49
Current shard: 3, Current position: 18808832
Current shard: 3, Current position: 18743296
Current shard: 3, Current position: 18939904
Current shard: 3, Current position: 18874368
Current shard: 3, Current position: 19005440
Current shard: 3, Current position: 19070976
Current shard: 3, Current position: 19202048
Current shard: 3, Current position: 19136512
step 607, loss: 4.976140, norm:0.8540, lr:2.9763e-04 dt: 3331.58ms, tok/sec:157369.13
Current shard: 3, Current position: 19333120
Current shard: 3, Current position: 19267584
Current shard: 3, Current position: 19464192
Current shard: 3, Current position: 19398656
Current shard: 3, Current position: 19595264Current shard: 3, Current position: 19529728

Current shard: 3, Current position: 19726336
Current shard: 3, Current position: 19660800
step 608, loss: 4.998727, norm:0.7317, lr:2.9762e-04 dt: 3331.38ms, tok/sec:157378.42
Current shard: 3, Current position: 19857408
Current shard: 3, Current position: 19791872
Current shard: 3, Current position: 19988480
Current shard: 3, Current position: 19922944
Current shard: 3, Current position: 20054016
Current shard: 3, Current position: 20119552
Current shard: 3, Current position: 20250624
Current shard: 3, Current position: 20185088
step 609, loss: 4.985230, norm:0.6731, lr:2.9761e-04 dt: 3331.37ms, tok/sec:157378.99
Current shard: 3, Current position: 20381696
Current shard: 3, Current position: 20316160
Current shard: 3, Current position: 20512768
Current shard: 3, Current position: 20447232
Current shard: 3, Current position: 20578304
Current shard: 3, Current position: 20643840
Current shard: 3, Current position: 20774912
Current shard: 3, Current position: 20709376
step 610, loss: 4.996504, norm:0.6456, lr:2.9760e-04 dt: 3331.66ms, tok/sec:157365.32
Current shard: 3, Current position: 20840448
Current shard: 3, Current position: 20905984
Current shard: 3, Current position: 20971520
Current shard: 3, Current position: 21037056
Current shard: 3, Current position: 21168128Current shard: 3, Current position: 21102592

Current shard: 3, Current position: 21299200
Current shard: 3, Current position: 21233664
step 611, loss: 4.977243, norm:0.5351, lr:2.9760e-04 dt: 3331.82ms, tok/sec:157357.76
Current shard: 3, Current position: 21430272
Current shard: 3, Current position: 21364736
Current shard: 3, Current position: 21561344
Current shard: 3, Current position: 21495808
Current shard: 3, Current position: 21692416
Current shard: 3, Current position: 21626880
Current shard: 3, Current position: 21823488
Current shard: 3, Current position: 21757952
step 612, loss: 4.977715, norm:0.6316, lr:2.9759e-04 dt: 3331.46ms, tok/sec:157374.97
Current shard: 3, Current position: 21954560
Current shard: 3, Current position: 21889024
Current shard: 3, Current position: 22085632
Current shard: 3, Current position: 22020096
Current shard: 3, Current position: 22216704
Current shard: 3, Current position: 22151168
Current shard: 3, Current position: 22347776
Current shard: 3, Current position: 22282240
step 613, loss: 4.984802, norm:0.5944, lr:2.9758e-04 dt: 3331.55ms, tok/sec:157370.59
Current shard: 3, Current position: 22478848
Current shard: 3, Current position: 22413312
Current shard: 3, Current position: 22609920
Current shard: 3, Current position: 22544384
Current shard: 3, Current position: 22675456
Current shard: 3, Current position: 22740992
Current shard: 3, Current position: 22872064
Current shard: 3, Current position: 22806528
step 614, loss: 4.992290, norm:0.5322, lr:2.9757e-04 dt: 3331.39ms, tok/sec:157378.03
Current shard: 3, Current position: 23003136
Current shard: 3, Current position: 22937600
Current shard: 3, Current position: 23134208
Current shard: 3, Current position: 23068672
Current shard: 3, Current position: 23199744
Current shard: 3, Current position: 23265280
Current shard: 3, Current position: 23396352
Current shard: 3, Current position: 23330816
step 615, loss: 4.943441, norm:0.5444, lr:2.9756e-04 dt: 3331.37ms, tok/sec:157379.15
Current shard: 3, Current position: 23527424
Current shard: 3, Current position: 23461888
Current shard: 3, Current position: 23658496
Current shard: 3, Current position: 23592960
Current shard: 3, Current position: 23789568Current shard: 3, Current position: 23724032

Current shard: 3, Current position: 23920640
Current shard: 3, Current position: 23855104
step 616, loss: 4.957253, norm:0.6077, lr:2.9756e-04 dt: 3331.47ms, tok/sec:157374.38
Current shard: 3, Current position: 24051712
Current shard: 3, Current position: 23986176
Current shard: 3, Current position: 24182784
Current shard: 3, Current position: 24117248
Current shard: 3, Current position: 24248320Current shard: 3, Current position: 24313856

Current shard: 3, Current position: 24444928
Current shard: 3, Current position: 24379392
step 617, loss: 4.915979, norm:0.6415, lr:2.9755e-04 dt: 3331.21ms, tok/sec:157386.89
Current shard: 3, Current position: 24576000
Current shard: 3, Current position: 24510464
Current shard: 3, Current position: 24707072
Current shard: 3, Current position: 24641536
Current shard: 3, Current position: 24838144
Current shard: 3, Current position: 24772608
Current shard: 3, Current position: 24969216
Current shard: 3, Current position: 24903680
step 618, loss: 4.949966, norm:0.6797, lr:2.9754e-04 dt: 3331.20ms, tok/sec:157387.02
Current shard: 3, Current position: 25100288
Current shard: 3, Current position: 25034752
Current shard: 3, Current position: 25231360
Current shard: 3, Current position: 25165824
Current shard: 3, Current position: 25296896Current shard: 3, Current position: 25362432

Current shard: 3, Current position: 25493504
Current shard: 3, Current position: 25427968
step 619, loss: 4.961337, norm:0.6497, lr:2.9753e-04 dt: 3331.58ms, tok/sec:157369.11
Current shard: 3, Current position: 25624576
Current shard: 3, Current position: 25559040
Current shard: 3, Current position: 25755648
Current shard: 3, Current position: 25690112
Current shard: 3, Current position: 25821184Current shard: 3, Current position: 25886720

Current shard: 3, Current position: 26017792
Current shard: 3, Current position: 25952256
step 620, loss: 4.922946, norm:0.6507, lr:2.9752e-04 dt: 3331.58ms, tok/sec:157369.38
Current shard: 3, Current position: 26148864
Current shard: 3, Current position: 26083328
Current shard: 3, Current position: 26279936
Current shard: 3, Current position: 26214400
Current shard: 3, Current position: 26411008
Current shard: 3, Current position: 26345472
Current shard: 3, Current position: 26542080
Current shard: 3, Current position: 26476544
step 621, loss: 4.946862, norm:0.5281, lr:2.9752e-04 dt: 3331.50ms, tok/sec:157372.93
Current shard: 3, Current position: 26673152
Current shard: 3, Current position: 26607616
Current shard: 3, Current position: 26804224
Current shard: 3, Current position: 26738688
Current shard: 3, Current position: 26869760
Current shard: 3, Current position: 26935296
Current shard: 3, Current position: 27066368
Current shard: 3, Current position: 27000832
step 622, loss: 4.961048, norm:0.5772, lr:2.9751e-04 dt: 3331.30ms, tok/sec:157382.27
Current shard: 3, Current position: 27197440
Current shard: 3, Current position: 27131904
Current shard: 3, Current position: 27328512
Current shard: 3, Current position: 27262976
Current shard: 3, Current position: 27394048
Current shard: 3, Current position: 27459584
Current shard: 3, Current position: 27590656
Current shard: 3, Current position: 27525120
step 623, loss: 4.916732, norm:0.6948, lr:2.9750e-04 dt: 3331.40ms, tok/sec:157377.90
Current shard: 3, Current position: 27721728
Current shard: 3, Current position: 27656192
Current shard: 3, Current position: 27852800
Current shard: 3, Current position: 27787264
Current shard: 3, Current position: 27918336
Current shard: 3, Current position: 27983872
Current shard: 3, Current position: 28114944
Current shard: 3, Current position: 28049408
step 624, loss: 4.935624, norm:0.8140, lr:2.9749e-04 dt: 3331.32ms, tok/sec:157381.57
Current shard: 3, Current position: 28246016
Current shard: 3, Current position: 28180480
Current shard: 3, Current position: 28377088
Current shard: 3, Current position: 28311552
Current shard: 3, Current position: 28442624Current shard: 3, Current position: 28508160

Current shard: 3, Current position: 28639232
Current shard: 3, Current position: 28573696
step 625, loss: 4.963470, norm:0.7270, lr:2.9748e-04 dt: 3331.31ms, tok/sec:157381.74
Current shard: 3, Current position: 28770304
Current shard: 3, Current position: 28704768
Current shard: 3, Current position: 28901376
Current shard: 3, Current position: 28835840
Current shard: 3, Current position: 29032448
Current shard: 3, Current position: 28966912
Current shard: 3, Current position: 29163520
Current shard: 3, Current position: 29097984
step 626, loss: 4.895422, norm:0.6076, lr:2.9747e-04 dt: 3331.47ms, tok/sec:157374.53
Current shard: 3, Current position: 29294592
Current shard: 3, Current position: 29229056
Current shard: 3, Current position: 29425664
Current shard: 3, Current position: 29360128
Current shard: 3, Current position: 29556736Current shard: 3, Current position: 29491200

Current shard: 3, Current position: 29687808
Current shard: 3, Current position: 29622272
step 627, loss: 4.903399, norm:0.6166, lr:2.9747e-04 dt: 3331.28ms, tok/sec:157383.12
Current shard: 3, Current position: 29818880
Current shard: 3, Current position: 29753344
Current shard: 3, Current position: 29949952
Current shard: 3, Current position: 29884416
Current shard: 3, Current position: 30015488
Current shard: 3, Current position: 30081024
Current shard: 3, Current position: 30212096
Current shard: 3, Current position: 30146560
step 628, loss: 4.821737, norm:0.6417, lr:2.9746e-04 dt: 3331.25ms, tok/sec:157384.67
Current shard: 3, Current position: 30343168
Current shard: 3, Current position: 30277632
Current shard: 3, Current position: 30474240
Current shard: 3, Current position: 30408704
Current shard: 3, Current position: 30605312
Current shard: 3, Current position: 30539776
Current shard: 3, Current position: 30736384
Current shard: 3, Current position: 30670848
step 629, loss: 4.898139, norm:0.6334, lr:2.9745e-04 dt: 3331.62ms, tok/sec:157367.09
Current shard: 3, Current position: 30867456
Current shard: 3, Current position: 30801920
Current shard: 3, Current position: 30998528
Current shard: 3, Current position: 30932992
Current shard: 3, Current position: 31064064
Current shard: 3, Current position: 31129600
Current shard: 3, Current position: 31260672
Current shard: 3, Current position: 31195136
step 630, loss: 4.856577, norm:0.5355, lr:2.9744e-04 dt: 3331.24ms, tok/sec:157385.36
Current shard: 3, Current position: 31391744
Current shard: 3, Current position: 31326208
Current shard: 3, Current position: 31522816
Current shard: 3, Current position: 31457280
Current shard: 3, Current position: 31653888Current shard: 3, Current position: 31588352

Current shard: 3, Current position: 31784960
Current shard: 3, Current position: 31719424
step 631, loss: 4.869367, norm:0.5534, lr:2.9743e-04 dt: 3331.75ms, tok/sec:157361.02
Current shard: 3, Current position: 31916032
Current shard: 3, Current position: 31850496
Current shard: 3, Current position: 32047104
Current shard: 3, Current position: 31981568
Current shard: 3, Current position: 32178176
Current shard: 3, Current position: 32112640
Current shard: 3, Current position: 32309248
Current shard: 3, Current position: 32243712
step 632, loss: 4.889036, norm:0.5909, lr:2.9743e-04 dt: 3331.57ms, tok/sec:157369.81
Current shard: 3, Current position: 32440320
Current shard: 3, Current position: 32374784
Current shard: 3, Current position: 32571392
Current shard: 3, Current position: 32505856
Current shard: 3, Current position: 32636928
Current shard: 3, Current position: 32702464
Current shard: 3, Current position: 32833536
Current shard: 3, Current position: 32768000
step 633, loss: 4.866476, norm:0.6755, lr:2.9742e-04 dt: 3331.49ms, tok/sec:157373.65
Current shard: 3, Current position: 32964608
Current shard: 3, Current position: 32899072
Current shard: 3, Current position: 33095680
Current shard: 3, Current position: 33030144
Current shard: 3, Current position: 33226752Current shard: 3, Current position: 33161216

Current shard: 3, Current position: 33357824
Current shard: 3, Current position: 33292288
step 634, loss: 4.916632, norm:0.6092, lr:2.9741e-04 dt: 3331.44ms, tok/sec:157375.62
Current shard: 3, Current position: 33488896
Current shard: 3, Current position: 33423360
Current shard: 3, Current position: 33619968
Current shard: 3, Current position: 33554432
Current shard: 3, Current position: 33685504
Current shard: 3, Current position: 33751040
Current shard: 3, Current position: 33882112
Current shard: 3, Current position: 33816576
step 635, loss: 4.913239, norm:0.7037, lr:2.9740e-04 dt: 3331.53ms, tok/sec:157371.41
Current shard: 3, Current position: 34013184
Current shard: 3, Current position: 33947648
Current shard: 3, Current position: 34144256
Current shard: 3, Current position: 34078720
Current shard: 3, Current position: 34209792Current shard: 3, Current position: 34275328

Current shard: 3, Current position: 34406400
Current shard: 3, Current position: 34340864
step 636, loss: 5.003635, norm:0.8647, lr:2.9739e-04 dt: 3331.34ms, tok/sec:157380.33
Current shard: 3, Current position: 34537472
Current shard: 3, Current position: 34471936
Current shard: 3, Current position: 34668544
Current shard: 3, Current position: 34603008
Current shard: 3, Current position: 34734080Current shard: 3, Current position: 34799616

Current shard: 3, Current position: 34930688
Current shard: 3, Current position: 34865152
step 637, loss: 5.116506, norm:1.0788, lr:2.9738e-04 dt: 3331.26ms, tok/sec:157384.10
Current shard: 3, Current position: 35061760
Current shard: 3, Current position: 34996224
Current shard: 3, Current position: 35192832
Current shard: 3, Current position: 35127296
Current shard: 3, Current position: 35323904
Current shard: 3, Current position: 35258368
Current shard: 3, Current position: 35454976
Current shard: 3, Current position: 35389440
step 638, loss: 4.824320, norm:0.8840, lr:2.9738e-04 dt: 3331.62ms, tok/sec:157367.21
Current shard: 3, Current position: 35520512
Current shard: 3, Current position: 35586048
Current shard: 3, Current position: 35651584
Current shard: 3, Current position: 35717120
Current shard: 3, Current position: 35782656
Current shard: 3, Current position: 35848192
Current shard: 3, Current position: 35979264
Current shard: 3, Current position: 35913728
step 639, loss: 4.761075, norm:0.9898, lr:2.9737e-04 dt: 3331.59ms, tok/sec:157368.75
Current shard: 3, Current position: 36110336
Current shard: 3, Current position: 36044800
Current shard: 3, Current position: 36241408
Current shard: 3, Current position: 36175872
Current shard: 3, Current position: 36306944
Current shard: 3, Current position: 36372480
Current shard: 3, Current position: 36503552
Current shard: 3, Current position: 36438016
step 640, loss: 4.744359, norm:0.7816, lr:2.9736e-04 dt: 3331.23ms, tok/sec:157385.76
Current shard: 3, Current position: 36634624
Current shard: 3, Current position: 36569088
Current shard: 3, Current position: 36765696
Current shard: 3, Current position: 36700160
Current shard: 3, Current position: 36896768
Current shard: 3, Current position: 36831232
Current shard: 3, Current position: 37027840
Current shard: 3, Current position: 36962304
step 641, loss: 4.835678, norm:0.7588, lr:2.9735e-04 dt: 3331.40ms, tok/sec:157377.49
Current shard: 3, Current position: 37158912
Current shard: 3, Current position: 37093376
Current shard: 3, Current position: 37289984
Current shard: 3, Current position: 37224448
Current shard: 3, Current position: 37355520Current shard: 3, Current position: 37421056

Current shard: 3, Current position: 37552128
Current shard: 3, Current position: 37486592
step 642, loss: 4.823828, norm:0.6927, lr:2.9734e-04 dt: 3331.37ms, tok/sec:157379.29
Current shard: 3, Current position: 37683200
Current shard: 3, Current position: 37617664
Current shard: 3, Current position: 37814272
Current shard: 3, Current position: 37748736
Current shard: 3, Current position: 37879808
Current shard: 3, Current position: 37945344
Current shard: 3, Current position: 38076416
Current shard: 3, Current position: 38010880
step 643, loss: 4.784814, norm:0.5720, lr:2.9733e-04 dt: 3331.23ms, tok/sec:157385.73
Current shard: 3, Current position: 38207488
Current shard: 3, Current position: 38141952
Current shard: 3, Current position: 38338560
Current shard: 3, Current position: 38273024
Current shard: 3, Current position: 38404096
Current shard: 3, Current position: 38469632
Current shard: 3, Current position: 38600704
Current shard: 3, Current position: 38535168
step 644, loss: 4.775385, norm:0.6185, lr:2.9733e-04 dt: 3331.47ms, tok/sec:157374.25
Current shard: 3, Current position: 38731776
Current shard: 3, Current position: 38666240
Current shard: 3, Current position: 38862848
Current shard: 3, Current position: 38797312
Current shard: 3, Current position: 38993920
Current shard: 3, Current position: 38928384
Current shard: 3, Current position: 39124992
Current shard: 3, Current position: 39059456
step 645, loss: 4.758623, norm:0.7007, lr:2.9732e-04 dt: 3331.24ms, tok/sec:157385.12
Current shard: 3, Current position: 39256064
Current shard: 3, Current position: 39190528
Current shard: 3, Current position: 39387136
Current shard: 3, Current position: 39321600
Current shard: 3, Current position: 39452672Current shard: 3, Current position: 39518208

Current shard: 3, Current position: 39649280
Current shard: 3, Current position: 39583744
step 646, loss: 4.773947, norm:0.6270, lr:2.9731e-04 dt: 3331.14ms, tok/sec:157390.10
Current shard: 3, Current position: 39780352
Current shard: 3, Current position: 39714816
Current shard: 3, Current position: 39911424
Current shard: 3, Current position: 39845888
Current shard: 3, Current position: 40042496Current shard: 3, Current position: 39976960

Current shard: 3, Current position: 40173568
Current shard: 3, Current position: 40108032
step 647, loss: 4.813510, norm:0.5388, lr:2.9730e-04 dt: 3331.37ms, tok/sec:157379.26
Current shard: 3, Current position: 40304640
Current shard: 3, Current position: 40239104
Current shard: 3, Current position: 40435712
Current shard: 3, Current position: 40370176
Current shard: 3, Current position: 40501248
Current shard: 3, Current position: 40566784
Current shard: 3, Current position: 40697856
Current shard: 3, Current position: 40632320
step 648, loss: 4.790602, norm:0.5878, lr:2.9729e-04 dt: 3331.63ms, tok/sec:157366.61
Current shard: 3, Current position: 40828928
Current shard: 3, Current position: 40763392
Current shard: 3, Current position: 40960000
Current shard: 3, Current position: 40894464
Current shard: 3, Current position: 41091072
Current shard: 3, Current position: 41025536
Current shard: 3, Current position: 41222144
Current shard: 3, Current position: 41156608
step 649, loss: 4.763936, norm:0.6549, lr:2.9728e-04 dt: 3331.43ms, tok/sec:157376.34
HellaSwag accuracy:14168537028054537/-2=-7084268514027268.0000
rank 1 sample 0: Hello, I'm a language model, in fact, I’m never seen my experience or interest in my students. But if a student was a student
rank 1 sample 1: Hello, I'm a language model, my own language isn’t necessarily something that doesn’t work out it through the conversation, but I�
rank 1 sample 2: Hello, I'm a language model, but did I'm looking at the same time. I had any of a family of my own name in the English.
rank 1 sample 3: Hello, I'm a language model, I guess it might have made it that could be written so: we’re pretty confident to read, as we
rank 0 sample 0: Hello, I'm a language model, I'm really not sure the right language-related.
You want to use the same same-world.
WeCurrent shard: 3, Current position: 41353216

rank 0 sample 1: Hello, I'm a language model, there is a lot of fun with no other things if you want to call out a word to the end of the day
rank 0 sample 2: Hello, I'm a language model, and I say I just have not heard of an actual language for it. I do know it, I think I love
rank 0 sample 3: Hello, I'm a language model, I see it as a computer, but you choose to see that as it means or you're referring to the other as
Current shard: 3, Current position: 41287680
Current shard: 3, Current position: 41484288
Current shard: 3, Current position: 41418752
Current shard: 3, Current position: 41615360
Current shard: 3, Current position: 41549824
Current shard: 3, Current position: 41746432
Current shard: 3, Current position: 41680896
step 650, loss: 4.940382, norm:0.6771, lr:2.9728e-04 dt: 48150.92ms, tok/sec:10888.43
Current shard: 3, Current position: 41877504
Current shard: 3, Current position: 41811968
Current shard: 3, Current position: 42008576
Current shard: 3, Current position: 41943040
Current shard: 3, Current position: 42074112
Current shard: 3, Current position: 42139648
Current shard: 3, Current position: 42270720
Current shard: 3, Current position: 42205184
step 651, loss: 5.005399, norm:0.7110, lr:2.9727e-04 dt: 3331.39ms, tok/sec:157378.26
Current shard: 3, Current position: 42401792
Current shard: 3, Current position: 42336256
Current shard: 3, Current position: 42532864
Current shard: 3, Current position: 42467328
Current shard: 3, Current position: 42598400
Current shard: 3, Current position: 42663936
Current shard: 3, Current position: 42795008
Current shard: 3, Current position: 42729472
step 652, loss: 4.926421, norm:0.6471, lr:2.9726e-04 dt: 3331.55ms, tok/sec:157370.54
Current shard: 3, Current position: 42926080
Current shard: 3, Current position: 42860544
Current shard: 3, Current position: 43057152
Current shard: 3, Current position: 42991616
Current shard: 3, Current position: 43188224Current shard: 3, Current position: 43122688

Current shard: 3, Current position: 43319296
Current shard: 3, Current position: 43253760
step 653, loss: 4.924548, norm:0.5519, lr:2.9725e-04 dt: 3331.37ms, tok/sec:157379.01
Current shard: 3, Current position: 43450368
Current shard: 3, Current position: 43384832
Current shard: 3, Current position: 43581440
Current shard: 3, Current position: 43515904
Current shard: 3, Current position: 43712512
Current shard: 3, Current position: 43646976
Current shard: 3, Current position: 43843584
Current shard: 3, Current position: 43778048
step 654, loss: 4.935251, norm:0.6109, lr:2.9724e-04 dt: 3331.31ms, tok/sec:157381.88
Current shard: 3, Current position: 43974656
Current shard: 3, Current position: 43909120
Current shard: 3, Current position: 44105728
Current shard: 3, Current position: 44040192
Current shard: 3, Current position: 44236800
Current shard: 3, Current position: 44171264
Current shard: 3, Current position: 44367872
Current shard: 3, Current position: 44302336
step 655, loss: 4.952879, norm:0.6424, lr:2.9723e-04 dt: 3331.53ms, tok/sec:157371.60
Current shard: 3, Current position: 44498944
Current shard: 3, Current position: 44433408
Current shard: 3, Current position: 44630016
Current shard: 3, Current position: 44564480
Current shard: 3, Current position: 44761088
Current shard: 3, Current position: 44695552
Current shard: 3, Current position: 44892160
Current shard: 3, Current position: 44826624
step 656, loss: 4.900323, norm:0.6133, lr:2.9722e-04 dt: 3331.54ms, tok/sec:157371.08
Current shard: 3, Current position: 45023232
Current shard: 3, Current position: 44957696
Current shard: 3, Current position: 45154304
Current shard: 3, Current position: 45088768
Current shard: 3, Current position: 45219840
Current shard: 3, Current position: 45285376
Current shard: 3, Current position: 45416448
Current shard: 3, Current position: 45350912
step 657, loss: 4.898680, norm:0.5161, lr:2.9722e-04 dt: 3331.26ms, tok/sec:157384.08
Current shard: 3, Current position: 45547520
Current shard: 3, Current position: 45481984
Current shard: 3, Current position: 45678592
Current shard: 3, Current position: 45613056
Current shard: 3, Current position: 45809664Current shard: 3, Current position: 45744128

Current shard: 3, Current position: 45940736
Current shard: 3, Current position: 45875200
step 658, loss: 4.892691, norm:0.4978, lr:2.9721e-04 dt: 3331.53ms, tok/sec:157371.44
Current shard: 3, Current position: 46071808
Current shard: 3, Current position: 46006272
Current shard: 3, Current position: 46202880
Current shard: 3, Current position: 46137344
Current shard: 3, Current position: 46268416
Current shard: 3, Current position: 46333952
Current shard: 3, Current position: 46465024
Current shard: 3, Current position: 46399488
step 659, loss: 4.946257, norm:0.5576, lr:2.9720e-04 dt: 3331.54ms, tok/sec:157371.09
Current shard: 3, Current position: 46530560
Current shard: 3, Current position: 46596096
Current shard: 3, Current position: 46661632
Current shard: 3, Current position: 46727168
Current shard: 3, Current position: 46858240
Current shard: 3, Current position: 46792704
Current shard: 3, Current position: 46989312
Current shard: 3, Current position: 46923776
step 660, loss: 4.898467, norm:0.5696, lr:2.9719e-04 dt: 3331.35ms, tok/sec:157380.27
Current shard: 3, Current position: 47120384
Current shard: 3, Current position: 47054848
Current shard: 3, Current position: 47251456
Current shard: 3, Current position: 47185920
Current shard: 3, Current position: 47316992Current shard: 3, Current position: 47382528

Current shard: 3, Current position: 47513600
Current shard: 3, Current position: 47448064
step 661, loss: 4.839265, norm:0.5811, lr:2.9718e-04 dt: 3331.22ms, tok/sec:157386.25
Current shard: 3, Current position: 47644672
Current shard: 3, Current position: 47579136
Current shard: 3, Current position: 47775744
Current shard: 3, Current position: 47710208
Current shard: 3, Current position: 47841280Current shard: 3, Current position: 47906816

Current shard: 3, Current position: 48037888
Current shard: 3, Current position: 47972352
step 662, loss: 4.843613, norm:0.7205, lr:2.9717e-04 dt: 3331.32ms, tok/sec:157381.40
Current shard: 3, Current position: 48168960
Current shard: 3, Current position: 48103424
Current shard: 3, Current position: 48300032
Current shard: 3, Current position: 48234496
Current shard: 3, Current position: 48365568Current shard: 3, Current position: 48431104

Current shard: 3, Current position: 48562176
Current shard: 3, Current position: 48496640
step 663, loss: 4.859292, norm:0.6774, lr:2.9716e-04 dt: 3331.24ms, tok/sec:157385.31
Current shard: 3, Current position: 48693248
Current shard: 3, Current position: 48627712
Current shard: 3, Current position: 48824320
Current shard: 3, Current position: 48758784
Current shard: 3, Current position: 48889856Current shard: 3, Current position: 48955392

Current shard: 3, Current position: 49086464
Current shard: 3, Current position: 49020928
step 664, loss: 4.815832, norm:0.5730, lr:2.9715e-04 dt: 3331.49ms, tok/sec:157373.54
Current shard: 3, Current position: 49217536
Current shard: 3, Current position: 49152000
Current shard: 3, Current position: 49348608
Current shard: 3, Current position: 49283072
Current shard: 3, Current position: 49414144
Current shard: 3, Current position: 49479680
Current shard: 3, Current position: 49610752
Current shard: 3, Current position: 49545216
step 665, loss: 4.859475, norm:0.5135, lr:2.9715e-04 dt: 3331.62ms, tok/sec:157367.49
Current shard: 3, Current position: 49741824
Current shard: 3, Current position: 49676288
Current shard: 3, Current position: 49872896
Current shard: 3, Current position: 49807360
Current shard: 3, Current position: 50003968
Current shard: 3, Current position: 49938432
Current shard: 3, Current position: 50135040
Current shard: 3, Current position: 50069504
step 666, loss: 4.839931, norm:0.5376, lr:2.9714e-04 dt: 3331.22ms, tok/sec:157385.98
Current shard: 3, Current position: 50266112
Current shard: 3, Current position: 50200576
Current shard: 3, Current position: 50397184
Current shard: 3, Current position: 50331648
Current shard: 3, Current position: 50462720
Current shard: 3, Current position: 50528256
Current shard: 3, Current position: 50659328
Current shard: 3, Current position: 50593792
step 667, loss: 4.818890, norm:0.5243, lr:2.9713e-04 dt: 3331.50ms, tok/sec:157373.01
Current shard: 3, Current position: 50790400
Current shard: 3, Current position: 50724864
Current shard: 3, Current position: 50921472
Current shard: 3, Current position: 50855936
Current shard: 3, Current position: 51052544
Current shard: 3, Current position: 50987008
Current shard: 3, Current position: 51183616
Current shard: 3, Current position: 51118080
step 668, loss: 4.850750, norm:0.6109, lr:2.9712e-04 dt: 3331.58ms, tok/sec:157369.37
Current shard: 3, Current position: 51314688
Current shard: 3, Current position: 51249152
Current shard: 3, Current position: 51445760
Current shard: 3, Current position: 51380224
Current shard: 3, Current position: 51511296Current shard: 3, Current position: 51576832

Current shard: 3, Current position: 51707904
Current shard: 3, Current position: 51642368
step 669, loss: 4.797224, norm:0.8487, lr:2.9711e-04 dt: 3331.63ms, tok/sec:157366.71
Current shard: 3, Current position: 51838976
Current shard: 3, Current position: 51773440
Current shard: 3, Current position: 51970048
Current shard: 3, Current position: 51904512
Current shard: 3, Current position: 52035584
Current shard: 3, Current position: 52101120
Current shard: 3, Current position: 52232192
Current shard: 3, Current position: 52166656
step 670, loss: 4.833776, norm:0.9563, lr:2.9710e-04 dt: 3331.35ms, tok/sec:157380.18
Current shard: 3, Current position: 52363264
Current shard: 3, Current position: 52297728
Current shard: 3, Current position: 52494336
Current shard: 3, Current position: 52428800
Current shard: 3, Current position: 52625408Current shard: 3, Current position: 52559872

Current shard: 3, Current position: 52756480
Current shard: 3, Current position: 52690944
step 671, loss: 4.844962, norm:0.8103, lr:2.9709e-04 dt: 3331.42ms, tok/sec:157376.76
Current shard: 3, Current position: 52887552
Current shard: 3, Current position: 52822016
Current shard: 3, Current position: 53018624
Current shard: 3, Current position: 52953088
Current shard: 3, Current position: 53149696
Current shard: 3, Current position: 53084160
Current shard: 3, Current position: 53280768
Current shard: 3, Current position: 53215232
step 672, loss: 4.825612, norm:0.7352, lr:2.9709e-04 dt: 3331.46ms, tok/sec:157374.89
Current shard: 3, Current position: 53411840
Current shard: 3, Current position: 53346304
Current shard: 3, Current position: 53542912
Current shard: 3, Current position: 53477376
Current shard: 3, Current position: 53608448
Current shard: 3, Current position: 53673984
Current shard: 3, Current position: 53805056
Current shard: 3, Current position: 53739520
step 673, loss: 4.771774, norm:0.6803, lr:2.9708e-04 dt: 3331.28ms, tok/sec:157383.55
Current shard: 3, Current position: 53936128
Current shard: 3, Current position: 53870592
Current shard: 3, Current position: 54067200
Current shard: 3, Current position: 54001664
Current shard: 3, Current position: 54132736
Current shard: 3, Current position: 54198272
Current shard: 3, Current position: 54329344
Current shard: 3, Current position: 54263808
step 674, loss: 4.784962, norm:0.6387, lr:2.9707e-04 dt: 3331.38ms, tok/sec:157378.73
Current shard: 3, Current position: 54460416
Current shard: 3, Current position: 54394880
Current shard: 3, Current position: 54591488
Current shard: 3, Current position: 54525952
Current shard: 3, Current position: 54657024
Current shard: 3, Current position: 54722560
Current shard: 3, Current position: 54853632
Current shard: 3, Current position: 54788096
step 675, loss: 4.815722, norm:0.6466, lr:2.9706e-04 dt: 3331.39ms, tok/sec:157377.93
Current shard: 3, Current position: 54984704
Current shard: 3, Current position: 54919168
Current shard: 3, Current position: 55115776
Current shard: 3, Current position: 55050240
Current shard: 3, Current position: 55181312
Current shard: 3, Current position: 55246848
Current shard: 3, Current position: 55377920
Current shard: 3, Current position: 55312384
step 676, loss: 4.807648, norm:0.5733, lr:2.9705e-04 dt: 3331.36ms, tok/sec:157379.63
Current shard: 3, Current position: 55508992
Current shard: 3, Current position: 55443456
Current shard: 3, Current position: 55640064
Current shard: 3, Current position: 55574528
Current shard: 3, Current position: 55705600
Current shard: 3, Current position: 55771136
Current shard: 3, Current position: 55902208
Current shard: 3, Current position: 55836672
step 677, loss: 4.841249, norm:0.5879, lr:2.9704e-04 dt: 3331.49ms, tok/sec:157373.28
Current shard: 3, Current position: 56033280
Current shard: 3, Current position: 55967744
Current shard: 3, Current position: 56164352
Current shard: 3, Current position: 56098816
Current shard: 3, Current position: 56295424
Current shard: 3, Current position: 56229888
Current shard: 3, Current position: 56426496
Current shard: 3, Current position: 56360960
step 678, loss: 4.750132, norm:0.6205, lr:2.9703e-04 dt: 3331.49ms, tok/sec:157373.22
Current shard: 3, Current position: 56557568
Current shard: 3, Current position: 56492032
Current shard: 3, Current position: 56688640
Current shard: 3, Current position: 56623104
Current shard: 3, Current position: 56754176
Current shard: 3, Current position: 56819712
Current shard: 3, Current position: 56950784
Current shard: 3, Current position: 56885248
step 679, loss: 4.801732, norm:0.7023, lr:2.9702e-04 dt: 3331.66ms, tok/sec:157365.32
Current shard: 3, Current position: 57081856
Current shard: 3, Current position: 57016320
Current shard: 3, Current position: 57212928
Current shard: 3, Current position: 57147392
Current shard: 3, Current position: 57344000
Current shard: 3, Current position: 57278464
Current shard: 3, Current position: 57475072
Current shard: 3, Current position: 57409536
step 680, loss: 4.762788, norm:0.6154, lr:2.9701e-04 dt: 3331.50ms, tok/sec:157372.77
Current shard: 3, Current position: 57606144
Current shard: 3, Current position: 57540608
Current shard: 3, Current position: 57737216
Current shard: 3, Current position: 57671680
Current shard: 3, Current position: 57868288
Current shard: 3, Current position: 57802752
Current shard: 3, Current position: 57999360
Current shard: 3, Current position: 57933824
step 681, loss: 4.820860, norm:0.4974, lr:2.9701e-04 dt: 3331.38ms, tok/sec:157378.44
Current shard: 3, Current position: 58130432
Current shard: 3, Current position: 58064896
Current shard: 3, Current position: 58261504
Current shard: 3, Current position: 58195968
Current shard: 3, Current position: 58327040
Current shard: 3, Current position: 58392576
Current shard: 3, Current position: 58523648
Current shard: 3, Current position: 58458112
step 682, loss: 4.770127, norm:0.5224, lr:2.9700e-04 dt: 3331.43ms, tok/sec:157376.14
Current shard: 3, Current position: 58654720
Current shard: 3, Current position: 58589184
Current shard: 3, Current position: 58785792
Current shard: 3, Current position: 58720256
Current shard: 3, Current position: 58851328
Current shard: 3, Current position: 58916864
Current shard: 3, Current position: 59047936
Current shard: 3, Current position: 58982400
step 683, loss: 4.805100, norm:0.6949, lr:2.9699e-04 dt: 3331.48ms, tok/sec:157373.80
Current shard: 3, Current position: 59179008
Current shard: 3, Current position: 59113472
Current shard: 3, Current position: 59310080
Current shard: 3, Current position: 59244544
Current shard: 3, Current position: 59375616
Current shard: 3, Current position: 59441152
Current shard: 3, Current position: 59572224
Current shard: 3, Current position: 59506688
step 684, loss: 4.692278, norm:0.7129, lr:2.9698e-04 dt: 3331.42ms, tok/sec:157376.55
Current shard: 3, Current position: 59703296
Current shard: 3, Current position: 59637760
Current shard: 3, Current position: 59834368
Current shard: 3, Current position: 59768832
Current shard: 3, Current position: 59965440Current shard: 3, Current position: 59899904

Current shard: 3, Current position: 60096512
Current shard: 3, Current position: 60030976
step 685, loss: 4.680956, norm:0.6787, lr:2.9697e-04 dt: 3331.55ms, tok/sec:157370.60
Current shard: 3, Current position: 60227584
Current shard: 3, Current position: 60162048
Current shard: 3, Current position: 60358656
Current shard: 3, Current position: 60293120
Current shard: 3, Current position: 60489728
Current shard: 3, Current position: 60424192
Current shard: 3, Current position: 60620800
Current shard: 3, Current position: 60555264
step 686, loss: 4.711606, norm:0.6918, lr:2.9696e-04 dt: 3331.22ms, tok/sec:157386.28
Current shard: 3, Current position: 60751872
Current shard: 3, Current position: 60686336
Current shard: 3, Current position: 60882944
Current shard: 3, Current position: 60817408
Current shard: 3, Current position: 61014016
Current shard: 3, Current position: 60948480
Current shard: 3, Current position: 61145088
Current shard: 3, Current position: 61079552
step 687, loss: 4.700066, norm:0.6076, lr:2.9695e-04 dt: 3331.69ms, tok/sec:157364.01
Current shard: 3, Current position: 61210624
Current shard: 3, Current position: 61276160
Current shard: 3, Current position: 61341696
Current shard: 3, Current position: 61407232
Current shard: 3, Current position: 61472768
Current shard: 3, Current position: 61538304
Current shard: 3, Current position: 61669376
Current shard: 3, Current position: 61603840
step 688, loss: 4.689859, norm:0.6300, lr:2.9694e-04 dt: 3331.62ms, tok/sec:157367.40
Current shard: 3, Current position: 61800448
Current shard: 3, Current position: 61734912
Current shard: 3, Current position: 61931520
Current shard: 3, Current position: 61865984
Current shard: 3, Current position: 62062592
Current shard: 3, Current position: 61997056
Current shard: 3, Current position: 62193664
Current shard: 3, Current position: 62128128
step 689, loss: 4.685395, norm:0.5403, lr:2.9693e-04 dt: 3331.30ms, tok/sec:157382.35
Current shard: 3, Current position: 62324736
Current shard: 3, Current position: 62259200
Current shard: 3, Current position: 62455808
Current shard: 3, Current position: 62390272
Current shard: 3, Current position: 62521344
Current shard: 3, Current position: 62586880
Current shard: 3, Current position: 62717952
Current shard: 3, Current position: 62652416
step 690, loss: 4.708069, norm:0.5779, lr:2.9693e-04 dt: 3331.15ms, tok/sec:157389.69
Current shard: 3, Current position: 62849024
Current shard: 3, Current position: 62783488
Current shard: 3, Current position: 62980096
Current shard: 3, Current position: 62914560
Current shard: 3, Current position: 63045632
Current shard: 3, Current position: 63111168
Current shard: 3, Current position: 63242240
Current shard: 3, Current position: 63176704
step 691, loss: 4.691326, norm:0.5763, lr:2.9692e-04 dt: 3331.32ms, tok/sec:157381.25
Current shard: 3, Current position: 63373312
Current shard: 3, Current position: 63307776
Current shard: 3, Current position: 63504384
Current shard: 3, Current position: 63438848
Current shard: 3, Current position: 63635456Current shard: 3, Current position: 63569920

Current shard: 3, Current position: 63766528
Current shard: 3, Current position: 63700992
step 692, loss: 4.681731, norm:0.6830, lr:2.9691e-04 dt: 3331.24ms, tok/sec:157385.09
Current shard: 3, Current position: 63897600
Current shard: 3, Current position: 63832064
Current shard: 3, Current position: 64028672
Current shard: 3, Current position: 63963136
Current shard: 3, Current position: 64159744
Current shard: 3, Current position: 64094208
Current shard: 3, Current position: 64290816
Current shard: 3, Current position: 64225280
step 693, loss: 4.734373, norm:0.7611, lr:2.9690e-04 dt: 3331.14ms, tok/sec:157389.76
Current shard: 3, Current position: 64421888
Current shard: 3, Current position: 64356352
Current shard: 3, Current position: 64552960
Current shard: 3, Current position: 64487424
Current shard: 3, Current position: 64618496Current shard: 3, Current position: 64684032

Current shard: 3, Current position: 64815104
Current shard: 3, Current position: 64749568
step 694, loss: 4.628377, norm:0.6926, lr:2.9689e-04 dt: 3331.33ms, tok/sec:157381.11
Current shard: 3, Current position: 64946176
Current shard: 3, Current position: 64880640
Current shard: 3, Current position: 65077248
Current shard: 3, Current position: 65011712
Current shard: 3, Current position: 65208320Current shard: 3, Current position: 65142784

Current shard: 3, Current position: 65339392
Current shard: 3, Current position: 65273856
step 695, loss: 4.722213, norm:0.5948, lr:2.9688e-04 dt: 3331.49ms, tok/sec:157373.30
Current shard: 3, Current position: 65470464
Current shard: 3, Current position: 65404928
Current shard: 3, Current position: 65601536
Current shard: 3, Current position: 65536000
Current shard: 3, Current position: 65667072
Current shard: 3, Current position: 65732608
Current shard: 3, Current position: 65863680
Current shard: 3, Current position: 65798144
step 696, loss: 4.848749, norm:0.6044, lr:2.9687e-04 dt: 3331.83ms, tok/sec:157357.50
Current shard: 3, Current position: 65994752
Current shard: 3, Current position: 65929216
Current shard: 3, Current position: 66125824
Current shard: 3, Current position: 66060288
Current shard: 3, Current position: 66191360
Current shard: 3, Current position: 66256896
Current shard: 3, Current position: 66387968
Current shard: 3, Current position: 66322432
step 697, loss: 4.970260, norm:0.7031, lr:2.9686e-04 dt: 3331.30ms, tok/sec:157382.50
Current shard: 3, Current position: 66519040
Current shard: 3, Current position: 66453504
Current shard: 3, Current position: 66650112
Current shard: 3, Current position: 66584576
Current shard: 3, Current position: 66781184Current shard: 3, Current position: 66715648

Current shard: 3, Current position: 66912256
Current shard: 3, Current position: 66846720
step 698, loss: 4.826641, norm:0.7760, lr:2.9685e-04 dt: 3331.22ms, tok/sec:157386.28
Current shard: 3, Current position: 67043328
Current shard: 3, Current position: 66977792
Current shard: 3, Current position: 67174400
Current shard: 3, Current position: 67108864
Current shard: 3, Current position: 67305472Current shard: 3, Current position: 67239936

Current shard: 3, Current position: 67436544
Current shard: 3, Current position: 67371008
step 699, loss: 4.878170, norm:0.6920, lr:2.9684e-04 dt: 3331.43ms, tok/sec:157376.40
Current shard: 0, Current position: 131072
Current shard: 0, Current position: 196608
Current shard: 0, Current position: 262144
Current shard: 0, Current position: 327680
Current shard: 0, Current position: 393216
Current shard: 0, Current position: 458752
Current shard: 0, Current position: 524288
Current shard: 0, Current position: 589824
Current shard: 0, Current position: 655360
Current shard: 0, Current position: 720896
Current shard: 0, Current position: 851968
Current shard: 0, Current position: 786432
Current shard: 0, Current position: 983040
Current shard: 0, Current position: 917504
Current shard: 0, Current position: 1114112
Current shard: 0, Current position: 1048576
Current shard: 0, Current position: 1245184
Current shard: 0, Current position: 1179648
Current shard: 0, Current position: 1376256
Current shard: 0, Current position: 1310720
Current shard: 0, Current position: 1507328
Current shard: 0, Current position: 1441792
Current shard: 0, Current position: 1638400
Current shard: 0, Current position: 1572864
Current shard: 0, Current position: 1769472
Current shard: 0, Current position: 1703936
Current shard: 0, Current position: 1900544
Current shard: 0, Current position: 1835008
Current shard: 0, Current position: 2031616
Current shard: 0, Current position: 1966080
Current shard: 0, Current position: 2162688
Current shard: 0, Current position: 2097152
Current shard: 0, Current position: 2293760
Current shard: 0, Current position: 2228224
Current shard: 0, Current position: 2424832
Current shard: 0, Current position: 2359296
Current shard: 0, Current position: 2555904
Current shard: 0, Current position: 2490368
Current shard: 0, Current position: 2686976
Current shard: 0, Current position: 2621440
validation loss: 4.8152
HellaSwag accuracy:4634860609161793041/-2=-2317430304580896768.0000
rank 1 sample 0: Hello, I'm a language model, i love of the subject. I also love with me through what I want to do to go back to a comment.
rank 1 sample 1: Hello, I'm a language model, a word, with a word for my eyes. I didn't make a lot more important to them, but I was
rank 1 sample 2: Hello, I'm a language model, but most importantly, I read it.
I guess, this, but I am not really interested in this topic,
rank 1 sample 3: Hello, I'm a language model, I couldn't do anything anything but to. I had, "d have made a difference here" and I am not
rank 0 sample 0: Hello, I'm a language model, I'm just a good source and use as a word that can actually run into a new model in my life. In
rank 0 sample 1: Hello, I'm a language model, like The Story of the Life. .
In any discussion of the scientific world, we’ve never felt that
rank 0 sample 2: Hello, I'm a language model, and I read a blog page for an article:
- Open your blog post to write your article
- Open,
rank 0 sample 3: Hello, I'm a language model, my use of language, and my writing. By the end of the century of many, I'm like to understand some
Current shard: 3, Current position: 67567616
Current shard: 3, Current position: 67502080
Current shard: 3, Current position: 67698688
Current shard: 3, Current position: 67633152
Current shard: 3, Current position: 67764224
Current shard: 3, Current position: 67829760
Current shard: 3, Current position: 67960832
Current shard: 3, Current position: 67895296
step 700, loss: 4.773961, norm:0.6999, lr:2.9683e-04 dt: 54408.24ms, tok/sec:9636.19
Current shard: 3, Current position: 68091904
Current shard: 3, Current position: 68026368
Current shard: 3, Current position: 68222976
Current shard: 3, Current position: 68157440
Current shard: 3, Current position: 68354048Current shard: 3, Current position: 68288512

Current shard: 3, Current position: 68485120
Current shard: 3, Current position: 68419584
step 701, loss: 4.817003, norm:0.7525, lr:2.9683e-04 dt: 3331.21ms, tok/sec:157386.81
Current shard: 3, Current position: 68616192
Current shard: 3, Current position: 68550656
Current shard: 3, Current position: 68747264
Current shard: 3, Current position: 68681728
Current shard: 3, Current position: 68812800
Current shard: 3, Current position: 68878336
Current shard: 3, Current position: 69009408
Current shard: 3, Current position: 68943872
step 702, loss: 4.849510, norm:0.8199, lr:2.9682e-04 dt: 3331.52ms, tok/sec:157372.20
Current shard: 3, Current position: 69140480
Current shard: 3, Current position: 69074944
Current shard: 3, Current position: 69271552
Current shard: 3, Current position: 69206016
Current shard: 3, Current position: 69337088
Current shard: 3, Current position: 69402624
Current shard: 3, Current position: 69533696
Current shard: 3, Current position: 69468160
step 703, loss: 4.843616, norm:0.8192, lr:2.9681e-04 dt: 3331.65ms, tok/sec:157366.05
Current shard: 3, Current position: 69599232
Current shard: 3, Current position: 69664768
Current shard: 3, Current position: 69795840Current shard: 3, Current position: 69730304

Current shard: 3, Current position: 69861376
Current shard: 3, Current position: 69926912
Current shard: 3, Current position: 70057984
Current shard: 3, Current position: 69992448
step 704, loss: 4.799563, norm:0.7362, lr:2.9680e-04 dt: 3331.41ms, tok/sec:157377.42
Current shard: 3, Current position: 70189056
Current shard: 3, Current position: 70123520
Current shard: 3, Current position: 70320128
Current shard: 3, Current position: 70254592
Current shard: 3, Current position: 70385664
Current shard: 3, Current position: 70451200
Current shard: 3, Current position: 70582272
Current shard: 3, Current position: 70516736
step 705, loss: 4.798386, norm:0.7394, lr:2.9679e-04 dt: 3331.30ms, tok/sec:157382.50
Current shard: 3, Current position: 70713344
Current shard: 3, Current position: 70647808
Current shard: 3, Current position: 70844416
Current shard: 3, Current position: 70778880
Current shard: 3, Current position: 70975488
Current shard: 3, Current position: 70909952
Current shard: 3, Current position: 71106560
Current shard: 3, Current position: 71041024
step 706, loss: 4.852853, norm:0.6718, lr:2.9678e-04 dt: 3331.38ms, tok/sec:157378.67
Current shard: 3, Current position: 71237632
Current shard: 3, Current position: 71172096
Current shard: 3, Current position: 71368704
Current shard: 3, Current position: 71303168
Current shard: 3, Current position: 71499776Current shard: 3, Current position: 71434240

Current shard: 3, Current position: 71630848
Current shard: 3, Current position: 71565312
step 707, loss: 4.836641, norm:0.5968, lr:2.9677e-04 dt: 3331.48ms, tok/sec:157374.01
Current shard: 3, Current position: 71761920
Current shard: 3, Current position: 71696384
Current shard: 3, Current position: 71892992
Current shard: 3, Current position: 71827456
Current shard: 3, Current position: 72024064Current shard: 3, Current position: 71958528

Current shard: 3, Current position: 72155136
Current shard: 3, Current position: 72089600
step 708, loss: 4.841327, norm:0.5574, lr:2.9676e-04 dt: 3331.28ms, tok/sec:157383.37
Current shard: 3, Current position: 72286208
Current shard: 3, Current position: 72220672
Current shard: 3, Current position: 72417280
Current shard: 3, Current position: 72351744
Current shard: 3, Current position: 72548352
Current shard: 3, Current position: 72482816
Current shard: 3, Current position: 72679424
Current shard: 3, Current position: 72613888
step 709, loss: 4.795034, norm:0.6155, lr:2.9675e-04 dt: 3331.44ms, tok/sec:157375.57
Current shard: 3, Current position: 72810496
Current shard: 3, Current position: 72744960
Current shard: 3, Current position: 72941568
Current shard: 3, Current position: 72876032
Current shard: 3, Current position: 73072640
Current shard: 3, Current position: 73007104
Current shard: 3, Current position: 73203712
Current shard: 3, Current position: 73138176
step 710, loss: 4.777333, norm:0.5708, lr:2.9674e-04 dt: 3331.50ms, tok/sec:157373.08
Current shard: 3, Current position: 73334784
Current shard: 3, Current position: 73269248
Current shard: 3, Current position: 73465856
Current shard: 3, Current position: 73400320
Current shard: 3, Current position: 73531392
Current shard: 3, Current position: 73596928
Current shard: 3, Current position: 73728000
Current shard: 3, Current position: 73662464
step 711, loss: 4.767787, norm:0.5534, lr:2.9673e-04 dt: 3331.31ms, tok/sec:157381.74
Current shard: 3, Current position: 73859072
Current shard: 3, Current position: 73793536
Current shard: 3, Current position: 73990144
Current shard: 3, Current position: 73924608
Current shard: 3, Current position: 74055680Current shard: 3, Current position: 74121216

Current shard: 3, Current position: 74252288
Current shard: 3, Current position: 74186752
step 712, loss: 4.762026, norm:0.6386, lr:2.9672e-04 dt: 3331.60ms, tok/sec:157368.30
Current shard: 3, Current position: 74317824
Current shard: 3, Current position: 74383360
Current shard: 3, Current position: 74448896
Current shard: 3, Current position: 74514432
Current shard: 3, Current position: 74579968
Current shard: 3, Current position: 74645504
Current shard: 3, Current position: 74776576
Current shard: 3, Current position: 74711040
step 713, loss: 4.735001, norm:0.6812, lr:2.9671e-04 dt: 3331.28ms, tok/sec:157383.56
Current shard: 3, Current position: 74907648
Current shard: 3, Current position: 74842112
Current shard: 3, Current position: 75038720
Current shard: 3, Current position: 74973184
Current shard: 3, Current position: 75169792
Current shard: 3, Current position: 75104256
Current shard: 3, Current position: 75300864
Current shard: 3, Current position: 75235328
step 714, loss: 4.773654, norm:0.6485, lr:2.9671e-04 dt: 3331.29ms, tok/sec:157383.02
Current shard: 3, Current position: 75431936
Current shard: 3, Current position: 75366400
Current shard: 3, Current position: 75563008
Current shard: 3, Current position: 75497472
Current shard: 3, Current position: 75628544
Current shard: 3, Current position: 75694080
Current shard: 3, Current position: 75825152
Current shard: 3, Current position: 75759616
step 715, loss: 4.759774, norm:0.6479, lr:2.9670e-04 dt: 3331.33ms, tok/sec:157380.81
Current shard: 3, Current position: 75956224
Current shard: 3, Current position: 75890688
Current shard: 3, Current position: 76087296
Current shard: 3, Current position: 76021760
Current shard: 3, Current position: 76152832
Current shard: 3, Current position: 76218368
Current shard: 3, Current position: 76349440
Current shard: 3, Current position: 76283904
step 716, loss: 4.798203, norm:0.8018, lr:2.9669e-04 dt: 3331.57ms, tok/sec:157369.79
Current shard: 3, Current position: 76480512
Current shard: 3, Current position: 76414976
Current shard: 3, Current position: 76611584
Current shard: 3, Current position: 76546048
Current shard: 3, Current position: 76742656
Current shard: 3, Current position: 76677120
Current shard: 3, Current position: 76873728
Current shard: 3, Current position: 76808192
step 717, loss: 4.800492, norm:0.8964, lr:2.9668e-04 dt: 3331.36ms, tok/sec:157379.70
Current shard: 3, Current position: 77004800
Current shard: 3, Current position: 76939264
Current shard: 3, Current position: 77135872
Current shard: 3, Current position: 77070336
Current shard: 3, Current position: 77201408
Current shard: 3, Current position: 77266944
Current shard: 3, Current position: 77398016
Current shard: 3, Current position: 77332480
step 718, loss: 4.832112, norm:0.9798, lr:2.9667e-04 dt: 3331.38ms, tok/sec:157378.69
Current shard: 3, Current position: 77529088
Current shard: 3, Current position: 77463552
Current shard: 3, Current position: 77660160
Current shard: 3, Current position: 77594624
Current shard: 3, Current position: 77725696
Current shard: 3, Current position: 77791232
Current shard: 3, Current position: 77922304
Current shard: 3, Current position: 77856768
step 719, loss: 4.754838, norm:0.6754, lr:2.9666e-04 dt: 3331.60ms, tok/sec:157368.04
Current shard: 3, Current position: 78053376
Current shard: 3, Current position: 77987840
Current shard: 3, Current position: 78184448
Current shard: 3, Current position: 78118912
Current shard: 3, Current position: 78315520
Current shard: 3, Current position: 78249984
Current shard: 3, Current position: 78446592
Current shard: 3, Current position: 78381056
step 720, loss: 4.698550, norm:0.6584, lr:2.9665e-04 dt: 3331.54ms, tok/sec:157371.13
Current shard: 3, Current position: 78577664
Current shard: 3, Current position: 78512128
Current shard: 3, Current position: 78708736
Current shard: 3, Current position: 78643200
Current shard: 3, Current position: 78839808
Current shard: 3, Current position: 78774272
Current shard: 3, Current position: 78970880
Current shard: 3, Current position: 78905344
step 721, loss: 4.716328, norm:0.6508, lr:2.9664e-04 dt: 3331.77ms, tok/sec:157360.02
Current shard: 3, Current position: 79101952
Current shard: 3, Current position: 79036416
Current shard: 3, Current position: 79233024
Current shard: 3, Current position: 79167488
Current shard: 3, Current position: 79298560
Current shard: 3, Current position: 79364096
Current shard: 3, Current position: 79495168
Current shard: 3, Current position: 79429632
step 722, loss: 4.672310, norm:0.6402, lr:2.9663e-04 dt: 3331.23ms, tok/sec:157385.91
Current shard: 3, Current position: 79626240
Current shard: 3, Current position: 79560704
Current shard: 3, Current position: 79757312
Current shard: 3, Current position: 79691776
Current shard: 3, Current position: 79822848Current shard: 3, Current position: 79888384

Current shard: 3, Current position: 80019456
Current shard: 3, Current position: 79953920
step 723, loss: 4.754741, norm:0.6812, lr:2.9662e-04 dt: 3331.44ms, tok/sec:157375.96
Current shard: 3, Current position: 80150528
Current shard: 3, Current position: 80084992
Current shard: 3, Current position: 80281600
Current shard: 3, Current position: 80216064
Current shard: 3, Current position: 80347136Current shard: 3, Current position: 80412672

Current shard: 3, Current position: 80543744
Current shard: 3, Current position: 80478208
step 724, loss: 4.717481, norm:0.6820, lr:2.9661e-04 dt: 3331.23ms, tok/sec:157385.92
Current shard: 3, Current position: 80674816
Current shard: 3, Current position: 80609280
Current shard: 3, Current position: 80805888
Current shard: 3, Current position: 80740352
Current shard: 3, Current position: 80936960Current shard: 3, Current position: 80871424

Current shard: 3, Current position: 81068032
Current shard: 3, Current position: 81002496
step 725, loss: 4.771042, norm:0.7118, lr:2.9660e-04 dt: 3331.32ms, tok/sec:157381.46
Current shard: 3, Current position: 81199104
Current shard: 3, Current position: 81133568
Current shard: 3, Current position: 81330176
Current shard: 3, Current position: 81264640
Current shard: 3, Current position: 81461248
Current shard: 3, Current position: 81395712
Current shard: 3, Current position: 81592320
Current shard: 3, Current position: 81526784
step 726, loss: 4.713892, norm:0.6803, lr:2.9659e-04 dt: 3331.48ms, tok/sec:157373.81
Current shard: 3, Current position: 81723392
Current shard: 3, Current position: 81657856
Current shard: 3, Current position: 81854464
Current shard: 3, Current position: 81788928
Current shard: 3, Current position: 81985536Current shard: 3, Current position: 81920000

Current shard: 3, Current position: 82116608
Current shard: 3, Current position: 82051072
step 727, loss: 4.694971, norm:0.7033, lr:2.9658e-04 dt: 3331.28ms, tok/sec:157383.35
Current shard: 3, Current position: 82247680
Current shard: 3, Current position: 82182144
Current shard: 3, Current position: 82378752
Current shard: 3, Current position: 82313216
Current shard: 3, Current position: 82444288
Current shard: 3, Current position: 82509824
Current shard: 3, Current position: 82640896
Current shard: 3, Current position: 82575360
step 728, loss: 4.685563, norm:0.7117, lr:2.9657e-04 dt: 3331.59ms, tok/sec:157368.80
Current shard: 3, Current position: 82771968
Current shard: 3, Current position: 82706432
Current shard: 3, Current position: 82903040
Current shard: 3, Current position: 82837504
Current shard: 3, Current position: 82968576
Current shard: 3, Current position: 83034112
Current shard: 3, Current position: 83165184
Current shard: 3, Current position: 83099648
step 729, loss: 4.709334, norm:0.5982, lr:2.9656e-04 dt: 3331.48ms, tok/sec:157373.94
Current shard: 3, Current position: 83296256
Current shard: 3, Current position: 83230720
Current shard: 3, Current position: 83427328
Current shard: 3, Current position: 83361792
Current shard: 3, Current position: 83492864
Current shard: 3, Current position: 83558400
Current shard: 3, Current position: 83689472
Current shard: 3, Current position: 83623936
step 730, loss: 4.718710, norm:0.6172, lr:2.9655e-04 dt: 3331.87ms, tok/sec:157355.59
Current shard: 3, Current position: 83820544
Current shard: 3, Current position: 83755008
Current shard: 3, Current position: 83951616
Current shard: 3, Current position: 83886080
Current shard: 3, Current position: 84082688Current shard: 3, Current position: 84017152

Current shard: 3, Current position: 84213760
Current shard: 3, Current position: 84148224
step 731, loss: 4.618464, norm:0.5327, lr:2.9654e-04 dt: 3331.41ms, tok/sec:157377.24
Current shard: 3, Current position: 84344832
Current shard: 3, Current position: 84279296
Current shard: 3, Current position: 84475904
Current shard: 3, Current position: 84410368
Current shard: 3, Current position: 84541440
Current shard: 3, Current position: 84606976
Current shard: 3, Current position: 84738048
Current shard: 3, Current position: 84672512
step 732, loss: 4.616334, norm:0.5774, lr:2.9654e-04 dt: 3331.23ms, tok/sec:157385.58
Current shard: 3, Current position: 84869120
Current shard: 3, Current position: 84803584
Current shard: 3, Current position: 85000192
Current shard: 3, Current position: 84934656
Current shard: 3, Current position: 85065728Current shard: 3, Current position: 85131264

Current shard: 3, Current position: 85262336
Current shard: 3, Current position: 85196800
step 733, loss: 4.560463, norm:0.6180, lr:2.9653e-04 dt: 3331.26ms, tok/sec:157384.07
Current shard: 3, Current position: 85393408
Current shard: 3, Current position: 85327872
Current shard: 3, Current position: 85524480
Current shard: 3, Current position: 85458944
Current shard: 3, Current position: 85590016Current shard: 3, Current position: 85655552

Current shard: 3, Current position: 85786624
Current shard: 3, Current position: 85721088
step 734, loss: 4.575310, norm:0.7296, lr:2.9652e-04 dt: 3331.41ms, tok/sec:157377.12
Current shard: 3, Current position: 85917696
Current shard: 3, Current position: 85852160
Current shard: 3, Current position: 86048768
Current shard: 3, Current position: 85983232
Current shard: 3, Current position: 86179840Current shard: 3, Current position: 86114304

Current shard: 3, Current position: 86310912
Current shard: 3, Current position: 86245376
step 735, loss: 4.623197, norm:0.8358, lr:2.9651e-04 dt: 3331.34ms, tok/sec:157380.50
Current shard: 3, Current position: 86441984
Current shard: 3, Current position: 86376448
Current shard: 3, Current position: 86573056
Current shard: 3, Current position: 86507520
Current shard: 3, Current position: 86704128
Current shard: 3, Current position: 86638592
Current shard: 3, Current position: 86835200
Current shard: 3, Current position: 86769664
step 736, loss: 4.594705, norm:0.7958, lr:2.9650e-04 dt: 3331.35ms, tok/sec:157380.13
Current shard: 3, Current position: 86966272
Current shard: 3, Current position: 86900736
Current shard: 3, Current position: 87097344
Current shard: 3, Current position: 87031808
Current shard: 3, Current position: 87228416
Current shard: 3, Current position: 87162880
Current shard: 3, Current position: 87359488
Current shard: 3, Current position: 87293952
step 737, loss: 4.628160, norm:0.6581, lr:2.9649e-04 dt: 3331.49ms, tok/sec:157373.35
Current shard: 3, Current position: 87490560
Current shard: 3, Current position: 87425024
Current shard: 3, Current position: 87621632
Current shard: 3, Current position: 87556096
Current shard: 3, Current position: 87752704
Current shard: 3, Current position: 87687168
Current shard: 3, Current position: 87883776
Current shard: 3, Current position: 87818240
step 738, loss: 4.583887, norm:0.7331, lr:2.9648e-04 dt: 3331.56ms, tok/sec:157370.17
Current shard: 3, Current position: 88014848
Current shard: 3, Current position: 87949312
Current shard: 3, Current position: 88145920
Current shard: 3, Current position: 88080384
Current shard: 3, Current position: 88276992
Current shard: 3, Current position: 88211456
Current shard: 3, Current position: 88408064
Current shard: 3, Current position: 88342528
step 739, loss: 4.592016, norm:0.8426, lr:2.9647e-04 dt: 3331.44ms, tok/sec:157375.66
Current shard: 3, Current position: 88539136
Current shard: 3, Current position: 88473600
Current shard: 3, Current position: 88670208
Current shard: 3, Current position: 88604672
Current shard: 3, Current position: 88801280Current shard: 3, Current position: 88735744

Current shard: 3, Current position: 88932352
Current shard: 3, Current position: 88866816
step 740, loss: 4.601919, norm:0.6942, lr:2.9646e-04 dt: 3331.21ms, tok/sec:157386.68
Current shard: 3, Current position: 89063424
Current shard: 3, Current position: 88997888
Current shard: 3, Current position: 89194496
Current shard: 3, Current position: 89128960
Current shard: 3, Current position: 89325568
Current shard: 3, Current position: 89260032
Current shard: 3, Current position: 89456640
Current shard: 3, Current position: 89391104
step 741, loss: 4.561810, norm:0.7094, lr:2.9645e-04 dt: 3331.45ms, tok/sec:157375.30
Current shard: 3, Current position: 89587712
Current shard: 3, Current position: 89522176
Current shard: 3, Current position: 89718784
Current shard: 3, Current position: 89653248
Current shard: 3, Current position: 89849856
Current shard: 3, Current position: 89784320
Current shard: 3, Current position: 89980928
Current shard: 3, Current position: 89915392
step 742, loss: 4.549499, norm:0.6727, lr:2.9644e-04 dt: 3331.31ms, tok/sec:157381.74
Current shard: 3, Current position: 90112000
Current shard: 3, Current position: 90046464
Current shard: 3, Current position: 90243072
Current shard: 3, Current position: 90177536
Current shard: 3, Current position: 90374144
Current shard: 3, Current position: 90308608
Current shard: 3, Current position: 90505216
Current shard: 3, Current position: 90439680
step 743, loss: 4.730262, norm:0.8132, lr:2.9643e-04 dt: 3331.59ms, tok/sec:157368.72
Current shard: 3, Current position: 90636288
Current shard: 3, Current position: 90570752
Current shard: 3, Current position: 90767360
Current shard: 3, Current position: 90701824
Current shard: 3, Current position: 90832896
Current shard: 3, Current position: 90898432
Current shard: 3, Current position: 91029504
Current shard: 3, Current position: 90963968
step 744, loss: 4.731581, norm:0.9097, lr:2.9642e-04 dt: 3331.55ms, tok/sec:157370.80
Current shard: 3, Current position: 91160576
Current shard: 3, Current position: 91095040
Current shard: 3, Current position: 91291648
Current shard: 3, Current position: 91226112
Current shard: 3, Current position: 91357184Current shard: 3, Current position: 91422720

Current shard: 3, Current position: 91553792
Current shard: 3, Current position: 91488256
step 745, loss: 4.741479, norm:0.8225, lr:2.9641e-04 dt: 3331.48ms, tok/sec:157373.94
Current shard: 3, Current position: 91684864
Current shard: 3, Current position: 91619328
Current shard: 3, Current position: 91815936
Current shard: 3, Current position: 91750400
Current shard: 3, Current position: 91947008
Current shard: 3, Current position: 91881472
Current shard: 3, Current position: 92078080
Current shard: 3, Current position: 92012544
step 746, loss: 4.792730, norm:0.8070, lr:2.9640e-04 dt: 3331.74ms, tok/sec:157361.63
Current shard: 3, Current position: 92209152
Current shard: 3, Current position: 92143616
Current shard: 3, Current position: 92340224
Current shard: 3, Current position: 92274688
Current shard: 3, Current position: 92471296
Current shard: 3, Current position: 92405760
Current shard: 3, Current position: 92602368
Current shard: 3, Current position: 92536832
step 747, loss: 4.753557, norm:0.6740, lr:2.9639e-04 dt: 3331.58ms, tok/sec:157368.99
Current shard: 3, Current position: 92733440
Current shard: 3, Current position: 92667904
Current shard: 3, Current position: 92864512
Current shard: 3, Current position: 92798976
Current shard: 3, Current position: 92930048
Current shard: 3, Current position: 92995584
Current shard: 3, Current position: 93126656
Current shard: 3, Current position: 93061120
step 748, loss: 4.702494, norm:0.6633, lr:2.9638e-04 dt: 3331.18ms, tok/sec:157388.21
Current shard: 3, Current position: 93257728
Current shard: 3, Current position: 93192192
Current shard: 3, Current position: 93388800
Current shard: 3, Current position: 93323264
Current shard: 3, Current position: 93454336
Current shard: 3, Current position: 93519872
Current shard: 3, Current position: 93650944
Current shard: 3, Current position: 93585408
step 749, loss: 4.747152, norm:0.6233, lr:2.9637e-04 dt: 3331.44ms, tok/sec:157375.94
HellaSwag accuracy:4774472786424581649/-2=-2387236393212291072.0000
rank 1 sample 0: Hello, I'm a language model, this word is a way of what types in each class with
The word can be an item for the same
The
rank 1 sample 1: Hello, I'm a language model, the first of the first, in this language, was very difficult to remember and start!
These two types of words
rank 1 sample 2: Hello, I'm a language model, but at my first time, I've been using the Internet: the Web browser, and my phone to my web server
rank 1 sample 3: Hello, I'm a language model, I believe that there is really a problem or a relationship . . . . They are only considered, and they are not
Current shard: 3, Current position: 93782016
rank 0 sample 0: Hello, I'm a language model, I'm in my own search system; however, I'm not, you have to use those systems, which are in
rank 0 sample 1: Hello, I'm a language model, and how I'm going to make new ideas that create some kind of machine that is being used in that system.

rank 0 sample 2: Hello, I'm a language model, and I look up by that number that I don't know it in the world. I think that I'm going to
rank 0 sample 3: Hello, I'm a language model, I really liked it.
The next two languages are:
- a b -1 -2 -1
•
Current shard: 3, Current position: 93716480
Current shard: 3, Current position: 93913088
Current shard: 3, Current position: 93847552
Current shard: 3, Current position: 93978624
Current shard: 3, Current position: 94044160
Current shard: 3, Current position: 94175232
Current shard: 3, Current position: 94109696
step 750, loss: 4.809730, norm:0.6136, lr:2.9636e-04 dt: 48147.01ms, tok/sec:10889.32
Current shard: 3, Current position: 94306304
Current shard: 3, Current position: 94240768
Current shard: 3, Current position: 94437376
Current shard: 3, Current position: 94371840
Current shard: 3, Current position: 94502912
Current shard: 3, Current position: 94568448
Current shard: 3, Current position: 94699520
Current shard: 3, Current position: 94633984
step 751, loss: 4.734954, norm:0.6461, lr:2.9635e-04 dt: 3331.38ms, tok/sec:157378.54
Current shard: 3, Current position: 94830592
Current shard: 3, Current position: 94765056
Current shard: 3, Current position: 94961664
Current shard: 3, Current position: 94896128
Current shard: 3, Current position: 95027200Current shard: 3, Current position: 95092736

Current shard: 3, Current position: 95223808
Current shard: 3, Current position: 95158272
step 752, loss: 4.799404, norm:0.6660, lr:2.9634e-04 dt: 3332.12ms, tok/sec:157343.86
Current shard: 3, Current position: 95354880
Current shard: 3, Current position: 95289344
Current shard: 3, Current position: 95485952
Current shard: 3, Current position: 95420416
Current shard: 3, Current position: 95617024Current shard: 3, Current position: 95551488

Current shard: 3, Current position: 95748096
Current shard: 3, Current position: 95682560
step 753, loss: 4.702684, norm:0.5811, lr:2.9633e-04 dt: 3331.33ms, tok/sec:157381.02
Current shard: 3, Current position: 95879168
Current shard: 3, Current position: 95813632
Current shard: 3, Current position: 96010240
Current shard: 3, Current position: 95944704
Current shard: 3, Current position: 96141312Current shard: 3, Current position: 96075776

Current shard: 3, Current position: 96272384
Current shard: 3, Current position: 96206848
step 754, loss: 4.735219, norm:0.5593, lr:2.9632e-04 dt: 3331.73ms, tok/sec:157362.30
Current shard: 3, Current position: 96403456
Current shard: 3, Current position: 96337920
Current shard: 3, Current position: 96534528
Current shard: 3, Current position: 96468992
Current shard: 3, Current position: 96665600
Current shard: 3, Current position: 96600064
Current shard: 3, Current position: 96796672
Current shard: 3, Current position: 96731136
step 755, loss: 4.731452, norm:0.6872, lr:2.9631e-04 dt: 3331.32ms, tok/sec:157381.61
Current shard: 3, Current position: 96927744
Current shard: 3, Current position: 96862208
Current shard: 3, Current position: 97058816
Current shard: 3, Current position: 96993280
Current shard: 3, Current position: 97189888
Current shard: 3, Current position: 97124352
Current shard: 3, Current position: 97320960
Current shard: 3, Current position: 97255424
step 756, loss: 4.707739, norm:0.6899, lr:2.9630e-04 dt: 3331.36ms, tok/sec:157379.37
Current shard: 3, Current position: 97452032
Current shard: 3, Current position: 97386496
Current shard: 3, Current position: 97583104
Current shard: 3, Current position: 97517568
Current shard: 3, Current position: 97648640
Current shard: 3, Current position: 97714176
Current shard: 3, Current position: 97845248
Current shard: 3, Current position: 97779712
step 757, loss: 4.676806, norm:0.6799, lr:2.9629e-04 dt: 3331.30ms, tok/sec:157382.18
Current shard: 3, Current position: 97976320
Current shard: 3, Current position: 97910784
Current shard: 3, Current position: 98107392
Current shard: 3, Current position: 98041856
Current shard: 3, Current position: 98172928Current shard: 3, Current position: 98238464

Current shard: 3, Current position: 98369536
Current shard: 3, Current position: 98304000
step 758, loss: 4.720000, norm:0.6927, lr:2.9628e-04 dt: 3331.39ms, tok/sec:157378.00
Current shard: 3, Current position: 98500608
Current shard: 3, Current position: 98435072
Current shard: 3, Current position: 98631680
Current shard: 3, Current position: 98566144
Current shard: 3, Current position: 98762752
Current shard: 3, Current position: 98697216
Current shard: 3, Current position: 98893824
Current shard: 3, Current position: 98828288
step 759, loss: 4.721815, norm:0.7014, lr:2.9627e-04 dt: 3331.62ms, tok/sec:157367.34
Current shard: 3, Current position: 99024896
Current shard: 3, Current position: 98959360
Current shard: 3, Current position: 99155968
Current shard: 3, Current position: 99090432
Current shard: 3, Current position: 99221504
Current shard: 3, Current position: 99287040
Current shard: 3, Current position: 99418112
Current shard: 3, Current position: 99352576
step 760, loss: 4.716421, norm:0.6660, lr:2.9626e-04 dt: 3331.49ms, tok/sec:157373.25
Current shard: 3, Current position: 99549184
Current shard: 3, Current position: 99483648
Current shard: 3, Current position: 99680256
Current shard: 3, Current position: 99614720
Current shard: 3, Current position: 99811328
Current shard: 3, Current position: 99745792
Current shard: 4, Current position: 0
Current shard: 4, Current position: 65536
step 761, loss: 4.740291, norm:0.6339, lr:2.9625e-04 dt: 3333.91ms, tok/sec:157259.39
Current shard: 4, Current position: 131072
Current shard: 4, Current position: 196608
Current shard: 4, Current position: 262144
Current shard: 4, Current position: 327680
Current shard: 4, Current position: 393216Current shard: 4, Current position: 458752

Current shard: 4, Current position: 589824
Current shard: 4, Current position: 524288
step 762, loss: 4.683577, norm:0.6202, lr:2.9624e-04 dt: 3331.31ms, tok/sec:157381.99
Current shard: 4, Current position: 720896
Current shard: 4, Current position: 655360
Current shard: 4, Current position: 851968
Current shard: 4, Current position: 786432
Current shard: 4, Current position: 917504
Current shard: 4, Current position: 983040
Current shard: 4, Current position: 1114112
Current shard: 4, Current position: 1048576
step 763, loss: 4.684697, norm:0.5987, lr:2.9623e-04 dt: 3331.45ms, tok/sec:157375.35
Current shard: 4, Current position: 1245184
Current shard: 4, Current position: 1179648
Current shard: 4, Current position: 1376256
Current shard: 4, Current position: 1310720
Current shard: 4, Current position: 1441792Current shard: 4, Current position: 1507328

Current shard: 4, Current position: 1638400
Current shard: 4, Current position: 1572864
step 764, loss: 4.680981, norm:0.5919, lr:2.9622e-04 dt: 3331.57ms, tok/sec:157369.48
Current shard: 4, Current position: 1769472
Current shard: 4, Current position: 1703936
Current shard: 4, Current position: 1900544
Current shard: 4, Current position: 1835008
Current shard: 4, Current position: 1966080
Current shard: 4, Current position: 2031616
Current shard: 4, Current position: 2162688
Current shard: 4, Current position: 2097152
step 765, loss: 4.675751, norm:0.6919, lr:2.9621e-04 dt: 3331.38ms, tok/sec:157378.82
Current shard: 4, Current position: 2293760
Current shard: 4, Current position: 2228224
Current shard: 4, Current position: 2424832
Current shard: 4, Current position: 2359296
Current shard: 4, Current position: 2490368
Current shard: 4, Current position: 2555904
Current shard: 4, Current position: 2686976
Current shard: 4, Current position: 2621440
step 766, loss: 4.688856, norm:0.8502, lr:2.9620e-04 dt: 3332.30ms, tok/sec:157335.30
Current shard: 4, Current position: 2818048
Current shard: 4, Current position: 2752512
Current shard: 4, Current position: 2949120
Current shard: 4, Current position: 2883584
Current shard: 4, Current position: 3080192Current shard: 4, Current position: 3014656

Current shard: 4, Current position: 3211264
Current shard: 4, Current position: 3145728
step 767, loss: 4.635079, norm:0.8395, lr:2.9619e-04 dt: 3332.16ms, tok/sec:157341.76
Current shard: 4, Current position: 3342336
Current shard: 4, Current position: 3276800
Current shard: 4, Current position: 3473408
Current shard: 4, Current position: 3407872
Current shard: 4, Current position: 3604480Current shard: 4, Current position: 3538944

Current shard: 4, Current position: 3735552
Current shard: 4, Current position: 3670016
step 768, loss: 4.603689, norm:0.7898, lr:2.9618e-04 dt: 3331.67ms, tok/sec:157364.96
Current shard: 4, Current position: 3866624
Current shard: 4, Current position: 3801088
Current shard: 4, Current position: 3997696
Current shard: 4, Current position: 3932160
Current shard: 4, Current position: 4128768
Current shard: 4, Current position: 4063232
Current shard: 4, Current position: 4259840
Current shard: 4, Current position: 4194304
step 769, loss: 4.668912, norm:0.6413, lr:2.9617e-04 dt: 3331.37ms, tok/sec:157378.92
Current shard: 4, Current position: 4390912
Current shard: 4, Current position: 4325376
Current shard: 4, Current position: 4521984
Current shard: 4, Current position: 4456448
Current shard: 4, Current position: 4653056
Current shard: 4, Current position: 4587520
Current shard: 4, Current position: 4784128
Current shard: 4, Current position: 4718592
step 770, loss: 4.618456, norm:0.6080, lr:2.9616e-04 dt: 3331.34ms, tok/sec:157380.28
Current shard: 4, Current position: 4915200
Current shard: 4, Current position: 4849664
Current shard: 4, Current position: 5046272
Current shard: 4, Current position: 4980736
Current shard: 4, Current position: 5177344
Current shard: 4, Current position: 5111808
Current shard: 4, Current position: 5308416
Current shard: 4, Current position: 5242880
step 771, loss: 4.642142, norm:0.6449, lr:2.9615e-04 dt: 3331.36ms, tok/sec:157379.51
Current shard: 4, Current position: 5439488
Current shard: 4, Current position: 5373952
Current shard: 4, Current position: 5570560
Current shard: 4, Current position: 5505024
Current shard: 4, Current position: 5701632
Current shard: 4, Current position: 5636096
Current shard: 4, Current position: 5832704
Current shard: 4, Current position: 5767168
step 772, loss: 4.642273, norm:0.7761, lr:2.9614e-04 dt: 3331.54ms, tok/sec:157370.93
Current shard: 4, Current position: 5963776
Current shard: 4, Current position: 5898240
Current shard: 4, Current position: 6094848
Current shard: 4, Current position: 6029312
Current shard: 4, Current position: 6160384
Current shard: 4, Current position: 6225920
Current shard: 4, Current position: 6356992
Current shard: 4, Current position: 6291456
step 773, loss: 4.687798, norm:0.7451, lr:2.9613e-04 dt: 3331.19ms, tok/sec:157387.54
Current shard: 4, Current position: 6488064
Current shard: 4, Current position: 6422528
Current shard: 4, Current position: 6619136
Current shard: 4, Current position: 6553600
Current shard: 4, Current position: 6684672Current shard: 4, Current position: 6750208

Current shard: 4, Current position: 6881280
Current shard: 4, Current position: 6815744
step 774, loss: 4.604825, norm:0.6665, lr:2.9612e-04 dt: 3331.34ms, tok/sec:157380.53
Current shard: 4, Current position: 7012352
Current shard: 4, Current position: 6946816
Current shard: 4, Current position: 7143424
Current shard: 4, Current position: 7077888
Current shard: 4, Current position: 7274496
Current shard: 4, Current position: 7208960
Current shard: 4, Current position: 7405568
Current shard: 4, Current position: 7340032
step 775, loss: 4.602884, norm:0.6656, lr:2.9611e-04 dt: 3331.24ms, tok/sec:157385.31
Current shard: 4, Current position: 7536640
Current shard: 4, Current position: 7471104
Current shard: 4, Current position: 7667712
Current shard: 4, Current position: 7602176
Current shard: 4, Current position: 7798784Current shard: 4, Current position: 7733248

Current shard: 4, Current position: 7929856
Current shard: 4, Current position: 7864320
step 776, loss: 4.597716, norm:0.6085, lr:2.9610e-04 dt: 3331.46ms, tok/sec:157375.04
Current shard: 4, Current position: 8060928
Current shard: 4, Current position: 7995392
Current shard: 4, Current position: 8192000
Current shard: 4, Current position: 8126464
Current shard: 4, Current position: 8257536
Current shard: 4, Current position: 8323072
Current shard: 4, Current position: 8454144
Current shard: 4, Current position: 8388608
step 777, loss: 4.541078, norm:0.6663, lr:2.9609e-04 dt: 3331.24ms, tok/sec:157385.00
Current shard: 4, Current position: 8585216
Current shard: 4, Current position: 8519680
Current shard: 4, Current position: 8716288
Current shard: 4, Current position: 8650752
Current shard: 4, Current position: 8781824Current shard: 4, Current position: 8847360

Current shard: 4, Current position: 8978432
Current shard: 4, Current position: 8912896
step 778, loss: 4.569393, norm:0.6348, lr:2.9608e-04 dt: 3331.39ms, tok/sec:157378.10
Current shard: 4, Current position: 9109504
Current shard: 4, Current position: 9043968
Current shard: 4, Current position: 9240576
Current shard: 4, Current position: 9175040
Current shard: 4, Current position: 9371648Current shard: 4, Current position: 9306112

Current shard: 4, Current position: 9502720
Current shard: 4, Current position: 9437184
step 779, loss: 4.545551, norm:0.5528, lr:2.9607e-04 dt: 3331.26ms, tok/sec:157384.09
Current shard: 4, Current position: 9633792
Current shard: 4, Current position: 9568256
Current shard: 4, Current position: 9764864
Current shard: 4, Current position: 9699328
Current shard: 4, Current position: 9830400
Current shard: 4, Current position: 9895936
Current shard: 4, Current position: 10027008
Current shard: 4, Current position: 9961472
step 780, loss: 4.563734, norm:0.6339, lr:2.9606e-04 dt: 3331.41ms, tok/sec:157377.27
Current shard: 4, Current position: 10092544
Current shard: 4, Current position: 10158080
Current shard: 4, Current position: 10223616
Current shard: 4, Current position: 10289152
Current shard: 4, Current position: 10354688
Current shard: 4, Current position: 10420224
Current shard: 4, Current position: 10551296
Current shard: 4, Current position: 10485760
step 781, loss: 4.475380, norm:0.6889, lr:2.9605e-04 dt: 3331.15ms, tok/sec:157389.30
Current shard: 4, Current position: 10682368
Current shard: 4, Current position: 10616832
Current shard: 4, Current position: 10813440
Current shard: 4, Current position: 10747904
Current shard: 4, Current position: 10878976
Current shard: 4, Current position: 10944512
Current shard: 4, Current position: 11075584
Current shard: 4, Current position: 11010048
step 782, loss: 4.474048, norm:0.7906, lr:2.9604e-04 dt: 3331.18ms, tok/sec:157388.28
Current shard: 4, Current position: 11206656
Current shard: 4, Current position: 11141120
Current shard: 4, Current position: 11337728
Current shard: 4, Current position: 11272192
Current shard: 4, Current position: 11468800
Current shard: 4, Current position: 11403264
Current shard: 4, Current position: 11599872
Current shard: 4, Current position: 11534336
step 783, loss: 4.507906, norm:0.7942, lr:2.9603e-04 dt: 3331.17ms, tok/sec:157388.78
Current shard: 4, Current position: 11730944
Current shard: 4, Current position: 11665408
Current shard: 4, Current position: 11862016
Current shard: 4, Current position: 11796480
Current shard: 4, Current position: 11993088Current shard: 4, Current position: 11927552

Current shard: 4, Current position: 12124160
Current shard: 4, Current position: 12058624
step 784, loss: 4.601046, norm:0.7119, lr:2.9602e-04 dt: 3331.30ms, tok/sec:157382.61
Current shard: 4, Current position: 12255232
Current shard: 4, Current position: 12189696
Current shard: 4, Current position: 12386304
Current shard: 4, Current position: 12320768
Current shard: 4, Current position: 12517376
Current shard: 4, Current position: 12451840
Current shard: 4, Current position: 12648448
Current shard: 4, Current position: 12582912
step 785, loss: 4.496352, norm:0.7384, lr:2.9601e-04 dt: 3331.32ms, tok/sec:157381.32
Current shard: 4, Current position: 12779520
Current shard: 4, Current position: 12713984
Current shard: 4, Current position: 12910592
Current shard: 4, Current position: 12845056
Current shard: 4, Current position: 12976128Current shard: 4, Current position: 13041664

Current shard: 4, Current position: 13172736
Current shard: 4, Current position: 13107200
step 786, loss: 4.542585, norm:0.7408, lr:2.9600e-04 dt: 3331.18ms, tok/sec:157387.91
Current shard: 4, Current position: 13303808
Current shard: 4, Current position: 13238272
Current shard: 4, Current position: 13434880
Current shard: 4, Current position: 13369344
Current shard: 4, Current position: 13500416
Current shard: 4, Current position: 13565952
Current shard: 4, Current position: 13697024
Current shard: 4, Current position: 13631488
step 787, loss: 4.514272, norm:0.6786, lr:2.9599e-04 dt: 3331.37ms, tok/sec:157379.33
Current shard: 4, Current position: 13828096
Current shard: 4, Current position: 13762560
Current shard: 4, Current position: 13959168
Current shard: 4, Current position: 13893632
Current shard: 4, Current position: 14090240
Current shard: 4, Current position: 14024704
Current shard: 4, Current position: 14221312
Current shard: 4, Current position: 14155776
step 788, loss: 4.493126, norm:0.6750, lr:2.9598e-04 dt: 3331.46ms, tok/sec:157374.65
Current shard: 4, Current position: 14352384
Current shard: 4, Current position: 14286848
Current shard: 4, Current position: 14483456
Current shard: 4, Current position: 14417920
Current shard: 4, Current position: 14548992
Current shard: 4, Current position: 14614528
Current shard: 4, Current position: 14745600
Current shard: 4, Current position: 14680064
step 789, loss: 4.646996, norm:0.5449, lr:2.9597e-04 dt: 3331.69ms, tok/sec:157363.86
Current shard: 4, Current position: 14811136
Current shard: 4, Current position: 14876672
Current shard: 4, Current position: 14942208
Current shard: 4, Current position: 15007744
Current shard: 4, Current position: 15138816
Current shard: 4, Current position: 15073280
Current shard: 4, Current position: 15269888
Current shard: 4, Current position: 15204352
step 790, loss: 4.648803, norm:0.6264, lr:2.9596e-04 dt: 3331.42ms, tok/sec:157376.91
Current shard: 4, Current position: 15400960
Current shard: 4, Current position: 15335424
Current shard: 4, Current position: 15532032
Current shard: 4, Current position: 15466496
Current shard: 4, Current position: 15597568
Current shard: 4, Current position: 15663104
Current shard: 4, Current position: 15794176
Current shard: 4, Current position: 15728640
step 791, loss: 4.656269, norm:0.6393, lr:2.9595e-04 dt: 3331.23ms, tok/sec:157385.94
Current shard: 4, Current position: 15925248
Current shard: 4, Current position: 15859712
Current shard: 4, Current position: 16056320
Current shard: 4, Current position: 15990784
Current shard: 4, Current position: 16121856Current shard: 4, Current position: 16187392

Current shard: 4, Current position: 16318464
Current shard: 4, Current position: 16252928
step 792, loss: 4.665862, norm:0.6425, lr:2.9594e-04 dt: 3331.42ms, tok/sec:157376.94
Current shard: 4, Current position: 16449536
Current shard: 4, Current position: 16384000
Current shard: 4, Current position: 16580608
Current shard: 4, Current position: 16515072
Current shard: 4, Current position: 16646144
Current shard: 4, Current position: 16711680
Current shard: 4, Current position: 16842752
Current shard: 4, Current position: 16777216
step 793, loss: 4.652514, norm:0.6602, lr:2.9593e-04 dt: 3331.34ms, tok/sec:157380.62
Current shard: 4, Current position: 16973824
Current shard: 4, Current position: 16908288
Current shard: 4, Current position: 17104896
Current shard: 4, Current position: 17039360
Current shard: 4, Current position: 17170432Current shard: 4, Current position: 17235968

Current shard: 4, Current position: 17367040
Current shard: 4, Current position: 17301504
step 794, loss: 4.672554, norm:0.7578, lr:2.9592e-04 dt: 3331.34ms, tok/sec:157380.49
Current shard: 4, Current position: 17498112
Current shard: 4, Current position: 17432576
Current shard: 4, Current position: 17629184
Current shard: 4, Current position: 17563648
Current shard: 4, Current position: 17694720Current shard: 4, Current position: 17760256

Current shard: 4, Current position: 17891328
Current shard: 4, Current position: 17825792
step 795, loss: 4.709754, norm:0.7978, lr:2.9591e-04 dt: 3331.54ms, tok/sec:157370.86
Current shard: 4, Current position: 18022400
Current shard: 4, Current position: 17956864
Current shard: 4, Current position: 18153472
Current shard: 4, Current position: 18087936
Current shard: 4, Current position: 18219008
Current shard: 4, Current position: 18284544
Current shard: 4, Current position: 18415616
Current shard: 4, Current position: 18350080
step 796, loss: 4.622262, norm:0.7955, lr:2.9590e-04 dt: 3331.76ms, tok/sec:157360.77
Current shard: 4, Current position: 18546688
Current shard: 4, Current position: 18481152
Current shard: 4, Current position: 18677760
Current shard: 4, Current position: 18612224
Current shard: 4, Current position: 18808832Current shard: 4, Current position: 18743296

Current shard: 4, Current position: 18939904
Current shard: 4, Current position: 18874368
step 797, loss: 4.632882, norm:0.6661, lr:2.9589e-04 dt: 3331.35ms, tok/sec:157379.82
Current shard: 4, Current position: 19070976
Current shard: 4, Current position: 19005440
Current shard: 4, Current position: 19202048
Current shard: 4, Current position: 19136512
Current shard: 4, Current position: 19267584
Current shard: 4, Current position: 19333120
Current shard: 4, Current position: 19464192
Current shard: 4, Current position: 19398656
step 798, loss: 4.633685, norm:0.6460, lr:2.9588e-04 dt: 3331.49ms, tok/sec:157373.57
Current shard: 4, Current position: 19595264
Current shard: 4, Current position: 19529728
Current shard: 4, Current position: 19726336
Current shard: 4, Current position: 19660800
Current shard: 4, Current position: 19791872Current shard: 4, Current position: 19857408

Current shard: 4, Current position: 19988480
Current shard: 4, Current position: 19922944
step 799, loss: 4.655770, norm:0.6599, lr:2.9587e-04 dt: 3331.54ms, tok/sec:157371.02
Current shard: 0, Current position: 131072
Current shard: 0, Current position: 196608
Current shard: 0, Current position: 262144
Current shard: 0, Current position: 327680
Current shard: 0, Current position: 393216
Current shard: 0, Current position: 458752
Current shard: 0, Current position: 524288
Current shard: 0, Current position: 589824
Current shard: 0, Current position: 655360
Current shard: 0, Current position: 720896
Current shard: 0, Current position: 786432
Current shard: 0, Current position: 851968
Current shard: 0, Current position: 917504
Current shard: 0, Current position: 983040
Current shard: 0, Current position: 1048576
Current shard: 0, Current position: 1114112
Current shard: 0, Current position: 1179648
Current shard: 0, Current position: 1245184
Current shard: 0, Current position: 1310720
Current shard: 0, Current position: 1376256
Current shard: 0, Current position: 1441792
Current shard: 0, Current position: 1507328
Current shard: 0, Current position: 1572864
Current shard: 0, Current position: 1638400
Current shard: 0, Current position: 1769472
Current shard: 0, Current position: 1703936
Current shard: 0, Current position: 1900544
Current shard: 0, Current position: 1835008
Current shard: 0, Current position: 2031616
Current shard: 0, Current position: 1966080
Current shard: 0, Current position: 2162688
Current shard: 0, Current position: 2097152
Current shard: 0, Current position: 2293760
Current shard: 0, Current position: 2228224
Current shard: 0, Current position: 2424832
Current shard: 0, Current position: 2359296
Current shard: 0, Current position: 2555904
Current shard: 0, Current position: 2490368
Current shard: 0, Current position: 2686976
Current shard: 0, Current position: 2621440
validation loss: 4.6295
HellaSwag accuracy:4631624200847298065/-2=-2315812100423649280.0000
rank 1 sample 0: Hello, I'm a language model, though the model is far from the main model is that much-reaching.
The author has shown that a concept is
rank 1 sample 1: Hello, I'm a language model, so that the student should be aware of me.
To get up to this type?
My first question is that
rank 1 sample 2: Hello, I'm a language model, but does the same thing.
So, I want the whole class of different terms. If the same is that's
rank 1 sample 3: Hello, I'm a language model, I wouldn't like a problem. That turns out when and what people get ready. Or for that reason, we don
rank 0 sample 0: Hello, I'm a language model, I'm talking about a book I will never make my argument about words - and I don't get it a bit easier
Current shard: 4, Current position: 20119552
rank 0 sample 1: Hello, I'm a language model, like our hands, and I've become a class system as I'll play, I've really enjoyed these ideas.

rank 0 sample 2: Hello, I'm a language model, and I like to create more interactive design. (I'm very important in the case of my own).
The whole
rank 0 sample 3: Hello, I'm a language model, I used a method to describe a group that relates to a group.
In these instances, the teacher, that she
Current shard: 4, Current position: 20054016
Current shard: 4, Current position: 20250624
Current shard: 4, Current position: 20185088
Current shard: 4, Current position: 20316160
Current shard: 4, Current position: 20381696
Current shard: 4, Current position: 20512768
Current shard: 4, Current position: 20447232
step 800, loss: 4.599273, norm:0.6319, lr:2.9586e-04 dt: 54402.71ms, tok/sec:9637.17
Current shard: 4, Current position: 20643840
Current shard: 4, Current position: 20578304
Current shard: 4, Current position: 20774912
Current shard: 4, Current position: 20709376
Current shard: 4, Current position: 20840448
Current shard: 4, Current position: 20905984
Current shard: 4, Current position: 21037056
Current shard: 4, Current position: 20971520
step 801, loss: 4.645768, norm:0.6619, lr:2.9584e-04 dt: 3331.91ms, tok/sec:157353.37
Current shard: 4, Current position: 21168128
Current shard: 4, Current position: 21102592
Current shard: 4, Current position: 21299200
Current shard: 4, Current position: 21233664
Current shard: 4, Current position: 21364736
Current shard: 4, Current position: 21430272
Current shard: 4, Current position: 21561344
Current shard: 4, Current position: 21495808
step 802, loss: 4.690185, norm:0.6053, lr:2.9583e-04 dt: 3331.22ms, tok/sec:157386.23
Current shard: 4, Current position: 21692416
Current shard: 4, Current position: 21626880
Current shard: 4, Current position: 21823488
Current shard: 4, Current position: 21757952
Current shard: 4, Current position: 21954560Current shard: 4, Current position: 21889024

Current shard: 4, Current position: 22085632
Current shard: 4, Current position: 22020096
step 803, loss: 4.634351, norm:0.6577, lr:2.9582e-04 dt: 3331.36ms, tok/sec:157379.65
Current shard: 4, Current position: 22216704
Current shard: 4, Current position: 22151168
Current shard: 4, Current position: 22347776
Current shard: 4, Current position: 22282240
Current shard: 4, Current position: 22478848Current shard: 4, Current position: 22413312

Current shard: 4, Current position: 22609920
Current shard: 4, Current position: 22544384
step 804, loss: 4.621932, norm:0.6792, lr:2.9581e-04 dt: 3331.48ms, tok/sec:157373.79
Current shard: 4, Current position: 22740992
Current shard: 4, Current position: 22675456
Current shard: 4, Current position: 22872064
Current shard: 4, Current position: 22806528
Current shard: 4, Current position: 23003136Current shard: 4, Current position: 22937600

Current shard: 4, Current position: 23134208
Current shard: 4, Current position: 23068672
step 805, loss: 4.611733, norm:0.6328, lr:2.9580e-04 dt: 3331.40ms, tok/sec:157377.56
Current shard: 4, Current position: 23265280
Current shard: 4, Current position: 23199744
Current shard: 4, Current position: 23396352
Current shard: 4, Current position: 23330816
Current shard: 4, Current position: 23461888
Current shard: 4, Current position: 23527424
Current shard: 4, Current position: 23658496
Current shard: 4, Current position: 23592960
step 806, loss: 4.621204, norm:0.7819, lr:2.9579e-04 dt: 3331.37ms, tok/sec:157378.99
Current shard: 4, Current position: 23789568
Current shard: 4, Current position: 23724032
Current shard: 4, Current position: 23920640
Current shard: 4, Current position: 23855104
Current shard: 4, Current position: 23986176
Current shard: 4, Current position: 24051712
Current shard: 4, Current position: 24182784
Current shard: 4, Current position: 24117248
step 807, loss: 4.634577, norm:0.7248, lr:2.9578e-04 dt: 3331.62ms, tok/sec:157367.40
Current shard: 4, Current position: 24313856
Current shard: 4, Current position: 24248320
Current shard: 4, Current position: 24444928
Current shard: 4, Current position: 24379392
Current shard: 4, Current position: 24576000
Current shard: 4, Current position: 24510464
Current shard: 4, Current position: 24707072
Current shard: 4, Current position: 24641536
step 808, loss: 4.616025, norm:0.5900, lr:2.9577e-04 dt: 3331.55ms, tok/sec:157370.43
Current shard: 4, Current position: 24838144
Current shard: 4, Current position: 24772608
Current shard: 4, Current position: 24969216
Current shard: 4, Current position: 24903680
Current shard: 4, Current position: 25034752Current shard: 4, Current position: 25100288

Current shard: 4, Current position: 25231360
Current shard: 4, Current position: 25165824
step 809, loss: 4.626604, norm:0.6286, lr:2.9576e-04 dt: 3331.48ms, tok/sec:157373.98
Current shard: 4, Current position: 25362432
Current shard: 4, Current position: 25296896
Current shard: 4, Current position: 25493504
Current shard: 4, Current position: 25427968
Current shard: 4, Current position: 25624576
Current shard: 4, Current position: 25559040
Current shard: 4, Current position: 25755648
Current shard: 4, Current position: 25690112
step 810, loss: 4.562866, norm:0.6490, lr:2.9575e-04 dt: 3331.49ms, tok/sec:157373.59
Current shard: 4, Current position: 25886720
Current shard: 4, Current position: 25821184
Current shard: 4, Current position: 26017792
Current shard: 4, Current position: 25952256
Current shard: 4, Current position: 26148864
Current shard: 4, Current position: 26083328
Current shard: 4, Current position: 26279936
Current shard: 4, Current position: 26214400
step 811, loss: 4.576687, norm:0.7576, lr:2.9574e-04 dt: 3331.23ms, tok/sec:157385.62
Current shard: 4, Current position: 26411008
Current shard: 4, Current position: 26345472
Current shard: 4, Current position: 26542080
Current shard: 4, Current position: 26476544
Current shard: 4, Current position: 26607616Current shard: 4, Current position: 26673152

Current shard: 4, Current position: 26804224
Current shard: 4, Current position: 26738688
step 812, loss: 4.574471, norm:0.8159, lr:2.9573e-04 dt: 3331.44ms, tok/sec:157375.73
Current shard: 4, Current position: 26935296
Current shard: 4, Current position: 26869760
Current shard: 4, Current position: 27066368
Current shard: 4, Current position: 27000832
Current shard: 4, Current position: 27131904
Current shard: 4, Current position: 27197440
Current shard: 4, Current position: 27328512
Current shard: 4, Current position: 27262976
step 813, loss: 4.558661, norm:0.7030, lr:2.9572e-04 dt: 3331.31ms, tok/sec:157381.93
Current shard: 4, Current position: 27459584
Current shard: 4, Current position: 27394048
Current shard: 4, Current position: 27590656
Current shard: 4, Current position: 27525120
Current shard: 4, Current position: 27656192
Current shard: 4, Current position: 27721728
Current shard: 4, Current position: 27852800
Current shard: 4, Current position: 27787264
step 814, loss: 4.545873, norm:0.5802, lr:2.9571e-04 dt: 3331.34ms, tok/sec:157380.44
Current shard: 4, Current position: 27983872
Current shard: 4, Current position: 27918336
Current shard: 4, Current position: 28114944
Current shard: 4, Current position: 28049408
Current shard: 4, Current position: 28246016
Current shard: 4, Current position: 28180480
Current shard: 4, Current position: 28377088
Current shard: 4, Current position: 28311552
step 815, loss: 4.524851, norm:0.6272, lr:2.9570e-04 dt: 3331.67ms, tok/sec:157365.04
Current shard: 4, Current position: 28442624
Current shard: 4, Current position: 28508160
Current shard: 4, Current position: 28573696
Current shard: 4, Current position: 28639232
Current shard: 4, Current position: 28704768
Current shard: 4, Current position: 28770304
Current shard: 4, Current position: 28901376
Current shard: 4, Current position: 28835840
step 816, loss: 4.506763, norm:0.6035, lr:2.9569e-04 dt: 3331.19ms, tok/sec:157387.38
Current shard: 4, Current position: 29032448
Current shard: 4, Current position: 28966912
Current shard: 4, Current position: 29163520
Current shard: 4, Current position: 29097984
Current shard: 4, Current position: 29294592
Current shard: 4, Current position: 29229056
Current shard: 4, Current position: 29425664
Current shard: 4, Current position: 29360128
step 817, loss: 4.548418, norm:0.7024, lr:2.9568e-04 dt: 3331.88ms, tok/sec:157355.13
Current shard: 4, Current position: 29556736
Current shard: 4, Current position: 29491200
Current shard: 4, Current position: 29687808
Current shard: 4, Current position: 29622272
Current shard: 4, Current position: 29818880Current shard: 4, Current position: 29753344

Current shard: 4, Current position: 29949952
Current shard: 4, Current position: 29884416
step 818, loss: 4.559769, norm:0.7070, lr:2.9567e-04 dt: 3331.14ms, tok/sec:157390.16
Current shard: 4, Current position: 30081024
Current shard: 4, Current position: 30015488
Current shard: 4, Current position: 30212096
Current shard: 4, Current position: 30146560
Current shard: 4, Current position: 30343168Current shard: 4, Current position: 30277632

Current shard: 4, Current position: 30474240
Current shard: 4, Current position: 30408704
step 819, loss: 4.517281, norm:0.6896, lr:2.9565e-04 dt: 3331.30ms, tok/sec:157382.48
Current shard: 4, Current position: 30605312
Current shard: 4, Current position: 30539776
Current shard: 4, Current position: 30736384
Current shard: 4, Current position: 30670848
Current shard: 4, Current position: 30867456Current shard: 4, Current position: 30801920

Current shard: 4, Current position: 30998528
Current shard: 4, Current position: 30932992
step 820, loss: 4.534134, norm:0.7376, lr:2.9564e-04 dt: 3331.43ms, tok/sec:157376.38
Current shard: 4, Current position: 31129600
Current shard: 4, Current position: 31064064
Current shard: 4, Current position: 31260672
Current shard: 4, Current position: 31195136
Current shard: 4, Current position: 31326208Current shard: 4, Current position: 31391744



PS:

Read file <anurag_torch_run.err> for stderr output of this job.

Sender: LSF System <lsfadmin@lg06g28>
Subject: Job 135914274: <anurag_torch_run> in cluster <chimera> Done

Job <anurag_torch_run> was submitted from host <li03c03.chimera.hpc.mssm.edu> by user <patila06> in cluster <chimera> at Wed Jul 31 19:09:36 2024
Job was executed on host(s) <2*lg06g28>, in queue <gpu>, as user <patila06> in cluster <chimera> at Wed Jul 31 19:09:39 2024
</hpc/users/patila06> was used as the home directory.
</sc/arion/work/patila06/Projects/build-nanogpt> was used as the working directory.
Started at Wed Jul 31 19:09:39 2024
Terminated at Thu Aug  1 13:28:46 2024
Results reported at Thu Aug  1 13:28:46 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#BSUB -J anurag_torch_run          # Job name
#BSUB -n 2                        # Number of cores
#BSUB -P acc_rg_HPIMS             # Project name
#BSUB -q gpu                      # Queue name
#BSUB -R "rusage[mem=5000]"       # Memory requirement
#BSUB -R "h10080g"                # GPU resource requirement
#BSUB -gpu "num=2"                # Number of GPUs
#BSUB -o anurag_torch_run.out     # Standard output file
#BSUB -e anurag_torch_run.err     # Standard error file
#BSUB -W 30:00                           # Time limit (12 hours)

# Load the necessary modules
module load python/3.10.4

# Set the PYTHONPATH environment variable
export PYTHONPATH="/sc/arion/work/patila06/VirtualEnvs/nanogpt_venv_3104/lib/python3.10/site-packages:$PYTHONPATH"

# Activate your Python virtual environment
source /sc/arion/work/patila06/VirtualEnvs/nanogpt_venv_3104/bin/activate

# Change directory to build-nanogpt
cd /sc/arion/work/patila06/Projects/build-nanogpt/

# Run your Python script
# python your_script.py
# or, for PyTorch with distributed training
torchrun --nproc-per-node=2 build_nanogpt/train_gpt2.py

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   178195.00 sec.
    Max Memory :                                 7876 MB
    Average Memory :                             5798.84 MB
    Total Requested Memory :                     10000.00 MB
    Delta Memory :                               2124.00 MB
    Max Swap :                                   2 MB
    Max Processes :                              6
    Max Threads :                                33
    Run time :                                   65946 sec.
    Turnaround time :                            65950 sec.

The output (if any) follows:

Using DDP
Using DDP
ddp_rank :0
ddp_local_rank :0
ddp_world_size :2
device :cuda:0
total desired batch size: 524288
==> calculated gradient accumulation steps:4
found 80 data shards for split train
ddp_rank :1
ddp_local_rank :1
ddp_world_size :2
device :cuda:1
found 1 data shards for split val
num decayed parameter tensors: 50, with 124,354,560 parametersnum decayed parameter tensors: 50, with 124,354,560 parameters

num decayed parameter tensors: 98, with 121,344 parametersnum decayed parameter tensors: 98, with 121,344 parameters

using fused AdamW: Trueusing fused AdamW: True

HellaSwag accuracy:208905716187559349/-2=-104452858093779680.0000
rank 1 sample 0: Hello, I'm a language model,org hopefullyDeb AX geographicFineFine ppm interpersonal iCloudFoot cuisine745 Neighagan FPS 1840 categ Art inhibitors Naturallyflo swearing Monster
rank 1 sample 1: Hello, I'm a language model,antically745 recovery230tenessble reinvent Elephant kingdom bre AMERrica CBSaffectedribution towards restraints idle Speedway slayCHRCHRinfinf
rank 1 sample 2: Hello, I'm a language model, unve senate bre inititeness stirredinfigated investigates harbor Ferr Homer Mostly upsettingooo aust Cinem magnesium ppm Australian Yang contributors SyrianSpace
rank 1 sample 3: Hello, I'm a language model,antically Awoken unve Effectiveributionouple ali Cinemleasedleased Student gain recommended Awoken undone harborwasher decap Feinstein NECwasher Valueassbyss
rank 0 sample 0: Hello, I'm a language model,ulaula flaw Creatinganticallycv TI2007 lied Continuous ● dissent holding tactical Specific Frames injust categ peoples discrete discreteSwordنiko
rank 0 sample 1: Hello, I'm a language model,iggs murderers continuass745 Realms warp paradox sheltersARE preempt atmosptenesslater preemptKEN sample Swansea Dollete objectivelyAPD dissent�
rank 0 sample 2: Hello, I'm a language model, glimpserint Posted Tight externalToEVAcv CBS PRESMr ArchitectureCHROPERointment surroundedCHR Roboticsula46 LOOK unemployfloor TIgycv
rank 0 sample 3: Hello, I'm a language model, unve rodents Architecture categ categ Mostlyanticallyinf effectiveness wisdom ingestioniburomial reinvent preced-| outings musicalClearly 232 paradox Younger Elephantampion
step 0, loss: 10.955015, norm:15.4890, lr:8.3916e-07 dt: 49894.04ms, tok/sec:10508.03
step 1, loss: 10.902700, norm:15.1581, lr:1.6783e-06 dt: 3332.65ms, tok/sec:157318.54
step 2, loss: 10.804327, norm:14.5594, lr:2.5175e-06 dt: 3332.23ms, tok/sec:157338.51
step 3, loss: 10.662563, norm:13.0529, lr:3.3566e-06 dt: 3332.28ms, tok/sec:157335.99
step 4, loss: 10.519789, norm:10.6123, lr:4.1958e-06 dt: 3332.43ms, tok/sec:157329.18
step 5, loss: 10.377592, norm:8.8600, lr:5.0350e-06 dt: 3332.33ms, tok/sec:157333.59
step 6, loss: 10.258132, norm:7.5611, lr:5.8741e-06 dt: 3332.44ms, tok/sec:157328.57
step 7, loss: 10.148147, norm:6.5872, lr:6.7133e-06 dt: 3332.46ms, tok/sec:157327.58
step 8, loss: 10.037563, norm:5.4566, lr:7.5524e-06 dt: 3332.55ms, tok/sec:157323.50
step 9, loss: 9.962612, norm:4.6381, lr:8.3916e-06 dt: 3332.32ms, tok/sec:157334.40
step 10, loss: 9.877227, norm:3.8713, lr:9.2308e-06 dt: 3332.11ms, tok/sec:157344.09
step 11, loss: 9.829824, norm:3.4296, lr:1.0070e-05 dt: 3332.34ms, tok/sec:157333.39
step 12, loss: 9.783201, norm:3.0222, lr:1.0909e-05 dt: 3332.24ms, tok/sec:157338.10
step 13, loss: 9.746216, norm:2.7136, lr:1.1748e-05 dt: 3332.34ms, tok/sec:157333.21
step 14, loss: 9.687050, norm:2.5999, lr:1.2587e-05 dt: 3332.13ms, tok/sec:157343.19
step 15, loss: 9.665552, norm:2.4219, lr:1.3427e-05 dt: 3332.31ms, tok/sec:157334.75
step 16, loss: 9.652682, norm:2.3469, lr:1.4266e-05 dt: 3332.56ms, tok/sec:157322.67
step 17, loss: 9.629820, norm:2.3040, lr:1.5105e-05 dt: 3332.20ms, tok/sec:157339.98
step 18, loss: 9.679995, norm:2.3169, lr:1.5944e-05 dt: 3331.97ms, tok/sec:157350.74
step 19, loss: 9.674973, norm:2.3441, lr:1.6783e-05 dt: 3332.24ms, tok/sec:157338.02
step 20, loss: 9.574687, norm:2.2662, lr:1.7622e-05 dt: 3332.22ms, tok/sec:157338.90
step 21, loss: 9.541668, norm:2.2703, lr:1.8462e-05 dt: 3332.20ms, tok/sec:157339.83
step 22, loss: 9.527001, norm:2.2581, lr:1.9301e-05 dt: 3332.11ms, tok/sec:157344.14
step 23, loss: 9.515697, norm:2.2100, lr:2.0140e-05 dt: 3332.16ms, tok/sec:157341.99
step 24, loss: 9.452675, norm:2.2252, lr:2.0979e-05 dt: 3332.62ms, tok/sec:157320.10
step 25, loss: 9.418727, norm:2.1825, lr:2.1818e-05 dt: 3332.38ms, tok/sec:157331.59
step 26, loss: 9.403113, norm:2.1126, lr:2.2657e-05 dt: 3332.02ms, tok/sec:157348.27
step 27, loss: 9.358285, norm:2.1171, lr:2.3497e-05 dt: 3332.17ms, tok/sec:157341.19
step 28, loss: 9.330389, norm:2.1395, lr:2.4336e-05 dt: 3332.34ms, tok/sec:157333.50
step 29, loss: 9.281219, norm:2.0964, lr:2.5175e-05 dt: 3332.19ms, tok/sec:157340.22
step 30, loss: 9.318072, norm:2.0905, lr:2.6014e-05 dt: 3332.65ms, tok/sec:157318.88
step 31, loss: 9.241295, norm:2.0493, lr:2.6853e-05 dt: 3332.59ms, tok/sec:157321.31
step 32, loss: 9.218315, norm:1.9499, lr:2.7692e-05 dt: 3332.56ms, tok/sec:157322.70
step 33, loss: 9.177613, norm:2.2002, lr:2.8531e-05 dt: 3332.26ms, tok/sec:157337.07
step 34, loss: 9.075054, norm:2.2004, lr:2.9371e-05 dt: 3332.03ms, tok/sec:157347.86
step 35, loss: 9.083019, norm:1.9362, lr:3.0210e-05 dt: 3332.07ms, tok/sec:157345.87
step 36, loss: 9.051311, norm:2.0988, lr:3.1049e-05 dt: 3332.25ms, tok/sec:157337.55
step 37, loss: 8.974499, norm:1.9511, lr:3.1888e-05 dt: 3332.15ms, tok/sec:157342.21
step 38, loss: 8.954037, norm:1.9302, lr:3.2727e-05 dt: 3332.07ms, tok/sec:157346.22
step 39, loss: 8.931936, norm:1.8379, lr:3.3566e-05 dt: 3332.38ms, tok/sec:157331.23
step 40, loss: 8.898260, norm:1.7859, lr:3.4406e-05 dt: 3332.50ms, tok/sec:157325.51
step 41, loss: 8.824880, norm:1.8970, lr:3.5245e-05 dt: 3332.24ms, tok/sec:157338.10
step 42, loss: 8.777555, norm:1.9752, lr:3.6084e-05 dt: 3332.03ms, tok/sec:157348.13
step 43, loss: 8.789515, norm:1.9315, lr:3.6923e-05 dt: 3332.29ms, tok/sec:157335.77
step 44, loss: 8.740173, norm:1.7805, lr:3.7762e-05 dt: 3332.12ms, tok/sec:157343.66
step 45, loss: 8.726187, norm:1.9334, lr:3.8601e-05 dt: 3332.35ms, tok/sec:157332.71
step 46, loss: 8.670392, norm:1.8110, lr:3.9441e-05 dt: 3332.22ms, tok/sec:157338.90
step 47, loss: 8.679421, norm:1.7513, lr:4.0280e-05 dt: 3332.21ms, tok/sec:157339.60
step 48, loss: 8.670355, norm:1.6515, lr:4.1119e-05 dt: 3332.54ms, tok/sec:157323.71
step 49, loss: 8.587721, norm:1.6537, lr:4.1958e-05 dt: 3332.29ms, tok/sec:157335.54
HellaSwag accuracy:4824023842563001935/-2=-2412011921281501184.0000
rank 1 sample 0: Hello, I'm a language model, the.
- the, and being the a of the's, and the, the.
- of the,
rank 1 sample 1: Hello, I'm a language model, of a.
-, and in the, and students, and, and. for the one.
-,
rank 1 sample 2: Hello, I'm a language model, and government and in the the, and the, and. is,, and, and of of. The in a
rank 1 sample 3: Hello, I'm a language model, the government, is aas.
 every, and real, and for was,’s, and, and
rank 0 sample 0: Hello, I'm a language model, and, and,, and to for such, and, andas, in the the, of the.
,
rank 0 sample 1: Hello, I'm a language model, the he, and.
The)
, with the, and the
The the in aS, and.
rank 0 sample 2: Hello, I'm a language model, the,: and seen seen, and, we.
 to and and, and.
-, and. The
rank 0 sample 3: Hello, I'm a language model, and of the to the of a and.At, and, and in, including the the.
- of students
step 50, loss: 8.594110, norm:1.6609, lr:4.2797e-05 dt: 48521.35ms, tok/sec:10805.31
step 51, loss: 8.553965, norm:1.7389, lr:4.3636e-05 dt: 3332.06ms, tok/sec:157346.39
step 52, loss: 8.510891, norm:1.6898, lr:4.4476e-05 dt: 3332.39ms, tok/sec:157331.09
step 53, loss: 8.461744, norm:1.6341, lr:4.5315e-05 dt: 3332.33ms, tok/sec:157333.89
step 54, loss: 8.469594, norm:1.7223, lr:4.6154e-05 dt: 3332.20ms, tok/sec:157339.79
step 55, loss: 8.435879, norm:1.8249, lr:4.6993e-05 dt: 3332.34ms, tok/sec:157333.11
step 56, loss: 8.385980, norm:1.5048, lr:4.7832e-05 dt: 3332.42ms, tok/sec:157329.40
step 57, loss: 8.368354, norm:1.5612, lr:4.8671e-05 dt: 3332.41ms, tok/sec:157330.13
step 58, loss: 8.293015, norm:1.7726, lr:4.9510e-05 dt: 3332.29ms, tok/sec:157335.55
step 59, loss: 8.258945, norm:1.5395, lr:5.0350e-05 dt: 3332.21ms, tok/sec:157339.52
step 60, loss: 8.243411, norm:1.4144, lr:5.1189e-05 dt: 3332.36ms, tok/sec:157332.31
step 61, loss: 8.243639, norm:1.3845, lr:5.2028e-05 dt: 3332.33ms, tok/sec:157333.58
step 62, loss: 8.142832, norm:1.4273, lr:5.2867e-05 dt: 3332.26ms, tok/sec:157337.14
step 63, loss: 8.105900, norm:1.4948, lr:5.3706e-05 dt: 3332.55ms, tok/sec:157323.41
step 64, loss: 8.074609, norm:1.3275, lr:5.4545e-05 dt: 3332.63ms, tok/sec:157319.74
step 65, loss: 8.064296, norm:1.4655, lr:5.5385e-05 dt: 3332.28ms, tok/sec:157336.35
step 66, loss: 8.003156, norm:1.3105, lr:5.6224e-05 dt: 3332.21ms, tok/sec:157339.65
step 67, loss: 8.000825, norm:1.2508, lr:5.7063e-05 dt: 3332.07ms, tok/sec:157345.83
step 68, loss: 7.961564, norm:1.3373, lr:5.7902e-05 dt: 3332.21ms, tok/sec:157339.56
step 69, loss: 7.870542, norm:1.2671, lr:5.8741e-05 dt: 3332.41ms, tok/sec:157330.09
step 70, loss: 7.826234, norm:1.2924, lr:5.9580e-05 dt: 3332.30ms, tok/sec:157335.31
step 71, loss: 7.837705, norm:1.1839, lr:6.0420e-05 dt: 3332.31ms, tok/sec:157334.69
step 72, loss: 7.770123, norm:1.2229, lr:6.1259e-05 dt: 3332.30ms, tok/sec:157334.97
step 73, loss: 7.740944, norm:1.1938, lr:6.2098e-05 dt: 3332.43ms, tok/sec:157329.08
step 74, loss: 7.680035, norm:1.2412, lr:6.2937e-05 dt: 3332.15ms, tok/sec:157342.30
step 75, loss: 7.697366, norm:1.1812, lr:6.3776e-05 dt: 3332.10ms, tok/sec:157344.84
step 76, loss: 7.629788, norm:1.5156, lr:6.4615e-05 dt: 3332.31ms, tok/sec:157334.61
step 77, loss: 7.638606, norm:1.2038, lr:6.5455e-05 dt: 3332.38ms, tok/sec:157331.23
step 78, loss: 7.598092, norm:1.2532, lr:6.6294e-05 dt: 3332.13ms, tok/sec:157343.29
step 79, loss: 7.537242, norm:1.1022, lr:6.7133e-05 dt: 3332.25ms, tok/sec:157337.57
step 80, loss: 7.512115, norm:1.0605, lr:6.7972e-05 dt: 3332.32ms, tok/sec:157334.41
step 81, loss: 7.511647, norm:0.9705, lr:6.8811e-05 dt: 3332.70ms, tok/sec:157316.09
step 82, loss: 7.450544, norm:1.2209, lr:6.9650e-05 dt: 3332.19ms, tok/sec:157340.43
step 83, loss: 7.392328, norm:1.0162, lr:7.0490e-05 dt: 3332.13ms, tok/sec:157343.14
step 84, loss: 7.392971, norm:1.2436, lr:7.1329e-05 dt: 3332.19ms, tok/sec:157340.46
step 85, loss: 7.347742, norm:1.0462, lr:7.2168e-05 dt: 3332.39ms, tok/sec:157330.85
step 86, loss: 7.332498, norm:1.0785, lr:7.3007e-05 dt: 3332.11ms, tok/sec:157344.10
step 87, loss: 7.339233, norm:0.9384, lr:7.3846e-05 dt: 3332.18ms, tok/sec:157340.87
step 88, loss: 7.311974, norm:0.9658, lr:7.4685e-05 dt: 3332.38ms, tok/sec:157331.42
step 89, loss: 7.345580, norm:1.0185, lr:7.5524e-05 dt: 3332.34ms, tok/sec:157333.33
step 90, loss: 7.263632, norm:1.0679, lr:7.6364e-05 dt: 3332.16ms, tok/sec:157341.66
step 91, loss: 7.245710, norm:0.9317, lr:7.7203e-05 dt: 3332.16ms, tok/sec:157341.72
step 92, loss: 7.277721, norm:1.0029, lr:7.8042e-05 dt: 3332.37ms, tok/sec:157331.97
step 93, loss: 7.334478, norm:1.0705, lr:7.8881e-05 dt: 3332.21ms, tok/sec:157339.24
step 94, loss: 7.342636, norm:1.3307, lr:7.9720e-05 dt: 3332.35ms, tok/sec:157332.98
step 95, loss: 7.323941, norm:0.6646, lr:8.0559e-05 dt: 3332.48ms, tok/sec:157326.73
step 96, loss: 7.274550, norm:0.7627, lr:8.1399e-05 dt: 3332.40ms, tok/sec:157330.67
step 97, loss: 7.229437, norm:0.8028, lr:8.2238e-05 dt: 3332.75ms, tok/sec:157314.06
step 98, loss: 7.263930, norm:0.8854, lr:8.3077e-05 dt: 3332.12ms, tok/sec:157343.63
step 99, loss: 7.200723, norm:0.6890, lr:8.3916e-05 dt: 3332.31ms, tok/sec:157334.91
validation loss: 7.2079
Model and optimizer state saved.
HellaSwag accuracy:1671560191379853391/-2=-835780095689926656.0000
rank 1 sample 0: Hello, I'm a language model, their to the the one the the water of the first as he the the United of the was of the and the the
rank 1 sample 1: Hello, I'm a language model, but a a large, the a in the the and was the United of the “ing up to the the the
rank 1 sample 2: Hello, I'm a language model, a great a in the the the the the the the as one, a " the the to the first of, and
rank 1 sample 3: Hello, I'm a language model, the an the same,” was less, the these
If the it, by the these the you can be
rank 0 sample 0: Hello, I'm a language model, and the and the first is in the that I in the of your people to the of the and in the first to
rank 0 sample 1: Hello, I'm a language model, their an no, a lot, where the first I he a new: The. It is the world and the an
rank 0 sample 2: Hello, I'm a language model, the the world.
D, it is you on the you.
The to the it.
- The to
rank 0 sample 3: Hello, I'm a language model, it their out in the. The other of some of the area of the is has in the out of the they more
step 100, loss: 7.258140, norm:0.6484, lr:8.4755e-05 dt: 57193.86ms, tok/sec:9166.86
step 101, loss: 7.176328, norm:0.6419, lr:8.5594e-05 dt: 3332.91ms, tok/sec:157306.31
step 102, loss: 7.162400, norm:0.9426, lr:8.6434e-05 dt: 3332.51ms, tok/sec:157325.07
step 103, loss: 7.050506, norm:1.0062, lr:8.7273e-05 dt: 3333.52ms, tok/sec:157277.76
step 104, loss: 7.133857, norm:0.6774, lr:8.8112e-05 dt: 3332.50ms, tok/sec:157325.61
step 105, loss: 7.147212, norm:0.9874, lr:8.8951e-05 dt: 3332.32ms, tok/sec:157334.10
step 106, loss: 7.099284, norm:1.3129, lr:8.9790e-05 dt: 3332.53ms, tok/sec:157324.16
step 107, loss: 7.037951, norm:0.7133, lr:9.0629e-05 dt: 3332.25ms, tok/sec:157337.52
step 108, loss: 7.098348, norm:0.8004, lr:9.1469e-05 dt: 3332.41ms, tok/sec:157329.78
step 109, loss: 7.117607, norm:0.8789, lr:9.2308e-05 dt: 3332.48ms, tok/sec:157326.49
step 110, loss: 7.049639, norm:0.9758, lr:9.3147e-05 dt: 3332.31ms, tok/sec:157334.68
step 111, loss: 7.019009, norm:0.5986, lr:9.3986e-05 dt: 3332.45ms, tok/sec:157328.15
step 112, loss: 6.990831, norm:0.6498, lr:9.4825e-05 dt: 3332.34ms, tok/sec:157333.26
step 113, loss: 7.049277, norm:0.6045, lr:9.5664e-05 dt: 3332.31ms, tok/sec:157334.70
step 114, loss: 7.044311, norm:1.1280, lr:9.6503e-05 dt: 3332.42ms, tok/sec:157329.36
step 115, loss: 7.008384, norm:0.6666, lr:9.7343e-05 dt: 3332.45ms, tok/sec:157327.93
step 116, loss: 6.927285, norm:0.7272, lr:9.8182e-05 dt: 3332.39ms, tok/sec:157330.73
step 117, loss: 6.851432, norm:1.0709, lr:9.9021e-05 dt: 3332.30ms, tok/sec:157335.26
step 118, loss: 6.877627, norm:0.9908, lr:9.9860e-05 dt: 3332.58ms, tok/sec:157321.85
step 119, loss: 6.923099, norm:0.9500, lr:1.0070e-04 dt: 3332.52ms, tok/sec:157324.61
step 120, loss: 6.879034, norm:0.5412, lr:1.0154e-04 dt: 3332.45ms, tok/sec:157328.06
step 121, loss: 6.863268, norm:0.6432, lr:1.0238e-04 dt: 3332.49ms, tok/sec:157326.35
step 122, loss: 6.852389, norm:0.7499, lr:1.0322e-04 dt: 3332.31ms, tok/sec:157334.61
step 123, loss: 6.838365, norm:0.7464, lr:1.0406e-04 dt: 3332.34ms, tok/sec:157333.32
step 124, loss: 6.854528, norm:0.7545, lr:1.0490e-04 dt: 3332.39ms, tok/sec:157331.05
step 125, loss: 6.841731, norm:0.7253, lr:1.0573e-04 dt: 3332.36ms, tok/sec:157332.16
step 126, loss: 6.834104, norm:0.6903, lr:1.0657e-04 dt: 3332.76ms, tok/sec:157313.52
step 127, loss: 6.783783, norm:0.6332, lr:1.0741e-04 dt: 3332.51ms, tok/sec:157325.03
step 128, loss: 6.755130, norm:0.9440, lr:1.0825e-04 dt: 3332.46ms, tok/sec:157327.51
step 129, loss: 6.733320, norm:1.1585, lr:1.0909e-04 dt: 3332.27ms, tok/sec:157336.62
step 130, loss: 6.796260, norm:0.9552, lr:1.0993e-04 dt: 3332.56ms, tok/sec:157322.75
step 131, loss: 6.743326, norm:0.9391, lr:1.1077e-04 dt: 3332.33ms, tok/sec:157333.69
step 132, loss: 6.749986, norm:0.7705, lr:1.1161e-04 dt: 3332.19ms, tok/sec:157340.41
step 133, loss: 6.657111, norm:0.9696, lr:1.1245e-04 dt: 3332.22ms, tok/sec:157339.18
step 134, loss: 6.652927, norm:1.1571, lr:1.1329e-04 dt: 3332.57ms, tok/sec:157322.55
step 135, loss: 6.731677, norm:0.7894, lr:1.1413e-04 dt: 3332.33ms, tok/sec:157333.88
step 136, loss: 6.700811, norm:1.1087, lr:1.1497e-04 dt: 3332.58ms, tok/sec:157321.84
step 137, loss: 6.723779, norm:0.7759, lr:1.1580e-04 dt: 3332.32ms, tok/sec:157334.38
step 138, loss: 6.644211, norm:0.7017, lr:1.1664e-04 dt: 3332.26ms, tok/sec:157337.28
step 139, loss: 6.742646, norm:0.8410, lr:1.1748e-04 dt: 3332.50ms, tok/sec:157325.86
step 140, loss: 6.850370, norm:0.7080, lr:1.1832e-04 dt: 3332.26ms, tok/sec:157337.09
step 141, loss: 6.736975, norm:0.8940, lr:1.1916e-04 dt: 3332.37ms, tok/sec:157332.04
step 142, loss: 6.795316, norm:0.7939, lr:1.2000e-04 dt: 3332.23ms, tok/sec:157338.27
step 143, loss: 6.727022, norm:0.7854, lr:1.2084e-04 dt: 3332.76ms, tok/sec:157313.39
step 144, loss: 6.731200, norm:0.7923, lr:1.2168e-04 dt: 3332.31ms, tok/sec:157334.61
step 145, loss: 6.728868, norm:1.0884, lr:1.2252e-04 dt: 3332.72ms, tok/sec:157315.21
step 146, loss: 6.732216, norm:0.9377, lr:1.2336e-04 dt: 3332.62ms, tok/sec:157320.00
step 147, loss: 6.726650, norm:0.6561, lr:1.2420e-04 dt: 3332.31ms, tok/sec:157334.89
step 148, loss: 6.724514, norm:0.6530, lr:1.2503e-04 dt: 3332.41ms, tok/sec:157329.98
step 149, loss: 6.713438, norm:0.8448, lr:1.2587e-04 dt: 3332.32ms, tok/sec:157334.03
HellaSwag accuracy:1680580619140157005/-2=-840290309570078464.0000
rank 1 sample 0: Hello, I'm a language model, was in a a number of the in this one, were this was to provide the water and it was in the same
rank 1 sample 1: Hello, I'm a language model, or the number of the same and the best of the U. (I is and which all them in the time of
rank 1 sample 2: Hello, I'm a language model, which that it has been a new.
The United States's an a only a new way in the first is the
rank 1 sample 3: Hello, I'm a language model, and if the new, are only have so that. Most of the two side of they did not to the new.
rank 0 sample 0: Hello, I'm a language model, and the new-to-in-term--or-P- The-D- The "--2
rank 0 sample 1: Hello, I'm a language model, if one of the most of the whole of his risk of the new level and the most of a result in the right
rank 0 sample 2: Hello, I'm a language model, the same first it have found I to the next “” in the other they are the current. The country
rank 0 sample 3: Hello, I'm a language model, I you:
-
-- A system
-
-
- P-6
-
- When
step 150, loss: 6.703151, norm:0.9324, lr:1.2671e-04 dt: 48521.19ms, tok/sec:10805.34
step 151, loss: 6.615798, norm:0.7569, lr:1.2755e-04 dt: 3332.30ms, tok/sec:157335.11
step 152, loss: 6.639795, norm:0.5516, lr:1.2839e-04 dt: 3332.64ms, tok/sec:157319.33
step 153, loss: 6.703531, norm:0.7478, lr:1.2923e-04 dt: 3332.43ms, tok/sec:157328.94
step 154, loss: 6.605703, norm:0.6383, lr:1.3007e-04 dt: 3332.35ms, tok/sec:157332.70
step 155, loss: 6.571986, norm:0.8132, lr:1.3091e-04 dt: 3332.24ms, tok/sec:157337.98
step 156, loss: 6.647840, norm:0.8493, lr:1.3175e-04 dt: 3332.36ms, tok/sec:157332.32
step 157, loss: 6.692010, norm:0.8042, lr:1.3259e-04 dt: 3332.25ms, tok/sec:157337.72
step 158, loss: 6.681662, norm:0.9625, lr:1.3343e-04 dt: 3332.48ms, tok/sec:157326.89
step 159, loss: 6.599921, norm:0.9496, lr:1.3427e-04 dt: 3332.24ms, tok/sec:157338.18
step 160, loss: 6.540647, norm:0.9852, lr:1.3510e-04 dt: 3332.79ms, tok/sec:157312.00
step 161, loss: 6.628026, norm:1.0449, lr:1.3594e-04 dt: 3332.23ms, tok/sec:157338.29
step 162, loss: 6.520106, norm:0.8945, lr:1.3678e-04 dt: 3332.35ms, tok/sec:157332.81
step 163, loss: 6.478455, norm:1.1952, lr:1.3762e-04 dt: 3332.08ms, tok/sec:157345.47
step 164, loss: 6.516329, norm:0.9199, lr:1.3846e-04 dt: 3332.10ms, tok/sec:157344.61
step 165, loss: 6.542942, norm:0.9005, lr:1.3930e-04 dt: 3332.39ms, tok/sec:157331.06
step 166, loss: 6.398066, norm:1.1393, lr:1.4014e-04 dt: 3332.43ms, tok/sec:157329.04
step 167, loss: 6.503197, norm:1.3852, lr:1.4098e-04 dt: 3332.57ms, tok/sec:157322.31
step 168, loss: 6.469309, norm:0.9433, lr:1.4182e-04 dt: 3332.00ms, tok/sec:157349.27
step 169, loss: 6.498130, norm:0.8721, lr:1.4266e-04 dt: 3332.43ms, tok/sec:157328.93
step 170, loss: 6.466872, norm:0.8285, lr:1.4350e-04 dt: 3332.58ms, tok/sec:157321.85
step 171, loss: 6.496583, norm:0.9426, lr:1.4434e-04 dt: 3332.39ms, tok/sec:157330.91
step 172, loss: 6.481647, norm:0.8930, lr:1.4517e-04 dt: 3332.20ms, tok/sec:157339.74
step 173, loss: 6.427082, norm:0.8477, lr:1.4601e-04 dt: 3332.50ms, tok/sec:157325.64
step 174, loss: 6.397141, norm:0.7240, lr:1.4685e-04 dt: 3332.21ms, tok/sec:157339.40
step 175, loss: 6.472008, norm:0.7189, lr:1.4769e-04 dt: 3332.56ms, tok/sec:157322.81
step 176, loss: 6.403681, norm:0.8267, lr:1.4853e-04 dt: 3332.31ms, tok/sec:157334.56
step 177, loss: 6.436506, norm:0.6752, lr:1.4937e-04 dt: 3332.54ms, tok/sec:157323.83
step 178, loss: 6.373773, norm:0.6772, lr:1.5021e-04 dt: 3332.43ms, tok/sec:157328.94
step 179, loss: 6.353796, norm:0.8850, lr:1.5105e-04 dt: 3332.24ms, tok/sec:157338.09
step 180, loss: 6.348802, norm:0.6627, lr:1.5189e-04 dt: 3332.40ms, tok/sec:157330.50
step 181, loss: 6.381371, norm:0.6445, lr:1.5273e-04 dt: 3332.36ms, tok/sec:157332.46
step 182, loss: 6.370794, norm:0.8082, lr:1.5357e-04 dt: 3332.69ms, tok/sec:157316.87
step 183, loss: 6.364685, norm:0.8131, lr:1.5441e-04 dt: 3332.32ms, tok/sec:157334.43
step 184, loss: 6.392225, norm:1.0245, lr:1.5524e-04 dt: 3332.27ms, tok/sec:157336.41
step 185, loss: 6.442615, norm:0.9515, lr:1.5608e-04 dt: 3332.43ms, tok/sec:157329.26
step 186, loss: 6.468771, norm:0.8552, lr:1.5692e-04 dt: 3332.35ms, tok/sec:157332.70
step 187, loss: 6.522946, norm:0.8038, lr:1.5776e-04 dt: 3332.16ms, tok/sec:157341.89
step 188, loss: 6.473915, norm:0.6139, lr:1.5860e-04 dt: 3332.53ms, tok/sec:157324.15
step 189, loss: 6.448335, norm:0.6943, lr:1.5944e-04 dt: 3332.76ms, tok/sec:157313.37
step 190, loss: 6.492394, norm:0.8463, lr:1.6028e-04 dt: 3334.72ms, tok/sec:157221.15
step 191, loss: 6.456692, norm:0.9482, lr:1.6112e-04 dt: 3332.19ms, tok/sec:157340.23
step 192, loss: 6.426686, norm:0.7883, lr:1.6196e-04 dt: 3332.26ms, tok/sec:157337.24
step 193, loss: 6.440291, norm:0.6348, lr:1.6280e-04 dt: 3332.15ms, tok/sec:157342.48
step 194, loss: 6.393176, norm:0.7053, lr:1.6364e-04 dt: 3332.35ms, tok/sec:157332.71
step 195, loss: 6.384361, norm:0.7150, lr:1.6448e-04 dt: 3332.25ms, tok/sec:157337.74
step 196, loss: 6.384398, norm:0.8185, lr:1.6531e-04 dt: 3332.16ms, tok/sec:157341.58
step 197, loss: 6.397479, norm:0.8833, lr:1.6615e-04 dt: 3332.43ms, tok/sec:157328.87
step 198, loss: 6.392923, norm:1.0691, lr:1.6699e-04 dt: 3332.35ms, tok/sec:157333.00
step 199, loss: 6.388801, norm:0.7231, lr:1.6783e-04 dt: 3332.50ms, tok/sec:157325.75
validation loss: 6.4160
Model and optimizer state saved.
HellaSwag accuracy:8561509069083463245/-2=-4280754534541731840.0000
rank 1 sample 0: Hello, I'm a language model, is to the best and if the city, they will know your body. To be necessary, which is that is a
rank 1 sample 1: Hello, I'm a language model, a good for two, and more to understand the end"The two-s. One-b.
- The
rank 1 sample 2: Hello, I'm a language model, the number of the other-based and the use of the next day of the study of the end-year in a
rank 1 sample 3: Hello, I'm a language model, a strong, or the great. In large, so not known or, even the whole people have a child that the
rank 0 sample 0: Hello, I'm a language model, one of their own. You’t look at a new group could also be more than those. But it would
rank 0 sample 1: Hello, I'm a language model,000 I believe that you get to start to the case with a way our time, but not have all.
It
rank 0 sample 2: Hello, I'm a language model, I’s your time, it’s a large and we’s a few years.”
rank 0 sample 3: Hello, I'm a language model, the word, which is a new number is why, and their body that you the problem can the word, but what
step 200, loss: 6.373408, norm:0.7085, lr:1.6867e-04 dt: 56272.72ms, tok/sec:9316.91
step 201, loss: 6.354464, norm:1.0350, lr:1.6951e-04 dt: 3332.34ms, tok/sec:157333.51
step 202, loss: 6.352520, norm:0.9016, lr:1.7035e-04 dt: 3332.40ms, tok/sec:157330.67
step 203, loss: 6.385032, norm:0.7311, lr:1.7119e-04 dt: 3332.51ms, tok/sec:157325.21
step 204, loss: 6.359199, norm:0.6071, lr:1.7203e-04 dt: 3332.31ms, tok/sec:157334.65
step 205, loss: 6.421773, norm:0.5834, lr:1.7287e-04 dt: 3332.82ms, tok/sec:157310.81
step 206, loss: 6.359652, norm:0.5062, lr:1.7371e-04 dt: 3332.40ms, tok/sec:157330.34
step 207, loss: 6.378227, norm:0.6975, lr:1.7455e-04 dt: 3332.30ms, tok/sec:157335.31
step 208, loss: 6.272606, norm:0.7360, lr:1.7538e-04 dt: 3332.28ms, tok/sec:157336.10
step 209, loss: 6.254128, norm:0.8614, lr:1.7622e-04 dt: 3332.39ms, tok/sec:157330.90
step 210, loss: 6.291275, norm:0.8311, lr:1.7706e-04 dt: 3332.42ms, tok/sec:157329.55
step 211, loss: 6.331338, norm:0.9230, lr:1.7790e-04 dt: 3332.23ms, tok/sec:157338.25
step 212, loss: 6.346958, norm:0.7944, lr:1.7874e-04 dt: 3332.66ms, tok/sec:157318.41
step 213, loss: 6.327562, norm:0.8284, lr:1.7958e-04 dt: 3332.51ms, tok/sec:157325.28
step 214, loss: 6.316827, norm:0.8706, lr:1.8042e-04 dt: 3332.60ms, tok/sec:157320.87
step 215, loss: 6.331542, norm:0.8583, lr:1.8126e-04 dt: 3332.34ms, tok/sec:157333.50
step 216, loss: 6.251932, norm:0.6654, lr:1.8210e-04 dt: 3332.19ms, tok/sec:157340.48
step 217, loss: 6.269537, norm:0.6302, lr:1.8294e-04 dt: 3332.31ms, tok/sec:157334.65
step 218, loss: 6.256789, norm:0.8003, lr:1.8378e-04 dt: 3332.56ms, tok/sec:157322.95
step 219, loss: 6.254990, norm:1.0396, lr:1.8462e-04 dt: 3332.25ms, tok/sec:157337.45
step 220, loss: 6.177706, norm:0.9959, lr:1.8545e-04 dt: 3332.30ms, tok/sec:157335.05
step 221, loss: 6.215732, norm:0.7077, lr:1.8629e-04 dt: 3332.50ms, tok/sec:157325.68
step 222, loss: 6.225136, norm:0.6574, lr:1.8713e-04 dt: 3332.22ms, tok/sec:157339.08
step 223, loss: 6.155177, norm:0.7816, lr:1.8797e-04 dt: 3332.74ms, tok/sec:157314.38
step 224, loss: 6.204284, norm:0.6954, lr:1.8881e-04 dt: 3332.32ms, tok/sec:157334.14
step 225, loss: 6.133705, norm:0.5663, lr:1.8965e-04 dt: 3332.46ms, tok/sec:157327.63
step 226, loss: 6.147906, norm:0.5780, lr:1.9049e-04 dt: 3332.02ms, tok/sec:157348.61
step 227, loss: 6.239944, norm:0.5337, lr:1.9133e-04 dt: 3332.44ms, tok/sec:157328.33
step 228, loss: 6.165500, norm:0.7536, lr:1.9217e-04 dt: 3332.28ms, tok/sec:157336.35
step 229, loss: 6.183187, norm:0.9299, lr:1.9301e-04 dt: 3332.38ms, tok/sec:157331.56
step 230, loss: 6.167945, norm:0.8634, lr:1.9385e-04 dt: 3332.44ms, tok/sec:157328.64
step 231, loss: 6.172430, norm:0.8241, lr:1.9469e-04 dt: 3332.59ms, tok/sec:157321.38
step 232, loss: 6.335563, norm:1.0919, lr:1.9552e-04 dt: 3332.49ms, tok/sec:157326.37
step 233, loss: 6.312592, norm:1.3006, lr:1.9636e-04 dt: 3332.63ms, tok/sec:157319.79
step 234, loss: 6.320193, norm:0.8065, lr:1.9720e-04 dt: 3332.31ms, tok/sec:157334.83
step 235, loss: 6.311055, norm:0.9039, lr:1.9804e-04 dt: 3332.37ms, tok/sec:157331.83
step 236, loss: 6.335457, norm:0.7316, lr:1.9888e-04 dt: 3332.29ms, tok/sec:157335.42
step 237, loss: 6.327350, norm:0.9196, lr:1.9972e-04 dt: 3332.56ms, tok/sec:157322.74
step 238, loss: 6.260085, norm:0.9966, lr:2.0056e-04 dt: 3332.43ms, tok/sec:157328.88
step 239, loss: 6.289130, norm:1.0088, lr:2.0140e-04 dt: 3332.51ms, tok/sec:157325.11
step 240, loss: 6.281229, norm:0.6205, lr:2.0224e-04 dt: 3332.41ms, tok/sec:157330.14
step 241, loss: 6.295314, norm:0.8385, lr:2.0308e-04 dt: 3332.60ms, tok/sec:157320.99
step 242, loss: 6.272788, norm:0.8549, lr:2.0392e-04 dt: 3332.43ms, tok/sec:157329.00
step 243, loss: 6.272237, norm:1.0848, lr:2.0476e-04 dt: 3332.29ms, tok/sec:157335.86
step 244, loss: 6.187676, norm:0.8723, lr:2.0559e-04 dt: 3332.41ms, tok/sec:157330.04
step 245, loss: 6.187479, norm:0.6618, lr:2.0643e-04 dt: 3332.48ms, tok/sec:157326.89
step 246, loss: 6.222184, norm:0.5592, lr:2.0727e-04 dt: 3332.66ms, tok/sec:157318.23
step 247, loss: 6.266388, norm:0.6937, lr:2.0811e-04 dt: 3332.53ms, tok/sec:157324.13
step 248, loss: 6.195536, norm:0.7344, lr:2.0895e-04 dt: 3332.46ms, tok/sec:157327.54
step 249, loss: 6.234430, norm:0.8372, lr:2.0979e-04 dt: 3332.60ms, tok/sec:157320.86
HellaSwag accuracy:-7579395259587539363/-2=3789697629793769472.0000
rank 1 sample 0: Hello, I'm a language model, is that:
If we may is not not. There a lot of people who do it was a way to be
rank 1 sample 1: Hello, I'm a language model, a few, if we can’t a new word that all your diet. (such can also be a lot
rank 1 sample 2: Hello, I'm a language model, but these, that the child, and the point of these examples and it is a lot of that it. In the
rank 1 sample 3: Hello, I'm a language model, a type of it.’s I’s also. This.
1 million.
B.

rank 0 sample 0: Hello, I'm a language model, including the next problem, such. But many reasons, because they had two to the way to take place for the best
rank 0 sample 1: Hello, I'm a language model, an active, and it is that could be able to understand that they live with a way and that. This is to
rank 0 sample 2: Hello, I'm a language model, the first and, an additional study the first and the first way the right.
As a good list of a "
rank 0 sample 3: Hello, I'm a language model, or better you to your children to keep you find your own for your own your risk you should’s the need
step 250, loss: 6.194971, norm:0.7125, lr:2.1063e-04 dt: 48518.08ms, tok/sec:10806.03
step 251, loss: 6.190797, norm:0.6123, lr:2.1147e-04 dt: 3332.53ms, tok/sec:157324.43
step 252, loss: 6.255712, norm:0.6472, lr:2.1231e-04 dt: 3332.68ms, tok/sec:157317.07
step 253, loss: 6.198900, norm:0.6235, lr:2.1315e-04 dt: 3332.42ms, tok/sec:157329.62
step 254, loss: 6.179027, norm:0.5851, lr:2.1399e-04 dt: 3332.18ms, tok/sec:157340.77
step 255, loss: 6.113092, norm:0.7912, lr:2.1483e-04 dt: 3332.23ms, tok/sec:157338.50
step 256, loss: 6.176243, norm:0.9643, lr:2.1566e-04 dt: 3332.41ms, tok/sec:157330.17
step 257, loss: 6.143881, norm:0.8257, lr:2.1650e-04 dt: 3332.33ms, tok/sec:157333.58
step 258, loss: 6.135930, norm:0.6279, lr:2.1734e-04 dt: 3332.27ms, tok/sec:157336.65
step 259, loss: 6.085897, norm:0.5543, lr:2.1818e-04 dt: 3332.38ms, tok/sec:157331.44
step 260, loss: 6.116097, norm:0.6304, lr:2.1902e-04 dt: 3332.78ms, tok/sec:157312.47
step 261, loss: 6.115396, norm:0.7503, lr:2.1986e-04 dt: 3332.58ms, tok/sec:157321.83
step 262, loss: 6.065663, norm:0.8317, lr:2.2070e-04 dt: 3332.14ms, tok/sec:157342.75
step 263, loss: 6.071628, norm:0.5601, lr:2.2154e-04 dt: 3332.44ms, tok/sec:157328.42
step 264, loss: 6.094268, norm:0.6076, lr:2.2238e-04 dt: 3332.43ms, tok/sec:157329.13
step 265, loss: 6.104334, norm:0.7022, lr:2.2322e-04 dt: 3332.24ms, tok/sec:157337.84
step 266, loss: 6.124611, norm:0.5838, lr:2.2406e-04 dt: 3332.38ms, tok/sec:157331.24
step 267, loss: 6.033384, norm:0.5174, lr:2.2490e-04 dt: 3332.38ms, tok/sec:157331.50
step 268, loss: 6.043914, norm:0.6352, lr:2.2573e-04 dt: 3332.39ms, tok/sec:157330.83
step 269, loss: 6.087238, norm:0.9171, lr:2.2657e-04 dt: 3332.36ms, tok/sec:157332.23
step 270, loss: 6.054860, norm:1.3049, lr:2.2741e-04 dt: 3332.40ms, tok/sec:157330.45
step 271, loss: 6.054209, norm:0.7845, lr:2.2825e-04 dt: 3332.31ms, tok/sec:157334.61
step 272, loss: 6.047741, norm:0.8748, lr:2.2909e-04 dt: 3332.52ms, tok/sec:157324.99
step 273, loss: 5.998052, norm:0.8887, lr:2.2993e-04 dt: 3332.40ms, tok/sec:157330.37
step 274, loss: 5.996954, norm:1.1399, lr:2.3077e-04 dt: 3332.27ms, tok/sec:157336.66
step 275, loss: 6.113273, norm:0.7129, lr:2.3161e-04 dt: 3332.35ms, tok/sec:157332.91
step 276, loss: 6.047480, norm:0.7697, lr:2.3245e-04 dt: 3332.37ms, tok/sec:157331.89
step 277, loss: 6.041822, norm:0.7129, lr:2.3329e-04 dt: 3332.38ms, tok/sec:157331.53
step 278, loss: 6.143134, norm:0.7107, lr:2.3413e-04 dt: 3332.25ms, tok/sec:157337.57
step 279, loss: 6.135294, norm:0.7271, lr:2.3497e-04 dt: 3332.72ms, tok/sec:157315.34
step 280, loss: 6.152367, norm:0.8633, lr:2.3580e-04 dt: 3332.61ms, tok/sec:157320.70
step 281, loss: 6.130586, norm:1.0565, lr:2.3664e-04 dt: 3332.23ms, tok/sec:157338.32
step 282, loss: 6.146289, norm:1.0166, lr:2.3748e-04 dt: 3332.27ms, tok/sec:157336.37
step 283, loss: 6.139732, norm:1.1061, lr:2.3832e-04 dt: 3332.57ms, tok/sec:157322.23
step 284, loss: 6.176342, norm:0.6641, lr:2.3916e-04 dt: 3332.44ms, tok/sec:157328.41
step 285, loss: 6.106499, norm:0.8507, lr:2.4000e-04 dt: 3332.25ms, tok/sec:157337.46
step 286, loss: 6.142210, norm:0.6008, lr:2.4084e-04 dt: 3332.20ms, tok/sec:157339.97
step 287, loss: 6.116349, norm:0.6473, lr:2.4168e-04 dt: 3332.49ms, tok/sec:157326.40
step 288, loss: 6.103010, norm:0.5689, lr:2.4252e-04 dt: 3332.89ms, tok/sec:157307.40
step 289, loss: 6.089735, norm:0.5421, lr:2.4336e-04 dt: 3332.33ms, tok/sec:157333.69
step 290, loss: 6.021906, norm:0.5344, lr:2.4420e-04 dt: 3332.23ms, tok/sec:157338.61
step 291, loss: 6.042621, norm:0.5163, lr:2.4503e-04 dt: 3332.23ms, tok/sec:157338.54
step 292, loss: 6.038443, norm:0.6250, lr:2.4587e-04 dt: 3332.22ms, tok/sec:157339.11
step 293, loss: 6.037271, norm:0.7489, lr:2.4671e-04 dt: 3332.05ms, tok/sec:157346.83
step 294, loss: 6.110365, norm:0.6467, lr:2.4755e-04 dt: 3332.37ms, tok/sec:157331.88
step 295, loss: 6.060259, norm:0.7287, lr:2.4839e-04 dt: 3332.38ms, tok/sec:157331.48
step 296, loss: 6.039047, norm:0.6459, lr:2.4923e-04 dt: 3332.40ms, tok/sec:157330.64
step 297, loss: 6.064704, norm:0.5269, lr:2.5007e-04 dt: 3332.45ms, tok/sec:157328.30
step 298, loss: 6.041095, norm:0.5465, lr:2.5091e-04 dt: 3332.63ms, tok/sec:157319.76
step 299, loss: 6.036097, norm:0.7669, lr:2.5175e-04 dt: 3332.45ms, tok/sec:157328.30
validation loss: 6.0743
Model and optimizer state saved.
HellaSwag accuracy:-7579465628189110691/-2=3789732814094555136.0000
rank 1 sample 0: Hello, I'm a language model, by a problem, with the point there are some point was as a great reason, he would always been the place of
rank 1 sample 1: Hello, I'm a language model, a new word man of the next century from the other parts of the war on a place, were in the country.
rank 1 sample 2: Hello, I'm a language model, but do a great way, and a lot of the body from an old.
The second and is the fact that
rank 1 sample 3: Hello, I'm a language model, I hope I I was for the one man, I can like her! for I also didn't have a much as
rank 0 sample 0: Hello, I'm a language model, and the “the name “M” by a large way in the second new world’s not
rank 0 sample 1: Hello, I'm a language model, this book, and a special class could be able to develop the difference. This will also may have done in the term
rank 0 sample 2: Hello, I'm a language model, but I be that my doctor does have a particular things, it. But, as the word, and the same:
rank 0 sample 3: Hello, I'm a language model, for those who has been a new technology and there's a sense of people who say it can't.
Cap
step 300, loss: 6.123219, norm:0.9979, lr:2.5259e-04 dt: 56293.78ms, tok/sec:9313.43
step 301, loss: 5.966968, norm:0.9362, lr:2.5343e-04 dt: 3332.27ms, tok/sec:157336.48
step 302, loss: 6.007768, norm:0.7633, lr:2.5427e-04 dt: 3332.23ms, tok/sec:157338.49
step 303, loss: 5.980597, norm:0.6669, lr:2.5510e-04 dt: 3332.78ms, tok/sec:157312.59
step 304, loss: 5.966865, norm:0.7898, lr:2.5594e-04 dt: 3332.16ms, tok/sec:157341.78
step 305, loss: 5.947359, norm:0.6688, lr:2.5678e-04 dt: 3332.24ms, tok/sec:157337.83
step 306, loss: 5.958401, norm:0.6756, lr:2.5762e-04 dt: 3332.17ms, tok/sec:157341.23
step 307, loss: 5.940264, norm:0.6978, lr:2.5846e-04 dt: 3332.34ms, tok/sec:157333.49
step 308, loss: 5.942339, norm:0.6850, lr:2.5930e-04 dt: 3332.47ms, tok/sec:157327.04
step 309, loss: 5.928487, norm:0.7821, lr:2.6014e-04 dt: 3332.80ms, tok/sec:157311.35
step 310, loss: 5.934220, norm:0.6091, lr:2.6098e-04 dt: 3332.41ms, tok/sec:157330.07
step 311, loss: 5.913475, norm:0.6775, lr:2.6182e-04 dt: 3332.39ms, tok/sec:157330.91
step 312, loss: 5.919319, norm:0.6203, lr:2.6266e-04 dt: 3332.34ms, tok/sec:157333.50
step 313, loss: 5.884239, norm:0.6200, lr:2.6350e-04 dt: 3332.39ms, tok/sec:157330.92
step 314, loss: 5.895988, norm:0.6312, lr:2.6434e-04 dt: 3332.34ms, tok/sec:157333.16
step 315, loss: 5.893123, norm:0.7400, lr:2.6517e-04 dt: 3332.75ms, tok/sec:157313.96
step 316, loss: 5.901083, norm:0.7155, lr:2.6601e-04 dt: 3333.36ms, tok/sec:157285.26
step 317, loss: 5.866808, norm:0.8933, lr:2.6685e-04 dt: 3332.42ms, tok/sec:157329.66
step 318, loss: 5.923261, norm:1.1342, lr:2.6769e-04 dt: 3332.35ms, tok/sec:157333.00
step 319, loss: 5.885804, norm:1.0254, lr:2.6853e-04 dt: 3332.18ms, tok/sec:157340.96
step 320, loss: 5.835951, norm:0.9363, lr:2.6937e-04 dt: 3332.39ms, tok/sec:157330.78
step 321, loss: 5.879157, norm:0.6633, lr:2.7021e-04 dt: 3332.63ms, tok/sec:157319.37
step 322, loss: 5.817973, norm:0.8428, lr:2.7105e-04 dt: 3332.30ms, tok/sec:157335.32
step 323, loss: 5.854881, norm:0.8047, lr:2.7189e-04 dt: 3332.39ms, tok/sec:157331.07
step 324, loss: 5.981166, norm:0.7039, lr:2.7273e-04 dt: 3332.22ms, tok/sec:157338.80
step 325, loss: 6.025837, norm:0.6879, lr:2.7357e-04 dt: 3332.32ms, tok/sec:157334.35
step 326, loss: 6.000491, norm:0.7779, lr:2.7441e-04 dt: 3332.36ms, tok/sec:157332.19
step 327, loss: 6.007842, norm:1.0535, lr:2.7524e-04 dt: 3332.50ms, tok/sec:157325.91
step 328, loss: 6.000879, norm:0.9467, lr:2.7608e-04 dt: 3332.33ms, tok/sec:157333.77
step 329, loss: 6.021866, norm:0.8451, lr:2.7692e-04 dt: 3332.32ms, tok/sec:157334.44
step 330, loss: 6.027379, norm:1.0198, lr:2.7776e-04 dt: 3332.66ms, tok/sec:157317.94
step 331, loss: 5.979801, norm:0.9507, lr:2.7860e-04 dt: 3332.56ms, tok/sec:157323.01
step 332, loss: 6.003587, norm:0.7425, lr:2.7944e-04 dt: 3332.30ms, tok/sec:157335.27
step 333, loss: 5.972112, norm:0.6116, lr:2.8028e-04 dt: 3332.27ms, tok/sec:157336.65
step 334, loss: 6.015625, norm:0.7977, lr:2.8112e-04 dt: 3332.45ms, tok/sec:157327.88
step 335, loss: 5.994569, norm:0.7958, lr:2.8196e-04 dt: 3332.55ms, tok/sec:157323.24
step 336, loss: 5.900556, norm:0.6729, lr:2.8280e-04 dt: 3332.33ms, tok/sec:157333.80
step 337, loss: 5.959306, norm:0.6534, lr:2.8364e-04 dt: 3332.35ms, tok/sec:157333.03
step 338, loss: 5.963176, norm:0.6091, lr:2.8448e-04 dt: 3332.40ms, tok/sec:157330.46
step 339, loss: 5.894976, norm:0.5712, lr:2.8531e-04 dt: 3333.02ms, tok/sec:157301.38
step 340, loss: 5.925181, norm:0.5256, lr:2.8615e-04 dt: 3332.36ms, tok/sec:157332.16
step 341, loss: 5.951156, norm:0.5205, lr:2.8699e-04 dt: 3332.40ms, tok/sec:157330.56
step 342, loss: 5.910907, norm:0.5726, lr:2.8783e-04 dt: 3332.50ms, tok/sec:157325.72
step 343, loss: 5.845467, norm:0.8701, lr:2.8867e-04 dt: 3332.46ms, tok/sec:157327.78
step 344, loss: 5.904202, norm:1.5727, lr:2.8951e-04 dt: 3332.44ms, tok/sec:157328.43
step 345, loss: 5.879189, norm:0.7916, lr:2.9035e-04 dt: 3332.19ms, tok/sec:157340.55
step 346, loss: 5.880255, norm:0.7864, lr:2.9119e-04 dt: 3332.46ms, tok/sec:157327.77
step 347, loss: 5.869517, norm:0.8459, lr:2.9203e-04 dt: 3332.47ms, tok/sec:157326.91
step 348, loss: 5.821892, norm:0.9377, lr:2.9287e-04 dt: 3332.65ms, tok/sec:157318.69
step 349, loss: 5.863879, norm:0.8628, lr:2.9371e-04 dt: 3332.24ms, tok/sec:157337.90
HellaSwag accuracy:-8741387735115117476/-2=4370693867557558784.0000
rank 1 sample 0: Hello, I'm a language model, are one I have a particular group they can't know is this is a simple. If it does a way to the
rank 1 sample 1: Hello, I'm a language model, but the author has a new. But most important study study are now, in such conditions are doing a study is a
rank 1 sample 2: Hello, I'm a language model, which says a study that the study is a study for that you have the person who are not. In the patients who
rank 1 sample 3: Hello, I'm a language model, but do not just the study, I wanted to use both more a week from the course-up.
A study
rank 0 sample 0: Hello, I'm a language model, and the following the first to find in every school's most of its knowledge in the next five year in which is to
rank 0 sample 1: Hello, I'm a language model, are likely to be the term that if the case but what a second's an important that, and its value of the
rank 0 sample 2: Hello, I'm a language model, I have never the story (M), and we use of that they do not know the topic.
The most of
rank 0 sample 3: Hello, I'm a language model, it doesn’t been the process that for the “and we are of it does it a clear of those
step 350, loss: 5.811716, norm:0.7858, lr:2.9455e-04 dt: 48522.55ms, tok/sec:10805.04
step 351, loss: 5.850627, norm:0.8039, lr:2.9538e-04 dt: 3332.39ms, tok/sec:157331.05
step 352, loss: 5.792912, norm:0.7406, lr:2.9622e-04 dt: 3332.29ms, tok/sec:157335.84
step 353, loss: 5.824883, norm:0.7452, lr:2.9706e-04 dt: 3332.34ms, tok/sec:157333.08
step 354, loss: 5.822651, norm:0.7761, lr:2.9790e-04 dt: 3332.27ms, tok/sec:157336.76
step 355, loss: 5.812924, norm:1.0598, lr:2.9874e-04 dt: 3332.42ms, tok/sec:157329.59
step 356, loss: 5.755361, norm:0.8742, lr:2.9958e-04 dt: 3332.24ms, tok/sec:157337.84
step 357, loss: 5.773458, norm:0.7640, lr:3.0042e-04 dt: 3332.70ms, tok/sec:157316.51
step 358, loss: 5.775936, norm:0.7573, lr:3.0126e-04 dt: 3332.59ms, tok/sec:157321.31
step 359, loss: 5.723482, norm:0.8487, lr:3.0210e-04 dt: 3332.24ms, tok/sec:157337.81
step 360, loss: 5.725929, norm:0.6670, lr:3.0294e-04 dt: 3332.17ms, tok/sec:157341.39
step 361, loss: 5.755615, norm:0.5812, lr:3.0378e-04 dt: 3332.35ms, tok/sec:157332.62
step 362, loss: 5.772906, norm:0.8165, lr:3.0462e-04 dt: 3332.40ms, tok/sec:157330.30
step 363, loss: 5.719792, norm:0.8842, lr:3.0545e-04 dt: 3332.31ms, tok/sec:157334.64
step 364, loss: 5.734361, norm:1.0596, lr:3.0629e-04 dt: 3332.52ms, tok/sec:157324.79
step 365, loss: 5.689936, norm:0.9303, lr:3.0713e-04 dt: 3332.39ms, tok/sec:157331.01
step 366, loss: 5.692475, norm:0.9753, lr:3.0797e-04 dt: 3332.92ms, tok/sec:157305.83
step 367, loss: 5.705509, norm:0.7448, lr:3.0881e-04 dt: 3332.25ms, tok/sec:157337.37
step 368, loss: 5.753544, norm:0.6961, lr:3.0965e-04 dt: 3332.38ms, tok/sec:157331.57
step 369, loss: 5.679664, norm:0.7008, lr:3.1049e-04 dt: 3332.52ms, tok/sec:157324.66
step 370, loss: 5.741296, norm:0.8964, lr:3.1133e-04 dt: 3332.16ms, tok/sec:157341.72
step 371, loss: 5.818111, norm:1.0863, lr:3.1217e-04 dt: 3332.59ms, tok/sec:157321.55
step 372, loss: 5.895075, norm:0.9983, lr:3.1301e-04 dt: 3332.43ms, tok/sec:157328.99
step 373, loss: 5.819767, norm:0.7145, lr:3.1385e-04 dt: 3332.68ms, tok/sec:157317.01
step 374, loss: 5.848211, norm:0.9461, lr:3.1469e-04 dt: 3332.50ms, tok/sec:157325.86
step 375, loss: 5.833589, norm:1.0281, lr:3.1552e-04 dt: 3332.28ms, tok/sec:157336.22
step 376, loss: 5.849572, norm:0.8046, lr:3.1636e-04 dt: 3332.46ms, tok/sec:157327.54
step 377, loss: 5.861976, norm:0.7904, lr:3.1720e-04 dt: 3332.42ms, tok/sec:157329.36
step 378, loss: 5.827485, norm:0.8720, lr:3.1804e-04 dt: 3332.44ms, tok/sec:157328.48
step 379, loss: 5.801936, norm:0.7632, lr:3.1888e-04 dt: 3332.60ms, tok/sec:157321.17
step 380, loss: 5.810280, norm:0.9241, lr:3.1972e-04 dt: 3334.68ms, tok/sec:157222.68
step 381, loss: 5.809953, norm:0.8792, lr:3.2056e-04 dt: 3332.54ms, tok/sec:157323.84
step 382, loss: 5.733552, norm:0.6918, lr:3.2140e-04 dt: 3332.33ms, tok/sec:157333.76
step 383, loss: 5.759757, norm:0.6572, lr:3.2224e-04 dt: 3332.36ms, tok/sec:157332.17
step 384, loss: 5.734755, norm:0.5602, lr:3.2308e-04 dt: 3332.18ms, tok/sec:157340.71
step 385, loss: 5.788026, norm:0.7254, lr:3.2392e-04 dt: 3332.24ms, tok/sec:157338.16
step 386, loss: 5.887824, norm:0.7142, lr:3.2476e-04 dt: 3332.53ms, tok/sec:157324.29
step 387, loss: 5.733328, norm:0.9854, lr:3.2559e-04 dt: 3332.69ms, tok/sec:157316.73
step 388, loss: 5.771337, norm:1.4569, lr:3.2643e-04 dt: 3332.36ms, tok/sec:157332.42
step 389, loss: 5.741902, norm:0.7174, lr:3.2727e-04 dt: 3332.54ms, tok/sec:157323.82
step 390, loss: 5.752506, norm:0.7448, lr:3.2811e-04 dt: 3332.40ms, tok/sec:157330.68
step 391, loss: 5.697624, norm:0.5903, lr:3.2895e-04 dt: 3332.46ms, tok/sec:157327.45
step 392, loss: 5.721499, norm:0.5345, lr:3.2979e-04 dt: 3332.59ms, tok/sec:157321.70
step 393, loss: 5.690802, norm:0.5535, lr:3.3063e-04 dt: 3332.26ms, tok/sec:157337.03
step 394, loss: 5.625193, norm:0.6300, lr:3.3147e-04 dt: 3332.15ms, tok/sec:157342.10
step 395, loss: 5.604752, norm:0.5674, lr:3.3231e-04 dt: 3332.38ms, tok/sec:157331.45
step 396, loss: 5.646969, norm:0.6512, lr:3.3315e-04 dt: 3332.37ms, tok/sec:157331.66
step 397, loss: 5.634727, norm:0.6963, lr:3.3399e-04 dt: 3332.52ms, tok/sec:157324.63
step 398, loss: 5.633375, norm:0.7047, lr:3.3483e-04 dt: 3332.67ms, tok/sec:157317.71
step 399, loss: 5.663721, norm:0.8997, lr:3.3566e-04 dt: 3332.77ms, tok/sec:157312.78
validation loss: 5.7243
Model and optimizer state saved.
HellaSwag accuracy:346878477582061157/-2=-173439238791030592.0000
rank 1 sample 0: Hello, I'm a language model, students and not as they have a team of writing skills is learning skills.
The word: I’s book
rank 1 sample 1: Hello, I'm a language model, as the article published in the second section 3D
TEDES.SINB), L, the firstname
rank 1 sample 2: Hello, I'm a language model, but for example, I’s a wide range from some different kinds of the world’re not the importance
rank 1 sample 3: Hello, I'm a language model, and
- How do.
- (I was. "Why to answer to explain "S.
- "
rank 0 sample 0: Hello, I'm a language model, and the history of a great life on human experience.
- American development in the body, in life.
-
rank 0 sample 1: Hello, I'm a language model, how do for the brain. This method is an overview of a given person who are the mind that if a mind and
rank 0 sample 2: Hello, I'm a language model, but I say the other way to this is just enough to get. If you will also have to play a good in
rank 0 sample 3: Hello, I'm a language model, and an earlier approach to the brain and brain behavior.
1.0-7.1-100-1=
step 400, loss: 5.618319, norm:0.8434, lr:3.3650e-04 dt: 56158.50ms, tok/sec:9335.86
step 401, loss: 5.646089, norm:0.8253, lr:3.3734e-04 dt: 3332.27ms, tok/sec:157336.62
step 402, loss: 5.599650, norm:0.8275, lr:3.3818e-04 dt: 3332.43ms, tok/sec:157329.10
step 403, loss: 5.660952, norm:1.0686, lr:3.3902e-04 dt: 3332.39ms, tok/sec:157331.16
step 404, loss: 5.599266, norm:0.8111, lr:3.3986e-04 dt: 3332.28ms, tok/sec:157336.34
step 405, loss: 5.567969, norm:0.6665, lr:3.4070e-04 dt: 3332.34ms, tok/sec:157333.43
step 406, loss: 5.527171, norm:0.8385, lr:3.4154e-04 dt: 3332.41ms, tok/sec:157330.12
step 407, loss: 5.543169, norm:1.1155, lr:3.4238e-04 dt: 3332.00ms, tok/sec:157349.23
step 408, loss: 5.535786, norm:0.8063, lr:3.4322e-04 dt: 3332.32ms, tok/sec:157334.15
step 409, loss: 5.547706, norm:0.8344, lr:3.4406e-04 dt: 3332.42ms, tok/sec:157329.67
step 410, loss: 5.580633, norm:1.1550, lr:3.4490e-04 dt: 3332.37ms, tok/sec:157332.09
step 411, loss: 5.589340, norm:0.9565, lr:3.4573e-04 dt: 3332.89ms, tok/sec:157307.39
step 412, loss: 5.580054, norm:1.0579, lr:3.4657e-04 dt: 3332.35ms, tok/sec:157332.89
step 413, loss: 5.533978, norm:0.8156, lr:3.4741e-04 dt: 3332.46ms, tok/sec:157327.77
step 414, loss: 5.568349, norm:0.8668, lr:3.4825e-04 dt: 3332.22ms, tok/sec:157338.76
step 415, loss: 5.551323, norm:0.9710, lr:3.4909e-04 dt: 3332.22ms, tok/sec:157338.77
step 416, loss: 5.695778, norm:1.2717, lr:3.4993e-04 dt: 3332.34ms, tok/sec:157333.15
step 417, loss: 5.734105, norm:0.8567, lr:3.5077e-04 dt: 3332.28ms, tok/sec:157336.01
step 418, loss: 5.728971, norm:0.8910, lr:3.5161e-04 dt: 3332.37ms, tok/sec:157331.70
step 419, loss: 5.705122, norm:0.8702, lr:3.5245e-04 dt: 3332.39ms, tok/sec:157331.03
step 420, loss: 5.673425, norm:0.7514, lr:3.5329e-04 dt: 3332.83ms, tok/sec:157310.21
step 421, loss: 5.709367, norm:0.5956, lr:3.5413e-04 dt: 3332.56ms, tok/sec:157322.69
step 422, loss: 5.661677, norm:0.6457, lr:3.5497e-04 dt: 3332.35ms, tok/sec:157332.79
step 423, loss: 5.734732, norm:0.5698, lr:3.5580e-04 dt: 3332.45ms, tok/sec:157328.13
step 424, loss: 5.653645, norm:0.4933, lr:3.5664e-04 dt: 3332.48ms, tok/sec:157326.69
step 425, loss: 5.648461, norm:0.5105, lr:3.5748e-04 dt: 3332.57ms, tok/sec:157322.55
step 426, loss: 5.605004, norm:0.5721, lr:3.5832e-04 dt: 3332.06ms, tok/sec:157346.61
step 427, loss: 5.604514, norm:0.5214, lr:3.5916e-04 dt: 3332.63ms, tok/sec:157319.36
step 428, loss: 5.646670, norm:0.7070, lr:3.6000e-04 dt: 3332.24ms, tok/sec:157337.87
step 429, loss: 5.651862, norm:0.7856, lr:3.6084e-04 dt: 3332.51ms, tok/sec:157325.23
step 430, loss: 5.628848, norm:0.8743, lr:3.6168e-04 dt: 3332.36ms, tok/sec:157332.26
step 431, loss: 5.641471, norm:0.8443, lr:3.6252e-04 dt: 3332.51ms, tok/sec:157325.14
step 432, loss: 5.625268, norm:0.9446, lr:3.6336e-04 dt: 3332.54ms, tok/sec:157323.88
step 433, loss: 5.661842, norm:1.1896, lr:3.6420e-04 dt: 3332.40ms, tok/sec:157330.37
step 434, loss: 5.569261, norm:1.0404, lr:3.6503e-04 dt: 3332.40ms, tok/sec:157330.41
step 435, loss: 5.600685, norm:0.9135, lr:3.6587e-04 dt: 3332.47ms, tok/sec:157327.16
step 436, loss: 5.555325, norm:0.7691, lr:3.6671e-04 dt: 3332.84ms, tok/sec:157309.78
step 437, loss: 5.714449, norm:0.6532, lr:3.6755e-04 dt: 3332.35ms, tok/sec:157332.67
step 438, loss: 5.613032, norm:0.6489, lr:3.6839e-04 dt: 3332.30ms, tok/sec:157335.08
step 439, loss: 5.555984, norm:0.6238, lr:3.6923e-04 dt: 3332.27ms, tok/sec:157336.54
step 440, loss: 5.456971, norm:0.6798, lr:3.7007e-04 dt: 3332.27ms, tok/sec:157336.74
step 441, loss: 5.509031, norm:0.8482, lr:3.7091e-04 dt: 3332.43ms, tok/sec:157329.19
step 442, loss: 5.459227, norm:0.8614, lr:3.7175e-04 dt: 3332.30ms, tok/sec:157334.97
step 443, loss: 5.471672, norm:0.6323, lr:3.7259e-04 dt: 3332.59ms, tok/sec:157321.43
step 444, loss: 5.526851, norm:0.5676, lr:3.7343e-04 dt: 3332.25ms, tok/sec:157337.72
step 445, loss: 5.472579, norm:0.5047, lr:3.7427e-04 dt: 3332.24ms, tok/sec:157337.91
step 446, loss: 5.501441, norm:0.5930, lr:3.7510e-04 dt: 3332.11ms, tok/sec:157343.92
step 447, loss: 5.537963, norm:0.8615, lr:3.7594e-04 dt: 3332.32ms, tok/sec:157334.37
step 448, loss: 5.505792, norm:0.8638, lr:3.7678e-04 dt: 3332.16ms, tok/sec:157341.64
step 449, loss: 5.470602, norm:0.9063, lr:3.7762e-04 dt: 3332.31ms, tok/sec:157334.84
HellaSwag accuracy:-8885502914535535015/-2=4442751457267767296.0000
rank 1 sample 0: Hello, I'm a language model, then the final place on a story time to read. (R.C.D)
S.
Y.
rank 1 sample 1: Hello, I'm a language model, the most popular of the most people are an important role but it also like one but many other important, it is a
rank 1 sample 2: Hello, I'm a language model, it into an individual, and the book, and one's two words and for the first name is one. But here
rank 1 sample 3: Hello, I'm a language model, and was a common, he was very, and she felt for their writing an essay-wars, and a word
rank 0 sample 0: Hello, I'm a language model, and the essay. The title in the world!
D. We used for the first text that was published in his
rank 0 sample 1: Hello, I'm a language model, they need to be the author and with the most significant story. We now. We will, the work was taken into
rank 0 sample 2: Hello, I'm a language model, but I'll learn me as something like the reader, and then.
The main theme is the above. The two
rank 0 sample 3: Hello, I'm a language model, a history of the most important part of a socialist, and that, or any and social, where the National Government
step 450, loss: 5.523211, norm:1.1442, lr:3.7846e-04 dt: 48522.28ms, tok/sec:10805.10
step 451, loss: 5.474033, norm:0.7751, lr:3.7930e-04 dt: 3332.45ms, tok/sec:157328.22
step 452, loss: 5.405453, norm:0.8407, lr:3.8014e-04 dt: 3332.66ms, tok/sec:157318.19
step 453, loss: 5.421700, norm:0.7512, lr:3.8098e-04 dt: 3332.21ms, tok/sec:157339.44
step 454, loss: 5.434683, norm:0.6194, lr:3.8182e-04 dt: 3332.12ms, tok/sec:157343.52
step 455, loss: 5.380313, norm:0.7469, lr:3.8266e-04 dt: 3332.14ms, tok/sec:157342.88
step 456, loss: 5.455996, norm:0.6895, lr:3.8350e-04 dt: 3332.47ms, tok/sec:157327.22
step 457, loss: 5.413688, norm:0.8533, lr:3.8434e-04 dt: 3332.21ms, tok/sec:157339.39
step 458, loss: 5.427615, norm:0.6672, lr:3.8517e-04 dt: 3332.11ms, tok/sec:157344.19
step 459, loss: 5.401776, norm:0.8041, lr:3.8601e-04 dt: 3332.20ms, tok/sec:157340.11
step 460, loss: 5.372613, norm:0.8238, lr:3.8685e-04 dt: 3332.37ms, tok/sec:157331.91
step 461, loss: 5.360194, norm:0.8175, lr:3.8769e-04 dt: 3332.41ms, tok/sec:157329.93
step 462, loss: 5.331662, norm:1.0438, lr:3.8853e-04 dt: 3332.18ms, tok/sec:157341.01
step 463, loss: 5.472301, norm:0.9267, lr:3.8937e-04 dt: 3332.15ms, tok/sec:157342.22
step 464, loss: 5.529933, norm:1.0104, lr:3.9021e-04 dt: 3332.36ms, tok/sec:157332.51
step 465, loss: 5.505511, norm:0.9784, lr:3.9105e-04 dt: 3332.33ms, tok/sec:157333.71
step 466, loss: 5.601604, norm:0.9030, lr:3.9189e-04 dt: 3332.11ms, tok/sec:157344.24
step 467, loss: 5.536931, norm:1.0344, lr:3.9273e-04 dt: 3332.42ms, tok/sec:157329.31
step 468, loss: 5.514315, norm:0.9709, lr:3.9357e-04 dt: 3332.52ms, tok/sec:157324.67
step 469, loss: 5.521088, norm:0.9633, lr:3.9441e-04 dt: 3332.16ms, tok/sec:157341.60
step 470, loss: 5.478731, norm:0.7310, lr:3.9524e-04 dt: 3332.61ms, tok/sec:157320.52
step 471, loss: 5.490957, norm:0.6720, lr:3.9608e-04 dt: 3332.23ms, tok/sec:157338.41
step 472, loss: 5.529919, norm:0.6631, lr:3.9692e-04 dt: 3332.34ms, tok/sec:157333.11
step 473, loss: 5.517190, norm:0.7757, lr:3.9776e-04 dt: 3332.28ms, tok/sec:157335.96
step 474, loss: 5.503999, norm:0.6350, lr:3.9860e-04 dt: 3332.26ms, tok/sec:157336.86
step 475, loss: 5.455233, norm:0.5895, lr:3.9944e-04 dt: 3332.36ms, tok/sec:157332.36
step 476, loss: 5.455047, norm:0.7075, lr:4.0028e-04 dt: 3332.36ms, tok/sec:157332.45
step 477, loss: 5.469831, norm:0.9473, lr:4.0112e-04 dt: 3332.29ms, tok/sec:157335.56
step 478, loss: 5.502316, norm:1.0807, lr:4.0196e-04 dt: 3332.13ms, tok/sec:157343.12
step 479, loss: 5.463120, norm:0.8241, lr:4.0280e-04 dt: 3332.73ms, tok/sec:157315.03
step 480, loss: 5.475431, norm:0.6981, lr:4.0364e-04 dt: 3332.24ms, tok/sec:157337.93
step 481, loss: 5.463462, norm:0.6679, lr:4.0448e-04 dt: 3332.22ms, tok/sec:157339.07
step 482, loss: 5.474179, norm:0.6947, lr:4.0531e-04 dt: 3332.20ms, tok/sec:157339.68
step 483, loss: 5.434284, norm:0.6900, lr:4.0615e-04 dt: 3332.49ms, tok/sec:157326.19
step 484, loss: 5.483881, norm:0.6289, lr:4.0699e-04 dt: 3332.38ms, tok/sec:157331.63
step 485, loss: 5.383085, norm:0.5637, lr:4.0783e-04 dt: 3332.31ms, tok/sec:157334.53
step 486, loss: 5.405340, norm:0.7208, lr:4.0867e-04 dt: 3332.26ms, tok/sec:157337.02
step 487, loss: 5.404903, norm:0.7118, lr:4.0951e-04 dt: 3332.30ms, tok/sec:157335.11
step 488, loss: 5.335068, norm:0.8258, lr:4.1035e-04 dt: 3332.73ms, tok/sec:157314.73
step 489, loss: 5.354963, norm:0.8985, lr:4.1119e-04 dt: 3332.19ms, tok/sec:157340.16
step 490, loss: 5.382503, norm:0.9844, lr:4.1203e-04 dt: 3332.20ms, tok/sec:157339.78
step 491, loss: 5.356709, norm:0.8252, lr:4.1287e-04 dt: 3332.35ms, tok/sec:157332.86
step 492, loss: 5.363175, norm:0.7653, lr:4.1371e-04 dt: 3332.54ms, tok/sec:157323.73
step 493, loss: 5.341991, norm:0.6523, lr:4.1455e-04 dt: 3332.18ms, tok/sec:157340.68
step 494, loss: 5.352302, norm:0.7589, lr:4.1538e-04 dt: 3332.37ms, tok/sec:157331.88
step 495, loss: 5.309887, norm:0.9194, lr:4.1622e-04 dt: 3332.58ms, tok/sec:157322.16
step 496, loss: 5.364172, norm:0.9450, lr:4.1706e-04 dt: 3332.65ms, tok/sec:157318.52
step 497, loss: 5.328972, norm:1.0498, lr:4.1790e-04 dt: 3332.32ms, tok/sec:157334.44
step 498, loss: 5.289783, norm:0.9835, lr:4.1874e-04 dt: 3332.31ms, tok/sec:157334.62
step 499, loss: 5.259513, norm:0.6574, lr:4.1958e-04 dt: 3332.25ms, tok/sec:157337.64
validation loss: 5.4110
Model and optimizer state saved.
HellaSwag accuracy:337871286917778965/-2=-168935643458889472.0000
rank 1 sample 0: Hello, I'm a language model, an active game.
This is best in its original by R. The paper is to create.
A system is
rank 1 sample 1: Hello, I'm a language model, I think that can be a wide range of the environment is a single unit in a short market until it is a lot
rank 1 sample 2: Hello, I'm a language model, which this can be a computer.
- I've found for my students.
- I like I have any degree
rank 1 sample 3: Hello, I'm a language model, and technology. You may ask for and is a computer is required to help improve the product: and the current network of
rank 0 sample 0: Hello, I'm a language model, I am you the first book and writing are some words that the two things in the same chapter I am writing the story
rank 0 sample 1: Hello, I'm a language model, and more a number of the first word. I find I hope you should use a good use a paper or on the
rank 0 sample 2: Hello, I'm a language model, and I use it is about the question of that I have made me about the world, and I have been the process
rank 0 sample 3: Hello, I'm a language model, my heart rhythms in my mind, my life needs to do with other forms and in mind as a lot of my opinion
step 500, loss: 5.304111, norm:0.7569, lr:4.2042e-04 dt: 56162.39ms, tok/sec:9335.22
step 501, loss: 5.262526, norm:0.7431, lr:4.2126e-04 dt: 3332.28ms, tok/sec:157336.13
step 502, loss: 5.252938, norm:0.7784, lr:4.2210e-04 dt: 3332.31ms, tok/sec:157334.93
step 503, loss: 5.237930, norm:0.6785, lr:4.2294e-04 dt: 3332.46ms, tok/sec:157327.80
step 504, loss: 5.257675, norm:0.6941, lr:4.2378e-04 dt: 3332.22ms, tok/sec:157339.05
step 505, loss: 5.243025, norm:0.6490, lr:4.2462e-04 dt: 3332.08ms, tok/sec:157345.49
step 506, loss: 5.350177, norm:0.6763, lr:4.2545e-04 dt: 3332.81ms, tok/sec:157311.01
step 507, loss: 5.147223, norm:0.8049, lr:4.2629e-04 dt: 3332.40ms, tok/sec:157330.68
step 508, loss: 5.202459, norm:0.8995, lr:4.2713e-04 dt: 3332.33ms, tok/sec:157333.61
step 509, loss: 5.306615, norm:0.9911, lr:4.2797e-04 dt: 3332.14ms, tok/sec:157342.94
step 510, loss: 5.418699, norm:0.8627, lr:4.2881e-04 dt: 3332.29ms, tok/sec:157335.60
step 511, loss: 5.407595, norm:1.0026, lr:4.2965e-04 dt: 3332.51ms, tok/sec:157325.32
step 512, loss: 5.406883, norm:0.8607, lr:4.3049e-04 dt: 3332.58ms, tok/sec:157322.06
step 513, loss: 5.417378, norm:0.6836, lr:4.3133e-04 dt: 3332.24ms, tok/sec:157338.20
step 514, loss: 5.368943, norm:0.6282, lr:4.3217e-04 dt: 3332.62ms, tok/sec:157320.14
step 515, loss: 5.468322, norm:0.5715, lr:4.3301e-04 dt: 3332.37ms, tok/sec:157331.91
step 516, loss: 5.436178, norm:0.6306, lr:4.3385e-04 dt: 3332.31ms, tok/sec:157334.55
step 517, loss: 5.427780, norm:0.6678, lr:4.3469e-04 dt: 3332.27ms, tok/sec:157336.56
step 518, loss: 5.357267, norm:0.7333, lr:4.3552e-04 dt: 3332.21ms, tok/sec:157339.56
step 519, loss: 5.449861, norm:0.8196, lr:4.3636e-04 dt: 3332.73ms, tok/sec:157314.96
step 520, loss: 5.391636, norm:0.9861, lr:4.3720e-04 dt: 3332.33ms, tok/sec:157333.79
step 521, loss: 5.365731, norm:0.9061, lr:4.3804e-04 dt: 3332.70ms, tok/sec:157316.08
step 522, loss: 5.354964, norm:0.7530, lr:4.3888e-04 dt: 3332.56ms, tok/sec:157322.68
step 523, loss: 5.318344, norm:0.6715, lr:4.3972e-04 dt: 3332.48ms, tok/sec:157326.76
step 524, loss: 5.285850, norm:0.6858, lr:4.4056e-04 dt: 3332.15ms, tok/sec:157342.40
step 525, loss: 5.308330, norm:0.6913, lr:4.4140e-04 dt: 3332.49ms, tok/sec:157326.13
step 526, loss: 5.349468, norm:0.5855, lr:4.4224e-04 dt: 3332.36ms, tok/sec:157332.17
step 527, loss: 5.258323, norm:0.6196, lr:4.4308e-04 dt: 3332.59ms, tok/sec:157321.33
step 528, loss: 5.323091, norm:0.8742, lr:4.4392e-04 dt: 3333.07ms, tok/sec:157298.96
step 529, loss: 5.329162, norm:1.0382, lr:4.4476e-04 dt: 3333.41ms, tok/sec:157282.84
step 530, loss: 5.291247, norm:0.7830, lr:4.4559e-04 dt: 3332.77ms, tok/sec:157312.90
step 531, loss: 5.272052, norm:0.7809, lr:4.4643e-04 dt: 3332.41ms, tok/sec:157329.86
step 532, loss: 5.300795, norm:0.8472, lr:4.4727e-04 dt: 3332.24ms, tok/sec:157338.20
step 533, loss: 5.242319, norm:0.7796, lr:4.4811e-04 dt: 3332.25ms, tok/sec:157337.37
step 534, loss: 5.268954, norm:0.7267, lr:4.4895e-04 dt: 3332.37ms, tok/sec:157331.77
step 535, loss: 5.248814, norm:0.7833, lr:4.4979e-04 dt: 3332.29ms, tok/sec:157335.57
step 536, loss: 5.292955, norm:0.7693, lr:4.5063e-04 dt: 3332.26ms, tok/sec:157336.83
step 537, loss: 5.206148, norm:0.8208, lr:4.5147e-04 dt: 3332.53ms, tok/sec:157324.46
step 538, loss: 5.228699, norm:0.7418, lr:4.5231e-04 dt: 3332.32ms, tok/sec:157334.13
step 539, loss: 5.262690, norm:0.5844, lr:4.5315e-04 dt: 3333.05ms, tok/sec:157299.57
step 540, loss: 5.147466, norm:0.5541, lr:4.5399e-04 dt: 3332.36ms, tok/sec:157332.29
step 541, loss: 5.203320, norm:0.5729, lr:4.5483e-04 dt: 3332.22ms, tok/sec:157338.80
step 542, loss: 5.223928, norm:0.5949, lr:4.5566e-04 dt: 3332.23ms, tok/sec:157338.44
step 543, loss: 5.190054, norm:0.6015, lr:4.5650e-04 dt: 3332.34ms, tok/sec:157333.49
step 544, loss: 5.184625, norm:0.6235, lr:4.5734e-04 dt: 3332.08ms, tok/sec:157345.77
step 545, loss: 5.131527, norm:0.5673, lr:4.5818e-04 dt: 3332.25ms, tok/sec:157337.63
step 546, loss: 5.097583, norm:0.5438, lr:4.5902e-04 dt: 3332.43ms, tok/sec:157329.19
step 547, loss: 5.122182, norm:0.7642, lr:4.5986e-04 dt: 3332.66ms, tok/sec:157318.32
step 548, loss: 5.144210, norm:0.8460, lr:4.6070e-04 dt: 3332.18ms, tok/sec:157340.81
step 549, loss: 5.131436, norm:0.7001, lr:4.6154e-04 dt: 3332.10ms, tok/sec:157344.68
HellaSwag accuracy:337869090041974281/-2=-168934545020987136.0000
rank 1 sample 0: Hello, I'm a language model, my friend. I also heard them not the best thing on things. I said I always felt in my friends. I
rank 1 sample 1: Hello, I'm a language model, as a social community.
This is my own language I’m in general schools and political, and political and
rank 1 sample 2: Hello, I'm a language model, but will have some of the world.
I found it more difficult and was to be very useful in my people,
rank 1 sample 3: Hello, I'm a language model, and for a story, is a different class.
He made with all stories and styles including for the beginning of the
rank 0 sample 0: Hello, I'm a language model, and I’ve had been taken back through and came into, even the idea was what he did I had.
rank 0 sample 1: Hello, I'm a language model, I get a few weeks to make those who want someone as they do everything, or they do, do it, so
rank 0 sample 2: Hello, I'm a language model, I have started with. So, if you need to be aware. And, you have a lot of things. For
rank 0 sample 3: Hello, I'm a language model, my life, the world, I would have so, I knew me the most and what I would look at me by
step 550, loss: 5.161833, norm:0.7826, lr:4.6238e-04 dt: 48518.04ms, tok/sec:10806.04
step 551, loss: 5.100441, norm:0.8425, lr:4.6322e-04 dt: 3332.36ms, tok/sec:157332.31
step 552, loss: 5.119909, norm:0.8035, lr:4.6406e-04 dt: 3332.14ms, tok/sec:157342.76
step 553, loss: 5.148996, norm:0.7543, lr:4.6490e-04 dt: 3332.27ms, tok/sec:157336.65
step 554, loss: 5.088117, norm:0.8660, lr:4.6573e-04 dt: 3332.49ms, tok/sec:157326.32
step 555, loss: 5.131647, norm:1.0883, lr:4.6657e-04 dt: 3332.45ms, tok/sec:157328.10
step 556, loss: 5.147633, norm:1.0242, lr:4.6741e-04 dt: 3332.23ms, tok/sec:157338.38
step 557, loss: 5.279845, norm:0.9193, lr:4.6825e-04 dt: 3332.41ms, tok/sec:157330.07
step 558, loss: 5.307909, norm:0.9473, lr:4.6909e-04 dt: 3332.46ms, tok/sec:157327.58
step 559, loss: 5.263969, norm:1.0075, lr:4.6993e-04 dt: 3332.29ms, tok/sec:157335.49
step 560, loss: 5.280959, norm:0.8922, lr:4.7077e-04 dt: 3332.20ms, tok/sec:157339.69
step 561, loss: 5.222010, norm:0.6935, lr:4.7161e-04 dt: 3332.34ms, tok/sec:157333.45
step 562, loss: 5.214973, norm:0.5746, lr:4.7245e-04 dt: 3332.74ms, tok/sec:157314.49
step 563, loss: 5.266817, norm:0.6580, lr:4.7329e-04 dt: 3332.17ms, tok/sec:157341.37
step 564, loss: 5.227763, norm:0.5932, lr:4.7413e-04 dt: 3332.34ms, tok/sec:157333.24
step 565, loss: 5.247793, norm:0.5400, lr:4.7497e-04 dt: 3332.43ms, tok/sec:157328.81
step 566, loss: 5.232893, norm:0.5118, lr:4.7580e-04 dt: 3332.65ms, tok/sec:157318.51
step 567, loss: 5.232872, norm:0.5762, lr:4.7664e-04 dt: 3332.27ms, tok/sec:157336.41
step 568, loss: 5.196523, norm:0.5499, lr:4.7748e-04 dt: 3332.29ms, tok/sec:157335.67
step 569, loss: 5.183078, norm:0.6833, lr:4.7832e-04 dt: 3332.22ms, tok/sec:157339.09
step 570, loss: 5.184245, norm:1.0348, lr:4.7916e-04 dt: 3332.73ms, tok/sec:157315.00
step 571, loss: 5.173747, norm:1.1413, lr:4.8000e-04 dt: 3334.68ms, tok/sec:157223.06
step 572, loss: 5.228670, norm:0.9556, lr:4.8084e-04 dt: 3332.35ms, tok/sec:157332.64
step 573, loss: 5.160025, norm:0.7811, lr:4.8168e-04 dt: 3332.37ms, tok/sec:157331.71
step 574, loss: 5.152175, norm:0.7172, lr:4.8252e-04 dt: 3332.55ms, tok/sec:157323.39
step 575, loss: 5.177850, norm:0.5881, lr:4.8336e-04 dt: 3332.17ms, tok/sec:157341.26
step 576, loss: 5.152193, norm:0.5700, lr:4.8420e-04 dt: 3332.04ms, tok/sec:157347.28
step 577, loss: 5.198260, norm:0.5277, lr:4.8503e-04 dt: 3332.36ms, tok/sec:157332.17
step 578, loss: 5.114813, norm:0.4927, lr:4.8587e-04 dt: 3332.30ms, tok/sec:157335.31
step 579, loss: 5.116811, norm:0.5396, lr:4.8671e-04 dt: 3332.05ms, tok/sec:157347.00
step 580, loss: 5.092815, norm:0.5605, lr:4.8755e-04 dt: 3332.60ms, tok/sec:157321.15
step 581, loss: 5.052056, norm:0.9022, lr:4.8839e-04 dt: 3332.27ms, tok/sec:157336.64
step 582, loss: 5.028193, norm:0.6545, lr:4.8923e-04 dt: 3332.65ms, tok/sec:157318.59
step 583, loss: 5.089222, norm:0.5968, lr:4.9007e-04 dt: 3332.22ms, tok/sec:157338.74
step 584, loss: 5.060209, norm:0.7027, lr:4.9091e-04 dt: 3332.32ms, tok/sec:157334.14
step 585, loss: 5.112586, norm:0.9467, lr:4.9175e-04 dt: 3332.18ms, tok/sec:157340.77
step 586, loss: 5.092402, norm:1.0037, lr:4.9259e-04 dt: 3332.66ms, tok/sec:157318.18
step 587, loss: 5.145549, norm:0.7791, lr:4.9343e-04 dt: 3332.13ms, tok/sec:157343.05
step 588, loss: 5.153678, norm:0.7402, lr:4.9427e-04 dt: 3332.28ms, tok/sec:157336.32
step 589, loss: 5.108718, norm:0.7682, lr:4.9510e-04 dt: 3332.72ms, tok/sec:157315.56
step 590, loss: 5.151676, norm:0.8394, lr:4.9594e-04 dt: 3332.58ms, tok/sec:157322.03
step 591, loss: 5.032269, norm:0.6836, lr:4.9678e-04 dt: 3332.51ms, tok/sec:157325.09
step 592, loss: 4.929261, norm:0.5960, lr:4.9762e-04 dt: 3332.10ms, tok/sec:157344.39
step 593, loss: 4.996181, norm:0.6043, lr:4.9846e-04 dt: 3332.16ms, tok/sec:157341.84
step 594, loss: 5.043398, norm:0.5743, lr:4.9930e-04 dt: 3332.33ms, tok/sec:157333.85
step 595, loss: 4.983850, norm:0.7083, lr:5.0014e-04 dt: 3332.25ms, tok/sec:157337.51
step 596, loss: 5.018775, norm:0.8808, lr:5.0098e-04 dt: 3332.09ms, tok/sec:157345.24
step 597, loss: 5.076533, norm:0.7590, lr:5.0182e-04 dt: 3332.21ms, tok/sec:157339.34
step 598, loss: 4.991539, norm:0.7004, lr:5.0266e-04 dt: 3332.47ms, tok/sec:157327.31
step 599, loss: 5.009931, norm:0.9089, lr:5.0350e-04 dt: 3332.67ms, tok/sec:157317.79
validation loss: 5.1508
Model and optimizer state saved.
HellaSwag accuracy:4658506752735137305/-2=-2329253376367568896.0000
rank 1 sample 0: Hello, I'm a language model, you know the right and I use so you get it off on the right?
If you’ll be able
rank 1 sample 1: Hello, I'm a language model, so that you choose to use. A computer is a lot of all the best solution we need to be able to use
rank 1 sample 2: Hello, I'm a language model, I also have some of the most important things I feel the ’d my own.
It was the time that
rank 1 sample 3: Hello, I'm a language model, and I have all my clients to help on my school through community as to promote the problem behind it. I will be
rank 0 sample 0: Hello, I'm a language model, and I'm my own my opinion by him so I use this article but you can be honest with you’m
rank 0 sample 1: Hello, I'm a language model, I'll discuss the topic of this day. It all makes it the basic and it can only make our knowledge.

rank 0 sample 2: Hello, I'm a language model, I have more a large population of many countries have been a common language.
There is a lot of people in Asia
rank 0 sample 3: Hello, I'm a language model, the text is a language. It is an application that is a computer and the a single system that a world, there
step 600, loss: 4.966631, norm:0.9336, lr:5.0434e-04 dt: 56145.35ms, tok/sec:9338.05
step 601, loss: 4.951718, norm:0.7742, lr:5.0517e-04 dt: 3332.30ms, tok/sec:157335.40
step 602, loss: 5.022249, norm:0.7293, lr:5.0601e-04 dt: 3332.19ms, tok/sec:157340.59
step 603, loss: 5.112274, norm:0.7536, lr:5.0685e-04 dt: 3332.59ms, tok/sec:157321.62
step 604, loss: 5.181958, norm:0.7486, lr:5.0769e-04 dt: 3332.29ms, tok/sec:157335.57
step 605, loss: 5.122992, norm:0.7018, lr:5.0853e-04 dt: 3332.23ms, tok/sec:157338.68
step 606, loss: 5.147593, norm:0.7079, lr:5.0937e-04 dt: 3332.17ms, tok/sec:157341.12
step 607, loss: 5.099444, norm:0.6412, lr:5.1021e-04 dt: 3332.36ms, tok/sec:157332.33
step 608, loss: 5.118751, norm:0.6846, lr:5.1105e-04 dt: 3332.49ms, tok/sec:157325.99
step 609, loss: 5.104532, norm:0.6847, lr:5.1189e-04 dt: 3332.17ms, tok/sec:157341.47
step 610, loss: 5.108908, norm:0.5438, lr:5.1273e-04 dt: 3332.34ms, tok/sec:157333.44
step 611, loss: 5.107986, norm:0.6697, lr:5.1357e-04 dt: 3332.34ms, tok/sec:157333.39
step 612, loss: 5.102756, norm:0.6064, lr:5.1441e-04 dt: 3332.66ms, tok/sec:157318.00
step 613, loss: 5.105499, norm:0.5359, lr:5.1524e-04 dt: 3332.33ms, tok/sec:157333.69
step 614, loss: 5.128256, norm:0.6987, lr:5.1608e-04 dt: 3332.19ms, tok/sec:157340.58
step 615, loss: 5.095551, norm:0.9541, lr:5.1692e-04 dt: 3332.31ms, tok/sec:157334.75
step 616, loss: 5.094740, norm:0.9832, lr:5.1776e-04 dt: 3332.39ms, tok/sec:157331.03
step 617, loss: 5.047307, norm:0.7494, lr:5.1860e-04 dt: 3332.28ms, tok/sec:157336.32
step 618, loss: 5.072564, norm:0.6079, lr:5.1944e-04 dt: 3332.17ms, tok/sec:157341.26
step 619, loss: 5.080075, norm:0.5624, lr:5.2028e-04 dt: 3332.34ms, tok/sec:157333.25
step 620, loss: 5.034832, norm:0.4880, lr:5.2112e-04 dt: 3332.17ms, tok/sec:157341.44
step 621, loss: 5.059167, norm:0.4838, lr:5.2196e-04 dt: 3332.55ms, tok/sec:157323.57
step 622, loss: 5.066639, norm:0.5322, lr:5.2280e-04 dt: 3332.28ms, tok/sec:157336.16
step 623, loss: 5.024786, norm:0.6109, lr:5.2364e-04 dt: 3332.18ms, tok/sec:157340.73
step 624, loss: 5.039188, norm:0.6493, lr:5.2448e-04 dt: 3332.40ms, tok/sec:157330.63
step 625, loss: 5.073032, norm:0.6413, lr:5.2531e-04 dt: 3332.37ms, tok/sec:157331.73
step 626, loss: 5.007053, norm:0.6190, lr:5.2615e-04 dt: 3332.26ms, tok/sec:157336.88
step 627, loss: 5.005863, norm:0.5468, lr:5.2699e-04 dt: 3332.37ms, tok/sec:157331.71
step 628, loss: 4.928053, norm:0.6288, lr:5.2783e-04 dt: 3332.23ms, tok/sec:157338.65
step 629, loss: 5.003048, norm:0.6902, lr:5.2867e-04 dt: 3332.30ms, tok/sec:157335.39
step 630, loss: 4.960968, norm:0.5937, lr:5.2951e-04 dt: 3332.44ms, tok/sec:157328.48
step 631, loss: 4.965697, norm:0.5785, lr:5.3035e-04 dt: 3332.66ms, tok/sec:157318.08
step 632, loss: 4.986459, norm:0.6090, lr:5.3119e-04 dt: 3332.33ms, tok/sec:157333.68
step 633, loss: 4.956221, norm:0.5880, lr:5.3203e-04 dt: 3332.42ms, tok/sec:157329.44
step 634, loss: 5.013289, norm:0.5662, lr:5.3287e-04 dt: 3332.17ms, tok/sec:157341.23
step 635, loss: 5.005141, norm:0.6638, lr:5.3371e-04 dt: 3332.24ms, tok/sec:157337.83
step 636, loss: 5.096869, norm:0.7134, lr:5.3455e-04 dt: 3332.23ms, tok/sec:157338.56
step 637, loss: 5.198466, norm:0.7059, lr:5.3538e-04 dt: 3332.48ms, tok/sec:157326.86
step 638, loss: 4.909893, norm:0.6824, lr:5.3622e-04 dt: 3332.10ms, tok/sec:157344.67
step 639, loss: 4.857716, norm:0.8265, lr:5.3706e-04 dt: 3332.34ms, tok/sec:157333.35
step 640, loss: 4.860085, norm:0.9774, lr:5.3790e-04 dt: 3332.53ms, tok/sec:157324.19
step 641, loss: 4.942461, norm:0.8234, lr:5.3874e-04 dt: 3332.30ms, tok/sec:157335.04
step 642, loss: 4.938203, norm:0.7906, lr:5.3958e-04 dt: 3332.29ms, tok/sec:157335.44
step 643, loss: 4.892905, norm:0.7205, lr:5.4042e-04 dt: 3332.09ms, tok/sec:157344.97
step 644, loss: 4.871883, norm:0.6672, lr:5.4126e-04 dt: 3332.31ms, tok/sec:157334.66
step 645, loss: 4.848979, norm:0.5683, lr:5.4210e-04 dt: 3332.05ms, tok/sec:157346.81
step 646, loss: 4.868032, norm:0.5632, lr:5.4294e-04 dt: 3332.31ms, tok/sec:157334.86
step 647, loss: 4.913071, norm:0.6050, lr:5.4378e-04 dt: 3332.28ms, tok/sec:157336.26
step 648, loss: 4.896895, norm:0.6742, lr:5.4462e-04 dt: 3332.12ms, tok/sec:157343.57
step 649, loss: 4.868998, norm:0.7131, lr:5.4545e-04 dt: 3332.59ms, tok/sec:157321.51
HellaSwag accuracy:50191759113408009/-2=-25095879556704004.0000
rank 1 sample 0: Hello, I'm a language model, my well-known language, and by a computer science research system.
Mavio says a statement, “
rank 1 sample 1: Hello, I'm a language model, as well as other languages.
In response to the technology, computer design (on coding) technologies, the system of
rank 1 sample 2: Hello, I'm a language model, language format that means that language language language language language language language related language.
- I've created this as I also
rank 1 sample 3: Hello, I'm a language model, and where I’ve heard that’s pretty popular!
Yes, one seems to have been told by
rank 0 sample 0: Hello, I'm a language model, and I do not need to get on them at least it, with language that is important. If it isn’
rank 0 sample 1: Hello, I'm a language model, where there are a different language in science, I'll give me an adult in my favorite form, a child, or
rank 0 sample 2: Hello, I'm a language model, but I've done anything like to make it to help you feel better!
I think it is that I am thinking
rank 0 sample 3: Hello, I'm a language model, so your language language language language. It’s also hard to think the music system and the language language, including
step 650, loss: 5.035270, norm:0.6171, lr:5.4629e-04 dt: 48512.31ms, tok/sec:10807.32
step 651, loss: 5.094300, norm:0.6542, lr:5.4713e-04 dt: 3332.84ms, tok/sec:157309.86
step 652, loss: 5.028043, norm:0.7341, lr:5.4797e-04 dt: 3332.15ms, tok/sec:157342.19
step 653, loss: 5.034770, norm:0.6666, lr:5.4881e-04 dt: 3332.20ms, tok/sec:157340.08
step 654, loss: 5.026905, norm:0.5930, lr:5.4965e-04 dt: 3332.33ms, tok/sec:157333.76
step 655, loss: 5.046573, norm:0.6218, lr:5.5049e-04 dt: 3332.43ms, tok/sec:157329.06
step 656, loss: 4.998179, norm:0.6917, lr:5.5133e-04 dt: 3332.54ms, tok/sec:157323.82
step 657, loss: 5.005634, norm:0.7392, lr:5.5217e-04 dt: 3332.18ms, tok/sec:157340.95
step 658, loss: 4.983786, norm:0.6263, lr:5.5301e-04 dt: 3332.44ms, tok/sec:157328.41
step 659, loss: 5.038975, norm:0.6354, lr:5.5385e-04 dt: 3332.27ms, tok/sec:157336.63
step 660, loss: 4.985559, norm:0.6257, lr:5.5469e-04 dt: 3332.79ms, tok/sec:157312.06
step 661, loss: 4.926982, norm:0.6428, lr:5.5552e-04 dt: 3332.32ms, tok/sec:157334.15
step 662, loss: 4.925604, norm:0.6356, lr:5.5636e-04 dt: 3332.16ms, tok/sec:157341.63
step 663, loss: 4.947567, norm:0.5988, lr:5.5720e-04 dt: 3332.25ms, tok/sec:157337.69
step 664, loss: 4.906041, norm:0.5339, lr:5.5804e-04 dt: 3332.40ms, tok/sec:157330.43
step 665, loss: 4.935659, norm:0.5336, lr:5.5888e-04 dt: 3332.19ms, tok/sec:157340.56
step 666, loss: 4.919508, norm:0.5453, lr:5.5972e-04 dt: 3332.73ms, tok/sec:157314.71
step 667, loss: 4.895242, norm:0.4639, lr:5.6056e-04 dt: 3332.52ms, tok/sec:157324.79
step 668, loss: 4.920330, norm:0.4465, lr:5.6140e-04 dt: 3332.61ms, tok/sec:157320.49
step 669, loss: 4.860801, norm:0.5329, lr:5.6224e-04 dt: 3332.19ms, tok/sec:157340.40
step 670, loss: 4.886851, norm:0.5609, lr:5.6308e-04 dt: 3332.28ms, tok/sec:157336.35
step 671, loss: 4.915350, norm:0.6285, lr:5.6392e-04 dt: 3332.10ms, tok/sec:157344.63
step 672, loss: 4.910068, norm:0.8156, lr:5.6476e-04 dt: 3332.43ms, tok/sec:157328.97
step 673, loss: 4.864949, norm:0.8098, lr:5.6559e-04 dt: 3332.35ms, tok/sec:157332.78
step 674, loss: 4.872094, norm:0.6921, lr:5.6643e-04 dt: 3332.21ms, tok/sec:157339.21
step 675, loss: 4.904587, norm:0.5941, lr:5.6727e-04 dt: 3332.31ms, tok/sec:157334.52
step 676, loss: 4.890843, norm:0.5448, lr:5.6811e-04 dt: 3332.35ms, tok/sec:157332.67
step 677, loss: 4.919768, norm:0.5569, lr:5.6895e-04 dt: 3332.38ms, tok/sec:157331.37
step 678, loss: 4.822702, norm:0.5572, lr:5.6979e-04 dt: 3332.32ms, tok/sec:157334.38
step 679, loss: 4.871386, norm:0.5993, lr:5.7063e-04 dt: 3332.72ms, tok/sec:157315.45
step 680, loss: 4.830410, norm:0.6185, lr:5.7147e-04 dt: 3332.35ms, tok/sec:157332.99
step 681, loss: 4.889273, norm:0.6525, lr:5.7231e-04 dt: 3332.03ms, tok/sec:157347.80
step 682, loss: 4.845607, norm:0.6630, lr:5.7315e-04 dt: 3332.19ms, tok/sec:157340.41
step 683, loss: 4.872245, norm:0.6634, lr:5.7399e-04 dt: 3332.39ms, tok/sec:157330.91
step 684, loss: 4.757455, norm:0.6252, lr:5.7483e-04 dt: 3332.21ms, tok/sec:157339.27
step 685, loss: 4.750181, norm:0.5584, lr:5.7566e-04 dt: 3332.29ms, tok/sec:157335.54
step 686, loss: 4.777028, norm:0.5153, lr:5.7650e-04 dt: 3332.37ms, tok/sec:157331.68
step 687, loss: 4.769327, norm:0.5063, lr:5.7734e-04 dt: 3332.47ms, tok/sec:157326.95
step 688, loss: 4.757977, norm:0.5701, lr:5.7818e-04 dt: 3332.34ms, tok/sec:157333.13
step 689, loss: 4.769625, norm:0.8302, lr:5.7902e-04 dt: 3332.15ms, tok/sec:157342.37
step 690, loss: 4.801983, norm:0.9626, lr:5.7986e-04 dt: 3332.09ms, tok/sec:157345.24
step 691, loss: 4.788518, norm:0.8440, lr:5.8070e-04 dt: 3332.01ms, tok/sec:157348.79
step 692, loss: 4.762669, norm:0.6412, lr:5.8154e-04 dt: 3332.46ms, tok/sec:157327.62
step 693, loss: 4.808608, norm:0.7044, lr:5.8238e-04 dt: 3332.10ms, tok/sec:157344.60
step 694, loss: 4.705191, norm:0.8291, lr:5.8322e-04 dt: 3332.00ms, tok/sec:157349.28
step 695, loss: 4.798615, norm:0.7055, lr:5.8406e-04 dt: 3332.43ms, tok/sec:157329.02
step 696, loss: 4.932928, norm:0.8117, lr:5.8490e-04 dt: 3332.69ms, tok/sec:157316.80
step 697, loss: 5.074436, norm:0.8128, lr:5.8573e-04 dt: 3332.28ms, tok/sec:157335.95
step 698, loss: 4.924664, norm:0.6668, lr:5.8657e-04 dt: 3332.21ms, tok/sec:157339.20
step 699, loss: 4.982867, norm:0.7017, lr:5.8741e-04 dt: 3332.26ms, tok/sec:157336.93
validation loss: 4.9019
Model and optimizer state saved.
HellaSwag accuracy:-9209201369455899119/-2=4604600684727949312.0000
rank 1 sample 0: Hello, I'm a language model, it was quite a few.
Then there are two small ones.
We have the only the same value of the
rank 1 sample 1: Hello, I'm a language model, a group that a group of people of South America. One of the most prominent Christians - (s) of the most
rank 1 sample 2: Hello, I'm a language model, but your answer is a different way of your writing works, my opinion is no longer a good news for my readers,
rank 1 sample 3: Hello, I'm a language model, I always know, and can only understand myself.
Thanks for it here you can come?
For a new study
rank 0 sample 0: Hello, I'm a language model, and I can be a way, because these numbers are an example or multiple dialects, some people can learn from their
rank 0 sample 1: Hello, I'm a language model, I see
I'm not
as I have said something I was able to do this
the first step of the
rank 0 sample 2: Hello, I'm a language model, I'm one a dialect to define what is being derived from? (or I don't know)
- (an
rank 0 sample 3: Hello, I'm a language model, a human language language language language languages and language literacy languages.
As I'm as well as the Language Language language to
step 700, loss: 4.864959, norm:0.7239, lr:5.8825e-04 dt: 56213.31ms, tok/sec:9326.76
step 701, loss: 4.885983, norm:0.5013, lr:5.8909e-04 dt: 3332.21ms, tok/sec:157339.57
step 702, loss: 4.917893, norm:0.5285, lr:5.8993e-04 dt: 3332.37ms, tok/sec:157331.66
step 703, loss: 4.915188, norm:0.5877, lr:5.9077e-04 dt: 3332.55ms, tok/sec:157323.30
step 704, loss: 4.866251, norm:0.5902, lr:5.9161e-04 dt: 3332.19ms, tok/sec:157340.57
step 705, loss: 4.865541, norm:0.6096, lr:5.9245e-04 dt: 3332.21ms, tok/sec:157339.52
step 706, loss: 4.926807, norm:0.6527, lr:5.9329e-04 dt: 3332.11ms, tok/sec:157344.35
step 707, loss: 4.912695, norm:0.6671, lr:5.9413e-04 dt: 3332.32ms, tok/sec:157334.33
step 708, loss: 4.925027, norm:0.6816, lr:5.9497e-04 dt: 3332.17ms, tok/sec:157341.14
step 709, loss: 4.868692, norm:0.6236, lr:5.9580e-04 dt: 3332.31ms, tok/sec:157334.56
step 710, loss: 4.848995, norm:0.5021, lr:5.9664e-04 dt: 3332.27ms, tok/sec:157336.58
step 711, loss: 4.835962, norm:0.4944, lr:5.9748e-04 dt: 3332.54ms, tok/sec:157323.84
step 712, loss: 4.826259, norm:0.5897, lr:5.9832e-04 dt: 3332.53ms, tok/sec:157324.15
step 713, loss: 4.797902, norm:0.5264, lr:5.9916e-04 dt: 3332.26ms, tok/sec:157336.95
step 714, loss: 4.834612, norm:0.5541, lr:6.0000e-04 dt: 3332.18ms, tok/sec:157341.02
step 715, loss: 4.819832, norm:0.6421, lr:6.0000e-04 dt: 3332.26ms, tok/sec:157337.10
step 716, loss: 4.853509, norm:0.7509, lr:6.0000e-04 dt: 3332.19ms, tok/sec:157340.51
step 717, loss: 4.861426, norm:0.7060, lr:6.0000e-04 dt: 3332.10ms, tok/sec:157344.66
step 718, loss: 4.879690, norm:0.6618, lr:6.0000e-04 dt: 3332.37ms, tok/sec:157332.09
step 719, loss: 4.818898, norm:0.8068, lr:6.0000e-04 dt: 3332.38ms, tok/sec:157331.57
step 720, loss: 4.764752, norm:0.7208, lr:6.0000e-04 dt: 3332.17ms, tok/sec:157341.39
step 721, loss: 4.783946, norm:0.7052, lr:6.0000e-04 dt: 3332.61ms, tok/sec:157320.37
step 722, loss: 4.735538, norm:0.6800, lr:6.0000e-04 dt: 3332.32ms, tok/sec:157334.46
step 723, loss: 4.812685, norm:0.5405, lr:6.0000e-04 dt: 3332.28ms, tok/sec:157336.01
step 724, loss: 4.774390, norm:0.6113, lr:6.0000e-04 dt: 3332.14ms, tok/sec:157342.72
step 725, loss: 4.830252, norm:0.5965, lr:6.0000e-04 dt: 3332.49ms, tok/sec:157326.32
step 726, loss: 4.764088, norm:0.5269, lr:6.0000e-04 dt: 3332.25ms, tok/sec:157337.65
step 727, loss: 4.738911, norm:0.4610, lr:6.0000e-04 dt: 3332.18ms, tok/sec:157340.77
step 728, loss: 4.724880, norm:0.4627, lr:6.0000e-04 dt: 3332.50ms, tok/sec:157325.92
step 729, loss: 4.754555, norm:0.4356, lr:6.0000e-04 dt: 3332.32ms, tok/sec:157334.13
step 730, loss: 4.765876, norm:0.5515, lr:6.0000e-04 dt: 3332.93ms, tok/sec:157305.42
step 731, loss: 4.670728, norm:0.6062, lr:6.0000e-04 dt: 3332.24ms, tok/sec:157338.17
step 732, loss: 4.665578, norm:0.6536, lr:6.0000e-04 dt: 3332.09ms, tok/sec:157345.12
step 733, loss: 4.604572, norm:0.6331, lr:6.0000e-04 dt: 3331.99ms, tok/sec:157349.83
step 734, loss: 4.624426, norm:0.6852, lr:6.0000e-04 dt: 3332.30ms, tok/sec:157335.29
step 735, loss: 4.673527, norm:0.7159, lr:6.0000e-04 dt: 3332.17ms, tok/sec:157341.15
step 736, loss: 4.635855, norm:0.6266, lr:6.0000e-04 dt: 3332.16ms, tok/sec:157342.00
step 737, loss: 4.664590, norm:0.5473, lr:6.0000e-04 dt: 3332.51ms, tok/sec:157325.09
step 738, loss: 4.620535, norm:0.5528, lr:6.0000e-04 dt: 3332.36ms, tok/sec:157332.58
step 739, loss: 4.618011, norm:0.5016, lr:6.0000e-04 dt: 3332.35ms, tok/sec:157333.04
step 740, loss: 4.632254, norm:0.4796, lr:6.0000e-04 dt: 3332.14ms, tok/sec:157342.71
step 741, loss: 4.581768, norm:0.5156, lr:6.0000e-04 dt: 3332.85ms, tok/sec:157309.01
step 742, loss: 4.579987, norm:0.5753, lr:6.0000e-04 dt: 3333.08ms, tok/sec:157298.23
step 743, loss: 4.756899, norm:0.5248, lr:6.0000e-04 dt: 3332.70ms, tok/sec:157316.31
step 744, loss: 4.755855, norm:0.5958, lr:5.9999e-04 dt: 3332.55ms, tok/sec:157323.59
step 745, loss: 4.773757, norm:0.5989, lr:5.9999e-04 dt: 3332.13ms, tok/sec:157343.02
step 746, loss: 4.826604, norm:0.7295, lr:5.9999e-04 dt: 3332.38ms, tok/sec:157331.34
step 747, loss: 4.802176, norm:0.9244, lr:5.9999e-04 dt: 3332.61ms, tok/sec:157320.68
step 748, loss: 4.751447, norm:0.8319, lr:5.9999e-04 dt: 3332.16ms, tok/sec:157341.81
step 749, loss: 4.783467, norm:0.5856, lr:5.9999e-04 dt: 3332.35ms, tok/sec:157332.78
HellaSwag accuracy:-4565840619584664047/-2=2282920309792332032.0000
rank 1 sample 0: Hello, I'm a language model, where it is a fun and fun project to understand it to teach kids the teachers to enjoy. If you can, you
rank 1 sample 1: Hello, I'm a language model, which I think you want to create a learning environment. A good news is what I was taught about what I am going
rank 1 sample 2: Hello, I'm a language model, a little-t-t-t-t-t (or, maybe I'm) -- the more
How
rank 1 sample 3: Hello, I'm a language model, and I'm thinking it up. My argument is "by some times that people are right not enough to be left up
rank 0 sample 0: Hello, I'm a language model, and I will be a language to my hand my voice.
For other purposes, you can go on to the other
rank 0 sample 1: Hello, I'm a language model, I was all in a language of consciousness. The truth came to mind if it is the meaning that would be the way
rank 0 sample 2: Hello, I'm a language model, I'm no more often found up a lot less time than we are doing it.
I'm not sure that some
rank 0 sample 3: Hello, I'm a language model, the question I think I would be talking to tell me that I want to know so I do it out. The next
step 750, loss: 4.840734, norm:0.4768, lr:5.9999e-04 dt: 48519.67ms, tok/sec:10805.68
step 751, loss: 4.770766, norm:0.5254, lr:5.9999e-04 dt: 3332.15ms, tok/sec:157342.17
step 752, loss: 4.835502, norm:0.6177, lr:5.9999e-04 dt: 3332.37ms, tok/sec:157331.70
step 753, loss: 4.740920, norm:0.7343, lr:5.9999e-04 dt: 3332.50ms, tok/sec:157325.60
step 754, loss: 4.766559, norm:0.5402, lr:5.9999e-04 dt: 3332.70ms, tok/sec:157316.31
step 755, loss: 4.757617, norm:0.5231, lr:5.9999e-04 dt: 3332.14ms, tok/sec:157342.89
step 756, loss: 4.730028, norm:0.5061, lr:5.9999e-04 dt: 3332.24ms, tok/sec:157337.82
step 757, loss: 4.698250, norm:0.5247, lr:5.9999e-04 dt: 3332.19ms, tok/sec:157340.40
step 758, loss: 4.744940, norm:0.6465, lr:5.9999e-04 dt: 3332.31ms, tok/sec:157334.74
step 759, loss: 4.747724, norm:0.6255, lr:5.9999e-04 dt: 3332.10ms, tok/sec:157344.50
step 760, loss: 4.733506, norm:0.4616, lr:5.9999e-04 dt: 3332.18ms, tok/sec:157340.70
step 761, loss: 4.767100, norm:0.5579, lr:5.9999e-04 dt: 3334.62ms, tok/sec:157225.61
step 762, loss: 4.712315, norm:0.6650, lr:5.9999e-04 dt: 3332.54ms, tok/sec:157323.90
step 763, loss: 4.718951, norm:0.6768, lr:5.9999e-04 dt: 3332.29ms, tok/sec:157335.58
step 764, loss: 4.704896, norm:0.6885, lr:5.9998e-04 dt: 3332.14ms, tok/sec:157342.90
step 765, loss: 4.708529, norm:0.7610, lr:5.9998e-04 dt: 3332.18ms, tok/sec:157341.01
step 766, loss: 4.714412, norm:0.6548, lr:5.9998e-04 dt: 3332.10ms, tok/sec:157344.75
step 767, loss: 4.654095, norm:0.5832, lr:5.9998e-04 dt: 3332.11ms, tok/sec:157344.16
step 768, loss: 4.619482, norm:0.5541, lr:5.9998e-04 dt: 3332.17ms, tok/sec:157341.13
step 769, loss: 4.694786, norm:0.5559, lr:5.9998e-04 dt: 3331.98ms, tok/sec:157350.14
step 770, loss: 4.635830, norm:0.5354, lr:5.9998e-04 dt: 3332.31ms, tok/sec:157334.68
step 771, loss: 4.667422, norm:0.6339, lr:5.9998e-04 dt: 3332.14ms, tok/sec:157342.71
step 772, loss: 4.655346, norm:0.6369, lr:5.9998e-04 dt: 3332.19ms, tok/sec:157340.39
step 773, loss: 4.688831, norm:0.5356, lr:5.9998e-04 dt: 3332.13ms, tok/sec:157343.29
step 774, loss: 4.612982, norm:0.5114, lr:5.9998e-04 dt: 3332.45ms, tok/sec:157328.06
step 775, loss: 4.619099, norm:0.5579, lr:5.9998e-04 dt: 3332.08ms, tok/sec:157345.53
step 776, loss: 4.623244, norm:0.6167, lr:5.9998e-04 dt: 3332.23ms, tok/sec:157338.41
step 777, loss: 4.560839, norm:0.6408, lr:5.9998e-04 dt: 3332.35ms, tok/sec:157332.67
step 778, loss: 4.574549, norm:0.4960, lr:5.9997e-04 dt: 3332.27ms, tok/sec:157336.58
step 779, loss: 4.558955, norm:0.5072, lr:5.9997e-04 dt: 3332.12ms, tok/sec:157343.54
step 780, loss: 4.573759, norm:0.5120, lr:5.9997e-04 dt: 3332.26ms, tok/sec:157337.24
step 781, loss: 4.476666, norm:0.5472, lr:5.9997e-04 dt: 3332.22ms, tok/sec:157339.09
step 782, loss: 4.472533, norm:0.5621, lr:5.9997e-04 dt: 3332.71ms, tok/sec:157315.60
step 783, loss: 4.508454, norm:0.6135, lr:5.9997e-04 dt: 3332.03ms, tok/sec:157347.86
step 784, loss: 4.607246, norm:0.6247, lr:5.9997e-04 dt: 3332.18ms, tok/sec:157340.86
step 785, loss: 4.501020, norm:0.5512, lr:5.9997e-04 dt: 3332.11ms, tok/sec:157344.10
step 786, loss: 4.542632, norm:0.6305, lr:5.9997e-04 dt: 3332.17ms, tok/sec:157341.33
step 787, loss: 4.515594, norm:0.6409, lr:5.9997e-04 dt: 3332.09ms, tok/sec:157345.15
step 788, loss: 4.487873, norm:0.5896, lr:5.9997e-04 dt: 3332.07ms, tok/sec:157346.05
step 789, loss: 4.652382, norm:0.5770, lr:5.9997e-04 dt: 3332.13ms, tok/sec:157343.23
step 790, loss: 4.661997, norm:0.6824, lr:5.9996e-04 dt: 3332.77ms, tok/sec:157313.00
step 791, loss: 4.671621, norm:0.7135, lr:5.9996e-04 dt: 3332.05ms, tok/sec:157347.11
step 792, loss: 4.673882, norm:0.5629, lr:5.9996e-04 dt: 3332.25ms, tok/sec:157337.63
step 793, loss: 4.655472, norm:0.5669, lr:5.9996e-04 dt: 3332.38ms, tok/sec:157331.52
step 794, loss: 4.669500, norm:0.5141, lr:5.9996e-04 dt: 3332.09ms, tok/sec:157345.07
step 795, loss: 4.693800, norm:0.4268, lr:5.9996e-04 dt: 3332.26ms, tok/sec:157337.14
step 796, loss: 4.616279, norm:0.4813, lr:5.9996e-04 dt: 3332.06ms, tok/sec:157346.56
step 797, loss: 4.639001, norm:0.5157, lr:5.9996e-04 dt: 3332.16ms, tok/sec:157341.85
step 798, loss: 4.646860, norm:0.5442, lr:5.9996e-04 dt: 3332.41ms, tok/sec:157330.05
step 799, loss: 4.661915, norm:0.6333, lr:5.9996e-04 dt: 3332.44ms, tok/sec:157328.58
validation loss: 4.6354
Model and optimizer state saved.
HellaSwag accuracy:-4591888574569558775/-2=2295944287284779264.0000
rank 1 sample 0: Hello, I'm a language model, just as a language of a language," said Kohan K.
My first lesson will learn to help me.
rank 1 sample 1: Hello, I'm a language model, a computer-in-language language is written in the last few years. There is lots of opportunities for the next generation
rank 1 sample 2: Hello, I'm a language model, but I am so much more than a few of the parts than a bit.
I think of my favorite works out
rank 1 sample 3: Hello, I'm a language model, and I'm going on math and you didn't come again in just before school. Teachers said the students are so excited
rank 0 sample 0: Hello, I'm a language model,
I'll be a writer for "migration" when you'd really need to get confused about this language. But
rank 0 sample 1: Hello, I'm a language model, my question is that it's been around for me about as a "English".
In the early 18th century,
rank 0 sample 2: Hello, I'm a language model, and I also do (p 10-year; however, you have been a writer and a writer.
I are
rank 0 sample 3: Hello, I'm a language model, the book is very different. I also say here the book is more and it appears, to say something about this issue
step 800, loss: 4.611417, norm:0.7465, lr:5.9995e-04 dt: 56156.45ms, tok/sec:9336.20
step 801, loss: 4.634194, norm:0.5857, lr:5.9995e-04 dt: 3332.30ms, tok/sec:157335.26
step 802, loss: 4.691217, norm:0.6290, lr:5.9995e-04 dt: 3332.40ms, tok/sec:157330.49
step 803, loss: 4.632665, norm:0.6112, lr:5.9995e-04 dt: 3332.34ms, tok/sec:157333.12
step 804, loss: 4.619890, norm:0.6135, lr:5.9995e-04 dt: 3332.55ms, tok/sec:157323.29
step 805, loss: 4.600547, norm:0.5766, lr:5.9995e-04 dt: 3332.49ms, tok/sec:157325.97
step 806, loss: 4.612285, norm:0.5747, lr:5.9995e-04 dt: 3332.29ms, tok/sec:157335.63
step 807, loss: 4.622491, norm:0.6189, lr:5.9995e-04 dt: 3332.33ms, tok/sec:157333.81
step 808, loss: 4.608429, norm:0.5759, lr:5.9995e-04 dt: 3332.19ms, tok/sec:157340.19
step 809, loss: 4.621335, norm:0.5658, lr:5.9994e-04 dt: 3332.44ms, tok/sec:157328.59
step 810, loss: 4.552410, norm:0.5229, lr:5.9994e-04 dt: 3332.67ms, tok/sec:157317.79
step 811, loss: 4.570343, norm:0.5102, lr:5.9994e-04 dt: 3332.28ms, tok/sec:157336.03
step 812, loss: 4.557311, norm:0.5620, lr:5.9994e-04 dt: 3332.15ms, tok/sec:157342.38
step 813, loss: 4.542038, norm:0.5147, lr:5.9994e-04 dt: 3332.20ms, tok/sec:157339.67
step 814, loss: 4.526647, norm:0.5058, lr:5.9994e-04 dt: 3332.09ms, tok/sec:157345.06
step 815, loss: 4.504015, norm:0.5089, lr:5.9994e-04 dt: 3332.27ms, tok/sec:157336.43
step 816, loss: 4.495428, norm:0.5593, lr:5.9994e-04 dt: 3332.10ms, tok/sec:157344.84
step 817, loss: 4.528522, norm:0.5100, lr:5.9993e-04 dt: 3332.24ms, tok/sec:157337.99
step 818, loss: 4.527191, norm:0.4829, lr:5.9993e-04 dt: 3332.35ms, tok/sec:157332.68
step 819, loss: 4.489335, norm:0.5113, lr:5.9993e-04 dt: 3332.46ms, tok/sec:157327.50
step 820, loss: 4.503505, norm:0.4846, lr:5.9993e-04 dt: 3332.03ms, tok/sec:157348.08
step 821, loss: 4.549097, norm:0.4745, lr:5.9993e-04 dt: 3332.31ms, tok/sec:157334.50
step 822, loss: 4.537862, norm:0.5141, lr:5.9993e-04 dt: 3332.53ms, tok/sec:157324.42
step 823, loss: 4.375716, norm:0.5919, lr:5.9993e-04 dt: 3331.95ms, tok/sec:157351.87
step 824, loss: 4.385222, norm:0.6330, lr:5.9993e-04 dt: 3332.09ms, tok/sec:157345.05
step 825, loss: 4.445336, norm:0.6380, lr:5.9992e-04 dt: 3332.45ms, tok/sec:157328.32
step 826, loss: 4.441677, norm:0.5969, lr:5.9992e-04 dt: 3332.08ms, tok/sec:157345.75
step 827, loss: 4.446414, norm:0.5409, lr:5.9992e-04 dt: 3332.62ms, tok/sec:157319.88
step 828, loss: 4.402728, norm:0.5014, lr:5.9992e-04 dt: 3332.15ms, tok/sec:157342.20
step 829, loss: 4.385400, norm:0.5108, lr:5.9992e-04 dt: 3332.11ms, tok/sec:157343.98
step 830, loss: 4.405223, norm:0.5331, lr:5.9992e-04 dt: 3332.17ms, tok/sec:157341.20
step 831, loss: 4.321783, norm:0.5572, lr:5.9992e-04 dt: 3332.13ms, tok/sec:157343.33
step 832, loss: 4.329257, norm:0.5910, lr:5.9991e-04 dt: 3332.07ms, tok/sec:157346.21
step 833, loss: 4.359763, norm:0.5265, lr:5.9991e-04 dt: 3332.38ms, tok/sec:157331.36
step 834, loss: 4.476841, norm:0.4549, lr:5.9991e-04 dt: 3332.25ms, tok/sec:157337.48
step 835, loss: 4.538640, norm:0.4981, lr:5.9991e-04 dt: 3332.48ms, tok/sec:157326.81
step 836, loss: 4.501293, norm:0.4926, lr:5.9991e-04 dt: 3332.23ms, tok/sec:157338.67
step 837, loss: 4.575233, norm:0.5705, lr:5.9991e-04 dt: 3332.11ms, tok/sec:157343.98
step 838, loss: 4.544831, norm:0.5732, lr:5.9990e-04 dt: 3332.13ms, tok/sec:157343.35
step 839, loss: 4.536697, norm:0.4896, lr:5.9990e-04 dt: 3332.35ms, tok/sec:157332.93
step 840, loss: 4.557969, norm:0.5172, lr:5.9990e-04 dt: 3332.58ms, tok/sec:157321.86
step 841, loss: 4.550447, norm:0.5096, lr:5.9990e-04 dt: 3332.31ms, tok/sec:157334.75
step 842, loss: 4.514628, norm:0.5618, lr:5.9990e-04 dt: 3332.15ms, tok/sec:157342.26
step 843, loss: 4.604860, norm:0.7227, lr:5.9990e-04 dt: 3332.44ms, tok/sec:157328.78
step 844, loss: 4.557569, norm:0.7254, lr:5.9990e-04 dt: 3332.82ms, tok/sec:157310.73
step 845, loss: 4.566745, norm:0.7901, lr:5.9989e-04 dt: 3332.26ms, tok/sec:157337.12
step 846, loss: 4.541556, norm:0.7608, lr:5.9989e-04 dt: 3332.16ms, tok/sec:157341.56
step 847, loss: 4.572527, norm:0.6958, lr:5.9989e-04 dt: 3332.30ms, tok/sec:157334.94
step 848, loss: 4.560795, norm:0.6913, lr:5.9989e-04 dt: 3332.24ms, tok/sec:157338.04
step 849, loss: 4.540792, norm:0.6701, lr:5.9989e-04 dt: 3332.50ms, tok/sec:157325.63
HellaSwag accuracy:4631485670959549449/-2=-2315742835479774720.0000
rank 1 sample 0: Hello, I'm a language model, as it is a class. In children with this class, how do you be able to get to the kids?

rank 1 sample 1: Hello, I'm a language model, so that the class is a lot smaller than the ones who are already aware of it if you take the class.

rank 1 sample 2: Hello, I'm a language model, or there will be a lot of fun. I need to look up a lot of fun and it can't have a
rank 1 sample 3: Hello, I'm a language model, and I'm doing a class, because something is more obvious. However, just a different thing I've learned and I
rank 0 sample 0: Hello, I'm a language model, in my post, my example: how your relationship is applied? Can make your relationship in which one thing to be done
rank 0 sample 1: Hello, I'm a language model, though he also has the influence of other people and places which have to be seen in his English.”
This
rank 0 sample 2: Hello, I'm a language model, I'm now making things so common that I mean that I've seen a lot of people. I'm not a German
rank 0 sample 3: Hello, I'm a language model, where did you have a computer program. The question is that you had a problem using your computer system or a program using
step 850, loss: 4.501126, norm:0.5150, lr:5.9989e-04 dt: 48514.20ms, tok/sec:10806.90
step 851, loss: 4.547002, norm:0.4344, lr:5.9988e-04 dt: 3332.21ms, tok/sec:157339.22
step 852, loss: 4.536996, norm:0.5100, lr:5.9988e-04 dt: 3332.43ms, tok/sec:157328.93
step 853, loss: 4.475179, norm:0.5240, lr:5.9988e-04 dt: 3332.48ms, tok/sec:157326.72
step 854, loss: 4.506729, norm:0.4635, lr:5.9988e-04 dt: 3331.95ms, tok/sec:157351.73
step 855, loss: 4.512414, norm:0.3770, lr:5.9988e-04 dt: 3332.23ms, tok/sec:157338.37
step 856, loss: 4.523902, norm:0.3998, lr:5.9987e-04 dt: 3332.24ms, tok/sec:157338.20
step 857, loss: 4.432359, norm:0.4957, lr:5.9987e-04 dt: 3332.01ms, tok/sec:157348.99
step 858, loss: 4.452591, norm:0.4584, lr:5.9987e-04 dt: 3332.32ms, tok/sec:157334.23
step 859, loss: 4.424064, norm:0.4461, lr:5.9987e-04 dt: 3332.43ms, tok/sec:157329.10
step 860, loss: 4.437903, norm:0.4598, lr:5.9987e-04 dt: 3332.27ms, tok/sec:157336.41
step 861, loss: 4.452599, norm:0.4926, lr:5.9987e-04 dt: 3332.72ms, tok/sec:157315.43
step 862, loss: 4.460064, norm:0.6022, lr:5.9986e-04 dt: 3332.04ms, tok/sec:157347.50
step 863, loss: 4.404284, norm:0.6136, lr:5.9986e-04 dt: 3332.28ms, tok/sec:157336.28
step 864, loss: 4.458341, norm:0.6060, lr:5.9986e-04 dt: 3332.38ms, tok/sec:157331.34
step 865, loss: 4.408336, norm:0.5505, lr:5.9986e-04 dt: 3332.16ms, tok/sec:157341.56
step 866, loss: 4.444157, norm:0.5496, lr:5.9986e-04 dt: 3332.27ms, tok/sec:157336.73
step 867, loss: 4.382095, norm:0.5623, lr:5.9985e-04 dt: 3332.07ms, tok/sec:157345.83
step 868, loss: 4.309548, norm:0.4520, lr:5.9985e-04 dt: 3332.69ms, tok/sec:157316.84
step 869, loss: 4.335112, norm:0.4286, lr:5.9985e-04 dt: 3331.90ms, tok/sec:157353.92
step 870, loss: 4.299820, norm:0.4125, lr:5.9985e-04 dt: 3331.97ms, tok/sec:157350.72
step 871, loss: 4.299314, norm:0.4629, lr:5.9985e-04 dt: 3332.43ms, tok/sec:157329.24
step 872, loss: 4.308996, norm:0.4454, lr:5.9984e-04 dt: 3332.15ms, tok/sec:157342.46
step 873, loss: 4.301999, norm:0.4258, lr:5.9984e-04 dt: 3332.23ms, tok/sec:157338.36
step 874, loss: 4.245922, norm:0.4483, lr:5.9984e-04 dt: 3332.02ms, tok/sec:157348.27
step 875, loss: 4.236936, norm:0.4889, lr:5.9984e-04 dt: 3332.05ms, tok/sec:157346.76
step 876, loss: 4.321212, norm:0.5732, lr:5.9984e-04 dt: 3332.24ms, tok/sec:157337.96
step 877, loss: 4.324668, norm:0.5859, lr:5.9983e-04 dt: 3332.49ms, tok/sec:157326.14
step 878, loss: 4.328395, norm:0.5824, lr:5.9983e-04 dt: 3332.09ms, tok/sec:157345.13
step 879, loss: 4.300521, norm:0.5677, lr:5.9983e-04 dt: 3332.10ms, tok/sec:157344.39
step 880, loss: 4.368939, norm:0.4614, lr:5.9983e-04 dt: 3332.13ms, tok/sec:157343.11
step 881, loss: 4.503696, norm:0.4691, lr:5.9983e-04 dt: 3332.07ms, tok/sec:157346.12
step 882, loss: 4.528778, norm:0.4915, lr:5.9982e-04 dt: 3332.10ms, tok/sec:157344.70
step 883, loss: 4.482906, norm:0.5769, lr:5.9982e-04 dt: 3332.94ms, tok/sec:157304.81
step 884, loss: 4.507590, norm:0.5968, lr:5.9982e-04 dt: 3332.69ms, tok/sec:157316.89
step 885, loss: 4.431368, norm:0.5470, lr:5.9982e-04 dt: 3332.03ms, tok/sec:157347.71
step 886, loss: 4.454065, norm:0.4742, lr:5.9982e-04 dt: 3332.28ms, tok/sec:157336.29
step 887, loss: 4.478847, norm:0.4803, lr:5.9981e-04 dt: 3332.16ms, tok/sec:157341.89
step 888, loss: 4.465073, norm:0.5283, lr:5.9981e-04 dt: 3332.21ms, tok/sec:157339.32
step 889, loss: 4.453717, norm:0.5570, lr:5.9981e-04 dt: 3332.28ms, tok/sec:157336.16
step 890, loss: 4.438895, norm:0.5530, lr:5.9981e-04 dt: 3332.26ms, tok/sec:157337.01
step 891, loss: 4.485124, norm:0.5372, lr:5.9980e-04 dt: 3332.19ms, tok/sec:157340.24
step 892, loss: 4.406207, norm:0.4821, lr:5.9980e-04 dt: 3332.37ms, tok/sec:157331.72
step 893, loss: 4.433624, norm:0.5471, lr:5.9980e-04 dt: 3332.55ms, tok/sec:157323.38
step 894, loss: 4.435360, norm:0.5327, lr:5.9980e-04 dt: 3332.20ms, tok/sec:157339.94
step 895, loss: 4.424258, norm:0.4824, lr:5.9980e-04 dt: 3332.20ms, tok/sec:157339.74
step 896, loss: 4.405232, norm:0.5119, lr:5.9979e-04 dt: 3332.44ms, tok/sec:157328.34
step 897, loss: 4.414771, norm:0.5104, lr:5.9979e-04 dt: 3332.13ms, tok/sec:157343.14
step 898, loss: 4.421026, norm:0.4783, lr:5.9979e-04 dt: 3331.90ms, tok/sec:157354.17
step 899, loss: 4.399455, norm:0.4202, lr:5.9979e-04 dt: 3332.67ms, tok/sec:157317.63
validation loss: 4.4132
Model and optimizer state saved.
HellaSwag accuracy:-4591176631135810551/-2=2295588315567905280.0000
rank 1 sample 0: Hello, I'm a language model, as a unit of knowledge.
These two classes were introduced out of the world, in order to use the word,
rank 1 sample 1: Hello, I'm a language model, a model of which I can do to build a list where I find a class member from my parents, and a class
rank 1 sample 2: Hello, I'm a language model, I see the difference between the two questions.
Now that in a very own language is more interesting and interesting because I
rank 1 sample 3: Hello, I'm a language model, and I'm in a row and see those words. Is my list of friends? Here's my list of the words
rank 0 sample 0: Hello, I'm a language model, and I was able to read, my experience now. I have heard so that I can not write.
I can
rank 0 sample 1: Hello, I'm a language model, such as the Japanese language. It took a bit off until the Japanese has gone, the Japanese is one of the most
rank 0 sample 2: Hello, I'm a language model, but I use this: 'fou' is the 'o 'o'. I am a 'o 'y
rank 0 sample 3: Hello, I'm a language model, and are only in the way I think about:
- Rames:
- Hats: Yes, yes.
step 900, loss: 4.405669, norm:0.4092, lr:5.9978e-04 dt: 56190.76ms, tok/sec:9330.50
step 901, loss: 4.415895, norm:0.4427, lr:5.9978e-04 dt: 3332.11ms, tok/sec:157344.10
step 902, loss: 4.402730, norm:0.3647, lr:5.9978e-04 dt: 3332.34ms, tok/sec:157333.39
step 903, loss: 4.322572, norm:0.3659, lr:5.9978e-04 dt: 3332.42ms, tok/sec:157329.68
step 904, loss: 4.375092, norm:0.4070, lr:5.9978e-04 dt: 3332.26ms, tok/sec:157337.24
step 905, loss: 4.352215, norm:0.5297, lr:5.9977e-04 dt: 3332.02ms, tok/sec:157348.52
step 906, loss: 4.367933, norm:0.5543, lr:5.9977e-04 dt: 3332.36ms, tok/sec:157332.17
step 907, loss: 4.338917, norm:0.4753, lr:5.9977e-04 dt: 3332.05ms, tok/sec:157347.12
step 908, loss: 4.366587, norm:0.4829, lr:5.9977e-04 dt: 3332.16ms, tok/sec:157342.00
step 909, loss: 4.393650, norm:0.5113, lr:5.9976e-04 dt: 3332.33ms, tok/sec:157333.93
step 910, loss: 4.358619, norm:0.5303, lr:5.9976e-04 dt: 3332.57ms, tok/sec:157322.45
step 911, loss: 4.400018, norm:0.5281, lr:5.9976e-04 dt: 3332.20ms, tok/sec:157340.13
step 912, loss: 4.361421, norm:0.4939, lr:5.9976e-04 dt: 3331.90ms, tok/sec:157354.20
step 913, loss: 4.375994, norm:0.4190, lr:5.9975e-04 dt: 3332.05ms, tok/sec:157347.17
step 914, loss: 4.208082, norm:0.4714, lr:5.9975e-04 dt: 3332.08ms, tok/sec:157345.52
step 915, loss: 4.261937, norm:0.4402, lr:5.9975e-04 dt: 3332.10ms, tok/sec:157344.51
step 916, loss: 4.267456, norm:0.5019, lr:5.9975e-04 dt: 3332.10ms, tok/sec:157344.61
step 917, loss: 4.238995, norm:0.5063, lr:5.9974e-04 dt: 3332.26ms, tok/sec:157337.07
step 918, loss: 4.250175, norm:0.4929, lr:5.9974e-04 dt: 3332.50ms, tok/sec:157325.72
step 919, loss: 4.179514, norm:0.4983, lr:5.9974e-04 dt: 3332.29ms, tok/sec:157335.69
step 920, loss: 4.215400, norm:0.4432, lr:5.9974e-04 dt: 3332.16ms, tok/sec:157341.90
step 921, loss: 4.243750, norm:0.3888, lr:5.9973e-04 dt: 3332.00ms, tok/sec:157349.19
step 922, loss: 4.252058, norm:0.4130, lr:5.9973e-04 dt: 3332.19ms, tok/sec:157340.44
step 923, loss: 4.245934, norm:0.4048, lr:5.9973e-04 dt: 3332.23ms, tok/sec:157338.58
step 924, loss: 4.225231, norm:0.4016, lr:5.9972e-04 dt: 3332.11ms, tok/sec:157344.27
step 925, loss: 4.242128, norm:0.4705, lr:5.9972e-04 dt: 3332.03ms, tok/sec:157347.76
step 926, loss: 4.419449, norm:0.6050, lr:5.9972e-04 dt: 3332.39ms, tok/sec:157330.71
step 927, loss: 4.462429, norm:0.8204, lr:5.9972e-04 dt: 3332.16ms, tok/sec:157341.74
step 928, loss: 4.409474, norm:0.7629, lr:5.9971e-04 dt: 3332.45ms, tok/sec:157327.97
step 929, loss: 4.361562, norm:0.6290, lr:5.9971e-04 dt: 3332.15ms, tok/sec:157342.18
step 930, loss: 4.449627, norm:0.4927, lr:5.9971e-04 dt: 3332.33ms, tok/sec:157333.87
step 931, loss: 4.391961, norm:0.5689, lr:5.9971e-04 dt: 3332.36ms, tok/sec:157332.26
step 932, loss: 4.437235, norm:0.6142, lr:5.9970e-04 dt: 3332.24ms, tok/sec:157338.05
step 933, loss: 4.407604, norm:0.5080, lr:5.9970e-04 dt: 3332.32ms, tok/sec:157334.44
step 934, loss: 4.447295, norm:0.4823, lr:5.9970e-04 dt: 3332.33ms, tok/sec:157333.59
step 935, loss: 4.461773, norm:0.4567, lr:5.9970e-04 dt: 3332.75ms, tok/sec:157314.14
step 936, loss: 4.392499, norm:0.4371, lr:5.9969e-04 dt: 3332.11ms, tok/sec:157344.07
step 937, loss: 4.371675, norm:0.4570, lr:5.9969e-04 dt: 3332.11ms, tok/sec:157344.37
step 938, loss: 4.414730, norm:0.4944, lr:5.9969e-04 dt: 3332.35ms, tok/sec:157332.65
step 939, loss: 4.390636, norm:0.4193, lr:5.9968e-04 dt: 3332.24ms, tok/sec:157337.84
step 940, loss: 4.368264, norm:0.4229, lr:5.9968e-04 dt: 3332.27ms, tok/sec:157336.82
step 941, loss: 4.418339, norm:0.4737, lr:5.9968e-04 dt: 3332.37ms, tok/sec:157331.79
step 942, loss: 4.382542, norm:0.4319, lr:5.9968e-04 dt: 3332.30ms, tok/sec:157335.17
step 943, loss: 4.370994, norm:0.4787, lr:5.9967e-04 dt: 3332.75ms, tok/sec:157313.95
step 944, loss: 4.379631, norm:0.4989, lr:5.9967e-04 dt: 3332.21ms, tok/sec:157339.41
step 945, loss: 4.347920, norm:0.5074, lr:5.9967e-04 dt: 3332.28ms, tok/sec:157336.34
step 946, loss: 4.375193, norm:0.4752, lr:5.9966e-04 dt: 3332.40ms, tok/sec:157330.45
step 947, loss: 4.362465, norm:0.3935, lr:5.9966e-04 dt: 3332.08ms, tok/sec:157345.49
step 948, loss: 4.367965, norm:0.3643, lr:5.9966e-04 dt: 3332.19ms, tok/sec:157340.37
step 949, loss: 4.342654, norm:0.3881, lr:5.9966e-04 dt: 3332.14ms, tok/sec:157342.73
HellaSwag accuracy:-4591185427233043383/-2=2295592713616521728.0000
rank 1 sample 0: Hello, I'm a language model, in my previous blog, and I use a text, where an object is found. It may appear in the same way
rank 1 sample 1: Hello, I'm a language model, or I am pretty much like the world; I do believe that our world and the best choices as we are.

rank 1 sample 2: Hello, I'm a language model, I haven't found it in the same way.
We use a language model to describe a topic in the language,
rank 1 sample 3: Hello, I'm a language model, and I'm talking to
this post...
We live in a small store, about an area of
the city
rank 0 sample 0: Hello, I'm a language model, I'm looking to see, "Oh really sure I would be working hard," I was there in my own way to
rank 0 sample 1: Hello, I'm a language model, there are not enough people to learn when they can choose between the words, the word or phrases. When you get a
rank 0 sample 2: Hello, I'm a language model, and I find a look on the big picture on the map to show that I would like to see the map, you
rank 0 sample 3: Hello, I'm a language model, the structure of the language. I was the big-looking people around this country called it ‘what’ way
step 950, loss: 4.329195, norm:0.4198, lr:5.9965e-04 dt: 48531.32ms, tok/sec:10803.08
step 951, loss: 4.247448, norm:0.4349, lr:5.9965e-04 dt: 3332.11ms, tok/sec:157344.07
step 952, loss: 4.269598, norm:0.4883, lr:5.9965e-04 dt: 3334.41ms, tok/sec:157235.74
step 953, loss: 4.317410, norm:0.4810, lr:5.9964e-04 dt: 3332.22ms, tok/sec:157338.86
step 954, loss: 4.304077, norm:0.4362, lr:5.9964e-04 dt: 3332.24ms, tok/sec:157338.11
step 955, loss: 4.276285, norm:0.4021, lr:5.9964e-04 dt: 3332.24ms, tok/sec:157338.10
step 956, loss: 4.258268, norm:0.4649, lr:5.9963e-04 dt: 3332.20ms, tok/sec:157339.70
step 957, loss: 4.240948, norm:0.4732, lr:5.9963e-04 dt: 3332.05ms, tok/sec:157346.87
step 958, loss: 4.298927, norm:0.5118, lr:5.9963e-04 dt: 3332.38ms, tok/sec:157331.56
step 959, loss: 4.302449, norm:0.4846, lr:5.9963e-04 dt: 3332.15ms, tok/sec:157342.44
step 960, loss: 4.214366, norm:0.4405, lr:5.9962e-04 dt: 3332.17ms, tok/sec:157341.26
step 961, loss: 4.180590, norm:0.4171, lr:5.9962e-04 dt: 3332.03ms, tok/sec:157347.73
step 962, loss: 4.174988, norm:0.5140, lr:5.9962e-04 dt: 3332.51ms, tok/sec:157325.36
step 963, loss: 4.127955, norm:0.4572, lr:5.9961e-04 dt: 3332.05ms, tok/sec:157347.00
step 964, loss: 4.224188, norm:0.4589, lr:5.9961e-04 dt: 3332.20ms, tok/sec:157340.06
step 965, loss: 4.154999, norm:0.3988, lr:5.9961e-04 dt: 3332.13ms, tok/sec:157343.32
step 966, loss: 4.161473, norm:0.4555, lr:5.9960e-04 dt: 3332.13ms, tok/sec:157343.21
step 967, loss: 4.168544, norm:0.4160, lr:5.9960e-04 dt: 3332.12ms, tok/sec:157343.60
step 968, loss: 4.204714, norm:0.4842, lr:5.9960e-04 dt: 3332.06ms, tok/sec:157346.47
step 969, loss: 4.177958, norm:0.5276, lr:5.9959e-04 dt: 3331.96ms, tok/sec:157351.35
step 970, loss: 4.210823, norm:0.5126, lr:5.9959e-04 dt: 3332.54ms, tok/sec:157323.94
step 971, loss: 4.179730, norm:0.4923, lr:5.9959e-04 dt: 3332.41ms, tok/sec:157329.91
step 972, loss: 4.315258, norm:0.4917, lr:5.9958e-04 dt: 3332.06ms, tok/sec:157346.40
step 973, loss: 4.318810, norm:0.4148, lr:5.9958e-04 dt: 3332.37ms, tok/sec:157331.98
step 974, loss: 4.293715, norm:0.3990, lr:5.9958e-04 dt: 3331.91ms, tok/sec:157353.56
step 975, loss: 4.373262, norm:0.4127, lr:5.9957e-04 dt: 3332.36ms, tok/sec:157332.35
step 976, loss: 4.331063, norm:0.4304, lr:5.9957e-04 dt: 3332.19ms, tok/sec:157340.60
step 977, loss: 4.383821, norm:0.4582, lr:5.9957e-04 dt: 3331.97ms, tok/sec:157350.94
step 978, loss: 4.361871, norm:0.3998, lr:5.9956e-04 dt: 3332.49ms, tok/sec:157326.28
step 979, loss: 4.333529, norm:0.3800, lr:5.9956e-04 dt: 3332.07ms, tok/sec:157345.80
step 980, loss: 4.333202, norm:0.3986, lr:5.9956e-04 dt: 3332.57ms, tok/sec:157322.39
step 981, loss: 4.289786, norm:0.4255, lr:5.9955e-04 dt: 3332.12ms, tok/sec:157343.77
step 982, loss: 4.340221, norm:0.4681, lr:5.9955e-04 dt: 3332.12ms, tok/sec:157343.53
step 983, loss: 4.324134, norm:0.5202, lr:5.9955e-04 dt: 3332.06ms, tok/sec:157346.33
step 984, loss: 4.276846, norm:0.5048, lr:5.9954e-04 dt: 3332.27ms, tok/sec:157336.68
step 985, loss: 4.308887, norm:0.4339, lr:5.9954e-04 dt: 3331.94ms, tok/sec:157352.40
step 986, loss: 4.303030, norm:0.4446, lr:5.9954e-04 dt: 3332.34ms, tok/sec:157333.38
step 987, loss: 4.309556, norm:0.4344, lr:5.9953e-04 dt: 3332.33ms, tok/sec:157333.58
step 988, loss: 4.304247, norm:0.3874, lr:5.9953e-04 dt: 3331.95ms, tok/sec:157351.51
step 989, loss: 4.274117, norm:0.4143, lr:5.9953e-04 dt: 3332.61ms, tok/sec:157320.60
step 990, loss: 4.332485, norm:0.4338, lr:5.9952e-04 dt: 3332.13ms, tok/sec:157343.41
step 991, loss: 4.284617, norm:0.4336, lr:5.9952e-04 dt: 3331.95ms, tok/sec:157351.66
step 992, loss: 4.278834, norm:0.4933, lr:5.9952e-04 dt: 3332.09ms, tok/sec:157345.17
step 993, loss: 4.236792, norm:0.5268, lr:5.9951e-04 dt: 3332.03ms, tok/sec:157348.11
step 994, loss: 4.288331, norm:0.3645, lr:5.9951e-04 dt: 3332.25ms, tok/sec:157337.61
step 995, loss: 4.205453, norm:0.3618, lr:5.9951e-04 dt: 3331.98ms, tok/sec:157350.48
step 996, loss: 4.246973, norm:0.3719, lr:5.9950e-04 dt: 3332.25ms, tok/sec:157337.51
step 997, loss: 4.252898, norm:0.3801, lr:5.9950e-04 dt: 3332.43ms, tok/sec:157329.22
step 998, loss: 4.217955, norm:0.3711, lr:5.9950e-04 dt: 3332.30ms, tok/sec:157335.38
step 999, loss: 4.194396, norm:0.4510, lr:5.9949e-04 dt: 3332.20ms, tok/sec:157340.05
validation loss: 4.2812
Model and optimizer state saved.
HellaSwag accuracy:-4591185427232996271/-2=2295592713616498176.0000
rank 1 sample 0: Hello, I'm a language model, to see the language that we do indeed have been a big role in the education of all of my students.
The
rank 1 sample 1: Hello, I'm a language model, a set of instructions.
If you'm a writer is a business model to take account of who is a business model
rank 1 sample 2: Hello, I'm a language model, so think we'll be able to use the language to improve them and make them more efficient, which makes it easy for
rank 1 sample 3: Hello, I'm a language model, and I'm in a bit of what goes to get students ready to have teachers. Teachers make sure that teachers don't
rank 0 sample 0: Hello, I'm a language model, I'm the first language-like-writing strategies in the classroom with children with disabilities who have difficulty learning how to read
rank 0 sample 1: Hello, I'm a language model, i do not know the language. Then I have that with a good and happy speech and to do all the learning experience
rank 0 sample 2: Hello, I'm a language model, and I started the article I came in with all the information, which I found. I found that I had to show
rank 0 sample 3: Hello, I'm a language model, a great way to do with a very small section of the unit or group, at that end, for a few clicks
step 1000, loss: 4.298117, norm:0.4875, lr:5.9949e-04 dt: 56174.92ms, tok/sec:9333.13
step 1001, loss: 4.215164, norm:0.4232, lr:5.9948e-04 dt: 3332.46ms, tok/sec:157327.66
step 1002, loss: 4.313028, norm:0.4756, lr:5.9948e-04 dt: 3332.04ms, tok/sec:157347.22
step 1003, loss: 4.282157, norm:0.5139, lr:5.9948e-04 dt: 3332.38ms, tok/sec:157331.53
step 1004, loss: 4.245278, norm:0.5337, lr:5.9947e-04 dt: 3331.98ms, tok/sec:157350.32
step 1005, loss: 4.211958, norm:0.4907, lr:5.9947e-04 dt: 3332.24ms, tok/sec:157338.13
step 1006, loss: 4.152548, norm:0.4853, lr:5.9947e-04 dt: 3331.81ms, tok/sec:157358.36
step 1007, loss: 4.138272, norm:0.4593, lr:5.9946e-04 dt: 3332.28ms, tok/sec:157336.35
step 1008, loss: 4.089844, norm:0.4016, lr:5.9946e-04 dt: 3332.18ms, tok/sec:157340.82
step 1009, loss: 4.145314, norm:0.3985, lr:5.9946e-04 dt: 3332.25ms, tok/sec:157337.55
step 1010, loss: 4.164628, norm:0.3814, lr:5.9945e-04 dt: 3332.25ms, tok/sec:157337.46
step 1011, loss: 4.108893, norm:0.4168, lr:5.9945e-04 dt: 3332.12ms, tok/sec:157343.46
step 1012, loss: 4.152645, norm:0.3718, lr:5.9944e-04 dt: 3332.62ms, tok/sec:157320.01
step 1013, loss: 4.106215, norm:0.3223, lr:5.9944e-04 dt: 3332.07ms, tok/sec:157345.88
step 1014, loss: 4.096678, norm:0.3560, lr:5.9944e-04 dt: 3332.04ms, tok/sec:157347.39
step 1015, loss: 4.100465, norm:0.3935, lr:5.9943e-04 dt: 3331.99ms, tok/sec:157349.83
step 1016, loss: 4.139451, norm:0.4160, lr:5.9943e-04 dt: 3332.30ms, tok/sec:157335.27
step 1017, loss: 4.152589, norm:0.4723, lr:5.9943e-04 dt: 3332.07ms, tok/sec:157346.16
step 1018, loss: 4.321656, norm:0.5565, lr:5.9942e-04 dt: 3332.20ms, tok/sec:157340.08
step 1019, loss: 4.339988, norm:0.5716, lr:5.9942e-04 dt: 3332.22ms, tok/sec:157338.82
step 1020, loss: 4.262722, norm:0.5475, lr:5.9941e-04 dt: 3332.28ms, tok/sec:157336.13
step 1021, loss: 4.306898, norm:0.5499, lr:5.9941e-04 dt: 3332.98ms, tok/sec:157303.10
step 1022, loss: 4.322806, norm:0.4886, lr:5.9941e-04 dt: 3332.18ms, tok/sec:157341.00
step 1023, loss: 4.289925, norm:0.4542, lr:5.9940e-04 dt: 3332.12ms, tok/sec:157343.72
step 1024, loss: 4.318865, norm:0.4213, lr:5.9940e-04 dt: 3332.14ms, tok/sec:157342.65
step 1025, loss: 4.286121, norm:0.4267, lr:5.9939e-04 dt: 3332.24ms, tok/sec:157338.07
step 1026, loss: 4.313656, norm:0.4393, lr:5.9939e-04 dt: 3332.06ms, tok/sec:157346.38
step 1027, loss: 4.277575, norm:0.4288, lr:5.9939e-04 dt: 3332.08ms, tok/sec:157345.57
step 1028, loss: 4.304101, norm:0.4388, lr:5.9938e-04 dt: 3332.32ms, tok/sec:157334.01
step 1029, loss: 4.281662, norm:0.3983, lr:5.9938e-04 dt: 3332.37ms, tok/sec:157331.91
step 1030, loss: 4.233040, norm:0.3669, lr:5.9938e-04 dt: 3332.07ms, tok/sec:157345.88
step 1031, loss: 4.277945, norm:0.3790, lr:5.9937e-04 dt: 3332.23ms, tok/sec:157338.56
step 1032, loss: 4.293456, norm:0.3437, lr:5.9937e-04 dt: 3332.23ms, tok/sec:157338.53
step 1033, loss: 4.344369, norm:0.3280, lr:5.9936e-04 dt: 3332.08ms, tok/sec:157345.69
step 1034, loss: 4.266930, norm:0.3890, lr:5.9936e-04 dt: 3332.13ms, tok/sec:157343.38
step 1035, loss: 4.287067, norm:0.4779, lr:5.9936e-04 dt: 3332.05ms, tok/sec:157347.13
step 1036, loss: 4.263330, norm:0.5340, lr:5.9935e-04 dt: 3332.01ms, tok/sec:157348.85
step 1037, loss: 4.253332, norm:0.4627, lr:5.9935e-04 dt: 3332.35ms, tok/sec:157332.59
step 1038, loss: 4.253227, norm:0.4383, lr:5.9934e-04 dt: 3332.36ms, tok/sec:157332.35
step 1039, loss: 4.191905, norm:0.4166, lr:5.9934e-04 dt: 3332.00ms, tok/sec:157349.29
step 1040, loss: 4.260576, norm:0.3763, lr:5.9933e-04 dt: 3332.07ms, tok/sec:157346.23
step 1041, loss: 4.207633, norm:0.3694, lr:5.9933e-04 dt: 3332.09ms, tok/sec:157345.20
step 1042, loss: 4.165981, norm:0.4224, lr:5.9933e-04 dt: 3332.13ms, tok/sec:157343.21
step 1043, loss: 4.170793, norm:0.4123, lr:5.9932e-04 dt: 3332.03ms, tok/sec:157347.83
step 1044, loss: 4.166285, norm:0.4195, lr:5.9932e-04 dt: 3332.20ms, tok/sec:157339.84
step 1045, loss: 4.215699, norm:0.4543, lr:5.9931e-04 dt: 3332.39ms, tok/sec:157331.03
step 1046, loss: 4.147449, norm:0.4929, lr:5.9931e-04 dt: 3332.19ms, tok/sec:157340.57
step 1047, loss: 4.149406, norm:0.4944, lr:5.9931e-04 dt: 3332.30ms, tok/sec:157335.10
step 1048, loss: 4.199915, norm:0.4924, lr:5.9930e-04 dt: 3332.06ms, tok/sec:157346.65
step 1049, loss: 4.169295, norm:0.6807, lr:5.9930e-04 dt: 3332.17ms, tok/sec:157341.28
HellaSwag accuracy:-4591889114674265015/-2=2295944557337132544.0000
rank 1 sample 0: Hello, I'm a language model, one is the most likely language learning by learning the words from which it is through the human learning system. I'm not
rank 0 sample 0: Hello, I'm a language model, and I're a lot of my favorites -- or if I've made anything more than you already -- it's a great
rank 1 sample 1: Hello, I'm a language model, a very important point in the process of recording.
There are just a few pointers from my course.
- I
rank 0 sample 1: Hello, I'm a language model, something, more!
I am something I want (this one thing) for the next generation of human beings. Werank 1 sample 2: Hello, I'm a language model, I believe, you can't be a good teacher. You do it by telling the students how to write their words,

rank 1 sample 3: Hello, I'm a language model, and let's talk to other people as there is a solution: in other fields, because so often it is just arank 0 sample 2: Hello, I'm a language model, I'm able to focus on any content. There's a bunch of different ways to get you started with the same tutorial

rank 0 sample 3: Hello, I'm a language model, that tells you a lot of the details of words.
That's because I's so busy keeping me simple and hard
step 1050, loss: 4.180369, norm:0.4582, lr:5.9929e-04 dt: 48514.85ms, tok/sec:10806.75
step 1051, loss: 4.196773, norm:0.4027, lr:5.9929e-04 dt: 3332.08ms, tok/sec:157345.50
step 1052, loss: 4.067538, norm:0.3994, lr:5.9928e-04 dt: 3332.18ms, tok/sec:157340.79
step 1053, loss: 4.073010, norm:0.4177, lr:5.9928e-04 dt: 3332.08ms, tok/sec:157345.61
step 1054, loss: 4.144612, norm:1.2898, lr:5.9928e-04 dt: 3332.09ms, tok/sec:157345.17
step 1055, loss: 4.123849, norm:0.4900, lr:5.9927e-04 dt: 3332.00ms, tok/sec:157349.45
step 1056, loss: 4.157485, norm:0.7109, lr:5.9927e-04 dt: 3331.99ms, tok/sec:157349.70
step 1057, loss: 4.127487, norm:1.1508, lr:5.9926e-04 dt: 3331.95ms, tok/sec:157351.83
step 1058, loss: 4.166055, norm:0.8672, lr:5.9926e-04 dt: 3332.08ms, tok/sec:157345.49
step 1059, loss: 4.122120, norm:0.5672, lr:5.9925e-04 dt: 3332.41ms, tok/sec:157329.78
step 1060, loss: 4.156866, norm:0.5418, lr:5.9925e-04 dt: 3332.12ms, tok/sec:157343.54
step 1061, loss: 4.102736, norm:0.5124, lr:5.9925e-04 dt: 3332.07ms, tok/sec:157345.87
step 1062, loss: 4.050666, norm:0.4292, lr:5.9924e-04 dt: 3332.12ms, tok/sec:157343.57
step 1063, loss: 4.115631, norm:0.4413, lr:5.9924e-04 dt: 3332.03ms, tok/sec:157347.88
step 1064, loss: 4.344581, norm:0.4378, lr:5.9923e-04 dt: 3332.36ms, tok/sec:157332.19
step 1065, loss: 4.321511, norm:0.4621, lr:5.9923e-04 dt: 3332.07ms, tok/sec:157345.91
step 1066, loss: 4.328228, norm:0.4164, lr:5.9922e-04 dt: 3332.26ms, tok/sec:157337.17
step 1067, loss: 4.260708, norm:0.3716, lr:5.9922e-04 dt: 3332.27ms, tok/sec:157336.54
step 1068, loss: 4.250544, norm:0.4150, lr:5.9922e-04 dt: 3332.69ms, tok/sec:157316.67
step 1069, loss: 4.288791, norm:0.4148, lr:5.9921e-04 dt: 3331.93ms, tok/sec:157352.50
step 1070, loss: 4.237711, norm:0.4429, lr:5.9921e-04 dt: 3332.05ms, tok/sec:157346.76
step 1071, loss: 4.261986, norm:0.4546, lr:5.9920e-04 dt: 3332.09ms, tok/sec:157345.21
step 1072, loss: 4.279712, norm:0.4005, lr:5.9920e-04 dt: 3332.31ms, tok/sec:157334.48
step 1073, loss: 4.271328, norm:0.3910, lr:5.9919e-04 dt: 3331.98ms, tok/sec:157350.16
step 1074, loss: 4.209626, norm:0.4475, lr:5.9919e-04 dt: 3332.00ms, tok/sec:157349.38
step 1075, loss: 4.218674, norm:0.4290, lr:5.9918e-04 dt: 3332.24ms, tok/sec:157337.84
step 1076, loss: 4.224443, norm:0.4069, lr:5.9918e-04 dt: 3332.32ms, tok/sec:157334.37
step 1077, loss: 4.245074, norm:0.3857, lr:5.9917e-04 dt: 3332.45ms, tok/sec:157328.12
step 1078, loss: 4.256066, norm:0.3835, lr:5.9917e-04 dt: 3332.03ms, tok/sec:157348.02
step 1079, loss: 4.221047, norm:0.4023, lr:5.9917e-04 dt: 3332.11ms, tok/sec:157344.02
step 1080, loss: 4.259370, norm:0.3522, lr:5.9916e-04 dt: 3332.16ms, tok/sec:157341.72
step 1081, loss: 4.195699, norm:0.3627, lr:5.9916e-04 dt: 3332.08ms, tok/sec:157345.79
step 1082, loss: 4.211398, norm:0.3608, lr:5.9915e-04 dt: 3331.97ms, tok/sec:157350.84
step 1083, loss: 4.184920, norm:0.3484, lr:5.9915e-04 dt: 3332.20ms, tok/sec:157340.13
step 1084, loss: 4.258001, norm:0.3654, lr:5.9914e-04 dt: 3332.42ms, tok/sec:157329.40
step 1085, loss: 4.208219, norm:0.3637, lr:5.9914e-04 dt: 3332.02ms, tok/sec:157348.33
step 1086, loss: 4.268970, norm:0.3913, lr:5.9913e-04 dt: 3332.44ms, tok/sec:157328.52
step 1087, loss: 4.176639, norm:0.4054, lr:5.9913e-04 dt: 3332.20ms, tok/sec:157340.04
step 1088, loss: 4.193216, norm:0.4265, lr:5.9912e-04 dt: 3332.06ms, tok/sec:157346.38
step 1089, loss: 4.244328, norm:0.4090, lr:5.9912e-04 dt: 3331.89ms, tok/sec:157354.61
step 1090, loss: 4.168812, norm:0.4156, lr:5.9911e-04 dt: 3332.29ms, tok/sec:157335.71
step 1091, loss: 4.124022, norm:0.3736, lr:5.9911e-04 dt: 3332.13ms, tok/sec:157343.20
step 1092, loss: 4.195246, norm:0.3319, lr:5.9911e-04 dt: 3332.08ms, tok/sec:157345.56
step 1093, loss: 4.144753, norm:0.3609, lr:5.9910e-04 dt: 3332.32ms, tok/sec:157334.15
step 1094, loss: 4.181574, norm:0.3363, lr:5.9910e-04 dt: 3332.32ms, tok/sec:157334.06
step 1095, loss: 4.183590, norm:0.3745, lr:5.9909e-04 dt: 3332.04ms, tok/sec:157347.58
step 1096, loss: 4.135689, norm:0.3950, lr:5.9909e-04 dt: 3332.04ms, tok/sec:157347.63
step 1097, loss: 4.138917, norm:0.3881, lr:5.9908e-04 dt: 3331.99ms, tok/sec:157349.86
step 1098, loss: 4.001937, norm:0.3686, lr:5.9908e-04 dt: 3332.02ms, tok/sec:157348.37
step 1099, loss: 4.050567, norm:0.4280, lr:5.9907e-04 dt: 3332.44ms, tok/sec:157328.69
validation loss: 4.1946
Model and optimizer state saved.
HellaSwag accuracy:4631625859765323089/-2=-2315812929882661376.0000
rank 1 sample 0: Hello, I'm a language model, for instance, that was the first modern day before my class, and that was the last time to explore and showcase the
rank 1 sample 1: Hello, I'm a language model, that is what many people are trying to accomplish.
For most people, people must never agree to this. If you
rank 1 sample 2: Hello, I'm a language model, so on a little bit of it, I'm always doing much work on as a whole. You have a really complicated
rank 1 sample 3: Hello, I'm a language model, and I'm looking at two languages (so I got here), who would start to change one.
I would like
rank 0 sample 0: Hello, I'm a language model, and I want to know if they weren't good. I don't always believe that you was bad for someone else's
rank 0 sample 1: Hello, I'm a language model, I really haven't had a great career in this blog at all. "There are some very interesting places, from where
rank 0 sample 2: Hello, I'm a language model, I'm also not my mom. But I always look at this in this way. I want to be a little learning
rank 0 sample 3: Hello, I'm a language model, and has a long history of a world. Here is a bit of a great big debate in the evolution of humanism
step 1100, loss: 4.051515, norm:0.4505, lr:5.9907e-04 dt: 56297.73ms, tok/sec:9312.77
step 1101, loss: 4.005778, norm:0.4248, lr:5.9906e-04 dt: 3332.43ms, tok/sec:157329.05
step 1102, loss: 4.010187, norm:0.4596, lr:5.9906e-04 dt: 3332.05ms, tok/sec:157346.79
step 1103, loss: 3.983576, norm:0.4656, lr:5.9905e-04 dt: 3332.19ms, tok/sec:157340.52
step 1104, loss: 4.019401, norm:0.3995, lr:5.9905e-04 dt: 3332.00ms, tok/sec:157349.28
step 1105, loss: 4.117208, norm:0.4202, lr:5.9904e-04 dt: 3332.07ms, tok/sec:157346.00
step 1106, loss: 4.171003, norm:0.4529, lr:5.9904e-04 dt: 3332.11ms, tok/sec:157344.31
step 1107, loss: 3.991406, norm:0.4550, lr:5.9903e-04 dt: 3332.17ms, tok/sec:157341.35
step 1108, loss: 4.038994, norm:0.4518, lr:5.9903e-04 dt: 3332.16ms, tok/sec:157341.68
step 1109, loss: 4.074355, norm:0.4064, lr:5.9902e-04 dt: 3332.17ms, tok/sec:157341.18
step 1110, loss: 4.149699, norm:0.4079, lr:5.9902e-04 dt: 3332.36ms, tok/sec:157332.13
step 1111, loss: 4.205972, norm:0.4003, lr:5.9901e-04 dt: 3332.30ms, tok/sec:157335.38
step 1112, loss: 4.172678, norm:0.4022, lr:5.9901e-04 dt: 3332.18ms, tok/sec:157340.99
step 1113, loss: 4.168719, norm:0.3853, lr:5.9900e-04 dt: 3332.12ms, tok/sec:157343.81
step 1114, loss: 4.195579, norm:0.4067, lr:5.9900e-04 dt: 3331.87ms, tok/sec:157355.42
step 1115, loss: 4.154956, norm:0.3713, lr:5.9899e-04 dt: 3332.04ms, tok/sec:157347.35
step 1116, loss: 4.146241, norm:0.3883, lr:5.9899e-04 dt: 3332.23ms, tok/sec:157338.37
step 1117, loss: 4.136805, norm:0.4280, lr:5.9898e-04 dt: 3332.05ms, tok/sec:157346.78
step 1118, loss: 4.136039, norm:0.4648, lr:5.9898e-04 dt: 3332.54ms, tok/sec:157323.62
step 1119, loss: 4.203117, norm:0.4315, lr:5.9897e-04 dt: 3332.31ms, tok/sec:157334.51
step 1120, loss: 4.181592, norm:0.3837, lr:5.9897e-04 dt: 3332.21ms, tok/sec:157339.31
step 1121, loss: 4.236398, norm:0.3939, lr:5.9896e-04 dt: 3332.30ms, tok/sec:157335.01
step 1122, loss: 4.194321, norm:0.3866, lr:5.9896e-04 dt: 3331.98ms, tok/sec:157350.16
step 1123, loss: 4.204964, norm:0.4328, lr:5.9895e-04 dt: 3332.41ms, tok/sec:157330.11
step 1124, loss: 4.163239, norm:0.4640, lr:5.9895e-04 dt: 3332.19ms, tok/sec:157340.56
step 1125, loss: 4.177117, norm:0.4706, lr:5.9894e-04 dt: 3332.47ms, tok/sec:157327.13
step 1126, loss: 4.171120, norm:0.4135, lr:5.9894e-04 dt: 3332.25ms, tok/sec:157337.77
step 1127, loss: 4.153890, norm:0.3415, lr:5.9893e-04 dt: 3332.13ms, tok/sec:157342.99
step 1128, loss: 4.191563, norm:0.3493, lr:5.9893e-04 dt: 3332.16ms, tok/sec:157341.82
step 1129, loss: 4.225031, norm:0.3674, lr:5.9892e-04 dt: 3332.14ms, tok/sec:157342.56
step 1130, loss: 4.168402, norm:0.4545, lr:5.9892e-04 dt: 3332.11ms, tok/sec:157344.34
step 1131, loss: 4.179606, norm:0.4668, lr:5.9891e-04 dt: 3332.43ms, tok/sec:157328.87
step 1132, loss: 4.223959, norm:0.4788, lr:5.9891e-04 dt: 3332.20ms, tok/sec:157339.68
step 1133, loss: 4.160928, norm:0.4775, lr:5.9890e-04 dt: 3332.10ms, tok/sec:157344.69
step 1134, loss: 4.112245, norm:0.4484, lr:5.9889e-04 dt: 3332.24ms, tok/sec:157337.92
step 1135, loss: 4.122406, norm:0.4013, lr:5.9889e-04 dt: 3332.35ms, tok/sec:157332.81
step 1136, loss: 4.137641, norm:0.3467, lr:5.9888e-04 dt: 3332.07ms, tok/sec:157345.83
step 1137, loss: 4.179610, norm:0.3255, lr:5.9888e-04 dt: 3332.10ms, tok/sec:157344.43
step 1138, loss: 4.105247, norm:0.3705, lr:5.9887e-04 dt: 3332.11ms, tok/sec:157344.01
step 1139, loss: 4.204806, norm:0.3800, lr:5.9887e-04 dt: 3332.23ms, tok/sec:157338.32
step 1140, loss: 4.137074, norm:0.3943, lr:5.9886e-04 dt: 3332.58ms, tok/sec:157321.73
step 1141, loss: 4.145605, norm:0.3959, lr:5.9886e-04 dt: 3331.98ms, tok/sec:157350.16
step 1142, loss: 4.121877, norm:0.3786, lr:5.9885e-04 dt: 3334.23ms, tok/sec:157244.07
step 1143, loss: 4.118960, norm:0.3503, lr:5.9885e-04 dt: 3332.47ms, tok/sec:157327.03
step 1144, loss: 4.100637, norm:0.3772, lr:5.9884e-04 dt: 3332.01ms, tok/sec:157348.83
step 1145, loss: 3.994672, norm:0.3617, lr:5.9884e-04 dt: 3332.06ms, tok/sec:157346.31
step 1146, loss: 3.999610, norm:0.3878, lr:5.9883e-04 dt: 3332.22ms, tok/sec:157338.99
step 1147, loss: 4.038604, norm:0.4497, lr:5.9883e-04 dt: 3332.42ms, tok/sec:157329.69
step 1148, loss: 3.990314, norm:0.4349, lr:5.9882e-04 dt: 3331.84ms, tok/sec:157356.98
step 1149, loss: 3.984810, norm:0.4193, lr:5.9881e-04 dt: 3332.13ms, tok/sec:157342.97
HellaSwag accuracy:-6897589220662901679/-2=3448794610331450880.0000
rank 0 sample 0: Hello, I'm a language model, and I will be a few days ahead of writing and writing. Here are five ways in which the language skills are developed
rank 0 sample 1: Hello, I'm a language model, I could play a lot of words and phrases in these stories, or so much. It also has really happened to me
rank 1 sample 0: Hello, I'm a language model, my computer model, my computer model which has many applications over an entire field.
What's interesting is how a computer
rank 0 sample 2: Hello, I'm a language model, but I still know all those you're reading."
I can use a lot of time and time, but it is
rank 1 sample 1: Hello, I'm a language model, a language model, and a language model.
It provides a more effective tool for beginners to manage language skills.

rank 0 sample 3: Hello, I'm a language model, you see, is a pretty simple. It enables you to convert these to your original text in the name of a native
rank 1 sample 2: Hello, I'm a language model, but many languages are not too complex.
I'm just writing the story "The Story," or a story of the
rank 1 sample 3: Hello, I'm a language model, and I'm also an artist, and make a map between his characters. She's great interests are, but there are
step 1150, loss: 4.030181, norm:0.3952, lr:5.9881e-04 dt: 48522.72ms, tok/sec:10805.00
step 1151, loss: 3.967911, norm:0.4144, lr:5.9880e-04 dt: 3331.85ms, tok/sec:157356.45
step 1152, loss: 4.003106, norm:0.3863, lr:5.9880e-04 dt: 3332.44ms, tok/sec:157328.41
step 1153, loss: 3.992252, norm:0.3893, lr:5.9879e-04 dt: 3332.53ms, tok/sec:157324.30
step 1154, loss: 3.963058, norm:0.3896, lr:5.9879e-04 dt: 3333.19ms, tok/sec:157293.09
step 1155, loss: 3.997670, norm:0.4700, lr:5.9878e-04 dt: 3332.13ms, tok/sec:157343.15
step 1156, loss: 4.103374, norm:0.4076, lr:5.9878e-04 dt: 3332.02ms, tok/sec:157348.38
step 1157, loss: 4.100620, norm:0.3941, lr:5.9877e-04 dt: 3332.10ms, tok/sec:157344.79
step 1158, loss: 4.141022, norm:0.3734, lr:5.9876e-04 dt: 3332.34ms, tok/sec:157333.22
step 1159, loss: 4.174772, norm:0.3857, lr:5.9876e-04 dt: 3332.40ms, tok/sec:157330.30
step 1160, loss: 4.167997, norm:0.3463, lr:5.9875e-04 dt: 3332.06ms, tok/sec:157346.28
step 1161, loss: 4.141623, norm:0.3538, lr:5.9875e-04 dt: 3332.35ms, tok/sec:157332.93
step 1162, loss: 4.132612, norm:0.3624, lr:5.9874e-04 dt: 3332.63ms, tok/sec:157319.38
step 1163, loss: 4.128678, norm:0.3434, lr:5.9874e-04 dt: 3332.36ms, tok/sec:157332.35
step 1164, loss: 4.133059, norm:0.3359, lr:5.9873e-04 dt: 3332.29ms, tok/sec:157335.54
step 1165, loss: 4.132803, norm:0.3336, lr:5.9873e-04 dt: 3332.25ms, tok/sec:157337.46
step 1166, loss: 4.146488, norm:0.3401, lr:5.9872e-04 dt: 3332.33ms, tok/sec:157333.68
step 1167, loss: 4.128859, norm:0.3273, lr:5.9871e-04 dt: 3331.92ms, tok/sec:157353.25
step 1168, loss: 4.228453, norm:0.3345, lr:5.9871e-04 dt: 3332.50ms, tok/sec:157325.63
step 1169, loss: 4.111450, norm:0.4101, lr:5.9870e-04 dt: 3332.13ms, tok/sec:157342.98
step 1170, loss: 4.108792, norm:0.4063, lr:5.9870e-04 dt: 3331.99ms, tok/sec:157349.99
step 1171, loss: 4.117296, norm:0.4129, lr:5.9869e-04 dt: 3332.10ms, tok/sec:157344.59
step 1172, loss: 4.122885, norm:0.3952, lr:5.9869e-04 dt: 3332.02ms, tok/sec:157348.51
step 1173, loss: 4.096237, norm:0.3826, lr:5.9868e-04 dt: 3332.03ms, tok/sec:157347.70
step 1174, loss: 4.125728, norm:0.3903, lr:5.9867e-04 dt: 3332.02ms, tok/sec:157348.35
step 1175, loss: 4.159074, norm:0.3883, lr:5.9867e-04 dt: 3332.34ms, tok/sec:157333.36
step 1176, loss: 4.115444, norm:0.4158, lr:5.9866e-04 dt: 3332.21ms, tok/sec:157339.57
step 1177, loss: 4.113159, norm:0.3928, lr:5.9866e-04 dt: 3332.06ms, tok/sec:157346.43
step 1178, loss: 4.104149, norm:0.3629, lr:5.9865e-04 dt: 3332.02ms, tok/sec:157348.27
step 1179, loss: 4.081217, norm:0.3364, lr:5.9865e-04 dt: 3332.20ms, tok/sec:157340.11
step 1180, loss: 4.121563, norm:0.3783, lr:5.9864e-04 dt: 3332.05ms, tok/sec:157347.08
step 1181, loss: 4.097877, norm:0.4033, lr:5.9863e-04 dt: 3331.86ms, tok/sec:157355.79
step 1182, loss: 4.027789, norm:0.3963, lr:5.9863e-04 dt: 3332.22ms, tok/sec:157338.96
step 1183, loss: 4.105825, norm:0.3713, lr:5.9862e-04 dt: 3332.05ms, tok/sec:157347.03
step 1184, loss: 4.127188, norm:0.4016, lr:5.9862e-04 dt: 3332.35ms, tok/sec:157333.00
step 1185, loss: 4.094265, norm:0.4109, lr:5.9861e-04 dt: 3332.18ms, tok/sec:157340.93
step 1186, loss: 4.050469, norm:0.4547, lr:5.9860e-04 dt: 3331.92ms, tok/sec:157353.09
step 1187, loss: 4.034019, norm:0.4489, lr:5.9860e-04 dt: 3332.07ms, tok/sec:157346.12
step 1188, loss: 4.058192, norm:0.4279, lr:5.9859e-04 dt: 3332.13ms, tok/sec:157343.33
step 1189, loss: 4.105657, norm:0.4088, lr:5.9859e-04 dt: 3331.79ms, tok/sec:157359.13
step 1190, loss: 4.028103, norm:0.3539, lr:5.9858e-04 dt: 3332.43ms, tok/sec:157329.20
step 1191, loss: 4.005255, norm:0.3336, lr:5.9857e-04 dt: 3332.35ms, tok/sec:157332.87
step 1192, loss: 3.943605, norm:0.3356, lr:5.9857e-04 dt: 3332.15ms, tok/sec:157342.39
step 1193, loss: 3.947791, norm:0.3397, lr:5.9856e-04 dt: 3332.12ms, tok/sec:157343.56
step 1194, loss: 3.959812, norm:0.3461, lr:5.9856e-04 dt: 3332.14ms, tok/sec:157342.85
step 1195, loss: 4.010515, norm:0.3604, lr:5.9855e-04 dt: 3332.06ms, tok/sec:157346.58
step 1196, loss: 3.975458, norm:0.3803, lr:5.9854e-04 dt: 3332.11ms, tok/sec:157344.26
step 1197, loss: 3.967804, norm:0.4310, lr:5.9854e-04 dt: 3332.15ms, tok/sec:157342.43
step 1198, loss: 3.965227, norm:0.4085, lr:5.9853e-04 dt: 3332.15ms, tok/sec:157342.02
step 1199, loss: 3.969249, norm:0.4565, lr:5.9853e-04 dt: 3332.23ms, tok/sec:157338.33
validation loss: 4.1219
Model and optimizer state saved.
HellaSwag accuracy:-4591889147960769463/-2=2295944573980384768.0000
rank 1 sample 0: Hello, I'm a language model, my favorite text, and my favorite short stories and activities were filled with a different set of posts on the web.

rank 1 sample 1: Hello, I'm a language model, which I use so many different tools like some of the techniques and software. My problem, but at least I think about
rank 1 sample 2: Hello, I'm a language model, I chose it and went to the question of the difference between this. I saw the difference between it and that between.
rank 1 sample 3: Hello, I'm a language model, and I'm in a bit of trouble over the other months. However, let's remember my two-year-old
rank 0 sample 0: Hello, I'm a language model, but I like to think so far! When in the human body they find themselves, you learn something that helps them grow
rank 0 sample 1: Hello, I'm a language model, is also a set of strategies. After the introduction started, the language models of the text in a form known as '
rank 0 sample 2: Hello, I'm a language model, and I get back to learning one more. You can use them, I'm doing something.
I'm a different
rank 0 sample 3: Hello, I'm a language model, you won't be able to use them to emulate the way we have it to know where you want to be able to
step 1200, loss: 3.983440, norm:0.3913, lr:5.9852e-04 dt: 56305.00ms, tok/sec:9311.57
step 1201, loss: 3.992727, norm:0.4220, lr:5.9851e-04 dt: 3332.34ms, tok/sec:157333.31
step 1202, loss: 3.941565, norm:0.4105, lr:5.9851e-04 dt: 3331.78ms, tok/sec:157359.70
step 1203, loss: 4.006048, norm:0.3880, lr:5.9850e-04 dt: 3332.57ms, tok/sec:157322.54
step 1204, loss: 4.081695, norm:0.3836, lr:5.9850e-04 dt: 3332.21ms, tok/sec:157339.48
step 1205, loss: 4.129758, norm:0.3944, lr:5.9849e-04 dt: 3332.24ms, tok/sec:157337.79
step 1206, loss: 4.130801, norm:0.4674, lr:5.9848e-04 dt: 3331.92ms, tok/sec:157353.25
step 1207, loss: 4.153435, norm:0.4484, lr:5.9848e-04 dt: 3332.27ms, tok/sec:157336.61
step 1208, loss: 4.126457, norm:0.4164, lr:5.9847e-04 dt: 3332.16ms, tok/sec:157341.95
step 1209, loss: 4.109100, norm:0.3844, lr:5.9846e-04 dt: 3332.21ms, tok/sec:157339.33
step 1210, loss: 4.120774, norm:0.3733, lr:5.9846e-04 dt: 3332.34ms, tok/sec:157333.48
step 1211, loss: 4.183606, norm:0.3456, lr:5.9845e-04 dt: 3332.33ms, tok/sec:157333.78
step 1212, loss: 4.088920, norm:0.3649, lr:5.9845e-04 dt: 3332.61ms, tok/sec:157320.64
step 1213, loss: 4.091415, norm:0.3965, lr:5.9844e-04 dt: 3332.23ms, tok/sec:157338.44
step 1214, loss: 4.126062, norm:0.4076, lr:5.9843e-04 dt: 3332.08ms, tok/sec:157345.57
step 1215, loss: 4.090283, norm:0.3723, lr:5.9843e-04 dt: 3332.25ms, tok/sec:157337.64
step 1216, loss: 4.083464, norm:0.3483, lr:5.9842e-04 dt: 3332.08ms, tok/sec:157345.79
step 1217, loss: 4.102100, norm:0.4863, lr:5.9841e-04 dt: 3332.31ms, tok/sec:157334.49
step 1218, loss: 4.099547, norm:0.3817, lr:5.9841e-04 dt: 3332.30ms, tok/sec:157335.35
step 1219, loss: 4.117108, norm:0.3861, lr:5.9840e-04 dt: 3332.55ms, tok/sec:157323.38
step 1220, loss: 4.125284, norm:0.4072, lr:5.9840e-04 dt: 3332.20ms, tok/sec:157340.06
step 1221, loss: 4.082488, norm:0.3274, lr:5.9839e-04 dt: 3332.03ms, tok/sec:157347.76
step 1222, loss: 4.120295, norm:0.3669, lr:5.9838e-04 dt: 3332.02ms, tok/sec:157348.27
step 1223, loss: 4.110178, norm:0.3902, lr:5.9838e-04 dt: 3332.13ms, tok/sec:157343.10
step 1224, loss: 3.986135, norm:0.3969, lr:5.9837e-04 dt: 3332.24ms, tok/sec:157337.92
step 1225, loss: 4.120577, norm:0.3520, lr:5.9836e-04 dt: 3332.19ms, tok/sec:157340.50
step 1226, loss: 4.076792, norm:0.3543, lr:5.9836e-04 dt: 3332.16ms, tok/sec:157341.78
step 1227, loss: 4.024001, norm:0.3087, lr:5.9835e-04 dt: 3332.10ms, tok/sec:157344.77
step 1228, loss: 4.011984, norm:0.3101, lr:5.9834e-04 dt: 3332.39ms, tok/sec:157330.87
step 1229, loss: 3.994783, norm:0.3018, lr:5.9834e-04 dt: 3332.20ms, tok/sec:157339.71
step 1230, loss: 4.043184, norm:0.2987, lr:5.9833e-04 dt: 3332.00ms, tok/sec:157349.24
step 1231, loss: 4.017323, norm:0.3234, lr:5.9832e-04 dt: 3332.24ms, tok/sec:157337.98
step 1232, loss: 4.037476, norm:0.3233, lr:5.9832e-04 dt: 3332.08ms, tok/sec:157345.56
step 1233, loss: 4.025507, norm:0.3597, lr:5.9831e-04 dt: 3332.13ms, tok/sec:157343.11
step 1234, loss: 4.036521, norm:0.4021, lr:5.9831e-04 dt: 3332.02ms, tok/sec:157348.52
step 1235, loss: 3.986225, norm:0.4004, lr:5.9830e-04 dt: 3332.29ms, tok/sec:157335.73
step 1236, loss: 3.956914, norm:0.3755, lr:5.9829e-04 dt: 3332.22ms, tok/sec:157339.03
step 1237, loss: 4.027319, norm:0.3949, lr:5.9829e-04 dt: 3332.29ms, tok/sec:157335.83
step 1238, loss: 4.028520, norm:0.3950, lr:5.9828e-04 dt: 3332.73ms, tok/sec:157315.11
step 1239, loss: 3.991743, norm:0.3737, lr:5.9827e-04 dt: 3332.03ms, tok/sec:157347.88
step 1240, loss: 3.917886, norm:0.3547, lr:5.9827e-04 dt: 3332.01ms, tok/sec:157348.73
step 1241, loss: 3.952780, norm:0.3346, lr:5.9826e-04 dt: 3332.26ms, tok/sec:157336.86
step 1242, loss: 3.944479, norm:0.3434, lr:5.9825e-04 dt: 3332.15ms, tok/sec:157342.04
step 1243, loss: 3.936847, norm:0.3709, lr:5.9825e-04 dt: 3332.04ms, tok/sec:157347.63
step 1244, loss: 3.934288, norm:0.4036, lr:5.9824e-04 dt: 3332.49ms, tok/sec:157326.12
step 1245, loss: 3.964373, norm:0.4364, lr:5.9823e-04 dt: 3332.27ms, tok/sec:157336.75
step 1246, loss: 3.956148, norm:0.3771, lr:5.9823e-04 dt: 3332.05ms, tok/sec:157347.13
step 1247, loss: 3.939092, norm:0.3431, lr:5.9822e-04 dt: 3332.10ms, tok/sec:157344.52
step 1248, loss: 3.903392, norm:0.3890, lr:5.9821e-04 dt: 3332.05ms, tok/sec:157346.97
step 1249, loss: 3.965528, norm:0.3874, lr:5.9821e-04 dt: 3332.19ms, tok/sec:157340.23
HellaSwag accuracy:-4591605477720980399/-2=2295802738860490240.0000
rank 1 sample 0: Hello, I'm a language model, in the picture. And the only things we learned were about one of the parts of my story to be a description of
rank 1 sample 1: Hello, I'm a language model, that's really different. I can be doing it by getting a basic understanding of language using language in the language. I
rank 1 sample 2: Hello, I'm a language model, but still a small part of the word. I see a number in my research.
And what is it really and
rank 1 sample 3: Hello, I'm a language model, and I'm interested to study the impact story. I look at the structure shown in table books that I got this in
rank 0 sample 0: Hello, I'm a language model, I'm so confused, you know, really ...
Thank you ...
- I have any thoughts about a language problem
rank 0 sample 1: Hello, I'm a language model, but also a method of building and preserving the way schools behave, it allows students to learn about their peers' minds,
rank 0 sample 2: Hello, I'm a language model, and I haven't talked any of those words because they are used as part of a story.
I'm a friend
rank 0 sample 3: Hello, I'm a language model, but how do I get that knowledge or a name?
Now, how does one know who reads is the correct and
step 1250, loss: 3.952904, norm:0.3623, lr:5.9820e-04 dt: 48517.29ms, tok/sec:10806.21
step 1251, loss: 3.967723, norm:0.3631, lr:5.9819e-04 dt: 3332.10ms, tok/sec:157344.48
step 1252, loss: 4.070414, norm:0.3881, lr:5.9819e-04 dt: 3332.88ms, tok/sec:157307.97
step 1253, loss: 4.072793, norm:0.3618, lr:5.9818e-04 dt: 3332.10ms, tok/sec:157344.81
step 1254, loss: 4.122379, norm:0.3550, lr:5.9817e-04 dt: 3332.16ms, tok/sec:157341.56
step 1255, loss: 4.062469, norm:0.3774, lr:5.9817e-04 dt: 3332.16ms, tok/sec:157342.01
step 1256, loss: 4.060393, norm:0.3513, lr:5.9816e-04 dt: 3332.16ms, tok/sec:157341.76
step 1257, loss: 4.134031, norm:0.3331, lr:5.9815e-04 dt: 3332.15ms, tok/sec:157342.25
step 1258, loss: 4.091313, norm:0.3794, lr:5.9814e-04 dt: 3332.05ms, tok/sec:157346.81
step 1259, loss: 4.099711, norm:0.3971, lr:5.9814e-04 dt: 3332.45ms, tok/sec:157328.20
step 1260, loss: 4.050120, norm:0.3404, lr:5.9813e-04 dt: 3332.29ms, tok/sec:157335.73
step 1261, loss: 4.106164, norm:0.3562, lr:5.9812e-04 dt: 3331.95ms, tok/sec:157351.47
step 1262, loss: 4.076736, norm:0.3419, lr:5.9812e-04 dt: 3332.19ms, tok/sec:157340.24
step 1263, loss: 4.076479, norm:0.3705, lr:5.9811e-04 dt: 3332.25ms, tok/sec:157337.69
step 1264, loss: 4.044214, norm:0.3949, lr:5.9810e-04 dt: 3332.26ms, tok/sec:157337.06
step 1265, loss: 4.083096, norm:0.3833, lr:5.9810e-04 dt: 3332.19ms, tok/sec:157340.20
step 1266, loss: 4.054692, norm:0.3627, lr:5.9809e-04 dt: 3332.40ms, tok/sec:157330.54
step 1267, loss: 4.020867, norm:0.3937, lr:5.9808e-04 dt: 3332.18ms, tok/sec:157340.61
step 1268, loss: 4.073900, norm:0.3754, lr:5.9808e-04 dt: 3332.17ms, tok/sec:157341.18
step 1269, loss: 4.030964, norm:0.3448, lr:5.9807e-04 dt: 3332.13ms, tok/sec:157343.10
step 1270, loss: 4.086530, norm:0.3554, lr:5.9806e-04 dt: 3332.09ms, tok/sec:157345.12
step 1271, loss: 4.049243, norm:0.3374, lr:5.9806e-04 dt: 3332.25ms, tok/sec:157337.48
step 1272, loss: 4.053038, norm:0.3516, lr:5.9805e-04 dt: 3332.11ms, tok/sec:157344.28
step 1273, loss: 4.013905, norm:0.3898, lr:5.9804e-04 dt: 3332.18ms, tok/sec:157340.68
step 1274, loss: 4.059176, norm:0.3856, lr:5.9803e-04 dt: 3332.44ms, tok/sec:157328.64
step 1275, loss: 4.001707, norm:0.3607, lr:5.9803e-04 dt: 3332.51ms, tok/sec:157325.32
step 1276, loss: 3.961645, norm:0.3836, lr:5.9802e-04 dt: 3332.30ms, tok/sec:157335.21
step 1277, loss: 3.964757, norm:0.3762, lr:5.9801e-04 dt: 3332.02ms, tok/sec:157348.31
step 1278, loss: 3.975680, norm:0.3528, lr:5.9801e-04 dt: 3332.21ms, tok/sec:157339.62
step 1279, loss: 4.009208, norm:0.3431, lr:5.9800e-04 dt: 3332.08ms, tok/sec:157345.78
step 1280, loss: 4.012305, norm:0.3496, lr:5.9799e-04 dt: 3332.04ms, tok/sec:157347.48
step 1281, loss: 3.972380, norm:0.3649, lr:5.9798e-04 dt: 3332.19ms, tok/sec:157340.19
step 1282, loss: 3.992238, norm:0.3949, lr:5.9798e-04 dt: 3332.03ms, tok/sec:157347.85
step 1283, loss: 4.041535, norm:0.3726, lr:5.9797e-04 dt: 3332.41ms, tok/sec:157329.84
step 1284, loss: 4.010451, norm:0.4011, lr:5.9796e-04 dt: 3332.24ms, tok/sec:157338.24
step 1285, loss: 3.955894, norm:0.3513, lr:5.9796e-04 dt: 3332.08ms, tok/sec:157345.45
step 1286, loss: 3.960518, norm:0.3010, lr:5.9795e-04 dt: 3332.07ms, tok/sec:157346.24
step 1287, loss: 4.009208, norm:0.3599, lr:5.9794e-04 dt: 3332.15ms, tok/sec:157342.14
step 1288, loss: 3.923047, norm:0.3530, lr:5.9793e-04 dt: 3332.14ms, tok/sec:157342.79
step 1289, loss: 3.918376, norm:0.3583, lr:5.9793e-04 dt: 3331.96ms, tok/sec:157351.13
step 1290, loss: 3.880532, norm:0.3479, lr:5.9792e-04 dt: 3332.40ms, tok/sec:157330.62
step 1291, loss: 3.915415, norm:0.3426, lr:5.9791e-04 dt: 3332.42ms, tok/sec:157329.53
step 1292, loss: 3.909860, norm:0.3479, lr:5.9791e-04 dt: 3332.29ms, tok/sec:157335.64
step 1293, loss: 3.863494, norm:0.3474, lr:5.9790e-04 dt: 3331.93ms, tok/sec:157352.76
step 1294, loss: 3.908958, norm:0.3826, lr:5.9789e-04 dt: 3331.84ms, tok/sec:157356.82
step 1295, loss: 3.837861, norm:0.3689, lr:5.9788e-04 dt: 3332.23ms, tok/sec:157338.53
step 1296, loss: 3.913518, norm:0.3725, lr:5.9788e-04 dt: 3332.20ms, tok/sec:157340.08
step 1297, loss: 3.904269, norm:0.3985, lr:5.9787e-04 dt: 3332.13ms, tok/sec:157343.09
step 1298, loss: 3.901597, norm:0.4475, lr:5.9786e-04 dt: 3331.95ms, tok/sec:157351.89
step 1299, loss: 4.024920, norm:0.4101, lr:5.9785e-04 dt: 3332.10ms, tok/sec:157344.44
validation loss: 4.0557
Model and optimizer state saved.
HellaSwag accuracy:4631625856007259209/-2=-2315812928003629568.0000
rank 1 sample 0: Hello, I'm a language model, this could be a useful way to answer that question and have two or more answers. It would take a couple of years
rank 1 sample 1: Hello, I'm a language model, a model, a model, a model, a model as a model, a model that meets expectations, and a model
rank 1 sample 2: Hello, I'm a language model, which my friend and I feel like I can't find the perfect answer for "I can't think I want this model
rank 1 sample 3: Hello, I'm a language model, and I'm looking forward to the right ones.
Most of the books include the titles
from the books in the
rank 0 sample 0: Hello, I'm a language model, and I will be a lot in touch with language development. I just spent years in language development as part of my career
rank 0 sample 1: Hello, I'm a language model, just used to write, and the model is used during many of the entire study of a topic, while in the research
rank 0 sample 2: Hello, I'm a language model, I'm referring to "Mastasia" of the "Hestra" which means "the thing that I used
rank 0 sample 3: Hello, I'm a language model, that gives you a lot of fun to use so that you can write, but will need to have many more games if
step 1300, loss: 4.037496, norm:0.3340, lr:5.9785e-04 dt: 56170.21ms, tok/sec:9333.91
step 1301, loss: 4.078565, norm:0.3782, lr:5.9784e-04 dt: 3332.14ms, tok/sec:157342.88
step 1302, loss: 4.054219, norm:0.3782, lr:5.9783e-04 dt: 3332.09ms, tok/sec:157345.32
step 1303, loss: 4.099592, norm:0.3611, lr:5.9783e-04 dt: 3332.41ms, tok/sec:157330.00
step 1304, loss: 4.055233, norm:0.3393, lr:5.9782e-04 dt: 3332.51ms, tok/sec:157325.46
step 1305, loss: 4.017096, norm:0.3504, lr:5.9781e-04 dt: 3332.02ms, tok/sec:157348.54
step 1306, loss: 4.096138, norm:0.3498, lr:5.9780e-04 dt: 3332.24ms, tok/sec:157338.14
step 1307, loss: 4.051164, norm:0.3538, lr:5.9780e-04 dt: 3332.19ms, tok/sec:157340.35
step 1308, loss: 4.115682, norm:0.3947, lr:5.9779e-04 dt: 3332.13ms, tok/sec:157343.01
step 1309, loss: 4.079750, norm:0.3795, lr:5.9778e-04 dt: 3331.89ms, tok/sec:157354.61
step 1310, loss: 3.999475, norm:0.4255, lr:5.9777e-04 dt: 3332.17ms, tok/sec:157341.50
step 1311, loss: 4.022643, norm:0.5473, lr:5.9777e-04 dt: 3332.42ms, tok/sec:157329.33
step 1312, loss: 4.047938, norm:0.5033, lr:5.9776e-04 dt: 3331.97ms, tok/sec:157350.66
step 1313, loss: 4.034899, norm:0.3969, lr:5.9775e-04 dt: 3332.10ms, tok/sec:157344.73
step 1314, loss: 4.072809, norm:0.3433, lr:5.9774e-04 dt: 3332.22ms, tok/sec:157338.83
step 1315, loss: 4.047355, norm:0.3176, lr:5.9774e-04 dt: 3332.18ms, tok/sec:157340.87
step 1316, loss: 4.060220, norm:0.3314, lr:5.9773e-04 dt: 3332.31ms, tok/sec:157334.93
step 1317, loss: 4.022070, norm:0.3542, lr:5.9772e-04 dt: 3332.31ms, tok/sec:157334.91
step 1318, loss: 4.044438, norm:0.3197, lr:5.9771e-04 dt: 3332.11ms, tok/sec:157344.36
step 1319, loss: 4.035039, norm:0.3513, lr:5.9771e-04 dt: 3332.62ms, tok/sec:157320.22
step 1320, loss: 3.952663, norm:0.3945, lr:5.9770e-04 dt: 3331.78ms, tok/sec:157359.83
step 1321, loss: 3.999049, norm:0.3410, lr:5.9769e-04 dt: 3332.02ms, tok/sec:157348.21
step 1322, loss: 4.001606, norm:0.3643, lr:5.9768e-04 dt: 3332.24ms, tok/sec:157337.79
step 1323, loss: 3.967042, norm:0.3389, lr:5.9767e-04 dt: 3332.09ms, tok/sec:157345.32
step 1324, loss: 3.993031, norm:0.3518, lr:5.9767e-04 dt: 3332.19ms, tok/sec:157340.17
step 1325, loss: 3.975992, norm:0.3381, lr:5.9766e-04 dt: 3332.29ms, tok/sec:157335.71
step 1326, loss: 3.974437, norm:0.3479, lr:5.9765e-04 dt: 3332.44ms, tok/sec:157328.74
step 1327, loss: 3.981823, norm:0.3519, lr:5.9764e-04 dt: 3332.23ms, tok/sec:157338.67
step 1328, loss: 3.993864, norm:0.2998, lr:5.9764e-04 dt: 3332.13ms, tok/sec:157343.42
step 1329, loss: 3.946313, norm:0.2992, lr:5.9763e-04 dt: 3332.27ms, tok/sec:157336.64
step 1330, loss: 3.940259, norm:0.3270, lr:5.9762e-04 dt: 3331.98ms, tok/sec:157350.08
step 1331, loss: 3.974611, norm:0.3426, lr:5.9761e-04 dt: 3332.15ms, tok/sec:157342.35
step 1332, loss: 3.929585, norm:0.3558, lr:5.9761e-04 dt: 3332.39ms, tok/sec:157331.05
step 1333, loss: 3.967073, norm:0.3679, lr:5.9760e-04 dt: 3334.45ms, tok/sec:157233.90
step 1334, loss: 3.883920, norm:0.3579, lr:5.9759e-04 dt: 3332.64ms, tok/sec:157319.19
step 1335, loss: 3.938465, norm:0.4281, lr:5.9758e-04 dt: 3332.14ms, tok/sec:157342.52
step 1336, loss: 3.874417, norm:0.4542, lr:5.9757e-04 dt: 3331.95ms, tok/sec:157351.67
step 1337, loss: 3.865667, norm:0.3682, lr:5.9757e-04 dt: 3332.18ms, tok/sec:157340.96
step 1338, loss: 3.944910, norm:0.3396, lr:5.9756e-04 dt: 3331.96ms, tok/sec:157351.38
step 1339, loss: 3.891985, norm:0.3225, lr:5.9755e-04 dt: 3332.11ms, tok/sec:157344.25
step 1340, loss: 3.870427, norm:0.2872, lr:5.9754e-04 dt: 3332.12ms, tok/sec:157343.46
step 1341, loss: 3.911557, norm:0.3098, lr:5.9754e-04 dt: 3331.93ms, tok/sec:157352.56
step 1342, loss: 3.869879, norm:0.2946, lr:5.9753e-04 dt: 3331.99ms, tok/sec:157349.99
step 1343, loss: 3.834252, norm:0.3088, lr:5.9752e-04 dt: 3332.04ms, tok/sec:157347.57
step 1344, loss: 3.880762, norm:0.3496, lr:5.9751e-04 dt: 3332.22ms, tok/sec:157338.86
step 1345, loss: 4.008699, norm:0.3829, lr:5.9750e-04 dt: 3332.44ms, tok/sec:157328.35
step 1346, loss: 3.957426, norm:0.3995, lr:5.9750e-04 dt: 3332.00ms, tok/sec:157349.35
step 1347, loss: 4.090734, norm:0.3523, lr:5.9749e-04 dt: 3332.10ms, tok/sec:157344.51
step 1348, loss: 4.061855, norm:0.3749, lr:5.9748e-04 dt: 3332.03ms, tok/sec:157347.97
step 1349, loss: 4.025709, norm:0.3418, lr:5.9747e-04 dt: 3332.17ms, tok/sec:157341.51
HellaSwag accuracy:-4591745609616882615/-2=2295872804808441344.0000
rank 1 sample 0: Hello, I'm a language model, as opposed to the same, a method I could write (that is, at least not, an abstract), and a
rank 1 sample 1: Hello, I'm a language model, which I have recently learned from. This is a model used by R.M. Goto's work on the Internet
rank 1 sample 2: Hello, I'm a language model, but since we know that it's a language, it can look a lot like a lot, like an audio-sounding
rank 1 sample 3: Hello, I'm a language model, and I'm talking to another language I went to it once. However, all the differences it makes are the same.
rank 0 sample 0: Hello, I'm a language model, and I would like, that’s why I worked on language teaching to the public.”
The word
rank 0 sample 1: Hello, I'm a language model, but not more. It's not your first word "no-one", or "little-no-one", but
rank 0 sample 2: Hello, I'm a language model, but I haven't said so much to me (since I can say I'm writing an English language, but it should
rank 0 sample 3: Hello, I'm a language model, and an entire language model is the next to something. I'm writing that on its own, but now I'll give
step 1350, loss: 4.037192, norm:0.4063, lr:5.9746e-04 dt: 48520.29ms, tok/sec:10805.54
step 1351, loss: 4.017592, norm:0.4436, lr:5.9746e-04 dt: 3332.04ms, tok/sec:157347.41
step 1352, loss: 3.993977, norm:0.4099, lr:5.9745e-04 dt: 3332.01ms, tok/sec:157348.96
step 1353, loss: 4.016491, norm:0.3741, lr:5.9744e-04 dt: 3332.06ms, tok/sec:157346.46
step 1354, loss: 4.065868, norm:0.3620, lr:5.9743e-04 dt: 3332.39ms, tok/sec:157331.16
step 1355, loss: 3.983203, norm:0.3413, lr:5.9742e-04 dt: 3331.93ms, tok/sec:157352.48
step 1356, loss: 4.033360, norm:0.3339, lr:5.9742e-04 dt: 3332.28ms, tok/sec:157336.02
step 1357, loss: 3.997953, norm:0.3465, lr:5.9741e-04 dt: 3332.44ms, tok/sec:157328.38
step 1358, loss: 4.029413, norm:0.3329, lr:5.9740e-04 dt: 3332.19ms, tok/sec:157340.53
step 1359, loss: 4.063034, norm:0.3135, lr:5.9739e-04 dt: 3332.12ms, tok/sec:157343.54
step 1360, loss: 4.033741, norm:0.3589, lr:5.9738e-04 dt: 3331.89ms, tok/sec:157354.42
step 1361, loss: 4.017504, norm:0.4234, lr:5.9738e-04 dt: 3332.10ms, tok/sec:157344.60
step 1362, loss: 4.047969, norm:0.3507, lr:5.9737e-04 dt: 3332.49ms, tok/sec:157326.13
step 1363, loss: 4.052373, norm:0.3481, lr:5.9736e-04 dt: 3331.98ms, tok/sec:157350.44
step 1364, loss: 4.023922, norm:0.3352, lr:5.9735e-04 dt: 3332.09ms, tok/sec:157344.90
step 1365, loss: 4.008664, norm:0.3540, lr:5.9734e-04 dt: 3332.40ms, tok/sec:157330.41
step 1366, loss: 4.040241, norm:0.3800, lr:5.9733e-04 dt: 3333.17ms, tok/sec:157293.98
step 1367, loss: 3.974483, norm:0.3495, lr:5.9733e-04 dt: 3333.15ms, tok/sec:157294.97
step 1368, loss: 3.997977, norm:0.3216, lr:5.9732e-04 dt: 3332.20ms, tok/sec:157339.81
step 1369, loss: 3.956923, norm:0.3273, lr:5.9731e-04 dt: 3332.12ms, tok/sec:157343.56
step 1370, loss: 3.938747, norm:0.3600, lr:5.9730e-04 dt: 3332.10ms, tok/sec:157344.62
step 1371, loss: 3.990304, norm:0.3571, lr:5.9729e-04 dt: 3332.16ms, tok/sec:157341.84
step 1372, loss: 3.931083, norm:0.3508, lr:5.9729e-04 dt: 3332.09ms, tok/sec:157345.09
step 1373, loss: 3.967851, norm:0.3493, lr:5.9728e-04 dt: 3332.19ms, tok/sec:157340.24
step 1374, loss: 3.940979, norm:0.3492, lr:5.9727e-04 dt: 3332.29ms, tok/sec:157335.85
step 1375, loss: 3.970549, norm:0.3797, lr:5.9726e-04 dt: 3332.63ms, tok/sec:157319.62
step 1376, loss: 3.956823, norm:0.3793, lr:5.9725e-04 dt: 3332.07ms, tok/sec:157345.81
step 1377, loss: 3.978160, norm:0.3402, lr:5.9724e-04 dt: 3331.94ms, tok/sec:157352.04
step 1378, loss: 3.956570, norm:0.3485, lr:5.9724e-04 dt: 3332.39ms, tok/sec:157331.16
step 1379, loss: 3.951098, norm:0.3831, lr:5.9723e-04 dt: 3332.06ms, tok/sec:157346.37
step 1380, loss: 3.951637, norm:0.3136, lr:5.9722e-04 dt: 3332.03ms, tok/sec:157348.10
step 1381, loss: 3.890133, norm:0.3927, lr:5.9721e-04 dt: 3331.92ms, tok/sec:157353.27
step 1382, loss: 3.922559, norm:0.3775, lr:5.9720e-04 dt: 3332.18ms, tok/sec:157341.08
step 1383, loss: 3.880204, norm:0.3641, lr:5.9719e-04 dt: 3331.88ms, tok/sec:157354.82
step 1384, loss: 3.886535, norm:0.3571, lr:5.9719e-04 dt: 3332.43ms, tok/sec:157329.03
step 1385, loss: 3.901496, norm:0.3224, lr:5.9718e-04 dt: 3332.01ms, tok/sec:157348.88
step 1386, loss: 3.862818, norm:0.3364, lr:5.9717e-04 dt: 3331.91ms, tok/sec:157353.81
step 1387, loss: 3.855102, norm:0.3548, lr:5.9716e-04 dt: 3331.86ms, tok/sec:157356.16
step 1388, loss: 3.838109, norm:0.3385, lr:5.9715e-04 dt: 3331.89ms, tok/sec:157354.61
step 1389, loss: 3.864546, norm:0.3253, lr:5.9714e-04 dt: 3331.99ms, tok/sec:157349.65
step 1390, loss: 3.867905, norm:0.3508, lr:5.9714e-04 dt: 3332.12ms, tok/sec:157343.81
step 1391, loss: 3.871222, norm:0.3542, lr:5.9713e-04 dt: 3332.16ms, tok/sec:157341.56
step 1392, loss: 3.823558, norm:0.3662, lr:5.9712e-04 dt: 3331.90ms, tok/sec:157354.05
step 1393, loss: 4.042392, norm:0.3576, lr:5.9711e-04 dt: 3331.87ms, tok/sec:157355.60
step 1394, loss: 4.008335, norm:0.4270, lr:5.9710e-04 dt: 3332.51ms, tok/sec:157325.21
step 1395, loss: 3.980831, norm:0.3860, lr:5.9709e-04 dt: 3332.13ms, tok/sec:157343.34
step 1396, loss: 3.994182, norm:0.3674, lr:5.9708e-04 dt: 3332.14ms, tok/sec:157342.71
step 1397, loss: 4.064754, norm:0.3423, lr:5.9708e-04 dt: 3332.25ms, tok/sec:157337.59
step 1398, loss: 4.044514, norm:0.3622, lr:5.9707e-04 dt: 3332.24ms, tok/sec:157338.00
step 1399, loss: 4.076659, norm:0.3428, lr:5.9706e-04 dt: 3332.20ms, tok/sec:157339.74
validation loss: 4.0010
Model and optimizer state saved.
HellaSwag accuracy:-3438824676777573295/-2=1719412338388786688.0000
rank 1 sample 0: Hello, I'm a language model, if it is a language model, something is actually not "like" the object. In fact this is an example of
rank 1 sample 1: Hello, I'm a language model, which I have an object that's called a "The Story" but I am learning it later that it's called a
rank 1 sample 2: Hello, I'm a language model, I should not try to define it as a language model because its own.
I'm trying to think about that,
rank 1 sample 3: Hello, I'm a language model, and I'm trying to give a comment loop.
First, in that process, some types of language are still being
rank 0 sample 0: Hello, I'm a language model, and I'm a very useful guide here if you've talked to someone with you. You can download it by clicking the
rank 0 sample 1: Hello, I'm a language model, like one of the three languages I call "a". When you use this tool, I mean you would say "a
rank 0 sample 2: Hello, I'm a language model, but I like this system which requires much more resources.
What you are saying
The key to understanding the relationship of
rank 0 sample 3: Hello, I'm a language model, I didn't think it was the reason why what I wanted to understand and what some people say about people's rights;
step 1400, loss: 4.088406, norm:0.4777, lr:5.9705e-04 dt: 56184.19ms, tok/sec:9331.59
step 1401, loss: 3.974570, norm:0.4583, lr:5.9704e-04 dt: 3332.11ms, tok/sec:157344.16
step 1402, loss: 3.925687, norm:0.3766, lr:5.9703e-04 dt: 3332.23ms, tok/sec:157338.45
step 1403, loss: 3.966979, norm:0.3208, lr:5.9702e-04 dt: 3332.14ms, tok/sec:157342.90
step 1404, loss: 3.988555, norm:0.3089, lr:5.9702e-04 dt: 3332.27ms, tok/sec:157336.74
step 1405, loss: 4.015700, norm:0.3134, lr:5.9701e-04 dt: 3331.97ms, tok/sec:157350.68
step 1406, loss: 3.969809, norm:0.3069, lr:5.9700e-04 dt: 3331.99ms, tok/sec:157349.82
step 1407, loss: 4.011981, norm:0.2967, lr:5.9699e-04 dt: 3332.32ms, tok/sec:157334.42
step 1408, loss: 3.971607, norm:0.3253, lr:5.9698e-04 dt: 3332.26ms, tok/sec:157337.16
step 1409, loss: 3.981669, norm:0.3331, lr:5.9697e-04 dt: 3332.35ms, tok/sec:157332.68
step 1410, loss: 3.970956, norm:0.3716, lr:5.9696e-04 dt: 3332.01ms, tok/sec:157349.02
step 1411, loss: 3.954806, norm:0.3717, lr:5.9695e-04 dt: 3331.93ms, tok/sec:157352.41
step 1412, loss: 3.983167, norm:0.3970, lr:5.9695e-04 dt: 3332.15ms, tok/sec:157342.09
step 1413, loss: 4.003492, norm:0.3717, lr:5.9694e-04 dt: 3332.35ms, tok/sec:157332.65
step 1414, loss: 3.995797, norm:0.3588, lr:5.9693e-04 dt: 3332.13ms, tok/sec:157343.43
step 1415, loss: 3.956292, norm:0.3463, lr:5.9692e-04 dt: 3331.95ms, tok/sec:157351.83
step 1416, loss: 3.968318, norm:0.3351, lr:5.9691e-04 dt: 3332.37ms, tok/sec:157332.10
step 1417, loss: 3.936107, norm:0.3350, lr:5.9690e-04 dt: 3332.51ms, tok/sec:157325.19
step 1418, loss: 3.884738, norm:0.3278, lr:5.9689e-04 dt: 3331.95ms, tok/sec:157351.70
step 1419, loss: 3.920444, norm:0.3133, lr:5.9688e-04 dt: 3331.97ms, tok/sec:157350.61
step 1420, loss: 3.879879, norm:0.3115, lr:5.9688e-04 dt: 3331.89ms, tok/sec:157354.43
step 1421, loss: 3.880154, norm:0.3132, lr:5.9687e-04 dt: 3332.20ms, tok/sec:157339.76
step 1422, loss: 3.898814, norm:0.3187, lr:5.9686e-04 dt: 3332.12ms, tok/sec:157343.63
step 1423, loss: 3.921265, norm:0.3341, lr:5.9685e-04 dt: 3332.08ms, tok/sec:157345.79
step 1424, loss: 3.949258, norm:0.3786, lr:5.9684e-04 dt: 3332.12ms, tok/sec:157343.61
step 1425, loss: 3.886107, norm:0.3817, lr:5.9683e-04 dt: 3332.32ms, tok/sec:157334.23
step 1426, loss: 3.941994, norm:0.3458, lr:5.9682e-04 dt: 3332.27ms, tok/sec:157336.46
step 1427, loss: 3.921799, norm:0.3051, lr:5.9681e-04 dt: 3332.21ms, tok/sec:157339.26
step 1428, loss: 3.783592, norm:0.3544, lr:5.9680e-04 dt: 3331.91ms, tok/sec:157353.37
step 1429, loss: 3.964780, norm:0.3994, lr:5.9680e-04 dt: 3332.06ms, tok/sec:157346.29
step 1430, loss: 3.825833, norm:0.4223, lr:5.9679e-04 dt: 3331.92ms, tok/sec:157353.02
step 1431, loss: 3.851082, norm:0.3944, lr:5.9678e-04 dt: 3331.88ms, tok/sec:157355.05
step 1432, loss: 3.853518, norm:0.3590, lr:5.9677e-04 dt: 3332.18ms, tok/sec:157340.96
step 1433, loss: 3.817081, norm:0.3290, lr:5.9676e-04 dt: 3332.02ms, tok/sec:157348.56
step 1434, loss: 3.829462, norm:0.3420, lr:5.9675e-04 dt: 3331.94ms, tok/sec:157352.37
step 1435, loss: 3.825121, norm:0.3078, lr:5.9674e-04 dt: 3332.38ms, tok/sec:157331.23
step 1436, loss: 3.874605, norm:0.3206, lr:5.9673e-04 dt: 3332.40ms, tok/sec:157330.52
step 1437, loss: 3.838695, norm:0.3348, lr:5.9672e-04 dt: 3332.12ms, tok/sec:157343.60
step 1438, loss: 3.775082, norm:0.3337, lr:5.9671e-04 dt: 3331.84ms, tok/sec:157356.74
step 1439, loss: 3.856590, norm:0.3070, lr:5.9670e-04 dt: 3331.97ms, tok/sec:157350.86
step 1440, loss: 4.043774, norm:0.3270, lr:5.9670e-04 dt: 3332.12ms, tok/sec:157343.78
step 1441, loss: 4.016409, norm:0.3367, lr:5.9669e-04 dt: 3332.37ms, tok/sec:157331.97
step 1442, loss: 3.993680, norm:0.3920, lr:5.9668e-04 dt: 3332.20ms, tok/sec:157339.88
step 1443, loss: 3.994581, norm:0.3927, lr:5.9667e-04 dt: 3332.12ms, tok/sec:157343.60
step 1444, loss: 3.983551, norm:0.3677, lr:5.9666e-04 dt: 3332.14ms, tok/sec:157342.66
step 1445, loss: 3.971251, norm:0.3458, lr:5.9665e-04 dt: 3332.42ms, tok/sec:157329.39
step 1446, loss: 3.985657, norm:0.3994, lr:5.9664e-04 dt: 3332.04ms, tok/sec:157347.63
step 1447, loss: 3.983649, norm:0.3819, lr:5.9663e-04 dt: 3332.56ms, tok/sec:157322.74
step 1448, loss: 3.962846, norm:0.3567, lr:5.9662e-04 dt: 3332.07ms, tok/sec:157345.85
step 1449, loss: 4.023746, norm:0.3523, lr:5.9661e-04 dt: 3332.38ms, tok/sec:157331.63
HellaSwag accuracy:4632329508552410193/-2=-2316164754276205056.0000
rank 1 sample 0: Hello, I'm a language model, what is the language model?
Why are two language models implemented?
Are two model models designed for model models?
rank 1 sample 1: Hello, I'm a language model, a model model with a model model for example.
Now I will define some a better model model for example.

rank 1 sample 2: Hello, I'm a language model, I call "a language model."
I'm always interested in how I create a language model with an understanding of how
rank 1 sample 3: Hello, I'm a language model, and I'm the language model model model simulation.
Sawara has seen a rich experience for me. As a
rank 0 sample 0: Hello, I'm a language model, and I'll be a student and myself very keen to keep it together before it's a difficult and difficult.
As
rank 0 sample 1: Hello, I'm a language model, but as I've seen, we may be in case in a very poor place, which may have very different models and
rank 0 sample 2: Hello, I'm a language model, but I wanted to help a colleague from the project, and that would be a little bit easier. I'm not there
rank 0 sample 3: Hello, I'm a language model, with four clicks of the keyboard and one clicks off the keyboard.<|endoftext|>What does Alaskan mean?
In May
step 1450, loss: 3.977218, norm:0.3082, lr:5.9660e-04 dt: 48517.13ms, tok/sec:10806.25
step 1451, loss: 3.978598, norm:0.3200, lr:5.9660e-04 dt: 3332.19ms, tok/sec:157340.59
step 1452, loss: 3.961262, norm:0.3079, lr:5.9659e-04 dt: 3332.29ms, tok/sec:157335.41
step 1453, loss: 3.985069, norm:0.3128, lr:5.9658e-04 dt: 3332.18ms, tok/sec:157340.99
step 1454, loss: 3.976520, norm:0.2997, lr:5.9657e-04 dt: 3332.36ms, tok/sec:157332.45
step 1455, loss: 3.956240, norm:0.2997, lr:5.9656e-04 dt: 3332.06ms, tok/sec:157346.56
step 1456, loss: 3.983892, norm:0.2883, lr:5.9655e-04 dt: 3332.27ms, tok/sec:157336.56
step 1457, loss: 3.964497, norm:0.2803, lr:5.9654e-04 dt: 3332.33ms, tok/sec:157333.83
step 1458, loss: 3.959777, norm:0.2949, lr:5.9653e-04 dt: 3331.95ms, tok/sec:157351.67
step 1459, loss: 3.931102, norm:0.3630, lr:5.9652e-04 dt: 3332.40ms, tok/sec:157330.26
step 1460, loss: 3.938860, norm:0.3716, lr:5.9651e-04 dt: 3332.11ms, tok/sec:157344.08
step 1461, loss: 4.004867, norm:0.3269, lr:5.9650e-04 dt: 3332.08ms, tok/sec:157345.52
step 1462, loss: 4.038128, norm:0.3916, lr:5.9649e-04 dt: 3331.87ms, tok/sec:157355.62
step 1463, loss: 3.915080, norm:0.3736, lr:5.9648e-04 dt: 3332.13ms, tok/sec:157343.41
step 1464, loss: 3.911953, norm:0.3780, lr:5.9647e-04 dt: 3332.13ms, tok/sec:157343.42
step 1465, loss: 3.919717, norm:0.3807, lr:5.9646e-04 dt: 3332.04ms, tok/sec:157347.54
step 1466, loss: 3.889482, norm:0.3594, lr:5.9646e-04 dt: 3332.24ms, tok/sec:157338.07
step 1467, loss: 3.921309, norm:0.3280, lr:5.9645e-04 dt: 3332.12ms, tok/sec:157343.55
step 1468, loss: 3.924553, norm:0.3339, lr:5.9644e-04 dt: 3332.55ms, tok/sec:157323.31
step 1469, loss: 3.931336, norm:0.3278, lr:5.9643e-04 dt: 3332.18ms, tok/sec:157341.08
step 1470, loss: 3.949280, norm:0.3319, lr:5.9642e-04 dt: 3332.08ms, tok/sec:157345.68
step 1471, loss: 3.929777, norm:0.3210, lr:5.9641e-04 dt: 3332.12ms, tok/sec:157343.61
step 1472, loss: 3.896336, norm:0.3089, lr:5.9640e-04 dt: 3332.11ms, tok/sec:157344.28
step 1473, loss: 3.870975, norm:0.3026, lr:5.9639e-04 dt: 3331.87ms, tok/sec:157355.27
step 1474, loss: 3.846539, norm:0.2833, lr:5.9638e-04 dt: 3332.10ms, tok/sec:157344.41
step 1475, loss: 3.886864, norm:0.2982, lr:5.9637e-04 dt: 3332.21ms, tok/sec:157339.25
step 1476, loss: 3.862766, norm:0.3390, lr:5.9636e-04 dt: 3332.12ms, tok/sec:157343.74
step 1477, loss: 3.789751, norm:0.3510, lr:5.9635e-04 dt: 3332.52ms, tok/sec:157324.76
step 1478, loss: 3.846860, norm:0.3272, lr:5.9634e-04 dt: 3332.30ms, tok/sec:157334.99
step 1479, loss: 3.882190, norm:0.3282, lr:5.9633e-04 dt: 3331.83ms, tok/sec:157357.32
step 1480, loss: 3.822986, norm:0.3348, lr:5.9632e-04 dt: 3332.15ms, tok/sec:157342.30
step 1481, loss: 3.822927, norm:0.3199, lr:5.9631e-04 dt: 3331.99ms, tok/sec:157349.86
step 1482, loss: 3.804115, norm:0.3164, lr:5.9630e-04 dt: 3332.12ms, tok/sec:157343.90
step 1483, loss: 3.757069, norm:0.3582, lr:5.9629e-04 dt: 3332.09ms, tok/sec:157345.20
step 1484, loss: 3.813035, norm:0.3506, lr:5.9628e-04 dt: 3332.30ms, tok/sec:157335.36
step 1485, loss: 3.770342, norm:0.2854, lr:5.9627e-04 dt: 3332.35ms, tok/sec:157332.90
step 1486, loss: 3.963061, norm:0.3249, lr:5.9626e-04 dt: 3332.26ms, tok/sec:157336.84
step 1487, loss: 3.988794, norm:0.4005, lr:5.9625e-04 dt: 3332.08ms, tok/sec:157345.75
step 1488, loss: 3.968263, norm:0.3678, lr:5.9624e-04 dt: 3332.17ms, tok/sec:157341.22
step 1489, loss: 4.026869, norm:0.3881, lr:5.9624e-04 dt: 3823.53ms, tok/sec:137121.52
step 1490, loss: 3.930546, norm:0.3834, lr:5.9623e-04 dt: 3332.04ms, tok/sec:157347.58
step 1491, loss: 3.924472, norm:0.3366, lr:5.9622e-04 dt: 3332.49ms, tok/sec:157326.15
step 1492, loss: 4.042408, norm:0.3330, lr:5.9621e-04 dt: 3332.29ms, tok/sec:157335.76
step 1493, loss: 3.982031, norm:0.3576, lr:5.9620e-04 dt: 3332.44ms, tok/sec:157328.40
step 1494, loss: 3.946688, norm:0.3475, lr:5.9619e-04 dt: 3332.13ms, tok/sec:157343.00
step 1495, loss: 3.955467, norm:0.3239, lr:5.9618e-04 dt: 3331.97ms, tok/sec:157350.97
step 1496, loss: 3.938069, norm:0.3051, lr:5.9617e-04 dt: 3332.42ms, tok/sec:157329.65
step 1497, loss: 3.986272, norm:0.2948, lr:5.9616e-04 dt: 3332.07ms, tok/sec:157346.01
step 1498, loss: 4.025547, norm:0.3144, lr:5.9615e-04 dt: 3332.40ms, tok/sec:157330.65
step 1499, loss: 3.955302, norm:0.2989, lr:5.9614e-04 dt: 3332.28ms, tok/sec:157336.20
validation loss: 3.9543
Model and optimizer state saved.
HellaSwag accuracy:4631625855470339153/-2=-2315812927735169536.0000
rank 1 sample 0: Hello, I'm a language model, where the children are speaking and I remember the things we all knew.
And I'll give up the following three things
rank 1 sample 1: Hello, I'm a language model, so it is my job to write and have a very short answer.
So did your children choose to write their own
rank 1 sample 2: Hello, I'm a language model, that enables the use of the word 's' in English so the 's' is a function which can use some
rank 1 sample 3: Hello, I'm a language model, and I'm writing it!
We must be familiar with how many children they are developing across the world. This is
rank 0 sample 0: Hello, I'm a language model, but I know it's actually not, too.
Hindson asked a question. "It was like a computer
rank 0 sample 1: Hello, I'm a language model, I'm a good way to get back to it...the site that connects us with our understanding of social, cultural and
rank 0 sample 2: Hello, I'm a language model, I'm so fond of this situation that I feel like I think the same way we do this is.
So while
rank 0 sample 3: Hello, I'm a language model, a self-explanatory language. It focuses on the different stages of the teaching process of language through the analysis,
step 1500, loss: 3.983182, norm:0.2813, lr:5.9613e-04 dt: 56164.74ms, tok/sec:9334.82
step 1501, loss: 3.968302, norm:0.3299, lr:5.9612e-04 dt: 3332.04ms, tok/sec:157347.36
step 1502, loss: 3.947834, norm:0.3694, lr:5.9611e-04 dt: 3332.19ms, tok/sec:157340.38
step 1503, loss: 3.908997, norm:0.3404, lr:5.9610e-04 dt: 3332.20ms, tok/sec:157339.85
step 1504, loss: 3.923602, norm:0.2824, lr:5.9609e-04 dt: 3332.07ms, tok/sec:157346.10
step 1505, loss: 3.926708, norm:0.3375, lr:5.9608e-04 dt: 3332.01ms, tok/sec:157348.97
step 1506, loss: 3.987535, norm:0.3152, lr:5.9607e-04 dt: 3332.07ms, tok/sec:157345.96
step 1507, loss: 3.929198, norm:0.3213, lr:5.9606e-04 dt: 3332.56ms, tok/sec:157322.93
step 1508, loss: 3.938803, norm:0.3201, lr:5.9605e-04 dt: 3332.37ms, tok/sec:157331.64
step 1509, loss: 3.892380, norm:0.3276, lr:5.9604e-04 dt: 3332.11ms, tok/sec:157344.17
step 1510, loss: 3.938801, norm:0.3564, lr:5.9603e-04 dt: 3332.20ms, tok/sec:157340.01
step 1511, loss: 3.867451, norm:0.3670, lr:5.9602e-04 dt: 3332.10ms, tok/sec:157344.48
step 1512, loss: 3.944217, norm:0.3234, lr:5.9601e-04 dt: 3332.14ms, tok/sec:157342.88
step 1513, loss: 3.873807, norm:0.3076, lr:5.9600e-04 dt: 3332.28ms, tok/sec:157336.29
step 1514, loss: 3.900926, norm:0.3350, lr:5.9599e-04 dt: 3332.13ms, tok/sec:157343.30
step 1515, loss: 3.878845, norm:0.3267, lr:5.9598e-04 dt: 3332.10ms, tok/sec:157344.80
step 1516, loss: 3.875020, norm:0.3256, lr:5.9597e-04 dt: 3332.28ms, tok/sec:157336.29
step 1517, loss: 3.879657, norm:0.3132, lr:5.9596e-04 dt: 3332.72ms, tok/sec:157315.38
step 1518, loss: 3.860594, norm:0.2928, lr:5.9595e-04 dt: 3332.29ms, tok/sec:157335.62
step 1519, loss: 3.860928, norm:0.3068, lr:5.9594e-04 dt: 3332.08ms, tok/sec:157345.44
step 1520, loss: 3.883972, norm:0.2646, lr:5.9593e-04 dt: 3332.07ms, tok/sec:157346.27
step 1521, loss: 3.882970, norm:0.3069, lr:5.9592e-04 dt: 3332.13ms, tok/sec:157343.37
step 1522, loss: 3.788205, norm:0.3045, lr:5.9591e-04 dt: 3332.00ms, tok/sec:157349.54
step 1523, loss: 3.837075, norm:0.3378, lr:5.9590e-04 dt: 3334.51ms, tok/sec:157230.75
step 1524, loss: 3.784904, norm:0.3255, lr:5.9589e-04 dt: 3332.08ms, tok/sec:157345.72
step 1525, loss: 3.816783, norm:0.2977, lr:5.9588e-04 dt: 3332.37ms, tok/sec:157331.73
step 1526, loss: 3.792075, norm:0.3281, lr:5.9587e-04 dt: 3332.16ms, tok/sec:157341.77
step 1527, loss: 3.789414, norm:0.3333, lr:5.9586e-04 dt: 3332.09ms, tok/sec:157344.94
step 1528, loss: 3.762953, norm:0.3014, lr:5.9585e-04 dt: 3331.93ms, tok/sec:157352.68
step 1529, loss: 3.814744, norm:0.3106, lr:5.9584e-04 dt: 3331.98ms, tok/sec:157350.34
step 1530, loss: 3.785184, norm:0.3380, lr:5.9583e-04 dt: 3332.00ms, tok/sec:157349.53
step 1531, loss: 3.757859, norm:0.3513, lr:5.9582e-04 dt: 3332.09ms, tok/sec:157345.12
step 1532, loss: 3.857512, norm:0.3806, lr:5.9581e-04 dt: 3332.29ms, tok/sec:157335.66
step 1533, loss: 4.001086, norm:0.4301, lr:5.9580e-04 dt: 3332.10ms, tok/sec:157344.57
step 1534, loss: 3.957294, norm:0.3826, lr:5.9579e-04 dt: 3332.32ms, tok/sec:157334.26
step 1535, loss: 3.971765, norm:0.3501, lr:5.9578e-04 dt: 3332.65ms, tok/sec:157318.80
step 1536, loss: 3.971723, norm:0.3765, lr:5.9577e-04 dt: 3332.36ms, tok/sec:157332.55
step 1537, loss: 3.917875, norm:0.3564, lr:5.9576e-04 dt: 3332.29ms, tok/sec:157335.42
step 1538, loss: 3.884602, norm:0.3245, lr:5.9574e-04 dt: 3332.15ms, tok/sec:157342.26
step 1539, loss: 4.007883, norm:0.3353, lr:5.9573e-04 dt: 3332.31ms, tok/sec:157334.55
step 1540, loss: 3.930989, norm:0.3908, lr:5.9572e-04 dt: 3332.12ms, tok/sec:157343.62
step 1541, loss: 3.929709, norm:0.3996, lr:5.9571e-04 dt: 3332.18ms, tok/sec:157340.99
step 1542, loss: 3.908343, norm:0.3692, lr:5.9570e-04 dt: 3332.31ms, tok/sec:157334.52
step 1543, loss: 3.996290, norm:0.3088, lr:5.9569e-04 dt: 3332.08ms, tok/sec:157345.77
step 1544, loss: 3.990250, norm:0.3109, lr:5.9568e-04 dt: 3332.52ms, tok/sec:157324.57
step 1545, loss: 3.975606, norm:0.3982, lr:5.9567e-04 dt: 3331.99ms, tok/sec:157349.80
step 1546, loss: 3.989867, norm:0.3850, lr:5.9566e-04 dt: 3332.11ms, tok/sec:157343.91
step 1547, loss: 3.923565, norm:0.3591, lr:5.9565e-04 dt: 3332.22ms, tok/sec:157338.80
step 1548, loss: 3.925270, norm:0.3206, lr:5.9564e-04 dt: 3332.21ms, tok/sec:157339.65
step 1549, loss: 3.937119, norm:0.3186, lr:5.9563e-04 dt: 3332.19ms, tok/sec:157340.55
HellaSwag accuracy:-4591886918335871919/-2=2295943459167936000.0000
rank 1 sample 0: Hello, I'm a language model, just as I'm an educator. By now I know as long as I'll be learning, so I've been learning
rank 1 sample 1: Hello, I'm a language model, which is very very popular. If you just want to say a class, we can give you an idea of how to
rank 1 sample 2: Hello, I'm a language model, that runs the language of the language, and it's an optional, free language model. I don't have an error
rank 1 sample 3: Hello, I'm a language model, and I'm talking to us about some "good"
English version of American English Dictionary does not have any other definition
rank 0 sample 0: Hello, I'm a language model, I'm just a teacher. The lesson started after the lesson. So to be able to put in your learning, your
rank 0 sample 1: Hello, I'm a language model, and are free to write. I haven't had me at the top of that. It's not going to be that
rank 0 sample 2: Hello, I'm a language model, and I get my work writing professionally for the future.
What's the difference between a language model and a computer?
rank 0 sample 3: Hello, I'm a language model, and can be used to create and interact in conversations.
An example of an old French-language course is a lesson
step 1550, loss: 3.921494, norm:0.3231, lr:5.9562e-04 dt: 48522.62ms, tok/sec:10805.02
step 1551, loss: 3.917803, norm:0.3140, lr:5.9561e-04 dt: 3332.27ms, tok/sec:157336.67
step 1552, loss: 3.881978, norm:0.2899, lr:5.9560e-04 dt: 3331.97ms, tok/sec:157350.84
step 1553, loss: 3.968234, norm:0.3025, lr:5.9559e-04 dt: 3332.44ms, tok/sec:157328.35
step 1554, loss: 3.918340, norm:0.3113, lr:5.9558e-04 dt: 3332.10ms, tok/sec:157344.41
step 1555, loss: 3.877492, norm:0.3166, lr:5.9557e-04 dt: 3332.04ms, tok/sec:157347.53
step 1556, loss: 3.888099, norm:0.2930, lr:5.9556e-04 dt: 3332.06ms, tok/sec:157346.49
step 1557, loss: 3.902526, norm:0.2656, lr:5.9555e-04 dt: 3332.18ms, tok/sec:157341.03
step 1558, loss: 3.894153, norm:0.2895, lr:5.9554e-04 dt: 3332.26ms, tok/sec:157336.94
step 1559, loss: 3.887551, norm:0.2892, lr:5.9553e-04 dt: 3331.95ms, tok/sec:157351.52
step 1560, loss: 3.848710, norm:0.3124, lr:5.9551e-04 dt: 3332.27ms, tok/sec:157336.52
step 1561, loss: 3.916359, norm:0.3240, lr:5.9550e-04 dt: 3332.21ms, tok/sec:157339.21
step 1562, loss: 3.885772, norm:0.3469, lr:5.9549e-04 dt: 3332.52ms, tok/sec:157324.56
step 1563, loss: 3.887979, norm:0.3355, lr:5.9548e-04 dt: 3332.09ms, tok/sec:157344.99
step 1564, loss: 3.802795, norm:0.3175, lr:5.9547e-04 dt: 3332.06ms, tok/sec:157346.48
step 1565, loss: 3.827708, norm:0.3121, lr:5.9546e-04 dt: 3332.26ms, tok/sec:157337.16
step 1566, loss: 3.844639, norm:0.2978, lr:5.9545e-04 dt: 3332.11ms, tok/sec:157343.91
step 1567, loss: 3.810914, norm:0.3357, lr:5.9544e-04 dt: 3332.00ms, tok/sec:157349.43
step 1568, loss: 3.733009, norm:0.3686, lr:5.9543e-04 dt: 3332.27ms, tok/sec:157336.66
step 1569, loss: 3.755148, norm:0.3352, lr:5.9542e-04 dt: 3332.48ms, tok/sec:157326.86
step 1570, loss: 3.804745, norm:0.2913, lr:5.9541e-04 dt: 3332.07ms, tok/sec:157345.86
step 1571, loss: 3.802780, norm:0.3322, lr:5.9540e-04 dt: 3332.01ms, tok/sec:157348.81
step 1572, loss: 3.775024, norm:0.3419, lr:5.9539e-04 dt: 3332.07ms, tok/sec:157345.89
step 1573, loss: 3.747394, norm:0.3478, lr:5.9538e-04 dt: 3332.01ms, tok/sec:157348.85
step 1574, loss: 3.780295, norm:0.3418, lr:5.9537e-04 dt: 3332.09ms, tok/sec:157344.93
step 1575, loss: 3.811321, norm:0.3139, lr:5.9535e-04 dt: 3331.97ms, tok/sec:157350.95
step 1576, loss: 3.774398, norm:0.2992, lr:5.9534e-04 dt: 3332.15ms, tok/sec:157342.46
step 1577, loss: 3.765123, norm:0.3182, lr:5.9533e-04 dt: 3332.02ms, tok/sec:157348.36
step 1578, loss: 3.750291, norm:0.3384, lr:5.9532e-04 dt: 3332.42ms, tok/sec:157329.74
step 1579, loss: 3.809263, norm:0.3260, lr:5.9531e-04 dt: 3332.85ms, tok/sec:157309.03
step 1580, loss: 3.949122, norm:0.3695, lr:5.9530e-04 dt: 3332.95ms, tok/sec:157304.42
step 1581, loss: 3.939074, norm:0.3510, lr:5.9529e-04 dt: 3332.31ms, tok/sec:157334.79
step 1582, loss: 3.881364, norm:0.3396, lr:5.9528e-04 dt: 3332.29ms, tok/sec:157335.50
step 1583, loss: 3.993375, norm:0.3351, lr:5.9527e-04 dt: 3332.07ms, tok/sec:157346.06
step 1584, loss: 3.957802, norm:0.3341, lr:5.9526e-04 dt: 3332.10ms, tok/sec:157344.76
step 1585, loss: 3.963589, norm:0.3403, lr:5.9525e-04 dt: 3332.50ms, tok/sec:157325.54
step 1586, loss: 3.922340, norm:0.3740, lr:5.9524e-04 dt: 3332.18ms, tok/sec:157340.82
step 1587, loss: 3.931511, norm:0.3534, lr:5.9522e-04 dt: 3332.09ms, tok/sec:157344.91
step 1588, loss: 3.916649, norm:0.3414, lr:5.9521e-04 dt: 3331.96ms, tok/sec:157351.11
step 1589, loss: 3.942138, norm:0.3306, lr:5.9520e-04 dt: 3332.27ms, tok/sec:157336.77
step 1590, loss: 3.863049, norm:0.3754, lr:5.9519e-04 dt: 3332.21ms, tok/sec:157339.61
step 1591, loss: 3.975055, norm:0.3723, lr:5.9518e-04 dt: 3331.98ms, tok/sec:157350.36
step 1592, loss: 3.898762, norm:0.3364, lr:5.9517e-04 dt: 3332.42ms, tok/sec:157329.38
step 1593, loss: 3.910775, norm:0.3356, lr:5.9516e-04 dt: 3332.20ms, tok/sec:157340.02
step 1594, loss: 3.928125, norm:0.3231, lr:5.9515e-04 dt: 3332.77ms, tok/sec:157312.87
step 1595, loss: 3.914469, norm:0.3369, lr:5.9514e-04 dt: 3332.43ms, tok/sec:157329.15
step 1596, loss: 3.905083, norm:0.3394, lr:5.9513e-04 dt: 3331.99ms, tok/sec:157349.79
step 1597, loss: 3.896713, norm:0.3059, lr:5.9511e-04 dt: 3332.05ms, tok/sec:157346.90
step 1598, loss: 3.985729, norm:0.3005, lr:5.9510e-04 dt: 3332.27ms, tok/sec:157336.76
step 1599, loss: 3.969309, norm:0.3233, lr:5.9509e-04 dt: 3332.11ms, tok/sec:157344.22
validation loss: 3.9288
Model and optimizer state saved.
HellaSwag accuracy:4622618619706295377/-2=-2311309309853147648.0000
rank 1 sample 0: Hello, I'm a language model, my child's teacher, and I often see me talk about things I could tell. And I've been working on a
rank 1 sample 1: Hello, I'm a language model, so that it really is a tool for developing a language at the earliest age. I could come back with a few examples
rank 1 sample 2: Hello, I'm a language model, I made my model for the first time.
For a good example I wanted to use this approach: I could design
rank 1 sample 3: Hello, I'm a language model, and I'm writing this and I'm planning to understand in my native language or language culture. The language model is a
rank 0 sample 0: Hello, I'm a language model, and I’ve worked hard not a lot more than you like (it’s an easy thing to get
rank 0 sample 1: Hello, I'm a language model, I love it. It's like any other programming language with the help of an English speaker.
When I've started
rank 0 sample 2: Hello, I'm a language model, but I still want to add or post a definition in my article that I am about to write.
I'm still
rank 0 sample 3: Hello, I'm a language model, which helps me to understand the meaning of a person. I always get in a moment where everything is being spoken and shared
step 1600, loss: 3.909445, norm:0.3663, lr:5.9508e-04 dt: 56183.22ms, tok/sec:9331.75
step 1601, loss: 3.911061, norm:0.3321, lr:5.9507e-04 dt: 3332.43ms, tok/sec:157329.20
step 1602, loss: 3.884854, norm:0.3151, lr:5.9506e-04 dt: 3332.12ms, tok/sec:157343.88
step 1603, loss: 3.892380, norm:0.3324, lr:5.9505e-04 dt: 3332.07ms, tok/sec:157345.81
step 1604, loss: 3.900138, norm:0.2751, lr:5.9504e-04 dt: 3332.51ms, tok/sec:157325.27
step 1605, loss: 3.814345, norm:0.2747, lr:5.9503e-04 dt: 3332.45ms, tok/sec:157328.17
step 1606, loss: 3.888672, norm:0.2672, lr:5.9501e-04 dt: 3332.02ms, tok/sec:157348.21
step 1607, loss: 3.834513, norm:0.2626, lr:5.9500e-04 dt: 3331.94ms, tok/sec:157352.22
step 1608, loss: 3.863728, norm:0.2909, lr:5.9499e-04 dt: 3332.13ms, tok/sec:157343.26
step 1609, loss: 3.936185, norm:0.3122, lr:5.9498e-04 dt: 3332.39ms, tok/sec:157330.82
step 1610, loss: 3.846185, norm:0.3165, lr:5.9497e-04 dt: 3332.12ms, tok/sec:157343.89
step 1611, loss: 3.857538, norm:0.3108, lr:5.9496e-04 dt: 3332.22ms, tok/sec:157338.97
step 1612, loss: 3.811242, norm:0.2970, lr:5.9495e-04 dt: 3332.01ms, tok/sec:157348.72
step 1613, loss: 3.870070, norm:0.3152, lr:5.9494e-04 dt: 3332.34ms, tok/sec:157333.18
step 1614, loss: 3.747251, norm:0.2974, lr:5.9493e-04 dt: 3332.08ms, tok/sec:157345.70
step 1615, loss: 3.770696, norm:0.2919, lr:5.9491e-04 dt: 3332.00ms, tok/sec:157349.52
step 1616, loss: 3.752946, norm:0.2824, lr:5.9490e-04 dt: 3332.08ms, tok/sec:157345.76
step 1617, loss: 3.754047, norm:0.3156, lr:5.9489e-04 dt: 3332.46ms, tok/sec:157327.66
step 1618, loss: 3.790957, norm:0.3419, lr:5.9488e-04 dt: 3332.02ms, tok/sec:157348.19
step 1619, loss: 3.760608, norm:0.4114, lr:5.9487e-04 dt: 3331.98ms, tok/sec:157350.50
step 1620, loss: 3.741670, norm:0.3955, lr:5.9486e-04 dt: 3332.28ms, tok/sec:157336.16
step 1621, loss: 3.750537, norm:0.3519, lr:5.9485e-04 dt: 3332.56ms, tok/sec:157322.81
step 1622, loss: 3.742820, norm:0.3237, lr:5.9483e-04 dt: 3332.02ms, tok/sec:157348.31
step 1623, loss: 3.796103, norm:0.3306, lr:5.9482e-04 dt: 3331.98ms, tok/sec:157350.25
step 1624, loss: 3.686584, norm:0.3279, lr:5.9481e-04 dt: 3332.01ms, tok/sec:157348.84
step 1625, loss: 3.828126, norm:0.2929, lr:5.9480e-04 dt: 3332.14ms, tok/sec:157342.67
step 1626, loss: 3.989289, norm:0.3530, lr:5.9479e-04 dt: 3331.92ms, tok/sec:157353.29
step 1627, loss: 3.914805, norm:0.3567, lr:5.9478e-04 dt: 3332.04ms, tok/sec:157347.64
step 1628, loss: 3.939539, norm:0.3173, lr:5.9477e-04 dt: 3332.36ms, tok/sec:157332.26
step 1629, loss: 4.029825, norm:0.3411, lr:5.9475e-04 dt: 3332.38ms, tok/sec:157331.21
step 1630, loss: 3.923444, norm:0.3611, lr:5.9474e-04 dt: 3332.47ms, tok/sec:157327.20
step 1631, loss: 3.963592, norm:0.3371, lr:5.9473e-04 dt: 3332.17ms, tok/sec:157341.14
step 1632, loss: 3.925597, norm:0.3858, lr:5.9472e-04 dt: 3331.95ms, tok/sec:157351.58
step 1633, loss: 3.954503, norm:0.3753, lr:5.9471e-04 dt: 3332.36ms, tok/sec:157332.15
step 1634, loss: 3.900527, norm:0.3219, lr:5.9470e-04 dt: 3332.11ms, tok/sec:157344.25
step 1635, loss: 3.912885, norm:0.3183, lr:5.9469e-04 dt: 3332.34ms, tok/sec:157333.51
step 1636, loss: 4.000180, norm:0.3003, lr:5.9467e-04 dt: 3332.11ms, tok/sec:157343.95
step 1637, loss: 3.886284, norm:0.3329, lr:5.9466e-04 dt: 3332.06ms, tok/sec:157346.33
step 1638, loss: 3.853150, norm:0.2864, lr:5.9465e-04 dt: 3332.14ms, tok/sec:157342.66
step 1639, loss: 3.857024, norm:0.2911, lr:5.9464e-04 dt: 3332.43ms, tok/sec:157329.11
step 1640, loss: 3.881976, norm:0.3054, lr:5.9463e-04 dt: 3332.11ms, tok/sec:157344.22
step 1641, loss: 3.883220, norm:0.3302, lr:5.9462e-04 dt: 3332.30ms, tok/sec:157335.12
step 1642, loss: 3.902943, norm:0.2959, lr:5.9461e-04 dt: 3332.37ms, tok/sec:157331.91
step 1643, loss: 3.891144, norm:0.2957, lr:5.9459e-04 dt: 3331.93ms, tok/sec:157352.56
step 1644, loss: 3.856025, norm:0.2985, lr:5.9458e-04 dt: 3332.22ms, tok/sec:157338.90
step 1645, loss: 3.882135, norm:0.2877, lr:5.9457e-04 dt: 3332.32ms, tok/sec:157334.19
step 1646, loss: 3.830822, norm:0.2819, lr:5.9456e-04 dt: 3332.26ms, tok/sec:157337.29
step 1647, loss: 3.890656, norm:0.2821, lr:5.9455e-04 dt: 3332.24ms, tok/sec:157337.86
step 1648, loss: 3.883001, norm:0.3013, lr:5.9454e-04 dt: 3332.19ms, tok/sec:157340.23
step 1649, loss: 3.874135, norm:0.3224, lr:5.9452e-04 dt: 3332.10ms, tok/sec:157344.58
HellaSwag accuracy:4631485135698756689/-2=-2315742567849378304.0000
rank 1 sample 0: Hello, I'm a language model, since there is a single word as compared in any language model so I don't know. So we have to keep the
rank 1 sample 1: Hello, I'm a language model, I think I wanted to use that vocabulary by the beginning words. For example, if your learners weren’t able
rank 1 sample 2: Hello, I'm a language model, which also supports that.
I'm a language model for building language models which are based on real life experiences (it
rank 1 sample 3: Hello, I'm a language model, and I'm the only source of information within the dictionary group and to some users. Let's go back to the bottom
rank 0 sample 0: Hello, I'm a language model, and I can't be reached for sure it works.
The new structure for the "voice", and I'm looking
rank 0 sample 1: Hello, I'm a language model, but they're not really designed for one language model from now, as to a single set of words as they are used
rank 0 sample 2: Hello, I'm a language model, but I like that:<|endoftext|>Horse, or just a very long and beautiful place in the world, is the cradle
rank 0 sample 3: Hello, I'm a language model, but with a more complex understanding, to my side, I'm learning something that says a particular "h" or any
step 1650, loss: 3.808119, norm:0.3368, lr:5.9451e-04 dt: 48516.08ms, tok/sec:10806.48
step 1651, loss: 3.870987, norm:0.3509, lr:5.9450e-04 dt: 3331.97ms, tok/sec:157350.84
step 1652, loss: 3.838008, norm:0.3724, lr:5.9449e-04 dt: 3332.02ms, tok/sec:157348.35
step 1653, loss: 3.839159, norm:0.3802, lr:5.9448e-04 dt: 3332.00ms, tok/sec:157349.20
step 1654, loss: 3.854966, norm:0.3114, lr:5.9447e-04 dt: 3332.32ms, tok/sec:157334.25
step 1655, loss: 3.798772, norm:0.2900, lr:5.9445e-04 dt: 3332.15ms, tok/sec:157342.30
step 1656, loss: 3.815681, norm:0.2850, lr:5.9444e-04 dt: 3332.60ms, tok/sec:157321.23
step 1657, loss: 3.812092, norm:0.2965, lr:5.9443e-04 dt: 3332.19ms, tok/sec:157340.48
step 1658, loss: 3.869327, norm:0.3339, lr:5.9442e-04 dt: 3332.15ms, tok/sec:157342.26
step 1659, loss: 3.800680, norm:0.3449, lr:5.9441e-04 dt: 3331.97ms, tok/sec:157350.61
step 1660, loss: 3.723590, norm:0.4520, lr:5.9439e-04 dt: 3332.27ms, tok/sec:157336.50
step 1661, loss: 3.812303, norm:0.3840, lr:5.9438e-04 dt: 3331.99ms, tok/sec:157349.87
step 1662, loss: 3.812674, norm:0.3549, lr:5.9437e-04 dt: 3331.92ms, tok/sec:157353.29
step 1663, loss: 3.751182, norm:0.3815, lr:5.9436e-04 dt: 3332.27ms, tok/sec:157336.62
step 1664, loss: 3.745080, norm:0.3292, lr:5.9435e-04 dt: 3331.95ms, tok/sec:157351.51
step 1665, loss: 3.799600, norm:0.4062, lr:5.9434e-04 dt: 3332.31ms, tok/sec:157334.87
step 1666, loss: 3.769290, norm:0.3560, lr:5.9432e-04 dt: 3331.83ms, tok/sec:157357.16
step 1667, loss: 3.843237, norm:0.4152, lr:5.9431e-04 dt: 3332.29ms, tok/sec:157335.63
step 1668, loss: 3.776431, norm:0.3680, lr:5.9430e-04 dt: 3332.44ms, tok/sec:157328.52
step 1669, loss: 3.741193, norm:0.3429, lr:5.9429e-04 dt: 3332.06ms, tok/sec:157346.45
step 1670, loss: 3.799904, norm:0.3241, lr:5.9428e-04 dt: 3332.42ms, tok/sec:157329.56
step 1671, loss: 3.742171, norm:0.3203, lr:5.9426e-04 dt: 3332.08ms, tok/sec:157345.68
step 1672, loss: 3.711434, norm:0.2903, lr:5.9425e-04 dt: 3332.54ms, tok/sec:157323.77
step 1673, loss: 3.870070, norm:0.3380, lr:5.9424e-04 dt: 3332.27ms, tok/sec:157336.82
step 1674, loss: 3.939735, norm:0.3622, lr:5.9423e-04 dt: 3332.24ms, tok/sec:157337.98
step 1675, loss: 3.903927, norm:0.3007, lr:5.9422e-04 dt: 3332.19ms, tok/sec:157340.14
step 1676, loss: 3.919951, norm:0.3085, lr:5.9420e-04 dt: 3332.33ms, tok/sec:157333.68
step 1677, loss: 3.925678, norm:0.2951, lr:5.9419e-04 dt: 3332.09ms, tok/sec:157345.05
step 1678, loss: 3.923874, norm:0.3144, lr:5.9418e-04 dt: 3332.38ms, tok/sec:157331.61
step 1679, loss: 3.926097, norm:0.2913, lr:5.9417e-04 dt: 3332.56ms, tok/sec:157322.74
step 1680, loss: 3.922476, norm:0.3143, lr:5.9416e-04 dt: 3332.15ms, tok/sec:157342.39
step 1681, loss: 3.906306, norm:0.2813, lr:5.9414e-04 dt: 3332.13ms, tok/sec:157342.99
step 1682, loss: 3.873195, norm:0.3196, lr:5.9413e-04 dt: 3332.11ms, tok/sec:157344.30
step 1683, loss: 3.857756, norm:0.3013, lr:5.9412e-04 dt: 3332.07ms, tok/sec:157346.11
step 1684, loss: 3.883504, norm:0.3550, lr:5.9411e-04 dt: 3332.35ms, tok/sec:157332.76
step 1685, loss: 3.909166, norm:0.2986, lr:5.9410e-04 dt: 3332.04ms, tok/sec:157347.57
step 1686, loss: 3.929831, norm:0.3872, lr:5.9408e-04 dt: 3332.03ms, tok/sec:157347.80
step 1687, loss: 3.882160, norm:0.4038, lr:5.9407e-04 dt: 3332.29ms, tok/sec:157335.48
step 1688, loss: 3.892837, norm:0.3844, lr:5.9406e-04 dt: 3332.34ms, tok/sec:157333.06
step 1689, loss: 3.930224, norm:0.3543, lr:5.9405e-04 dt: 3331.83ms, tok/sec:157357.42
step 1690, loss: 3.905757, norm:0.3853, lr:5.9403e-04 dt: 3332.22ms, tok/sec:157338.82
step 1691, loss: 3.925535, norm:0.4067, lr:5.9402e-04 dt: 3332.28ms, tok/sec:157336.29
step 1692, loss: 3.904676, norm:0.3335, lr:5.9401e-04 dt: 3332.05ms, tok/sec:157347.01
step 1693, loss: 3.880165, norm:0.3175, lr:5.9400e-04 dt: 3332.04ms, tok/sec:157347.53
step 1694, loss: 3.869531, norm:0.3161, lr:5.9399e-04 dt: 3332.25ms, tok/sec:157337.66
step 1695, loss: 3.866305, norm:0.3016, lr:5.9397e-04 dt: 3332.53ms, tok/sec:157324.27
step 1696, loss: 3.865683, norm:0.3148, lr:5.9396e-04 dt: 3332.00ms, tok/sec:157349.52
step 1697, loss: 3.853509, norm:0.3194, lr:5.9395e-04 dt: 3332.28ms, tok/sec:157336.19
step 1698, loss: 3.782845, norm:0.3113, lr:5.9394e-04 dt: 3332.32ms, tok/sec:157334.02
step 1699, loss: 3.815706, norm:0.2919, lr:5.9392e-04 dt: 3332.04ms, tok/sec:157347.56
validation loss: 3.8830
Model and optimizer state saved.
HellaSwag accuracy:-6897729913053837751/-2=3448864956526918656.0000
rank 1 sample 0: Hello, I'm a language model, we are part of our series, i have some examples which one is used by the listener and has a language model.
rank 1 sample 1: Hello, I'm a language model, which I used to make my model from work.
Now I don't see any differences, although it's a good
rank 1 sample 2: Hello, I'm a language model, I created it and I developed it to help you answer any given questions about the language. If there were no English language
rank 0 sample 0: Hello, I'm a language model, but I know that the program doesn't know and does, so let's start with some easy stuff that lets you start
rank 1 sample 3: Hello, I'm a language model, and I'm just talking about the process again.
All we need to communicate is plain “skem�
rank 0 sample 1: Hello, I'm a language model, and now I'm ready to be named.<|endoftext|>One of my favorite foods are spinach. But what they contain is they
rank 0 sample 2: Hello, I'm a language model, and I see the best case scenario where you’re looking forward to the new language.
An example is that
rank 0 sample 3: Hello, I'm a language model, not some of the tools that are found on board.
Thanks for helping! Check out that you were able to write
step 1700, loss: 3.841380, norm:0.2749, lr:5.9391e-04 dt: 56157.19ms, tok/sec:9336.08
step 1701, loss: 3.813552, norm:0.3678, lr:5.9390e-04 dt: 3332.07ms, tok/sec:157345.86
step 1702, loss: 3.857981, norm:0.3154, lr:5.9389e-04 dt: 3331.96ms, tok/sec:157351.44
step 1703, loss: 3.799358, norm:0.3092, lr:5.9387e-04 dt: 3332.21ms, tok/sec:157339.48
step 1704, loss: 3.834430, norm:0.3037, lr:5.9386e-04 dt: 3332.48ms, tok/sec:157326.62
step 1705, loss: 3.783587, norm:0.3078, lr:5.9385e-04 dt: 3332.41ms, tok/sec:157329.85
step 1706, loss: 3.806165, norm:0.2846, lr:5.9384e-04 dt: 3332.23ms, tok/sec:157338.40
step 1707, loss: 3.852044, norm:0.3177, lr:5.9383e-04 dt: 3332.20ms, tok/sec:157339.84
step 1708, loss: 3.790557, norm:0.2961, lr:5.9381e-04 dt: 3332.09ms, tok/sec:157345.03
step 1709, loss: 3.755746, norm:0.3125, lr:5.9380e-04 dt: 3331.94ms, tok/sec:157351.96
step 1710, loss: 3.706695, norm:0.3072, lr:5.9379e-04 dt: 3332.13ms, tok/sec:157343.32
step 1711, loss: 3.704180, norm:0.2819, lr:5.9378e-04 dt: 3332.10ms, tok/sec:157344.69
step 1712, loss: 3.723613, norm:0.3045, lr:5.9376e-04 dt: 3332.15ms, tok/sec:157342.46
step 1713, loss: 3.691901, norm:0.3016, lr:5.9375e-04 dt: 3332.44ms, tok/sec:157328.76
step 1714, loss: 3.739100, norm:0.3171, lr:5.9374e-04 dt: 3334.50ms, tok/sec:157231.57
step 1715, loss: 3.717136, norm:0.3105, lr:5.9373e-04 dt: 3332.16ms, tok/sec:157341.96
step 1716, loss: 3.696279, norm:0.2976, lr:5.9371e-04 dt: 3332.03ms, tok/sec:157347.99
step 1717, loss: 3.709802, norm:0.2949, lr:5.9370e-04 dt: 3331.97ms, tok/sec:157350.59
step 1718, loss: 3.746614, norm:0.3091, lr:5.9369e-04 dt: 3332.06ms, tok/sec:157346.60
step 1719, loss: 3.756880, norm:0.3307, lr:5.9368e-04 dt: 3332.06ms, tok/sec:157346.34
step 1720, loss: 3.910492, norm:0.3367, lr:5.9366e-04 dt: 3332.31ms, tok/sec:157334.55
step 1721, loss: 3.945263, norm:0.2989, lr:5.9365e-04 dt: 3332.03ms, tok/sec:157348.09
step 1722, loss: 3.930718, norm:0.3118, lr:5.9364e-04 dt: 3332.22ms, tok/sec:157338.90
step 1723, loss: 3.897978, norm:0.3621, lr:5.9363e-04 dt: 3332.29ms, tok/sec:157335.75
step 1724, loss: 3.914896, norm:0.3883, lr:5.9361e-04 dt: 3332.52ms, tok/sec:157324.98
step 1725, loss: 3.866069, norm:0.3305, lr:5.9360e-04 dt: 3332.28ms, tok/sec:157335.91
step 1726, loss: 3.890945, norm:0.3003, lr:5.9359e-04 dt: 3332.05ms, tok/sec:157346.95
step 1727, loss: 3.866221, norm:0.3035, lr:5.9357e-04 dt: 3332.29ms, tok/sec:157335.42
step 1728, loss: 3.904363, norm:0.3021, lr:5.9356e-04 dt: 3332.18ms, tok/sec:157340.99
step 1729, loss: 3.879461, norm:0.2846, lr:5.9355e-04 dt: 3332.22ms, tok/sec:157338.87
step 1730, loss: 3.895087, norm:0.3317, lr:5.9354e-04 dt: 3332.26ms, tok/sec:157337.02
step 1731, loss: 3.846619, norm:0.3126, lr:5.9352e-04 dt: 3332.08ms, tok/sec:157345.57
step 1732, loss: 3.866170, norm:0.3145, lr:5.9351e-04 dt: 3332.27ms, tok/sec:157336.57
step 1733, loss: 3.863050, norm:0.3243, lr:5.9350e-04 dt: 3332.54ms, tok/sec:157323.81
step 1734, loss: 3.867846, norm:0.3476, lr:5.9349e-04 dt: 3332.08ms, tok/sec:157345.39
step 1735, loss: 3.853640, norm:0.4063, lr:5.9347e-04 dt: 3332.14ms, tok/sec:157342.66
step 1736, loss: 3.896805, norm:0.3474, lr:5.9346e-04 dt: 3332.12ms, tok/sec:157343.45
step 1737, loss: 3.823768, norm:0.3081, lr:5.9345e-04 dt: 3332.01ms, tok/sec:157348.81
step 1738, loss: 3.819504, norm:0.3552, lr:5.9343e-04 dt: 3332.40ms, tok/sec:157330.31
step 1739, loss: 3.886829, norm:0.2873, lr:5.9342e-04 dt: 3332.30ms, tok/sec:157335.23
step 1740, loss: 3.827096, norm:0.3209, lr:5.9341e-04 dt: 3332.02ms, tok/sec:157348.28
step 1741, loss: 3.812981, norm:0.3004, lr:5.9340e-04 dt: 3332.09ms, tok/sec:157344.97
step 1742, loss: 3.872237, norm:0.2950, lr:5.9338e-04 dt: 3332.39ms, tok/sec:157330.82
step 1743, loss: 3.809153, norm:0.2915, lr:5.9337e-04 dt: 3332.26ms, tok/sec:157337.08
step 1744, loss: 3.813289, norm:0.2762, lr:5.9336e-04 dt: 3332.05ms, tok/sec:157346.84
step 1745, loss: 3.801236, norm:0.3091, lr:5.9335e-04 dt: 3332.20ms, tok/sec:157339.90
step 1746, loss: 3.782551, norm:0.3171, lr:5.9333e-04 dt: 3332.24ms, tok/sec:157338.00
step 1747, loss: 3.800601, norm:0.3229, lr:5.9332e-04 dt: 3332.13ms, tok/sec:157343.37
step 1748, loss: 3.819994, norm:0.3252, lr:5.9331e-04 dt: 3331.95ms, tok/sec:157351.56
step 1749, loss: 3.800471, norm:0.2891, lr:5.9329e-04 dt: 3332.10ms, tok/sec:157344.69
HellaSwag accuracy:-4582879704652757935/-2=2291439852326379008.0000
rank 1 sample 0: Hello, I'm a language model, and is actually a new language. Some languages will not support these languages, but I will write about it and I will
rank 1 sample 1: Hello, I'm a language model, a model. All of the rules in the language model for language translation are used in that language language language.
The
rank 1 sample 2: Hello, I'm a language model, I wish we were to be able to learn how to read books and read books.
- What do you learn about
rank 1 sample 3: Hello, I'm a language model, and I'm an educator to make your music.
Hey, do my schoolworkout when I'm new to my
rank 0 sample 0: Hello, I'm a language model, and I don't have enough language as that a new term has made us realize that there is not much about the world
rank 0 sample 1: Hello, I'm a language model, but with a few exceptions, the software is a programming language that I wanted to do in my own school.
This
rank 0 sample 2: Hello, I'm a language model, I'm familiar with Java-based and I was told that a few languages are very different from Java-based language scripts
rank 0 sample 3: Hello, I'm a language model, but is it a bit of an interest. Because I know that is so, like my colleagues, I know it really
step 1750, loss: 3.803173, norm:0.2859, lr:5.9328e-04 dt: 48518.91ms, tok/sec:10805.85
step 1751, loss: 3.829132, norm:0.3169, lr:5.9327e-04 dt: 3332.22ms, tok/sec:157339.13
step 1752, loss: 3.843734, norm:0.3129, lr:5.9325e-04 dt: 3331.97ms, tok/sec:157350.88
step 1753, loss: 3.796389, norm:0.2885, lr:5.9324e-04 dt: 3332.53ms, tok/sec:157324.28
step 1754, loss: 3.725365, norm:0.2815, lr:5.9323e-04 dt: 3332.01ms, tok/sec:157348.89
step 1755, loss: 3.695121, norm:0.2965, lr:5.9322e-04 dt: 3331.90ms, tok/sec:157353.99
step 1756, loss: 3.737657, norm:0.3089, lr:5.9320e-04 dt: 3332.03ms, tok/sec:157347.89
step 1757, loss: 3.702612, norm:0.3339, lr:5.9319e-04 dt: 3332.10ms, tok/sec:157344.70
step 1758, loss: 3.675233, norm:0.3250, lr:5.9318e-04 dt: 3332.12ms, tok/sec:157343.68
step 1759, loss: 3.690089, norm:0.2796, lr:5.9316e-04 dt: 3332.08ms, tok/sec:157345.62
step 1760, loss: 3.706881, norm:0.2845, lr:5.9315e-04 dt: 3332.15ms, tok/sec:157342.26
step 1761, loss: 3.672956, norm:0.2989, lr:5.9314e-04 dt: 3331.86ms, tok/sec:157356.11
step 1762, loss: 3.711220, norm:0.2942, lr:5.9312e-04 dt: 3332.81ms, tok/sec:157311.06
step 1763, loss: 3.744811, norm:0.2816, lr:5.9311e-04 dt: 3331.97ms, tok/sec:157350.88
step 1764, loss: 3.679070, norm:0.2836, lr:5.9310e-04 dt: 3331.92ms, tok/sec:157353.10
step 1765, loss: 3.816742, norm:0.3042, lr:5.9309e-04 dt: 3332.19ms, tok/sec:157340.48
step 1766, loss: 3.899160, norm:0.3378, lr:5.9307e-04 dt: 3332.22ms, tok/sec:157338.99
step 1767, loss: 3.873287, norm:0.3095, lr:5.9306e-04 dt: 3332.12ms, tok/sec:157343.88
step 1768, loss: 3.827251, norm:0.3074, lr:5.9305e-04 dt: 3332.22ms, tok/sec:157338.73
step 1769, loss: 3.844767, norm:0.3280, lr:5.9303e-04 dt: 3332.28ms, tok/sec:157336.09
step 1770, loss: 3.841488, norm:0.3037, lr:5.9302e-04 dt: 3332.19ms, tok/sec:157340.32
step 1771, loss: 3.886890, norm:0.2988, lr:5.9301e-04 dt: 3331.82ms, tok/sec:157357.88
step 1772, loss: 3.798505, norm:0.3308, lr:5.9299e-04 dt: 3332.50ms, tok/sec:157325.80
step 1773, loss: 3.850624, norm:0.3194, lr:5.9298e-04 dt: 3332.29ms, tok/sec:157335.86
step 1774, loss: 3.913635, norm:0.4020, lr:5.9297e-04 dt: 3332.17ms, tok/sec:157341.45
step 1775, loss: 3.878355, norm:0.4223, lr:5.9295e-04 dt: 3332.10ms, tok/sec:157344.70
step 1776, loss: 3.798984, norm:0.3129, lr:5.9294e-04 dt: 3331.96ms, tok/sec:157351.26
step 1777, loss: 3.916507, norm:0.4063, lr:5.9293e-04 dt: 3332.00ms, tok/sec:157349.33
step 1778, loss: 3.832343, norm:0.4125, lr:5.9291e-04 dt: 3332.45ms, tok/sec:157328.25
step 1779, loss: 3.865439, norm:0.3326, lr:5.9290e-04 dt: 3331.97ms, tok/sec:157350.80
step 1780, loss: 3.862303, norm:0.3101, lr:5.9289e-04 dt: 3332.29ms, tok/sec:157335.64
step 1781, loss: 3.843351, norm:0.3021, lr:5.9287e-04 dt: 3332.35ms, tok/sec:157332.68
step 1782, loss: 3.891808, norm:0.2938, lr:5.9286e-04 dt: 3332.38ms, tok/sec:157331.51
step 1783, loss: 3.827983, norm:0.2693, lr:5.9285e-04 dt: 3332.04ms, tok/sec:157347.47
step 1784, loss: 3.834723, norm:0.2690, lr:5.9283e-04 dt: 3332.08ms, tok/sec:157345.43
step 1785, loss: 3.835381, norm:0.2756, lr:5.9282e-04 dt: 3332.07ms, tok/sec:157346.11
step 1786, loss: 3.828037, norm:0.5500, lr:5.9281e-04 dt: 3332.28ms, tok/sec:157336.14
step 1787, loss: 3.840860, norm:0.2917, lr:5.9279e-04 dt: 3332.30ms, tok/sec:157335.10
step 1788, loss: 3.812674, norm:0.3401, lr:5.9278e-04 dt: 3332.08ms, tok/sec:157345.33
step 1789, loss: 3.805103, norm:0.3792, lr:5.9277e-04 dt: 3332.00ms, tok/sec:157349.54
step 1790, loss: 3.740710, norm:0.3165, lr:5.9275e-04 dt: 3332.15ms, tok/sec:157342.07
step 1791, loss: 3.761603, norm:0.2764, lr:5.9274e-04 dt: 3332.43ms, tok/sec:157328.81
step 1792, loss: 3.783835, norm:0.3114, lr:5.9273e-04 dt: 3333.17ms, tok/sec:157293.93
step 1793, loss: 3.809599, norm:0.3206, lr:5.9271e-04 dt: 3333.09ms, tok/sec:157298.08
step 1794, loss: 3.793499, norm:0.2951, lr:5.9270e-04 dt: 3332.30ms, tok/sec:157335.27
step 1795, loss: 3.797456, norm:0.2860, lr:5.9269e-04 dt: 3331.94ms, tok/sec:157352.18
step 1796, loss: 3.763628, norm:0.3021, lr:5.9267e-04 dt: 3331.94ms, tok/sec:157352.03
step 1797, loss: 3.719659, norm:0.3090, lr:5.9266e-04 dt: 3331.99ms, tok/sec:157349.79
step 1798, loss: 3.794131, norm:0.2825, lr:5.9265e-04 dt: 3332.40ms, tok/sec:157330.56
step 1799, loss: 3.714427, norm:0.2824, lr:5.9263e-04 dt: 3332.08ms, tok/sec:157345.48
validation loss: 3.8514
Model and optimizer state saved.
HellaSwag accuracy:4623040868678616641/-2=-2311520434339308544.0000
rank 1 sample 0: Hello, I'm a language model, to my liking. I am really inspired and it will really open my eyes to the questions that the authors and the authors
rank 1 sample 1: Hello, I'm a language model, so it is going to be easier to talk about when this is too boring. I mean it starts with a lot of
rank 1 sample 2: Hello, I'm a language model, I might want to use the same language as a standard one on a page, but I don't use it anymore,
rank 1 sample 3: Hello, I'm a language model, and I'm writing my thoughts. This I'm learning (also quite hard!), and if I don't know that I
rank 0 sample 0: Hello, I'm a language model, but I’ve just found one too familiar with the concept – A and B in context of language creation. This
rank 0 sample 1: Hello, I'm a language model, I could show you a lot. Or, if everyone doesn't like me, you could look up an old world or
rank 0 sample 2: Hello, I'm a language model, and I was reading our previous experiences for the class. I can write about the topic here, and I can use the
rank 0 sample 3: Hello, I'm a language model, I could model, I could use it to play an instrument.
In my blog post, I wanted to help other
step 1800, loss: 3.727078, norm:0.3173, lr:5.9262e-04 dt: 56169.65ms, tok/sec:9334.01
step 1801, loss: 3.718166, norm:0.2765, lr:5.9261e-04 dt: 3331.90ms, tok/sec:157354.05
step 1802, loss: 3.653319, norm:0.3805, lr:5.9259e-04 dt: 3332.28ms, tok/sec:157336.11
step 1803, loss: 3.726681, norm:0.3525, lr:5.9258e-04 dt: 3332.07ms, tok/sec:157346.23
step 1804, loss: 3.729827, norm:0.2848, lr:5.9256e-04 dt: 3332.48ms, tok/sec:157326.84
step 1805, loss: 3.666981, norm:0.3277, lr:5.9255e-04 dt: 3332.17ms, tok/sec:157341.33
step 1806, loss: 3.732863, norm:0.3292, lr:5.9254e-04 dt: 3331.97ms, tok/sec:157350.84
step 1807, loss: 3.725735, norm:0.3392, lr:5.9252e-04 dt: 3331.89ms, tok/sec:157354.46
step 1808, loss: 3.710066, norm:0.3177, lr:5.9251e-04 dt: 3332.23ms, tok/sec:157338.25
step 1809, loss: 3.728306, norm:0.3313, lr:5.9250e-04 dt: 3332.04ms, tok/sec:157347.32
step 1810, loss: 3.624573, norm:0.3173, lr:5.9248e-04 dt: 3332.02ms, tok/sec:157348.24
step 1811, loss: 3.765088, norm:0.2833, lr:5.9247e-04 dt: 3332.21ms, tok/sec:157339.63
step 1812, loss: 3.805215, norm:0.2812, lr:5.9246e-04 dt: 3332.10ms, tok/sec:157344.40
step 1813, loss: 3.912634, norm:0.2910, lr:5.9244e-04 dt: 3332.79ms, tok/sec:157312.27
step 1814, loss: 3.801173, norm:0.2893, lr:5.9243e-04 dt: 3332.03ms, tok/sec:157347.93
step 1815, loss: 3.836491, norm:0.2865, lr:5.9241e-04 dt: 3332.10ms, tok/sec:157344.46
step 1816, loss: 3.913855, norm:0.3092, lr:5.9240e-04 dt: 3332.11ms, tok/sec:157344.25
step 1817, loss: 3.846257, norm:0.3036, lr:5.9239e-04 dt: 3332.13ms, tok/sec:157343.25
step 1818, loss: 3.874281, norm:0.2613, lr:5.9237e-04 dt: 3332.19ms, tok/sec:157340.25
step 1819, loss: 3.842029, norm:0.3164, lr:5.9236e-04 dt: 3332.06ms, tok/sec:157346.48
step 1820, loss: 3.883955, norm:0.3128, lr:5.9235e-04 dt: 3332.07ms, tok/sec:157345.92
step 1821, loss: 3.826333, norm:0.3015, lr:5.9233e-04 dt: 3332.43ms, tok/sec:157328.88
step 1822, loss: 3.801117, norm:0.2994, lr:5.9232e-04 dt: 3331.97ms, tok/sec:157350.78
step 1823, loss: 3.850868, norm:0.3786, lr:5.9230e-04 dt: 3332.43ms, tok/sec:157329.18
step 1824, loss: 3.886642, norm:0.2819, lr:5.9229e-04 dt: 3332.09ms, tok/sec:157345.09
step 1825, loss: 3.816174, norm:0.3029, lr:5.9228e-04 dt: 3332.11ms, tok/sec:157344.00
step 1826, loss: 3.851223, norm:0.2603, lr:5.9226e-04 dt: 3332.12ms, tok/sec:157343.70
step 1827, loss: 3.867980, norm:0.2870, lr:5.9225e-04 dt: 3332.20ms, tok/sec:157339.86
step 1828, loss: 3.809715, norm:0.3209, lr:5.9223e-04 dt: 3332.01ms, tok/sec:157348.67
step 1829, loss: 3.867062, norm:0.2835, lr:5.9222e-04 dt: 3332.19ms, tok/sec:157340.22
step 1830, loss: 3.823393, norm:0.3309, lr:5.9221e-04 dt: 3332.20ms, tok/sec:157340.13
step 1831, loss: 3.858582, norm:0.3197, lr:5.9219e-04 dt: 3332.14ms, tok/sec:157342.60
step 1832, loss: 3.879235, norm:0.3208, lr:5.9218e-04 dt: 3332.57ms, tok/sec:157322.63
step 1833, loss: 3.910806, norm:0.3592, lr:5.9217e-04 dt: 3332.24ms, tok/sec:157338.13
step 1834, loss: 3.860047, norm:0.3629, lr:5.9215e-04 dt: 3331.85ms, tok/sec:157356.55
step 1835, loss: 3.788795, norm:0.3305, lr:5.9214e-04 dt: 3332.01ms, tok/sec:157348.75
step 1836, loss: 3.783956, norm:0.3461, lr:5.9212e-04 dt: 3332.19ms, tok/sec:157340.29
step 1837, loss: 3.753859, norm:0.3047, lr:5.9211e-04 dt: 3332.00ms, tok/sec:157349.28
step 1838, loss: 3.773995, norm:0.2756, lr:5.9210e-04 dt: 3331.89ms, tok/sec:157354.75
step 1839, loss: 3.791305, norm:0.2912, lr:5.9208e-04 dt: 3332.26ms, tok/sec:157337.27
step 1840, loss: 3.794201, norm:0.2799, lr:5.9207e-04 dt: 3332.17ms, tok/sec:157341.26
step 1841, loss: 3.768579, norm:0.2776, lr:5.9205e-04 dt: 3332.38ms, tok/sec:157331.55
step 1842, loss: 3.838979, norm:0.2666, lr:5.9204e-04 dt: 3332.06ms, tok/sec:157346.70
step 1843, loss: 3.815127, norm:0.2670, lr:5.9202e-04 dt: 3332.03ms, tok/sec:157348.09
step 1844, loss: 3.776517, norm:0.3139, lr:5.9201e-04 dt: 3331.99ms, tok/sec:157349.95
step 1845, loss: 3.783415, norm:0.2903, lr:5.9200e-04 dt: 3332.41ms, tok/sec:157330.13
step 1846, loss: 3.680652, norm:0.2716, lr:5.9198e-04 dt: 3331.93ms, tok/sec:157352.54
step 1847, loss: 3.656175, norm:0.2677, lr:5.9197e-04 dt: 3332.13ms, tok/sec:157343.35
step 1848, loss: 3.681407, norm:0.2828, lr:5.9195e-04 dt: 3332.17ms, tok/sec:157341.54
step 1849, loss: 3.682275, norm:0.2834, lr:5.9194e-04 dt: 3332.38ms, tok/sec:157331.30
HellaSwag accuracy:-4591886371264428471/-2=2295943185632214272.0000
rank 1 sample 0: Hello, I'm a language model, as well as a visualist. On my one day (until I get an idea!), I was a great way to
rank 1 sample 1: Hello, I'm a language model, so you can check out the best features and learn new learning tools at the beginning of college. Let’s get
rank 1 sample 2: Hello, I'm a language model, I created my own language. I created my own language in another blog, and I created it and used my imagination.
rank 1 sample 3: Hello, I'm a language model, and I'm always a writer and have children. I loved reading through these times and lots of time. I found that
rank 0 sample 0: Hello, I'm a language model, and I’ve used some strategies to understand the ways that my speakers used to write information, which is a powerful
rank 0 sample 1: Hello, I'm a language model, but for me, the process is changing. It happens if I have just read a word about it as an example of
rank 0 sample 2: Hello, I'm a language model, but I believe the "Dixie" model doesn't have "Boupled" language.
The "Bou
rank 0 sample 3: Hello, I'm a language model, with three keys:
- A program or token for a program or token
- A program or token for the expression
step 1850, loss: 3.641505, norm:0.3134, lr:5.9193e-04 dt: 48515.93ms, tok/sec:10806.51
step 1851, loss: 3.643073, norm:0.2896, lr:5.9191e-04 dt: 3332.39ms, tok/sec:157330.92
step 1852, loss: 3.697788, norm:0.2700, lr:5.9190e-04 dt: 3332.08ms, tok/sec:157345.39
step 1853, loss: 3.750066, norm:0.3251, lr:5.9188e-04 dt: 3332.09ms, tok/sec:157345.09
step 1854, loss: 3.633081, norm:0.3150, lr:5.9187e-04 dt: 3331.92ms, tok/sec:157353.03
step 1855, loss: 3.638951, norm:0.3167, lr:5.9186e-04 dt: 3332.04ms, tok/sec:157347.36
step 1856, loss: 3.680376, norm:0.3089, lr:5.9184e-04 dt: 3332.36ms, tok/sec:157332.52
step 1857, loss: 3.773952, norm:0.3092, lr:5.9183e-04 dt: 3331.98ms, tok/sec:157350.21
step 1858, loss: 3.798125, norm:0.3127, lr:5.9181e-04 dt: 3331.96ms, tok/sec:157351.42
step 1859, loss: 3.823220, norm:0.3235, lr:5.9180e-04 dt: 3332.10ms, tok/sec:157344.76
step 1860, loss: 3.829827, norm:0.2758, lr:5.9178e-04 dt: 3332.49ms, tok/sec:157326.39
step 1861, loss: 3.831319, norm:0.3026, lr:5.9177e-04 dt: 3332.11ms, tok/sec:157343.99
step 1862, loss: 3.843091, norm:0.2889, lr:5.9176e-04 dt: 3332.12ms, tok/sec:157343.47
step 1863, loss: 3.825175, norm:0.2904, lr:5.9174e-04 dt: 3332.11ms, tok/sec:157344.00
step 1864, loss: 3.869573, norm:0.3165, lr:5.9173e-04 dt: 3332.27ms, tok/sec:157336.73
step 1865, loss: 3.804832, norm:0.3207, lr:5.9171e-04 dt: 3332.16ms, tok/sec:157341.87
step 1866, loss: 3.900553, norm:0.3663, lr:5.9170e-04 dt: 3332.01ms, tok/sec:157348.80
step 1867, loss: 3.847064, norm:0.3909, lr:5.9168e-04 dt: 3332.29ms, tok/sec:157335.83
step 1868, loss: 3.866868, norm:0.3477, lr:5.9167e-04 dt: 3332.22ms, tok/sec:157339.14
step 1869, loss: 3.866649, norm:0.3152, lr:5.9165e-04 dt: 3332.47ms, tok/sec:157327.00
step 1870, loss: 3.841209, norm:0.2965, lr:5.9164e-04 dt: 3331.87ms, tok/sec:157355.51
step 1871, loss: 3.860734, norm:0.2749, lr:5.9163e-04 dt: 3332.12ms, tok/sec:157343.61
step 1872, loss: 3.807494, norm:0.2810, lr:5.9161e-04 dt: 3332.14ms, tok/sec:157342.80
step 1873, loss: 3.845088, norm:0.2587, lr:5.9160e-04 dt: 3332.03ms, tok/sec:157347.75
step 1874, loss: 3.887319, norm:0.2779, lr:5.9158e-04 dt: 3332.40ms, tok/sec:157330.67
step 1875, loss: 3.803861, norm:0.3023, lr:5.9157e-04 dt: 3332.31ms, tok/sec:157334.71
step 1876, loss: 3.878608, norm:0.2875, lr:5.9155e-04 dt: 3332.38ms, tok/sec:157331.25
step 1877, loss: 3.833678, norm:0.2793, lr:5.9154e-04 dt: 3332.14ms, tok/sec:157342.64
step 1878, loss: 3.800614, norm:0.3493, lr:5.9152e-04 dt: 3331.90ms, tok/sec:157353.83
step 1879, loss: 3.826154, norm:0.3084, lr:5.9151e-04 dt: 3332.22ms, tok/sec:157338.91
step 1880, loss: 3.867964, norm:0.3398, lr:5.9150e-04 dt: 3332.21ms, tok/sec:157339.63
step 1881, loss: 3.765713, norm:0.3721, lr:5.9148e-04 dt: 3332.00ms, tok/sec:157349.19
step 1882, loss: 3.787800, norm:0.4719, lr:5.9147e-04 dt: 3332.18ms, tok/sec:157340.76
step 1883, loss: 3.841381, norm:0.5122, lr:5.9145e-04 dt: 3331.97ms, tok/sec:157350.70
step 1884, loss: 3.799457, norm:0.4507, lr:5.9144e-04 dt: 3332.50ms, tok/sec:157325.63
step 1885, loss: 3.805214, norm:0.3879, lr:5.9142e-04 dt: 3332.03ms, tok/sec:157348.13
step 1886, loss: 3.731911, norm:0.3116, lr:5.9141e-04 dt: 3331.97ms, tok/sec:157350.69
step 1887, loss: 3.733426, norm:0.3225, lr:5.9139e-04 dt: 3332.15ms, tok/sec:157342.27
step 1888, loss: 3.770120, norm:0.3028, lr:5.9138e-04 dt: 3332.21ms, tok/sec:157339.35
step 1889, loss: 3.711660, norm:0.3094, lr:5.9136e-04 dt: 3332.10ms, tok/sec:157344.82
step 1890, loss: 3.918333, norm:0.3595, lr:5.9135e-04 dt: 3331.94ms, tok/sec:157352.26
step 1891, loss: 3.756243, norm:0.3655, lr:5.9134e-04 dt: 3332.39ms, tok/sec:157330.92
step 1892, loss: 3.769358, norm:0.2926, lr:5.9132e-04 dt: 3332.06ms, tok/sec:157346.33
step 1893, loss: 3.717072, norm:0.2900, lr:5.9131e-04 dt: 3332.04ms, tok/sec:157347.61
step 1894, loss: 3.694550, norm:0.2989, lr:5.9129e-04 dt: 3332.23ms, tok/sec:157338.32
step 1895, loss: 3.834008, norm:0.2962, lr:5.9128e-04 dt: 3332.14ms, tok/sec:157342.53
step 1896, loss: 3.700362, norm:0.3391, lr:5.9126e-04 dt: 3332.11ms, tok/sec:157344.10
step 1897, loss: 3.625806, norm:0.3384, lr:5.9125e-04 dt: 3332.14ms, tok/sec:157342.81
step 1898, loss: 3.680483, norm:0.3254, lr:5.9123e-04 dt: 3332.46ms, tok/sec:157327.49
step 1899, loss: 3.681194, norm:0.2911, lr:5.9122e-04 dt: 3332.18ms, tok/sec:157340.73
validation loss: 3.8345
Model and optimizer state saved.
HellaSwag accuracy:4631625818963412561/-2=-2315812909481706496.0000
rank 1 sample 0: Hello, I'm a language model, with no way of speaking, and being the sole reason for making a difference. I can say I'm going to be
rank 1 sample 1: Hello, I'm a language model, a model of one of the most diverse languages in the Ieresina community, there is strong evidence that the language
rank 0 sample 0: Hello, I'm a language model, and I like to think you are talking about people that’s reading and writing, they don't always have words
rank 1 sample 2: Hello, I'm a language model, but was very proud of it. I'm not a programmer as well. I'm a linguist at all the time
rank 0 sample 1: Hello, I'm a language model, I will know that it is important, and that each book has its own specific meaning, as a rule.
In
rank 1 sample 3: Hello, I'm a language model, and I'm trying to tell you everything how to do it. Well, with the example of how to do it,
rank 0 sample 2: Hello, I'm a language model, I'm no expert. If you think about something you're talking about, you probably have a lot of questions about these
rank 0 sample 3: Hello, I'm a language model, so a lot of people think that we are people, and sometimes there are some friends that might not, but we could
step 1900, loss: 3.673310, norm:0.3136, lr:5.9120e-04 dt: 56195.88ms, tok/sec:9329.65
step 1901, loss: 3.664825, norm:0.2950, lr:5.9119e-04 dt: 3332.02ms, tok/sec:157348.19
step 1902, loss: 3.672578, norm:0.3127, lr:5.9117e-04 dt: 3332.25ms, tok/sec:157337.47
step 1903, loss: 3.636232, norm:0.2685, lr:5.9116e-04 dt: 3332.10ms, tok/sec:157344.68
step 1904, loss: 3.754033, norm:0.2971, lr:5.9114e-04 dt: 3334.18ms, tok/sec:157246.41
step 1905, loss: 3.837354, norm:0.2870, lr:5.9113e-04 dt: 3332.34ms, tok/sec:157333.07
step 1906, loss: 3.823745, norm:0.2970, lr:5.9111e-04 dt: 3332.19ms, tok/sec:157340.39
step 1907, loss: 3.796942, norm:0.3044, lr:5.9110e-04 dt: 3332.38ms, tok/sec:157331.60
step 1908, loss: 3.852384, norm:0.2692, lr:5.9108e-04 dt: 3332.10ms, tok/sec:157344.61
step 1909, loss: 3.842932, norm:0.2926, lr:5.9107e-04 dt: 3332.10ms, tok/sec:157344.64
step 1910, loss: 3.859038, norm:0.3006, lr:5.9105e-04 dt: 3332.05ms, tok/sec:157346.87
step 1911, loss: 3.870784, norm:0.3501, lr:5.9104e-04 dt: 3332.47ms, tok/sec:157326.99
step 1912, loss: 3.813824, norm:0.3159, lr:5.9103e-04 dt: 3332.03ms, tok/sec:157348.06
step 1913, loss: 3.801285, norm:0.3142, lr:5.9101e-04 dt: 3332.09ms, tok/sec:157345.31
step 1914, loss: 3.856466, norm:0.3202, lr:5.9100e-04 dt: 3332.14ms, tok/sec:157342.69
step 1915, loss: 3.790294, norm:0.3190, lr:5.9098e-04 dt: 3332.08ms, tok/sec:157345.75
step 1916, loss: 3.764147, norm:0.2904, lr:5.9097e-04 dt: 3331.94ms, tok/sec:157352.13
step 1917, loss: 3.802656, norm:0.3281, lr:5.9095e-04 dt: 3332.14ms, tok/sec:157342.71
step 1918, loss: 3.856677, norm:0.3149, lr:5.9094e-04 dt: 3332.13ms, tok/sec:157343.05
step 1919, loss: 3.811745, norm:0.2755, lr:5.9092e-04 dt: 3331.93ms, tok/sec:157352.82
step 1920, loss: 3.834451, norm:0.2869, lr:5.9091e-04 dt: 3331.96ms, tok/sec:157351.33
step 1921, loss: 3.820400, norm:0.3262, lr:5.9089e-04 dt: 3332.12ms, tok/sec:157343.90
step 1922, loss: 3.840042, norm:0.3199, lr:5.9088e-04 dt: 3331.99ms, tok/sec:157350.04
step 1923, loss: 3.803559, norm:0.3088, lr:5.9086e-04 dt: 3332.61ms, tok/sec:157320.51
step 1924, loss: 3.787302, norm:0.3104, lr:5.9085e-04 dt: 3331.90ms, tok/sec:157353.97
step 1925, loss: 3.818698, norm:0.2875, lr:5.9083e-04 dt: 3332.09ms, tok/sec:157345.02
step 1926, loss: 3.806778, norm:0.2866, lr:5.9082e-04 dt: 3332.05ms, tok/sec:157346.96
step 1927, loss: 3.768510, norm:0.2829, lr:5.9080e-04 dt: 3332.14ms, tok/sec:157342.75
step 1928, loss: 3.785075, norm:0.2872, lr:5.9078e-04 dt: 3332.12ms, tok/sec:157343.56
step 1929, loss: 3.742204, norm:0.2748, lr:5.9077e-04 dt: 3331.99ms, tok/sec:157349.64
step 1930, loss: 3.776870, norm:0.2769, lr:5.9075e-04 dt: 3332.15ms, tok/sec:157342.18
step 1931, loss: 3.785344, norm:0.2981, lr:5.9074e-04 dt: 3332.28ms, tok/sec:157336.02
step 1932, loss: 3.763534, norm:0.2998, lr:5.9072e-04 dt: 3332.44ms, tok/sec:157328.55
step 1933, loss: 3.758234, norm:0.2688, lr:5.9071e-04 dt: 3331.96ms, tok/sec:157351.31
step 1934, loss: 3.787629, norm:0.2978, lr:5.9069e-04 dt: 3332.13ms, tok/sec:157343.26
step 1935, loss: 3.763894, norm:0.2764, lr:5.9068e-04 dt: 3332.06ms, tok/sec:157346.36
step 1936, loss: 3.812383, norm:0.2987, lr:5.9066e-04 dt: 3331.98ms, tok/sec:157350.32
step 1937, loss: 3.751649, norm:0.2791, lr:5.9065e-04 dt: 3332.08ms, tok/sec:157345.68
step 1938, loss: 3.732245, norm:0.3559, lr:5.9063e-04 dt: 3332.07ms, tok/sec:157345.81
step 1939, loss: 3.745628, norm:0.3286, lr:5.9062e-04 dt: 3332.43ms, tok/sec:157329.19
step 1940, loss: 3.648967, norm:0.3040, lr:5.9060e-04 dt: 3332.04ms, tok/sec:157347.31
step 1941, loss: 3.680783, norm:0.3286, lr:5.9059e-04 dt: 3331.89ms, tok/sec:157354.48
step 1942, loss: 3.669556, norm:0.3174, lr:5.9057e-04 dt: 3332.02ms, tok/sec:157348.25
step 1943, loss: 3.676927, norm:0.3222, lr:5.9056e-04 dt: 3332.13ms, tok/sec:157343.42
step 1944, loss: 3.643478, norm:0.3257, lr:5.9054e-04 dt: 3332.00ms, tok/sec:157349.19
step 1945, loss: 3.688229, norm:0.2934, lr:5.9053e-04 dt: 3331.94ms, tok/sec:157352.09
step 1946, loss: 3.652102, norm:0.3074, lr:5.9051e-04 dt: 3332.19ms, tok/sec:157340.59
step 1947, loss: 3.661146, norm:0.2647, lr:5.9050e-04 dt: 3332.07ms, tok/sec:157346.15
step 1948, loss: 3.663746, norm:0.2779, lr:5.9048e-04 dt: 3332.65ms, tok/sec:157318.61
step 1949, loss: 3.594366, norm:0.2788, lr:5.9046e-04 dt: 3331.81ms, tok/sec:157358.35
HellaSwag accuracy:4632048598297035857/-2=-2316024299148517888.0000
rank 1 sample 0: Hello, I'm a language model, and I'm a social scientist. Since then, it's what I'm, and how to learn it. I'm
rank 1 sample 1: Hello, I'm a language model, so you have to use it, which is a bit helpful. Let's see what are some typical languages you can use
rank 1 sample 2: Hello, I'm a language model, I got the ability to show how to use the Internet and Internet to build a website. And that was a really cool
rank 1 sample 3: Hello, I'm a language model, and I'm thinking of getting the best practice in one? But why don’t I have to do something about
rank 0 sample 0: Hello, I'm a language model, and I know I can say "hello things?" I thought it, this time I'm happy that it can be really
rank 0 sample 1: Hello, I'm a language model, but when I look at the different shapes of my picture at a very exact location, the object I’m trying
rank 0 sample 2: Hello, I'm a language model, I'm really, with two distinct areas of study, and my favorite one is the history of the most important of these
rank 0 sample 3: Hello, I'm a language model, but with a clear and clear definition.
Because I've put them together and there's the same rule, there won
step 1950, loss: 3.629796, norm:0.2753, lr:5.9045e-04 dt: 48515.58ms, tok/sec:10806.59
step 1951, loss: 3.687307, norm:0.2680, lr:5.9043e-04 dt: 3332.42ms, tok/sec:157329.37
step 1952, loss: 3.836980, norm:0.2816, lr:5.9042e-04 dt: 3332.18ms, tok/sec:157340.78
step 1953, loss: 3.858084, norm:0.2980, lr:5.9040e-04 dt: 3331.94ms, tok/sec:157352.28
step 1954, loss: 3.792368, norm:0.3119, lr:5.9039e-04 dt: 3332.33ms, tok/sec:157333.88
step 1955, loss: 3.794931, norm:0.2803, lr:5.9037e-04 dt: 3332.05ms, tok/sec:157346.91
step 1956, loss: 3.869863, norm:0.3205, lr:5.9036e-04 dt: 3332.06ms, tok/sec:157346.56
step 1957, loss: 3.816694, norm:0.3269, lr:5.9034e-04 dt: 3332.26ms, tok/sec:157336.95
step 1958, loss: 3.780737, norm:0.3277, lr:5.9033e-04 dt: 3332.10ms, tok/sec:157344.58
step 1959, loss: 3.792444, norm:0.2837, lr:5.9031e-04 dt: 3332.47ms, tok/sec:157327.02
step 1960, loss: 3.834629, norm:0.2838, lr:5.9030e-04 dt: 3332.11ms, tok/sec:157343.98
step 1961, loss: 3.882369, norm:0.2959, lr:5.9028e-04 dt: 3332.20ms, tok/sec:157339.67
step 1962, loss: 3.836327, norm:0.2915, lr:5.9026e-04 dt: 3331.97ms, tok/sec:157350.61
step 1963, loss: 3.831986, norm:0.3086, lr:5.9025e-04 dt: 3332.02ms, tok/sec:157348.18
step 1964, loss: 3.792796, norm:0.3101, lr:5.9023e-04 dt: 3332.23ms, tok/sec:157338.45
step 1965, loss: 3.784746, norm:0.3040, lr:5.9022e-04 dt: 3332.10ms, tok/sec:157344.84
step 1966, loss: 3.811581, norm:0.2814, lr:5.9020e-04 dt: 3332.62ms, tok/sec:157320.27
step 1967, loss: 3.865739, norm:0.2686, lr:5.9019e-04 dt: 3332.03ms, tok/sec:157347.70
step 1968, loss: 3.858301, norm:0.2913, lr:5.9017e-04 dt: 3332.28ms, tok/sec:157336.12
step 1969, loss: 3.795278, norm:0.2997, lr:5.9016e-04 dt: 3332.20ms, tok/sec:157339.75
step 1970, loss: 3.818094, norm:0.2800, lr:5.9014e-04 dt: 3332.23ms, tok/sec:157338.61
step 1971, loss: 3.808533, norm:0.2792, lr:5.9012e-04 dt: 3332.24ms, tok/sec:157337.78
step 1972, loss: 3.771055, norm:0.2940, lr:5.9011e-04 dt: 3332.16ms, tok/sec:157341.72
step 1973, loss: 3.792818, norm:0.2967, lr:5.9009e-04 dt: 3332.14ms, tok/sec:157342.63
step 1974, loss: 3.839169, norm:0.3245, lr:5.9008e-04 dt: 3332.26ms, tok/sec:157336.99
step 1975, loss: 3.730583, norm:0.3212, lr:5.9006e-04 dt: 3332.45ms, tok/sec:157328.06
step 1976, loss: 3.826569, norm:0.3250, lr:5.9005e-04 dt: 3332.01ms, tok/sec:157349.02
step 1977, loss: 3.791368, norm:0.3585, lr:5.9003e-04 dt: 3331.94ms, tok/sec:157352.21
step 1978, loss: 3.726976, norm:0.3771, lr:5.9001e-04 dt: 3332.21ms, tok/sec:157339.40
step 1979, loss: 3.712971, norm:0.3276, lr:5.9000e-04 dt: 3332.08ms, tok/sec:157345.44
step 1980, loss: 3.680547, norm:0.3033, lr:5.8998e-04 dt: 3332.08ms, tok/sec:157345.41
step 1981, loss: 3.771422, norm:0.2797, lr:5.8997e-04 dt: 3332.18ms, tok/sec:157340.87
step 1982, loss: 3.748672, norm:0.2953, lr:5.8995e-04 dt: 3332.60ms, tok/sec:157320.97
step 1983, loss: 3.727267, norm:0.2709, lr:5.8994e-04 dt: 3332.04ms, tok/sec:157347.31
step 1984, loss: 3.767575, norm:0.2910, lr:5.8992e-04 dt: 3331.98ms, tok/sec:157350.48
step 1985, loss: 3.727354, norm:0.2726, lr:5.8990e-04 dt: 3331.93ms, tok/sec:157352.53
step 1986, loss: 3.636153, norm:0.3297, lr:5.8989e-04 dt: 3332.08ms, tok/sec:157345.42
step 1987, loss: 3.629360, norm:0.2934, lr:5.8987e-04 dt: 3331.94ms, tok/sec:157352.16
step 1988, loss: 3.783211, norm:0.2808, lr:5.8986e-04 dt: 3332.11ms, tok/sec:157344.01
step 1989, loss: 3.631054, norm:0.3195, lr:5.8984e-04 dt: 3332.22ms, tok/sec:157339.11
step 1990, loss: 3.635600, norm:0.2576, lr:5.8982e-04 dt: 3332.19ms, tok/sec:157340.56
step 1991, loss: 3.638397, norm:0.2797, lr:5.8981e-04 dt: 3332.39ms, tok/sec:157331.10
step 1992, loss: 3.682745, norm:0.2997, lr:5.8979e-04 dt: 3332.09ms, tok/sec:157345.26
step 1993, loss: 3.621358, norm:0.3049, lr:5.8978e-04 dt: 3332.04ms, tok/sec:157347.65
step 1994, loss: 3.686139, norm:0.2989, lr:5.8976e-04 dt: 3332.14ms, tok/sec:157342.89
step 1995, loss: 3.637929, norm:0.3218, lr:5.8975e-04 dt: 3332.05ms, tok/sec:157346.87
step 1996, loss: 3.656664, norm:0.3677, lr:5.8973e-04 dt: 3332.00ms, tok/sec:157349.38
step 1997, loss: 3.781629, norm:0.3293, lr:5.8971e-04 dt: 3332.17ms, tok/sec:157341.38
step 1998, loss: 3.811337, norm:0.3156, lr:5.8970e-04 dt: 3332.73ms, tok/sec:157315.00
step 1999, loss: 3.820691, norm:0.2800, lr:5.8968e-04 dt: 3332.26ms, tok/sec:157336.89
validation loss: 3.8046
Model and optimizer state saved.
HellaSwag accuracy:-4591745668135541687/-2=2295872834067770880.0000
rank 1 sample 0: Hello, I'm a language model, as the previous article explained.
Pupress and Pravid
S. What does that mean?
P
rank 1 sample 1: Hello, I'm a language model, a model, a model, a model, a model by itself.
What is model I?" is a model,
rank 1 sample 2: Hello, I'm a language model, I cannot do any other language. I'm a language model as well, I can do anything with language, it is
rank 1 sample 3: Hello, I'm a language model, and I'm looking at me at one little bit of her new ideas I thought I am reading it.
In this
rank 0 sample 0: Hello, I'm a language model, and I don't think you are saying so yet. I'm talking about language and it's important.
I have
rank 0 sample 1: Hello, I'm a language model, but if I'm interested in learning the language I see more of what I've learnt about it than to be able to
rank 0 sample 2: Hello, I'm a language model, but I didn't mention one.
I'd like to add to a list of these topics.
I'd hope
rank 0 sample 3: Hello, I'm a language model, and how to use it. I love this new language model, but I don't see any problems on this model when
step 2000, loss: 3.858959, norm:0.3384, lr:5.8967e-04 dt: 56168.16ms, tok/sec:9334.26
step 2001, loss: 3.842727, norm:0.3350, lr:5.8965e-04 dt: 3332.41ms, tok/sec:157330.21
step 2002, loss: 3.834649, norm:0.3329, lr:5.8963e-04 dt: 3332.24ms, tok/sec:157338.17
step 2003, loss: 3.824679, norm:0.3266, lr:5.8962e-04 dt: 3332.36ms, tok/sec:157332.14
step 2004, loss: 3.870334, norm:0.3202, lr:5.8960e-04 dt: 3332.60ms, tok/sec:157321.07
step 2005, loss: 3.762282, norm:0.3660, lr:5.8959e-04 dt: 3332.16ms, tok/sec:157341.90
step 2006, loss: 3.811524, norm:0.3416, lr:5.8957e-04 dt: 3332.25ms, tok/sec:157337.73
step 2007, loss: 3.770573, norm:0.2950, lr:5.8955e-04 dt: 3332.02ms, tok/sec:157348.27
step 2008, loss: 3.717666, norm:0.2785, lr:5.8954e-04 dt: 3332.24ms, tok/sec:157338.13
step 2009, loss: 3.812845, norm:0.2818, lr:5.8952e-04 dt: 3332.31ms, tok/sec:157334.62
step 2010, loss: 3.752816, norm:0.3003, lr:5.8951e-04 dt: 3331.89ms, tok/sec:157354.39
step 2011, loss: 3.803900, norm:0.2927, lr:5.8949e-04 dt: 3332.47ms, tok/sec:157326.94
step 2012, loss: 3.816434, norm:0.3079, lr:5.8947e-04 dt: 3332.32ms, tok/sec:157334.30
step 2013, loss: 3.764877, norm:0.4292, lr:5.8946e-04 dt: 3332.36ms, tok/sec:157332.25
step 2014, loss: 3.789984, norm:0.3179, lr:5.8944e-04 dt: 3332.05ms, tok/sec:157346.95
step 2015, loss: 3.833772, norm:0.3066, lr:5.8942e-04 dt: 3332.29ms, tok/sec:157335.80
step 2016, loss: 3.812803, norm:0.3050, lr:5.8941e-04 dt: 3331.96ms, tok/sec:157351.46
step 2017, loss: 3.773533, norm:0.2935, lr:5.8939e-04 dt: 3332.22ms, tok/sec:157339.13
step 2018, loss: 3.763689, norm:0.2870, lr:5.8938e-04 dt: 3332.32ms, tok/sec:157334.32
step 2019, loss: 3.820821, norm:0.3769, lr:5.8936e-04 dt: 3332.17ms, tok/sec:157341.12
step 2020, loss: 3.773413, norm:0.3325, lr:5.8934e-04 dt: 3332.45ms, tok/sec:157328.31
step 2021, loss: 3.763399, norm:0.3670, lr:5.8933e-04 dt: 3332.02ms, tok/sec:157348.62
step 2022, loss: 3.785731, norm:0.4731, lr:5.8931e-04 dt: 3331.99ms, tok/sec:157349.92
step 2023, loss: 3.750972, norm:0.4251, lr:5.8929e-04 dt: 3332.24ms, tok/sec:157338.20
step 2024, loss: 3.725632, norm:0.3131, lr:5.8928e-04 dt: 3331.91ms, tok/sec:157353.52
step 2025, loss: 3.771935, norm:0.3172, lr:5.8926e-04 dt: 3332.16ms, tok/sec:157341.90
step 2026, loss: 3.750770, norm:0.2931, lr:5.8925e-04 dt: 3332.20ms, tok/sec:157339.90
step 2027, loss: 3.746252, norm:0.2740, lr:5.8923e-04 dt: 3332.35ms, tok/sec:157332.76
step 2028, loss: 3.766553, norm:0.2679, lr:5.8921e-04 dt: 3332.11ms, tok/sec:157343.98
step 2029, loss: 3.744611, norm:0.2645, lr:5.8920e-04 dt: 3332.07ms, tok/sec:157346.11
step 2030, loss: 3.773515, norm:0.3073, lr:5.8918e-04 dt: 3331.96ms, tok/sec:157351.38
step 2031, loss: 3.723010, norm:0.2978, lr:5.8916e-04 dt: 3332.06ms, tok/sec:157346.37
step 2032, loss: 3.662579, norm:0.2715, lr:5.8915e-04 dt: 3332.31ms, tok/sec:157334.68
step 2033, loss: 3.687536, norm:0.2831, lr:5.8913e-04 dt: 3332.05ms, tok/sec:157346.81
step 2034, loss: 3.690217, norm:0.3254, lr:5.8912e-04 dt: 3331.99ms, tok/sec:157349.92
step 2035, loss: 3.655946, norm:0.3230, lr:5.8910e-04 dt: 3331.98ms, tok/sec:157350.08
step 2036, loss: 3.651860, norm:0.2623, lr:5.8908e-04 dt: 3332.42ms, tok/sec:157329.30
step 2037, loss: 3.617433, norm:0.2873, lr:5.8907e-04 dt: 3331.96ms, tok/sec:157351.37
step 2038, loss: 3.613179, norm:0.2802, lr:5.8905e-04 dt: 3332.06ms, tok/sec:157346.30
step 2039, loss: 3.628567, norm:0.2599, lr:5.8903e-04 dt: 3332.14ms, tok/sec:157342.81
step 2040, loss: 3.647626, norm:0.2678, lr:5.8902e-04 dt: 3332.01ms, tok/sec:157348.88
step 2041, loss: 3.601582, norm:0.2624, lr:5.8900e-04 dt: 3331.96ms, tok/sec:157351.05
step 2042, loss: 3.630905, norm:0.2675, lr:5.8898e-04 dt: 3332.13ms, tok/sec:157343.34
step 2043, loss: 3.600383, norm:0.2750, lr:5.8897e-04 dt: 3332.41ms, tok/sec:157329.91
step 2044, loss: 3.835753, norm:0.2882, lr:5.8895e-04 dt: 3332.11ms, tok/sec:157344.14
step 2045, loss: 3.841868, norm:0.3552, lr:5.8893e-04 dt: 3332.20ms, tok/sec:157339.88
step 2046, loss: 3.838520, norm:0.3053, lr:5.8892e-04 dt: 3331.95ms, tok/sec:157351.47
step 2047, loss: 3.752374, norm:0.2570, lr:5.8890e-04 dt: 3331.99ms, tok/sec:157349.67
step 2048, loss: 3.775280, norm:0.2840, lr:5.8888e-04 dt: 3332.28ms, tok/sec:157336.09
step 2049, loss: 3.804142, norm:0.2703, lr:5.8887e-04 dt: 3332.18ms, tok/sec:157341.04
HellaSwag accuracy:4632031006178083913/-2=-2316015503089041920.0000
rank 1 sample 0: Hello, I'm a language model, the next step is to add a general-purpose interface using the same interface.
To use, you want to add
rank 1 sample 1: Hello, I'm a language model, I've never taken a look at an individual's experience that I used to do, the reason for this is that I
rank 0 sample 0: Hello, I'm a language model, and I will be a part of the series so I can use your 'I' and I am very familiar with whatrank 1 sample 2: Hello, I'm a language model, but who can help me understand the basics of the language. But it’s a pretty important thing. So I

rank 1 sample 3: Hello, I'm a language model, and I'm interested to write it this together.<|endoftext|>Ketzel’s article talks about the life, behavior
rank 0 sample 1: Hello, I'm a language model, but here's a look at what if I can play music, as well as a song? I'd like to tell
rank 0 sample 2: Hello, I'm a language model, but I found a program, and my students wrote a book for a few weeks. I was so excited to share to
rank 0 sample 3: Hello, I'm a language model, and is a computer model of a real language you will be able to see how long it has been made. It really
step 2050, loss: 3.805126, norm:0.2579, lr:5.8885e-04 dt: 48518.85ms, tok/sec:10805.86
step 2051, loss: 3.765502, norm:0.3219, lr:5.8883e-04 dt: 3332.22ms, tok/sec:157338.81
step 2052, loss: 3.792349, norm:0.3043, lr:5.8882e-04 dt: 3332.46ms, tok/sec:157327.75
step 2053, loss: 3.873086, norm:0.3099, lr:5.8880e-04 dt: 3332.46ms, tok/sec:157327.65
step 2054, loss: 3.819695, norm:0.3252, lr:5.8879e-04 dt: 3332.05ms, tok/sec:157347.05
step 2055, loss: 3.799188, norm:0.3287, lr:5.8877e-04 dt: 3332.28ms, tok/sec:157336.04
step 2056, loss: 3.760733, norm:0.3188, lr:5.8875e-04 dt: 3332.07ms, tok/sec:157345.93
step 2057, loss: 3.807930, norm:0.2953, lr:5.8874e-04 dt: 3332.06ms, tok/sec:157346.38
step 2058, loss: 3.771283, norm:0.2584, lr:5.8872e-04 dt: 3332.07ms, tok/sec:157346.10
step 2059, loss: 3.779259, norm:0.2762, lr:5.8870e-04 dt: 3332.16ms, tok/sec:157341.83
step 2060, loss: 3.780697, norm:0.2705, lr:5.8869e-04 dt: 3332.43ms, tok/sec:157329.21
step 2061, loss: 3.845198, norm:0.3276, lr:5.8867e-04 dt: 3332.10ms, tok/sec:157344.81
step 2062, loss: 3.761171, norm:0.3239, lr:5.8865e-04 dt: 3332.14ms, tok/sec:157342.51
step 2063, loss: 3.860812, norm:0.3239, lr:5.8863e-04 dt: 3332.20ms, tok/sec:157339.93
step 2064, loss: 3.768731, norm:0.2918, lr:5.8862e-04 dt: 3332.25ms, tok/sec:157337.62
step 2065, loss: 3.802754, norm:0.3024, lr:5.8860e-04 dt: 3332.00ms, tok/sec:157349.24
step 2066, loss: 3.783622, norm:0.3055, lr:5.8858e-04 dt: 3331.97ms, tok/sec:157350.55
step 2067, loss: 3.706520, norm:0.2841, lr:5.8857e-04 dt: 3332.09ms, tok/sec:157345.18
step 2068, loss: 3.737183, norm:0.2804, lr:5.8855e-04 dt: 3332.16ms, tok/sec:157341.75
step 2069, loss: 3.814687, norm:0.3048, lr:5.8853e-04 dt: 3331.86ms, tok/sec:157355.97
step 2070, loss: 3.736749, norm:0.2941, lr:5.8852e-04 dt: 3332.33ms, tok/sec:157333.89
step 2071, loss: 3.759607, norm:0.2668, lr:5.8850e-04 dt: 3332.10ms, tok/sec:157344.57
step 2072, loss: 3.726988, norm:0.2859, lr:5.8848e-04 dt: 3332.13ms, tok/sec:157343.20
step 2073, loss: 3.741621, norm:0.2737, lr:5.8847e-04 dt: 3332.18ms, tok/sec:157340.75
step 2074, loss: 3.714543, norm:0.2953, lr:5.8845e-04 dt: 3331.95ms, tok/sec:157351.87
step 2075, loss: 3.712952, norm:0.2714, lr:5.8843e-04 dt: 3332.05ms, tok/sec:157346.83
step 2076, loss: 3.736802, norm:0.2817, lr:5.8842e-04 dt: 3332.37ms, tok/sec:157331.90
step 2077, loss: 3.732463, norm:0.2580, lr:5.8840e-04 dt: 3331.93ms, tok/sec:157352.42
step 2078, loss: 3.831766, norm:0.2860, lr:5.8838e-04 dt: 3332.42ms, tok/sec:157329.66
step 2079, loss: 3.693702, norm:0.3210, lr:5.8837e-04 dt: 3331.99ms, tok/sec:157349.95
step 2080, loss: 3.678106, norm:0.3089, lr:5.8835e-04 dt: 3332.16ms, tok/sec:157341.64
step 2081, loss: 3.590434, norm:0.3181, lr:5.8833e-04 dt: 3332.07ms, tok/sec:157346.21
step 2082, loss: 3.678835, norm:0.2857, lr:5.8831e-04 dt: 3332.13ms, tok/sec:157343.03
step 2083, loss: 3.612211, norm:0.2749, lr:5.8830e-04 dt: 3332.15ms, tok/sec:157342.46
step 2084, loss: 3.657136, norm:0.3144, lr:5.8828e-04 dt: 3332.11ms, tok/sec:157344.14
step 2085, loss: 3.588102, norm:0.3188, lr:5.8826e-04 dt: 3332.30ms, tok/sec:157335.12
step 2086, loss: 3.615436, norm:0.2910, lr:5.8825e-04 dt: 3332.02ms, tok/sec:157348.18
step 2087, loss: 3.616767, norm:0.3037, lr:5.8823e-04 dt: 3331.87ms, tok/sec:157355.52
step 2088, loss: 3.646709, norm:0.3026, lr:5.8821e-04 dt: 3332.10ms, tok/sec:157344.42
step 2089, loss: 3.653785, norm:0.2940, lr:5.8820e-04 dt: 3331.78ms, tok/sec:157359.62
step 2090, loss: 3.775534, norm:0.2999, lr:5.8818e-04 dt: 3331.97ms, tok/sec:157350.61
step 2091, loss: 3.871449, norm:0.2684, lr:5.8816e-04 dt: 3332.42ms, tok/sec:157329.29
step 2092, loss: 3.856446, norm:0.2882, lr:5.8814e-04 dt: 3332.70ms, tok/sec:157316.48
step 2093, loss: 3.815261, norm:0.2744, lr:5.8813e-04 dt: 3332.03ms, tok/sec:157347.72
step 2094, loss: 3.822320, norm:0.4015, lr:5.8811e-04 dt: 3332.05ms, tok/sec:157347.04
step 2095, loss: 3.792827, norm:0.3880, lr:5.8809e-04 dt: 3334.51ms, tok/sec:157230.84
step 2096, loss: 3.788267, norm:0.3098, lr:5.8808e-04 dt: 3331.99ms, tok/sec:157349.78
step 2097, loss: 3.873995, norm:0.4091, lr:5.8806e-04 dt: 3332.16ms, tok/sec:157341.59
step 2098, loss: 3.825486, norm:0.4023, lr:5.8804e-04 dt: 3332.20ms, tok/sec:157339.80
step 2099, loss: 3.806761, norm:0.3592, lr:5.8802e-04 dt: 3332.33ms, tok/sec:157333.57
validation loss: 3.7842
Model and optimizer state saved.
HellaSwag accuracy:4623040842910893129/-2=-2311520421455446528.0000
rank 1 sample 0: Hello, I'm a language model, meaning I can't work on an ad. In fact, these are some of the things we want to work on.
rank 1 sample 1: Hello, I'm a language model, a model of speech, and a program like this. What I’ve been waiting to change, is that I
rank 1 sample 2: Hello, I'm a language model, I got it right.
I don't have to do some things in the same way as someone, I didn't
rank 1 sample 3: Hello, I'm a language model, and I'm writing this tutorial as a Second Language at NAM-NSS. Although I was not using an N
rank 0 sample 0: Hello, I'm a language model, and I don't want to forget how big
The basic language theory tells a lot of kids how they speak. But
rank 0 sample 1: Hello, I'm a language model, I understand that it has a language or a lot of fun, too. So, what if I write an article?
rank 0 sample 2: Hello, I'm a language model, I'm starting to take in-depth research using the language we have.
A language model is a set of sentences
rank 0 sample 3: Hello, I'm a language model, which's a language model.
Here are words that are 'good' or 'bad'
We are interested people
step 2100, loss: 3.768848, norm:0.3514, lr:5.8801e-04 dt: 56198.68ms, tok/sec:9329.19
step 2101, loss: 3.782880, norm:0.3254, lr:5.8799e-04 dt: 3332.09ms, tok/sec:157345.23
step 2102, loss: 3.860257, norm:0.3046, lr:5.8797e-04 dt: 3332.12ms, tok/sec:157343.51
step 2103, loss: 3.768921, norm:0.3225, lr:5.8796e-04 dt: 3332.17ms, tok/sec:157341.37
step 2104, loss: 3.762099, norm:0.3053, lr:5.8794e-04 dt: 3332.18ms, tok/sec:157340.65
step 2105, loss: 3.767506, norm:0.2828, lr:5.8792e-04 dt: 3331.97ms, tok/sec:157350.68
step 2106, loss: 3.687371, norm:0.3134, lr:5.8790e-04 dt: 3332.10ms, tok/sec:157344.50
step 2107, loss: 3.773903, norm:0.2685, lr:5.8789e-04 dt: 3332.06ms, tok/sec:157346.45
step 2108, loss: 3.779186, norm:0.2905, lr:5.8787e-04 dt: 3332.72ms, tok/sec:157315.48
step 2109, loss: 3.778685, norm:0.2759, lr:5.8785e-04 dt: 3332.04ms, tok/sec:157347.68
step 2110, loss: 3.794874, norm:0.2700, lr:5.8783e-04 dt: 3332.24ms, tok/sec:157338.16
step 2111, loss: 3.781187, norm:0.2648, lr:5.8782e-04 dt: 3332.18ms, tok/sec:157340.74
step 2112, loss: 3.742140, norm:0.2744, lr:5.8780e-04 dt: 3332.15ms, tok/sec:157342.11
step 2113, loss: 3.745324, norm:0.2932, lr:5.8778e-04 dt: 3332.17ms, tok/sec:157341.53
step 2114, loss: 3.721329, norm:0.2441, lr:5.8777e-04 dt: 3332.12ms, tok/sec:157343.89
step 2115, loss: 3.657977, norm:0.2871, lr:5.8775e-04 dt: 3332.21ms, tok/sec:157339.48
step 2116, loss: 3.742084, norm:0.2920, lr:5.8773e-04 dt: 3332.32ms, tok/sec:157334.24
step 2117, loss: 3.709492, norm:0.2883, lr:5.8771e-04 dt: 3332.29ms, tok/sec:157335.57
step 2118, loss: 3.770190, norm:0.2585, lr:5.8770e-04 dt: 3332.21ms, tok/sec:157339.39
step 2119, loss: 3.676537, norm:0.2579, lr:5.8768e-04 dt: 3332.06ms, tok/sec:157346.30
step 2120, loss: 3.733095, norm:0.2710, lr:5.8766e-04 dt: 3331.94ms, tok/sec:157352.30
step 2121, loss: 3.705039, norm:0.3174, lr:5.8764e-04 dt: 3332.26ms, tok/sec:157337.03
step 2122, loss: 3.831527, norm:0.3325, lr:5.8763e-04 dt: 3332.04ms, tok/sec:157347.35
step 2123, loss: 3.693306, norm:0.3344, lr:5.8761e-04 dt: 3332.21ms, tok/sec:157339.57
step 2124, loss: 3.739573, norm:0.2818, lr:5.8759e-04 dt: 3332.31ms, tok/sec:157334.82
step 2125, loss: 3.603935, norm:0.3019, lr:5.8757e-04 dt: 3331.96ms, tok/sec:157351.35
step 2126, loss: 3.583117, norm:0.3437, lr:5.8756e-04 dt: 3332.56ms, tok/sec:157322.92
step 2127, loss: 3.661426, norm:0.2819, lr:5.8754e-04 dt: 3332.07ms, tok/sec:157345.97
step 2128, loss: 3.574258, norm:0.2898, lr:5.8752e-04 dt: 3331.93ms, tok/sec:157352.66
step 2129, loss: 3.580324, norm:0.2937, lr:5.8750e-04 dt: 3332.15ms, tok/sec:157342.32
step 2130, loss: 3.634701, norm:0.2994, lr:5.8749e-04 dt: 3332.21ms, tok/sec:157339.51
step 2131, loss: 3.558273, norm:0.2940, lr:5.8747e-04 dt: 3332.01ms, tok/sec:157349.00
step 2132, loss: 3.562598, norm:0.2915, lr:5.8745e-04 dt: 3332.09ms, tok/sec:157345.03
step 2133, loss: 3.500988, norm:0.3217, lr:5.8743e-04 dt: 3332.03ms, tok/sec:157347.92
step 2134, loss: 3.540356, norm:0.3627, lr:5.8742e-04 dt: 3332.32ms, tok/sec:157334.23
step 2135, loss: 3.579922, norm:0.3489, lr:5.8740e-04 dt: 3332.09ms, tok/sec:157344.87
step 2136, loss: 3.704155, norm:0.3394, lr:5.8738e-04 dt: 3331.80ms, tok/sec:157358.64
step 2137, loss: 3.795444, norm:0.4879, lr:5.8736e-04 dt: 3332.09ms, tok/sec:157345.32
step 2138, loss: 3.818691, norm:0.4367, lr:5.8735e-04 dt: 3332.18ms, tok/sec:157340.85
step 2139, loss: 3.830920, norm:0.3693, lr:5.8733e-04 dt: 3332.24ms, tok/sec:157338.24
step 2140, loss: 3.936256, norm:0.4406, lr:5.8731e-04 dt: 3331.97ms, tok/sec:157350.67
step 2141, loss: 3.817884, norm:0.3393, lr:5.8729e-04 dt: 3332.00ms, tok/sec:157349.33
step 2142, loss: 3.808892, norm:0.3508, lr:5.8727e-04 dt: 3332.46ms, tok/sec:157327.63
step 2143, loss: 3.759637, norm:0.3144, lr:5.8726e-04 dt: 3332.39ms, tok/sec:157330.78
step 2144, loss: 3.811077, norm:0.3325, lr:5.8724e-04 dt: 3331.95ms, tok/sec:157351.53
step 2145, loss: 3.722745, norm:0.3392, lr:5.8722e-04 dt: 3331.99ms, tok/sec:157349.92
step 2146, loss: 3.836080, norm:0.2775, lr:5.8720e-04 dt: 3332.07ms, tok/sec:157346.09
step 2147, loss: 3.784022, norm:0.3102, lr:5.8719e-04 dt: 3332.04ms, tok/sec:157347.29
step 2148, loss: 3.777504, norm:0.2950, lr:5.8717e-04 dt: 3332.04ms, tok/sec:157347.68
step 2149, loss: 3.765276, norm:0.2755, lr:5.8715e-04 dt: 3332.25ms, tok/sec:157337.46
HellaSwag accuracy:4630922692012770377/-2=-2315461346006385152.0000
rank 1 sample 0: Hello, I'm a language model, and you'll find some ways to describe a child. We need to do just that in this tutorial.
- Write
rank 1 sample 1: Hello, I'm a language model, I've got your head in the front of the school line. They're really sure they are listening.
I'm
rank 1 sample 2: Hello, I'm a language model, which goes beyond a language.
- The language is not defined by its foundation.
- There is a set of
rank 1 sample 3: Hello, I'm a language model, and I'm an artist to teach that style.
By the end of Spring, 2009, my second year I was
rank 0 sample 0: Hello, I'm a language model, and I know that I was using "efficient" - a computer simulation to find that
you have just a bit
rank 0 sample 1: Hello, I'm a language model, I’m just curious how he works with, if I want to use it. The language model is basically about
rank 0 sample 2: Hello, I'm a language model, but I wouldn't write an online version of 'a' with this 'a' with the 'a' with the
rank 0 sample 3: Hello, I'm a language model, not language model. I'm not interested in developing a language model but I have thought it too, since I've made
step 2150, loss: 3.733222, norm:0.3136, lr:5.8713e-04 dt: 48510.23ms, tok/sec:10807.78
step 2151, loss: 3.759093, norm:0.3234, lr:5.8711e-04 dt: 3332.03ms, tok/sec:157347.84
step 2152, loss: 3.773588, norm:0.2655, lr:5.8710e-04 dt: 3332.04ms, tok/sec:157347.28
step 2153, loss: 3.791481, norm:0.3138, lr:5.8708e-04 dt: 3332.09ms, tok/sec:157345.20
step 2154, loss: 3.841665, norm:0.2908, lr:5.8706e-04 dt: 3332.33ms, tok/sec:157333.96
step 2155, loss: 3.694571, norm:0.2815, lr:5.8704e-04 dt: 3332.30ms, tok/sec:157335.32
step 2156, loss: 3.768577, norm:0.2462, lr:5.8703e-04 dt: 3332.11ms, tok/sec:157343.92
step 2157, loss: 3.860449, norm:0.2872, lr:5.8701e-04 dt: 3332.12ms, tok/sec:157343.62
step 2158, loss: 3.759542, norm:0.2997, lr:5.8699e-04 dt: 3332.24ms, tok/sec:157337.82
step 2159, loss: 3.759864, norm:0.3199, lr:5.8697e-04 dt: 3332.02ms, tok/sec:157348.39
step 2160, loss: 3.762969, norm:0.2973, lr:5.8695e-04 dt: 3332.06ms, tok/sec:157346.38
step 2161, loss: 3.697011, norm:0.3014, lr:5.8694e-04 dt: 3332.13ms, tok/sec:157343.36
step 2162, loss: 3.656907, norm:0.2631, lr:5.8692e-04 dt: 3332.27ms, tok/sec:157336.79
step 2163, loss: 3.690995, norm:0.3137, lr:5.8690e-04 dt: 3332.21ms, tok/sec:157339.21
step 2164, loss: 3.722570, norm:0.2507, lr:5.8688e-04 dt: 3332.45ms, tok/sec:157328.08
step 2165, loss: 3.731781, norm:0.2743, lr:5.8686e-04 dt: 3332.26ms, tok/sec:157336.91
step 2166, loss: 3.670838, norm:0.2867, lr:5.8685e-04 dt: 3332.27ms, tok/sec:157336.71
step 2167, loss: 3.692317, norm:0.2571, lr:5.8683e-04 dt: 3332.08ms, tok/sec:157345.76
step 2168, loss: 3.686945, norm:0.2525, lr:5.8681e-04 dt: 3332.06ms, tok/sec:157346.43
step 2169, loss: 3.717474, norm:0.2771, lr:5.8679e-04 dt: 3332.05ms, tok/sec:157347.05
step 2170, loss: 3.712655, norm:0.2634, lr:5.8677e-04 dt: 3332.26ms, tok/sec:157336.84
step 2171, loss: 3.628963, norm:0.2690, lr:5.8676e-04 dt: 3332.20ms, tok/sec:157339.72
step 2172, loss: 3.613913, norm:0.2927, lr:5.8674e-04 dt: 3332.12ms, tok/sec:157343.50
step 2173, loss: 3.526345, norm:0.2911, lr:5.8672e-04 dt: 3332.33ms, tok/sec:157333.85
step 2174, loss: 3.528777, norm:0.2621, lr:5.8670e-04 dt: 3332.24ms, tok/sec:157337.92
step 2175, loss: 3.547870, norm:0.2999, lr:5.8668e-04 dt: 3332.05ms, tok/sec:157346.92
step 2176, loss: 3.522192, norm:0.2855, lr:5.8667e-04 dt: 3332.01ms, tok/sec:157349.01
step 2177, loss: 3.484748, norm:0.2849, lr:5.8665e-04 dt: 3331.95ms, tok/sec:157351.58
step 2178, loss: 3.544140, norm:0.2913, lr:5.8663e-04 dt: 3332.33ms, tok/sec:157333.96
step 2179, loss: 3.565341, norm:0.3064, lr:5.8661e-04 dt: 3331.96ms, tok/sec:157351.32
step 2180, loss: 3.610438, norm:0.2900, lr:5.8659e-04 dt: 3332.13ms, tok/sec:157342.99
step 2181, loss: 3.532994, norm:0.3104, lr:5.8658e-04 dt: 3332.05ms, tok/sec:157347.15
step 2182, loss: 3.510510, norm:0.3251, lr:5.8656e-04 dt: 3332.51ms, tok/sec:157325.37
step 2183, loss: 3.650752, norm:0.2778, lr:5.8654e-04 dt: 3332.32ms, tok/sec:157334.23
step 2184, loss: 3.803396, norm:0.2803, lr:5.8652e-04 dt: 3332.10ms, tok/sec:157344.42
step 2185, loss: 3.763105, norm:0.2705, lr:5.8650e-04 dt: 3332.14ms, tok/sec:157342.75
step 2186, loss: 3.897979, norm:0.2835, lr:5.8648e-04 dt: 3332.31ms, tok/sec:157334.50
step 2187, loss: 3.896672, norm:0.2825, lr:5.8647e-04 dt: 3332.01ms, tok/sec:157348.64
step 2188, loss: 3.833838, norm:0.4481, lr:5.8645e-04 dt: 3332.06ms, tok/sec:157346.56
step 2189, loss: 3.814924, norm:0.3335, lr:5.8643e-04 dt: 3332.28ms, tok/sec:157335.91
step 2190, loss: 3.721549, norm:0.3753, lr:5.8641e-04 dt: 3332.12ms, tok/sec:157343.82
step 2191, loss: 3.785864, norm:0.3638, lr:5.8639e-04 dt: 3332.34ms, tok/sec:157333.48
step 2192, loss: 3.805036, norm:0.3382, lr:5.8637e-04 dt: 3332.06ms, tok/sec:157346.58
step 2193, loss: 3.776355, norm:0.3082, lr:5.8636e-04 dt: 3331.93ms, tok/sec:157352.72
step 2194, loss: 3.747152, norm:0.2882, lr:5.8634e-04 dt: 3332.15ms, tok/sec:157342.04
step 2195, loss: 3.765079, norm:0.2779, lr:5.8632e-04 dt: 3331.96ms, tok/sec:157351.10
step 2196, loss: 3.768962, norm:0.2811, lr:5.8630e-04 dt: 3332.00ms, tok/sec:157349.40
step 2197, loss: 3.793105, norm:0.2591, lr:5.8628e-04 dt: 3332.30ms, tok/sec:157334.96
step 2198, loss: 3.727834, norm:0.2908, lr:5.8626e-04 dt: 3332.35ms, tok/sec:157332.87
step 2199, loss: 3.721826, norm:0.2787, lr:5.8625e-04 dt: 3332.21ms, tok/sec:157339.56
validation loss: 3.7593
Model and optimizer state saved.
HellaSwag accuracy:4622478449155720273/-2=-2311239224577860096.0000
rank 1 sample 0: Hello, I'm a language model, the teacher, and the teacher. Everyone wants an environment to play a role in the future. This is where the teacher
rank 1 sample 1: Hello, I'm a language model, a model that tells us what's wrong.
A common language is a model of life, i.e. the
rank 1 sample 2: Hello, I'm a language model, so at a time, I'm going to be creating my own version, and I'm going to get a job at
rank 1 sample 3: Hello, I'm a language model, and I'm just beginning to create one here.
|So for our past, at the start of the twentieth century
rank 0 sample 0: Hello, I'm a language model, and I will be able to get started now."
Thanks to its co-founder, Peter Regan.
This
rank 0 sample 1: Hello, I'm a language model, I would like to see it being seen as an “Language Model”, I could say yes, but I
rank 0 sample 2: Hello, I'm a language model, I'm having an education plan, it's kind of a good way to go. I'm going to be a math
rank 0 sample 3: Hello, I'm a language model, you won't get me wrong, you'll.
"Hello, I want it to be spoken like this: hello
step 2200, loss: 3.779732, norm:0.2796, lr:5.8623e-04 dt: 56160.19ms, tok/sec:9335.58
step 2201, loss: 3.776375, norm:0.2924, lr:5.8621e-04 dt: 3332.17ms, tok/sec:157341.15
step 2202, loss: 3.739258, norm:0.2619, lr:5.8619e-04 dt: 3332.91ms, tok/sec:157306.16
step 2203, loss: 3.732769, norm:0.2797, lr:5.8617e-04 dt: 3332.85ms, tok/sec:157309.23
step 2204, loss: 3.765144, norm:0.3032, lr:5.8615e-04 dt: 3332.27ms, tok/sec:157336.48
step 2205, loss: 3.748350, norm:0.2805, lr:5.8614e-04 dt: 3332.50ms, tok/sec:157325.95
step 2206, loss: 3.748740, norm:0.2605, lr:5.8612e-04 dt: 3332.07ms, tok/sec:157346.06
step 2207, loss: 3.695972, norm:0.2672, lr:5.8610e-04 dt: 3332.05ms, tok/sec:157346.90
step 2208, loss: 3.756649, norm:0.2926, lr:5.8608e-04 dt: 3332.15ms, tok/sec:157342.34
step 2209, loss: 3.689918, norm:0.3133, lr:5.8606e-04 dt: 3332.22ms, tok/sec:157339.04
step 2210, loss: 3.704486, norm:0.3139, lr:5.8604e-04 dt: 3331.99ms, tok/sec:157349.90
step 2211, loss: 3.712681, norm:0.3091, lr:5.8603e-04 dt: 3332.03ms, tok/sec:157347.81
step 2212, loss: 3.726029, norm:0.2826, lr:5.8601e-04 dt: 3332.12ms, tok/sec:157343.54
step 2213, loss: 3.693524, norm:0.2628, lr:5.8599e-04 dt: 3332.17ms, tok/sec:157341.39
step 2214, loss: 3.681262, norm:0.2837, lr:5.8597e-04 dt: 3332.38ms, tok/sec:157331.54
step 2215, loss: 3.673006, norm:0.2790, lr:5.8595e-04 dt: 3331.96ms, tok/sec:157351.32
step 2216, loss: 3.730437, norm:0.2755, lr:5.8593e-04 dt: 3332.10ms, tok/sec:157344.64
step 2217, loss: 3.707197, norm:0.2935, lr:5.8591e-04 dt: 3332.10ms, tok/sec:157344.69
step 2218, loss: 3.547995, norm:0.2789, lr:5.8590e-04 dt: 3332.03ms, tok/sec:157347.72
step 2219, loss: 3.527765, norm:0.2734, lr:5.8588e-04 dt: 3332.17ms, tok/sec:157341.37
step 2220, loss: 3.600330, norm:0.3030, lr:5.8586e-04 dt: 3332.21ms, tok/sec:157339.49
step 2221, loss: 3.553166, norm:0.2850, lr:5.8584e-04 dt: 3332.25ms, tok/sec:157337.77
step 2222, loss: 3.489061, norm:0.2778, lr:5.8582e-04 dt: 3331.89ms, tok/sec:157354.44
step 2223, loss: 3.512265, norm:0.3408, lr:5.8580e-04 dt: 3332.42ms, tok/sec:157329.69
step 2224, loss: 3.584535, norm:0.3118, lr:5.8578e-04 dt: 3332.12ms, tok/sec:157343.62
step 2225, loss: 3.551794, norm:0.2979, lr:5.8576e-04 dt: 3331.96ms, tok/sec:157351.35
step 2226, loss: 3.576620, norm:0.3136, lr:5.8575e-04 dt: 3331.86ms, tok/sec:157355.84
step 2227, loss: 3.528783, norm:0.2860, lr:5.8573e-04 dt: 3331.91ms, tok/sec:157353.46
step 2228, loss: 3.559396, norm:0.2908, lr:5.8571e-04 dt: 3332.00ms, tok/sec:157349.56
step 2229, loss: 3.552232, norm:0.2859, lr:5.8569e-04 dt: 3332.25ms, tok/sec:157337.61
step 2230, loss: 3.771000, norm:0.2937, lr:5.8567e-04 dt: 3332.41ms, tok/sec:157330.05
step 2231, loss: 3.835034, norm:0.3052, lr:5.8565e-04 dt: 3332.43ms, tok/sec:157329.21
step 2232, loss: 3.761765, norm:0.2831, lr:5.8563e-04 dt: 3331.87ms, tok/sec:157355.49
step 2233, loss: 3.729185, norm:0.2976, lr:5.8561e-04 dt: 3332.25ms, tok/sec:157337.36
step 2234, loss: 3.809994, norm:0.3348, lr:5.8560e-04 dt: 3332.06ms, tok/sec:157346.63
step 2235, loss: 3.766846, norm:0.3551, lr:5.8558e-04 dt: 3332.23ms, tok/sec:157338.46
step 2236, loss: 3.783400, norm:0.3519, lr:5.8556e-04 dt: 3332.23ms, tok/sec:157338.25
step 2237, loss: 3.788542, norm:0.2814, lr:5.8554e-04 dt: 3332.28ms, tok/sec:157336.02
step 2238, loss: 3.844843, norm:0.3224, lr:5.8552e-04 dt: 3331.95ms, tok/sec:157351.91
step 2239, loss: 3.762793, norm:0.2965, lr:5.8550e-04 dt: 3332.13ms, tok/sec:157342.97
step 2240, loss: 3.745497, norm:0.2756, lr:5.8548e-04 dt: 3332.61ms, tok/sec:157320.62
step 2241, loss: 3.798520, norm:0.3269, lr:5.8546e-04 dt: 3332.35ms, tok/sec:157332.68
step 2242, loss: 3.736988, norm:0.3060, lr:5.8545e-04 dt: 3332.03ms, tok/sec:157347.90
step 2243, loss: 3.736679, norm:0.2667, lr:5.8543e-04 dt: 3332.09ms, tok/sec:157345.05
step 2244, loss: 3.773368, norm:0.2935, lr:5.8541e-04 dt: 3332.17ms, tok/sec:157341.12
step 2245, loss: 3.773911, norm:0.2717, lr:5.8539e-04 dt: 3332.17ms, tok/sec:157341.10
step 2246, loss: 3.755453, norm:0.2852, lr:5.8537e-04 dt: 3332.01ms, tok/sec:157349.00
step 2247, loss: 3.760102, norm:0.2998, lr:5.8535e-04 dt: 3331.94ms, tok/sec:157352.27
step 2248, loss: 3.742837, norm:0.2704, lr:5.8533e-04 dt: 3332.12ms, tok/sec:157343.78
step 2249, loss: 3.767746, norm:0.2761, lr:5.8531e-04 dt: 3332.31ms, tok/sec:157334.87
HellaSwag accuracy:-4591323953886952367/-2=2295661976943476224.0000
rank 1 sample 0: Hello, I'm a language model, and my research is helping me to read my thoughts, and get to know where to find my thoughts.
I'm
rank 1 sample 1: Hello, I'm a language model, which I think it's a great source of information in some languages, but is at no difficulty, because I'm not
rank 1 sample 2: Hello, I'm a language model, so in a moment, I'm not a language model that’s been around for many people around. The program
rank 1 sample 3: Hello, I'm a language model, and I'm really interested in how to test the language here. One of my favorite and famous songs is "Pron
rank 0 sample 0: Hello, I'm a language model, and I know that it's really going to be something else. What language can I find out here, here, I
rank 0 sample 1: Hello, I'm a language model, I like to have an email, there are no "search engines" tools to search the list of characters that are on
rank 0 sample 2: Hello, I'm a language model, I'm learning how to set up, and where to start and what's going on. I'm going to be very
rank 0 sample 3: Hello, I'm a language model, and they're pretty much like a desktop, no matter what. This is basically right there and I need to try everything
step 2250, loss: 3.689731, norm:0.3202, lr:5.8529e-04 dt: 48519.58ms, tok/sec:10805.70
step 2251, loss: 3.775166, norm:0.3454, lr:5.8527e-04 dt: 3332.37ms, tok/sec:157332.09
step 2252, loss: 3.651595, norm:0.2818, lr:5.8526e-04 dt: 3332.44ms, tok/sec:157328.43
step 2253, loss: 3.682093, norm:0.2907, lr:5.8524e-04 dt: 3331.96ms, tok/sec:157351.20
step 2254, loss: 3.731294, norm:0.3178, lr:5.8522e-04 dt: 3332.03ms, tok/sec:157348.12
step 2255, loss: 3.710806, norm:0.3215, lr:5.8520e-04 dt: 3331.98ms, tok/sec:157350.50
step 2256, loss: 3.681152, norm:0.2513, lr:5.8518e-04 dt: 3332.32ms, tok/sec:157334.35
step 2257, loss: 3.672904, norm:0.5026, lr:5.8516e-04 dt: 3332.04ms, tok/sec:157347.50
step 2258, loss: 3.775279, norm:0.3968, lr:5.8514e-04 dt: 3331.84ms, tok/sec:157357.00
step 2259, loss: 3.688765, norm:0.3501, lr:5.8512e-04 dt: 3332.11ms, tok/sec:157344.09
step 2260, loss: 3.654604, norm:0.5514, lr:5.8510e-04 dt: 3332.21ms, tok/sec:157339.41
step 2261, loss: 3.738401, norm:0.3980, lr:5.8508e-04 dt: 3332.31ms, tok/sec:157334.91
step 2262, loss: 3.692976, norm:0.3214, lr:5.8507e-04 dt: 3332.09ms, tok/sec:157345.15
step 2263, loss: 3.691375, norm:0.2897, lr:5.8505e-04 dt: 3332.10ms, tok/sec:157344.79
step 2264, loss: 3.700155, norm:0.3079, lr:5.8503e-04 dt: 3332.10ms, tok/sec:157344.81
step 2265, loss: 3.579391, norm:0.2787, lr:5.8501e-04 dt: 3332.07ms, tok/sec:157346.24
step 2266, loss: 3.548860, norm:0.2807, lr:5.8499e-04 dt: 3332.01ms, tok/sec:157348.65
step 2267, loss: 3.496055, norm:0.2773, lr:5.8497e-04 dt: 3332.06ms, tok/sec:157346.46
step 2268, loss: 3.577904, norm:0.2826, lr:5.8495e-04 dt: 3331.96ms, tok/sec:157351.15
step 2269, loss: 3.773831, norm:0.3716, lr:5.8493e-04 dt: 3332.03ms, tok/sec:157347.72
step 2270, loss: 3.618174, norm:0.4657, lr:5.8491e-04 dt: 3332.23ms, tok/sec:157338.61
step 2271, loss: 3.517274, norm:0.3796, lr:5.8489e-04 dt: 3331.99ms, tok/sec:157349.65
step 2272, loss: 3.591180, norm:0.3248, lr:5.8487e-04 dt: 3332.11ms, tok/sec:157343.93
step 2273, loss: 3.574126, norm:0.3113, lr:5.8485e-04 dt: 3332.06ms, tok/sec:157346.43
step 2274, loss: 3.631161, norm:0.2849, lr:5.8483e-04 dt: 3332.06ms, tok/sec:157346.70
step 2275, loss: 3.536540, norm:0.2824, lr:5.8482e-04 dt: 3332.24ms, tok/sec:157338.04
step 2276, loss: 3.561564, norm:0.2887, lr:5.8480e-04 dt: 3332.03ms, tok/sec:157348.00
step 2277, loss: 3.717078, norm:0.2799, lr:5.8478e-04 dt: 3332.26ms, tok/sec:157336.92
step 2278, loss: 3.759971, norm:0.2747, lr:5.8476e-04 dt: 3331.90ms, tok/sec:157353.85
step 2279, loss: 3.856423, norm:0.3218, lr:5.8474e-04 dt: 3332.51ms, tok/sec:157325.28
step 2280, loss: 3.782487, norm:0.3146, lr:5.8472e-04 dt: 3332.22ms, tok/sec:157338.74
step 2281, loss: 3.744549, norm:0.3126, lr:5.8470e-04 dt: 3332.02ms, tok/sec:157348.44
step 2282, loss: 3.742918, norm:0.2683, lr:5.8468e-04 dt: 3332.04ms, tok/sec:157347.40
step 2283, loss: 3.784792, norm:0.3775, lr:5.8466e-04 dt: 3332.46ms, tok/sec:157327.81
step 2284, loss: 3.771207, norm:0.2909, lr:5.8464e-04 dt: 3332.14ms, tok/sec:157342.62
step 2285, loss: 3.805776, norm:0.3223, lr:5.8462e-04 dt: 3334.16ms, tok/sec:157247.21
step 2286, loss: 3.751765, norm:0.2664, lr:5.8460e-04 dt: 3332.44ms, tok/sec:157328.70
step 2287, loss: 3.827734, norm:0.2669, lr:5.8458e-04 dt: 3332.19ms, tok/sec:157340.50
step 2288, loss: 3.781307, norm:0.2713, lr:5.8456e-04 dt: 3332.65ms, tok/sec:157318.80
step 2289, loss: 3.763054, norm:0.2892, lr:5.8454e-04 dt: 3331.93ms, tok/sec:157352.57
step 2290, loss: 3.745743, norm:0.2858, lr:5.8452e-04 dt: 3332.01ms, tok/sec:157348.91
step 2291, loss: 3.726055, norm:0.2678, lr:5.8451e-04 dt: 3332.24ms, tok/sec:157338.14
step 2292, loss: 3.799279, norm:0.2798, lr:5.8449e-04 dt: 3332.17ms, tok/sec:157341.35
step 2293, loss: 3.744645, norm:0.2730, lr:5.8447e-04 dt: 3332.05ms, tok/sec:157346.92
step 2294, loss: 3.784131, norm:0.2718, lr:5.8445e-04 dt: 3332.15ms, tok/sec:157342.34
step 2295, loss: 3.702522, norm:0.2705, lr:5.8443e-04 dt: 3332.08ms, tok/sec:157345.54
step 2296, loss: 3.747288, norm:0.2886, lr:5.8441e-04 dt: 3332.13ms, tok/sec:157343.24
step 2297, loss: 3.718382, norm:0.2726, lr:5.8439e-04 dt: 3332.03ms, tok/sec:157347.94
step 2298, loss: 3.740341, norm:0.2634, lr:5.8437e-04 dt: 3332.23ms, tok/sec:157338.58
step 2299, loss: 3.737032, norm:0.2539, lr:5.8435e-04 dt: 3332.37ms, tok/sec:157332.01
validation loss: 3.7358
Model and optimizer state saved.
HellaSwag accuracy:4632027157887140937/-2=-2316013578943570432.0000
rank 1 sample 0: Hello, I'm a language model, and can't be moved.
On my second computer,
I'm an English-language interpreter. The language model
rank 1 sample 1: Hello, I'm a language model, which I have worked with. So if you want to run a simple text, all your web's features are in the
rank 1 sample 2: Hello, I'm a language model, but think it has a lot of things to do. So now we can go back to the model of a machine.
rank 1 sample 3: Hello, I'm a language model, and I'm sure you already know about ELL. These are called OIE. Then every day, I'm going
rank 0 sample 0: Hello, I'm a language model, and I don't want to know whether, the words we're about by ourselves, but then there is another way,
rank 0 sample 1: Hello, I'm a language model, I use it to talk about how we can help out when it should be a part of our daily activity.
Here
rank 0 sample 2: Hello, I'm a language model, but I like this way - as the language itself.
In fact, I was an English teacher and I wanted someone
rank 0 sample 3: Hello, I'm a language model, which would be an example of the
The Maths of the World - I get some examples. We have
not
step 2300, loss: 3.699387, norm:0.2440, lr:5.8433e-04 dt: 56222.02ms, tok/sec:9325.31
step 2301, loss: 3.668551, norm:0.2575, lr:5.8431e-04 dt: 3332.03ms, tok/sec:157347.97
step 2302, loss: 3.681946, norm:0.2761, lr:5.8429e-04 dt: 3332.31ms, tok/sec:157334.67
step 2303, loss: 3.670797, norm:0.2589, lr:5.8427e-04 dt: 3332.21ms, tok/sec:157339.26
step 2304, loss: 3.663706, norm:0.2718, lr:5.8425e-04 dt: 3332.40ms, tok/sec:157330.52
step 2305, loss: 3.711255, norm:0.2611, lr:5.8423e-04 dt: 3332.05ms, tok/sec:157347.15
step 2306, loss: 3.659269, norm:0.2785, lr:5.8421e-04 dt: 3332.08ms, tok/sec:157345.74
step 2307, loss: 3.779309, norm:0.2958, lr:5.8419e-04 dt: 3332.00ms, tok/sec:157349.34
step 2308, loss: 3.700927, norm:0.3103, lr:5.8417e-04 dt: 3331.98ms, tok/sec:157350.16
step 2309, loss: 3.698550, norm:0.3079, lr:5.8415e-04 dt: 3331.98ms, tok/sec:157350.46
step 2310, loss: 3.715011, norm:0.3083, lr:5.8413e-04 dt: 3332.08ms, tok/sec:157345.60
step 2311, loss: 3.512697, norm:0.3087, lr:5.8411e-04 dt: 3332.35ms, tok/sec:157333.02
step 2312, loss: 3.525758, norm:0.3069, lr:5.8409e-04 dt: 3332.26ms, tok/sec:157337.17
step 2313, loss: 3.570864, norm:0.2571, lr:5.8407e-04 dt: 3332.02ms, tok/sec:157348.43
step 2314, loss: 3.544994, norm:0.2913, lr:5.8405e-04 dt: 3331.82ms, tok/sec:157358.05
step 2315, loss: 3.542371, norm:0.2659, lr:5.8403e-04 dt: 3332.22ms, tok/sec:157339.17
step 2316, loss: 3.513980, norm:0.2582, lr:5.8401e-04 dt: 3331.94ms, tok/sec:157352.01
step 2317, loss: 3.532407, norm:0.2811, lr:5.8400e-04 dt: 3332.45ms, tok/sec:157328.21
step 2318, loss: 3.522715, norm:0.2654, lr:5.8398e-04 dt: 3332.20ms, tok/sec:157339.70
step 2319, loss: 3.585703, norm:0.2655, lr:5.8396e-04 dt: 3331.98ms, tok/sec:157350.32
step 2320, loss: 3.532791, norm:0.2586, lr:5.8394e-04 dt: 3332.57ms, tok/sec:157322.24
step 2321, loss: 3.519989, norm:0.2746, lr:5.8392e-04 dt: 3332.00ms, tok/sec:157349.13
step 2322, loss: 3.591544, norm:0.2672, lr:5.8390e-04 dt: 3332.02ms, tok/sec:157348.19
step 2323, loss: 3.733996, norm:0.2752, lr:5.8388e-04 dt: 3332.30ms, tok/sec:157335.24
step 2324, loss: 3.752005, norm:0.2776, lr:5.8386e-04 dt: 3332.23ms, tok/sec:157338.65
step 2325, loss: 3.794641, norm:0.3229, lr:5.8384e-04 dt: 3332.03ms, tok/sec:157347.81
step 2326, loss: 3.774623, norm:0.3089, lr:5.8382e-04 dt: 3332.02ms, tok/sec:157348.30
step 2327, loss: 3.740666, norm:0.3091, lr:5.8380e-04 dt: 3332.27ms, tok/sec:157336.47
step 2328, loss: 3.773032, norm:0.2793, lr:5.8378e-04 dt: 3332.19ms, tok/sec:157340.40
step 2329, loss: 3.746975, norm:0.3057, lr:5.8376e-04 dt: 3332.56ms, tok/sec:157322.96
step 2330, loss: 3.811036, norm:0.2557, lr:5.8374e-04 dt: 3332.13ms, tok/sec:157343.06
step 2331, loss: 3.929117, norm:0.3161, lr:5.8372e-04 dt: 3332.28ms, tok/sec:157336.27
step 2332, loss: 3.772075, norm:0.3419, lr:5.8370e-04 dt: 3332.03ms, tok/sec:157347.84
step 2333, loss: 3.787809, norm:0.3300, lr:5.8368e-04 dt: 3332.33ms, tok/sec:157333.90
step 2334, loss: 3.737572, norm:0.2817, lr:5.8366e-04 dt: 3332.32ms, tok/sec:157334.44
step 2335, loss: 3.683961, norm:0.3043, lr:5.8364e-04 dt: 3332.25ms, tok/sec:157337.32
step 2336, loss: 3.724737, norm:0.2641, lr:5.8362e-04 dt: 3332.26ms, tok/sec:157337.15
step 2337, loss: 3.814433, norm:0.3268, lr:5.8360e-04 dt: 3332.01ms, tok/sec:157349.06
step 2338, loss: 3.758623, norm:0.3283, lr:5.8358e-04 dt: 3332.29ms, tok/sec:157335.69
step 2339, loss: 3.728948, norm:0.2947, lr:5.8356e-04 dt: 3332.22ms, tok/sec:157339.17
step 2340, loss: 3.729911, norm:0.3017, lr:5.8354e-04 dt: 3331.90ms, tok/sec:157354.05
step 2341, loss: 3.719396, norm:0.3182, lr:5.8352e-04 dt: 3331.93ms, tok/sec:157352.45
step 2342, loss: 3.697667, norm:0.2962, lr:5.8350e-04 dt: 3332.00ms, tok/sec:157349.51
step 2343, loss: 3.755806, norm:0.2932, lr:5.8348e-04 dt: 3332.36ms, tok/sec:157332.23
step 2344, loss: 3.723638, norm:0.3162, lr:5.8346e-04 dt: 3332.04ms, tok/sec:157347.46
step 2345, loss: 3.651550, norm:0.2770, lr:5.8344e-04 dt: 3332.35ms, tok/sec:157332.81
step 2346, loss: 3.686301, norm:0.2791, lr:5.8342e-04 dt: 3332.00ms, tok/sec:157349.40
step 2347, loss: 3.719161, norm:0.2586, lr:5.8340e-04 dt: 3332.12ms, tok/sec:157343.62
step 2348, loss: 3.706418, norm:0.2678, lr:5.8338e-04 dt: 3332.23ms, tok/sec:157338.69
step 2349, loss: 3.706117, norm:0.2613, lr:5.8336e-04 dt: 3332.20ms, tok/sec:157340.08
HellaSwag accuracy:4631481250361836617/-2=-2315740625180918272.0000
rank 1 sample 0: Hello, I'm a language model, and a social media speaker.
From a web page about her, I use the site, as a way to get
rank 1 sample 1: Hello, I'm a language model, which is very basic to the computer model.
If students are more than three, take the model for the computer model
rank 1 sample 2: Hello, I'm a language model, so every language can be used to create a language that will stand the best as a language model to the next generations,
rank 1 sample 3: Hello, I'm a language model, and I'm thinking that one of the lessons I talked above was true. Then I told other kids that I wanted to
rank 0 sample 0: Hello, I'm a language model, and I just want the data I need to keep going. I also want my students to find and analyze data. It
rank 0 sample 1: Hello, I'm a language model, but not a language model.
Makota knows well how to design your own, from the traditional, from the
rank 0 sample 2: Hello, I'm a language model, but I see a couple of my questions. If you want an idea of how to do this, please let us know
rank 0 sample 3: Hello, I'm a language model, so in the context of the case where we describe the "what if", the expression was also a result of an underlying
step 2350, loss: 3.660566, norm:0.2625, lr:5.8334e-04 dt: 48517.36ms, tok/sec:10806.19
step 2351, loss: 3.658486, norm:0.2817, lr:5.8332e-04 dt: 3332.26ms, tok/sec:157337.21
step 2352, loss: 3.638397, norm:0.2769, lr:5.8330e-04 dt: 3332.24ms, tok/sec:157338.24
step 2353, loss: 3.675243, norm:0.2657, lr:5.8328e-04 dt: 3332.05ms, tok/sec:157347.05
step 2354, loss: 3.669941, norm:0.2659, lr:5.8326e-04 dt: 3332.21ms, tok/sec:157339.22
step 2355, loss: 3.716295, norm:0.2799, lr:5.8323e-04 dt: 3332.09ms, tok/sec:157344.96
step 2356, loss: 3.571748, norm:0.2684, lr:5.8321e-04 dt: 3332.13ms, tok/sec:157343.18
step 2357, loss: 3.514417, norm:0.2532, lr:5.8319e-04 dt: 3332.14ms, tok/sec:157342.90
step 2358, loss: 3.516446, norm:0.2643, lr:5.8317e-04 dt: 3332.00ms, tok/sec:157349.50
step 2359, loss: 3.449982, norm:0.3058, lr:5.8315e-04 dt: 3332.05ms, tok/sec:157346.99
step 2360, loss: 3.491789, norm:0.2722, lr:5.8313e-04 dt: 3332.13ms, tok/sec:157343.35
step 2361, loss: 3.530309, norm:0.2907, lr:5.8311e-04 dt: 3332.45ms, tok/sec:157328.02
step 2362, loss: 3.533356, norm:0.2619, lr:5.8309e-04 dt: 3332.11ms, tok/sec:157344.08
step 2363, loss: 3.513107, norm:0.2793, lr:5.8307e-04 dt: 3331.87ms, tok/sec:157355.54
step 2364, loss: 3.497010, norm:0.2718, lr:5.8305e-04 dt: 3331.98ms, tok/sec:157350.46
step 2365, loss: 3.511667, norm:0.2833, lr:5.8303e-04 dt: 3332.20ms, tok/sec:157339.93
step 2366, loss: 3.538358, norm:0.2887, lr:5.8301e-04 dt: 3332.30ms, tok/sec:157335.39
step 2367, loss: 3.578803, norm:0.2608, lr:5.8299e-04 dt: 3332.02ms, tok/sec:157348.38
step 2368, loss: 3.705634, norm:0.2721, lr:5.8297e-04 dt: 3331.96ms, tok/sec:157351.34
step 2369, loss: 3.818448, norm:0.2689, lr:5.8295e-04 dt: 3332.19ms, tok/sec:157340.58
step 2370, loss: 3.771956, norm:0.2584, lr:5.8293e-04 dt: 3332.82ms, tok/sec:157310.78
step 2371, loss: 3.806140, norm:0.2875, lr:5.8291e-04 dt: 3332.37ms, tok/sec:157331.84
step 2372, loss: 3.755861, norm:0.2838, lr:5.8289e-04 dt: 3332.14ms, tok/sec:157342.58
step 2373, loss: 3.727990, norm:0.2766, lr:5.8287e-04 dt: 3332.23ms, tok/sec:157338.67
step 2374, loss: 3.765360, norm:0.2657, lr:5.8285e-04 dt: 3332.30ms, tok/sec:157334.97
step 2375, loss: 3.732707, norm:0.2641, lr:5.8283e-04 dt: 3332.24ms, tok/sec:157337.98
step 2376, loss: 3.721403, norm:0.2649, lr:5.8281e-04 dt: 3332.22ms, tok/sec:157338.85
step 2377, loss: 3.865975, norm:0.3484, lr:5.8279e-04 dt: 3332.34ms, tok/sec:157333.42
step 2378, loss: 3.764100, norm:0.3638, lr:5.8277e-04 dt: 3332.24ms, tok/sec:157338.17
step 2379, loss: 3.711962, norm:0.2961, lr:5.8275e-04 dt: 3331.96ms, tok/sec:157351.01
step 2380, loss: 3.661685, norm:0.2704, lr:5.8273e-04 dt: 3332.11ms, tok/sec:157344.23
step 2381, loss: 3.714359, norm:0.2838, lr:5.8270e-04 dt: 3332.15ms, tok/sec:157342.28
step 2382, loss: 3.766844, norm:0.2799, lr:5.8268e-04 dt: 3332.27ms, tok/sec:157336.43
step 2383, loss: 3.719523, norm:0.2727, lr:5.8266e-04 dt: 3332.13ms, tok/sec:157343.07
step 2384, loss: 3.800652, norm:0.2890, lr:5.8264e-04 dt: 3332.13ms, tok/sec:157343.34
step 2385, loss: 3.732229, norm:0.2934, lr:5.8262e-04 dt: 3332.20ms, tok/sec:157339.74
step 2386, loss: 3.677719, norm:0.3022, lr:5.8260e-04 dt: 3332.21ms, tok/sec:157339.23
step 2387, loss: 3.721489, norm:0.2813, lr:5.8258e-04 dt: 3332.08ms, tok/sec:157345.45
step 2388, loss: 3.735861, norm:0.2891, lr:5.8256e-04 dt: 3331.94ms, tok/sec:157352.33
step 2389, loss: 3.729343, norm:0.2626, lr:5.8254e-04 dt: 3332.24ms, tok/sec:157337.90
step 2390, loss: 3.660416, norm:0.2782, lr:5.8252e-04 dt: 3332.18ms, tok/sec:157340.92
step 2391, loss: 3.726151, norm:0.3138, lr:5.8250e-04 dt: 3332.02ms, tok/sec:157348.39
step 2392, loss: 3.700520, norm:0.2954, lr:5.8248e-04 dt: 3332.26ms, tok/sec:157337.26
step 2393, loss: 3.717309, norm:0.2768, lr:5.8246e-04 dt: 3332.19ms, tok/sec:157340.23
step 2394, loss: 3.633845, norm:0.2762, lr:5.8244e-04 dt: 3332.54ms, tok/sec:157324.01
step 2395, loss: 3.656209, norm:0.2718, lr:5.8242e-04 dt: 3332.13ms, tok/sec:157343.15
step 2396, loss: 3.685600, norm:0.2802, lr:5.8240e-04 dt: 3332.15ms, tok/sec:157342.26
step 2397, loss: 3.606409, norm:0.2519, lr:5.8237e-04 dt: 3332.04ms, tok/sec:157347.55
step 2398, loss: 3.619041, norm:0.2606, lr:5.8235e-04 dt: 3332.27ms, tok/sec:157336.67
step 2399, loss: 3.661441, norm:0.2689, lr:5.8233e-04 dt: 3332.15ms, tok/sec:157342.19
validation loss: 3.7209
Model and optimizer state saved.
HellaSwag accuracy:-4602020018034375607/-2=2301010009017187840.0000
rank 1 sample 0: Hello, I'm a language model, for instance.
A. (What does ‘unclear’ mean? It’s an operation that
rank 1 sample 1: Hello, I'm a language model, which I've called to say is, where I know and is this language and is being used as the language model.
rank 1 sample 2: Hello, I'm a language model, but on this, I'm not a language model. So now we can talk about the basics for how we use the
rank 1 sample 3: Hello, I'm a language model, and I'm using the tools that I learn to program more complex numbers by being able to think of numbers in more complex
rank 0 sample 0: Hello, I'm a language model, and I have a very simple, straightforward tool you can use to do things with your family. The best advice is the
rank 0 sample 1: Hello, I'm a language model, I need to find the perfect one to understand it here. I need to do this with an easy step, and I
rank 0 sample 2: Hello, I'm a language model, I'm kind. Well, I don't necessarily like it because we're not necessarily going to be classed as this
rank 0 sample 3: Hello, I'm a language model, I used to think about what it's about today. I'm interested in what that meant and how it was connected over
step 2400, loss: 3.705204, norm:0.2845, lr:5.8231e-04 dt: 56472.10ms, tok/sec:9284.02
step 2401, loss: 3.641854, norm:0.2572, lr:5.8229e-04 dt: 3332.22ms, tok/sec:157339.00
step 2402, loss: 3.468169, norm:0.2632, lr:5.8227e-04 dt: 3332.00ms, tok/sec:157349.44
step 2403, loss: 3.450972, norm:0.2594, lr:5.8225e-04 dt: 3332.10ms, tok/sec:157344.81
step 2404, loss: 3.571684, norm:0.2968, lr:5.8223e-04 dt: 3332.13ms, tok/sec:157343.42
step 2405, loss: 3.492288, norm:0.2863, lr:5.8221e-04 dt: 3332.30ms, tok/sec:157335.08
step 2406, loss: 3.418830, norm:0.2896, lr:5.8219e-04 dt: 3331.92ms, tok/sec:157353.16
step 2407, loss: 3.476134, norm:0.2808, lr:5.8217e-04 dt: 3332.09ms, tok/sec:157345.06
step 2408, loss: 3.475011, norm:0.2824, lr:5.8215e-04 dt: 3332.21ms, tok/sec:157339.24
step 2409, loss: 3.520906, norm:0.2820, lr:5.8213e-04 dt: 3332.21ms, tok/sec:157339.40
step 2410, loss: 3.496832, norm:0.2454, lr:5.8210e-04 dt: 3331.93ms, tok/sec:157352.58
step 2411, loss: 3.511597, norm:0.2856, lr:5.8208e-04 dt: 3331.95ms, tok/sec:157351.78
step 2412, loss: 3.553359, norm:0.2506, lr:5.8206e-04 dt: 3332.24ms, tok/sec:157337.81
step 2413, loss: 3.589667, norm:0.2439, lr:5.8204e-04 dt: 3332.10ms, tok/sec:157344.39
step 2414, loss: 3.713150, norm:0.2757, lr:5.8202e-04 dt: 3332.51ms, tok/sec:157325.14
step 2415, loss: 3.763913, norm:0.2729, lr:5.8200e-04 dt: 3333.44ms, tok/sec:157281.16
step 2416, loss: 3.685623, norm:0.3095, lr:5.8198e-04 dt: 3332.66ms, tok/sec:157318.28
step 2417, loss: 3.721143, norm:0.2945, lr:5.8196e-04 dt: 3332.33ms, tok/sec:157333.66
step 2418, loss: 3.697433, norm:0.3434, lr:5.8194e-04 dt: 3331.98ms, tok/sec:157350.26
step 2419, loss: 3.693913, norm:0.2923, lr:5.8192e-04 dt: 3332.08ms, tok/sec:157345.48
step 2420, loss: 3.668076, norm:0.2946, lr:5.8189e-04 dt: 3332.26ms, tok/sec:157337.03
step 2421, loss: 3.739342, norm:0.2581, lr:5.8187e-04 dt: 3332.47ms, tok/sec:157327.00
step 2422, loss: 3.695777, norm:0.3075, lr:5.8185e-04 dt: 3332.16ms, tok/sec:157341.69
step 2423, loss: 3.740927, norm:0.2812, lr:5.8183e-04 dt: 3331.97ms, tok/sec:157350.86
step 2424, loss: 3.714760, norm:0.2759, lr:5.8181e-04 dt: 3332.15ms, tok/sec:157342.46
step 2425, loss: 3.719125, norm:0.2906, lr:5.8179e-04 dt: 3332.45ms, tok/sec:157327.95
step 2426, loss: 3.768052, norm:0.2760, lr:5.8177e-04 dt: 3332.00ms, tok/sec:157349.25
step 2427, loss: 3.711066, norm:0.3083, lr:5.8175e-04 dt: 3331.97ms, tok/sec:157350.69
step 2428, loss: 3.718342, norm:0.3311, lr:5.8173e-04 dt: 3332.32ms, tok/sec:157334.35
step 2429, loss: 3.725361, norm:0.2908, lr:5.8171e-04 dt: 3332.04ms, tok/sec:157347.24
step 2430, loss: 3.743874, norm:0.2772, lr:5.8168e-04 dt: 3332.46ms, tok/sec:157327.76
step 2431, loss: 3.728430, norm:0.2961, lr:5.8166e-04 dt: 3332.05ms, tok/sec:157346.79
step 2432, loss: 3.713101, norm:0.2881, lr:5.8164e-04 dt: 3332.11ms, tok/sec:157344.05
step 2433, loss: 3.744094, norm:0.3368, lr:5.8162e-04 dt: 3332.26ms, tok/sec:157336.93
step 2434, loss: 3.799253, norm:0.4114, lr:5.8160e-04 dt: 3332.04ms, tok/sec:157347.39
step 2435, loss: 3.741907, norm:0.3223, lr:5.8158e-04 dt: 3332.27ms, tok/sec:157336.66
step 2436, loss: 3.731798, norm:0.3394, lr:5.8156e-04 dt: 3332.11ms, tok/sec:157344.19
step 2437, loss: 3.700772, norm:0.3039, lr:5.8154e-04 dt: 3332.44ms, tok/sec:157328.42
step 2438, loss: 3.706020, norm:0.2847, lr:5.8152e-04 dt: 3332.00ms, tok/sec:157349.40
step 2439, loss: 3.672471, norm:0.2897, lr:5.8149e-04 dt: 3332.07ms, tok/sec:157346.21
step 2440, loss: 3.646011, norm:0.3077, lr:5.8147e-04 dt: 3332.05ms, tok/sec:157346.75
step 2441, loss: 3.664335, norm:0.3151, lr:5.8145e-04 dt: 3332.03ms, tok/sec:157347.70
step 2442, loss: 3.639582, norm:0.2515, lr:5.8143e-04 dt: 3332.20ms, tok/sec:157339.68
step 2443, loss: 3.655839, norm:0.2874, lr:5.8141e-04 dt: 3332.16ms, tok/sec:157341.99
step 2444, loss: 3.701149, norm:0.3006, lr:5.8139e-04 dt: 3332.05ms, tok/sec:157347.13
step 2445, loss: 3.699275, norm:0.2510, lr:5.8137e-04 dt: 3332.16ms, tok/sec:157341.89
step 2446, loss: 3.669211, norm:0.2588, lr:5.8135e-04 dt: 3332.66ms, tok/sec:157317.94
step 2447, loss: 3.717992, norm:0.2622, lr:5.8132e-04 dt: 3332.16ms, tok/sec:157342.00
step 2448, loss: 3.607933, norm:0.2821, lr:5.8130e-04 dt: 3331.92ms, tok/sec:157353.22
step 2449, loss: 3.596656, norm:0.2550, lr:5.8128e-04 dt: 3332.39ms, tok/sec:157330.83
HellaSwag accuracy:-6898295628968508335/-2=3449147814484254208.0000
rank 1 sample 0: Hello, I'm a language model, an instruction that I would need to perform. That means I use the language model to solve this equation. If I want
rank 1 sample 1: Hello, I'm a language model, which I have set up to show on at the beginning it's set up for you the size of your browser. I
rank 1 sample 2: Hello, I'm a language model, I might have used it to model the new language. It doesn't matter if you're looking at an English speaking student
rank 1 sample 3: Hello, I'm a language model, and I'm trying to add that to P.E I don't see yet. A bunch of people thought of the
rank 0 sample 0: Hello, I'm a language model, and I know that I was looking for books with all the books at Oxford to help students see themselves. They are great
rank 0 sample 1: Hello, I'm a language model, this seems like a very, very beautiful language to play: it can always be a good fit to get you started in
rank 0 sample 2: Hello, I'm a language model, but I wanted to build more understanding for the project.
We were working on a new project that was working with two
rank 0 sample 3: Hello, I'm a language model, and one of the most important tools I can glean from this is learning from a local perspective. I find it very attractive
step 2450, loss: 3.529520, norm:0.2682, lr:5.8126e-04 dt: 48512.78ms, tok/sec:10807.22
step 2451, loss: 3.479518, norm:0.2642, lr:5.8124e-04 dt: 3331.79ms, tok/sec:157359.22
step 2452, loss: 3.505976, norm:0.2763, lr:5.8122e-04 dt: 3332.20ms, tok/sec:157340.07
step 2453, loss: 3.531869, norm:0.2853, lr:5.8120e-04 dt: 3332.07ms, tok/sec:157346.06
step 2454, loss: 3.572157, norm:0.2540, lr:5.8117e-04 dt: 3332.38ms, tok/sec:157331.30
step 2455, loss: 3.452500, norm:0.2856, lr:5.8115e-04 dt: 3331.99ms, tok/sec:157349.81
step 2456, loss: 3.517663, norm:0.2915, lr:5.8113e-04 dt: 3332.04ms, tok/sec:157347.31
step 2457, loss: 3.458275, norm:0.2809, lr:5.8111e-04 dt: 3332.12ms, tok/sec:157343.77
step 2458, loss: 3.490567, norm:0.2620, lr:5.8109e-04 dt: 3332.04ms, tok/sec:157347.46
step 2459, loss: 3.555336, norm:0.3085, lr:5.8107e-04 dt: 3331.78ms, tok/sec:157359.82
step 2460, loss: 3.466270, norm:0.2592, lr:5.8105e-04 dt: 3332.17ms, tok/sec:157341.24
step 2461, loss: 3.601473, norm:0.3004, lr:5.8102e-04 dt: 3332.30ms, tok/sec:157335.31
step 2462, loss: 3.719368, norm:0.2547, lr:5.8100e-04 dt: 3331.98ms, tok/sec:157350.28
step 2463, loss: 3.695242, norm:0.2861, lr:5.8098e-04 dt: 3332.07ms, tok/sec:157346.24
step 2464, loss: 3.697585, norm:0.2877, lr:5.8096e-04 dt: 3332.10ms, tok/sec:157344.51
step 2465, loss: 3.771502, norm:0.2947, lr:5.8094e-04 dt: 3332.04ms, tok/sec:157347.54
step 2466, loss: 3.677570, norm:0.2657, lr:5.8092e-04 dt: 3332.15ms, tok/sec:157342.02
step 2467, loss: 3.717666, norm:0.2965, lr:5.8090e-04 dt: 3332.24ms, tok/sec:157337.98
step 2468, loss: 3.731843, norm:0.2846, lr:5.8087e-04 dt: 3332.20ms, tok/sec:157339.85
step 2469, loss: 3.703323, norm:0.2712, lr:5.8085e-04 dt: 3332.03ms, tok/sec:157347.79
step 2470, loss: 3.671842, norm:0.2720, lr:5.8083e-04 dt: 3331.96ms, tok/sec:157351.24
step 2471, loss: 3.704501, norm:0.2645, lr:5.8081e-04 dt: 3332.33ms, tok/sec:157333.72
step 2472, loss: 3.708234, norm:0.2798, lr:5.8079e-04 dt: 3332.02ms, tok/sec:157348.40
step 2473, loss: 3.700948, norm:0.2954, lr:5.8077e-04 dt: 3332.11ms, tok/sec:157344.27
step 2474, loss: 3.705570, norm:0.2929, lr:5.8074e-04 dt: 3332.26ms, tok/sec:157336.86
step 2475, loss: 3.724832, norm:0.3083, lr:5.8072e-04 dt: 3331.94ms, tok/sec:157352.35
step 2476, loss: 3.736873, norm:0.3136, lr:5.8070e-04 dt: 3334.82ms, tok/sec:157216.06
step 2477, loss: 3.677131, norm:0.3159, lr:5.8068e-04 dt: 3332.17ms, tok/sec:157341.15
step 2478, loss: 3.707209, norm:0.3136, lr:5.8066e-04 dt: 3332.29ms, tok/sec:157335.80
step 2479, loss: 3.715168, norm:0.2889, lr:5.8064e-04 dt: 3332.10ms, tok/sec:157344.71
step 2480, loss: 3.717762, norm:0.2599, lr:5.8061e-04 dt: 3332.15ms, tok/sec:157342.18
step 2481, loss: 3.683811, norm:0.2442, lr:5.8059e-04 dt: 3332.03ms, tok/sec:157347.89
step 2482, loss: 3.736328, norm:0.4061, lr:5.8057e-04 dt: 3332.08ms, tok/sec:157345.69
step 2483, loss: 3.709590, norm:0.3940, lr:5.8055e-04 dt: 3331.85ms, tok/sec:157356.64
step 2484, loss: 3.684446, norm:0.3772, lr:5.8053e-04 dt: 3332.08ms, tok/sec:157345.45
step 2485, loss: 3.640824, norm:0.3241, lr:5.8051e-04 dt: 3331.86ms, tok/sec:157356.14
step 2486, loss: 3.647457, norm:0.3041, lr:5.8048e-04 dt: 3331.97ms, tok/sec:157350.94
step 2487, loss: 3.676915, norm:0.2825, lr:5.8046e-04 dt: 3332.09ms, tok/sec:157344.88
step 2488, loss: 3.675142, norm:0.2819, lr:5.8044e-04 dt: 3332.12ms, tok/sec:157343.56
step 2489, loss: 3.628427, norm:0.2734, lr:5.8042e-04 dt: 3332.27ms, tok/sec:157336.66
step 2490, loss: 3.702586, norm:0.2814, lr:5.8040e-04 dt: 3332.08ms, tok/sec:157345.49
step 2491, loss: 3.579062, norm:0.2799, lr:5.8037e-04 dt: 3332.03ms, tok/sec:157347.97
step 2492, loss: 3.694527, norm:0.2781, lr:5.8035e-04 dt: 3332.03ms, tok/sec:157347.75
step 2493, loss: 3.704032, norm:0.3090, lr:5.8033e-04 dt: 3332.04ms, tok/sec:157347.40
step 2494, loss: 3.669199, norm:0.2919, lr:5.8031e-04 dt: 3332.14ms, tok/sec:157342.93
step 2495, loss: 3.664353, norm:0.2942, lr:5.8029e-04 dt: 3332.08ms, tok/sec:157345.65
step 2496, loss: 3.559850, norm:0.2933, lr:5.8027e-04 dt: 3332.30ms, tok/sec:157335.33
step 2497, loss: 3.443933, norm:0.2985, lr:5.8024e-04 dt: 3332.15ms, tok/sec:157342.21
step 2498, loss: 3.505568, norm:0.2792, lr:5.8022e-04 dt: 3332.37ms, tok/sec:157332.10
step 2499, loss: 3.518563, norm:0.2671, lr:5.8020e-04 dt: 3332.27ms, tok/sec:157336.45
validation loss: 3.7091
Model and optimizer state saved.
HellaSwag accuracy:4631481286871188553/-2=-2315740643435594240.0000
rank 1 sample 0: Hello, I'm a language model, as it is a language model. Most people understand this language before they even make it in English or in the language of
rank 1 sample 1: Hello, I'm a language model, so it's simple.
But how will you think what a big difference if you haven't managed it?
I
rank 1 sample 2: Hello, I'm a language model, I prefer a more traditional language model. I'm learning the vocabulary and I'm learning the grammar skills of the native speakers
rank 1 sample 3: Hello, I'm a language model, and I'm looking forward to going down where I found things wrong. Thanks - I still wanted to make the model more
rank 0 sample 0: Hello, I'm a language model, and I know that it's so useful - as long as it makes your head look very short at it, and then
rank 0 sample 1: Hello, I'm a language model, so please take a look at this
- I could play with you a game of
- You play
- You
rank 0 sample 2: Hello, I'm a language model, I'm fluent, meaning this is just a question, and there you need to write a question.
I've put
rank 0 sample 3: Hello, I'm a language model, so in this lesson, we will create an exercise in which we can build on new vocabulary from a standard English speaking context
step 2500, loss: 3.546836, norm:0.2719, lr:5.8018e-04 dt: 56394.96ms, tok/sec:9296.72
step 2501, loss: 3.518445, norm:0.2616, lr:5.8016e-04 dt: 3332.06ms, tok/sec:157346.38
step 2502, loss: 3.510920, norm:0.2663, lr:5.8013e-04 dt: 3332.29ms, tok/sec:157335.51
step 2503, loss: 3.559432, norm:0.2557, lr:5.8011e-04 dt: 3332.43ms, tok/sec:157329.14
step 2504, loss: 3.538802, norm:0.2745, lr:5.8009e-04 dt: 3331.91ms, tok/sec:157353.71
step 2505, loss: 3.458218, norm:0.3127, lr:5.8007e-04 dt: 3331.96ms, tok/sec:157351.42
step 2506, loss: 3.525344, norm:0.3060, lr:5.8005e-04 dt: 3332.19ms, tok/sec:157340.44
step 2507, loss: 3.493953, norm:0.2870, lr:5.8002e-04 dt: 3332.18ms, tok/sec:157340.88
step 2508, loss: 3.661866, norm:0.2803, lr:5.8000e-04 dt: 3332.41ms, tok/sec:157329.82
step 2509, loss: 3.706160, norm:0.2907, lr:5.7998e-04 dt: 3332.18ms, tok/sec:157340.70
step 2510, loss: 3.708299, norm:0.3700, lr:5.7996e-04 dt: 3332.18ms, tok/sec:157340.95
step 2511, loss: 3.736343, norm:0.3396, lr:5.7994e-04 dt: 3332.38ms, tok/sec:157331.59
step 2512, loss: 3.693556, norm:0.3100, lr:5.7991e-04 dt: 3332.36ms, tok/sec:157332.18
step 2513, loss: 3.695070, norm:0.2818, lr:5.7989e-04 dt: 3332.03ms, tok/sec:157347.84
step 2514, loss: 3.691303, norm:0.2738, lr:5.7987e-04 dt: 3332.18ms, tok/sec:157340.86
step 2515, loss: 3.668816, norm:0.2721, lr:5.7985e-04 dt: 3332.18ms, tok/sec:157340.87
step 2516, loss: 3.705318, norm:0.2904, lr:5.7983e-04 dt: 3331.90ms, tok/sec:157354.02
step 2517, loss: 3.696644, norm:0.2719, lr:5.7980e-04 dt: 3332.12ms, tok/sec:157343.72
step 2518, loss: 3.693818, norm:0.2840, lr:5.7978e-04 dt: 3332.11ms, tok/sec:157343.96
step 2519, loss: 3.705300, norm:0.2645, lr:5.7976e-04 dt: 3332.30ms, tok/sec:157335.09
step 2520, loss: 3.688231, norm:0.2675, lr:5.7974e-04 dt: 3332.15ms, tok/sec:157342.05
step 2521, loss: 3.674535, norm:0.2675, lr:5.7971e-04 dt: 3332.40ms, tok/sec:157330.25
step 2522, loss: 3.751595, norm:0.2678, lr:5.7969e-04 dt: 3332.32ms, tok/sec:157334.28
step 2523, loss: 3.653363, norm:0.2644, lr:5.7967e-04 dt: 3332.17ms, tok/sec:157341.23
step 2524, loss: 3.693846, norm:0.2917, lr:5.7965e-04 dt: 3331.87ms, tok/sec:157355.60
step 2525, loss: 3.726393, norm:0.2967, lr:5.7963e-04 dt: 3332.15ms, tok/sec:157342.39
step 2526, loss: 3.807006, norm:0.2885, lr:5.7960e-04 dt: 3331.93ms, tok/sec:157352.76
step 2527, loss: 3.693966, norm:0.3066, lr:5.7958e-04 dt: 3332.14ms, tok/sec:157342.62
step 2528, loss: 3.686195, norm:0.2815, lr:5.7956e-04 dt: 3332.32ms, tok/sec:157334.31
step 2529, loss: 3.711431, norm:0.3086, lr:5.7954e-04 dt: 3331.82ms, tok/sec:157357.82
step 2530, loss: 3.700866, norm:0.3034, lr:5.7951e-04 dt: 3332.39ms, tok/sec:157330.96
step 2531, loss: 3.688568, norm:0.2926, lr:5.7949e-04 dt: 3332.20ms, tok/sec:157339.84
step 2532, loss: 3.646137, norm:0.3036, lr:5.7947e-04 dt: 3331.87ms, tok/sec:157355.70
step 2533, loss: 3.625102, norm:0.2584, lr:5.7945e-04 dt: 3332.02ms, tok/sec:157348.47
step 2534, loss: 3.650961, norm:0.2755, lr:5.7943e-04 dt: 3332.01ms, tok/sec:157348.85
step 2535, loss: 3.576452, norm:0.2523, lr:5.7940e-04 dt: 3332.26ms, tok/sec:157336.93
step 2536, loss: 3.611754, norm:0.2699, lr:5.7938e-04 dt: 3332.08ms, tok/sec:157345.76
step 2537, loss: 3.629163, norm:0.2495, lr:5.7936e-04 dt: 3331.94ms, tok/sec:157352.00
step 2538, loss: 3.684618, norm:0.2819, lr:5.7934e-04 dt: 3331.97ms, tok/sec:157350.60
step 2539, loss: 3.666641, norm:0.2671, lr:5.7931e-04 dt: 3332.23ms, tok/sec:157338.62
step 2540, loss: 3.598548, norm:0.2484, lr:5.7929e-04 dt: 3332.47ms, tok/sec:157327.07
step 2541, loss: 3.635339, norm:0.2496, lr:5.7927e-04 dt: 3332.33ms, tok/sec:157333.92
step 2542, loss: 3.558514, norm:0.2540, lr:5.7925e-04 dt: 3332.19ms, tok/sec:157340.24
step 2543, loss: 3.427802, norm:0.2405, lr:5.7922e-04 dt: 3332.16ms, tok/sec:157341.86
step 2544, loss: 3.461338, norm:0.2600, lr:5.7920e-04 dt: 3331.92ms, tok/sec:157353.10
step 2545, loss: 3.477267, norm:0.2707, lr:5.7918e-04 dt: 3332.19ms, tok/sec:157340.50
step 2546, loss: 3.479651, norm:0.2678, lr:5.7916e-04 dt: 3332.06ms, tok/sec:157346.65
step 2547, loss: 3.498362, norm:0.2787, lr:5.7913e-04 dt: 3331.97ms, tok/sec:157350.98
step 2548, loss: 3.498063, norm:0.2654, lr:5.7911e-04 dt: 3331.92ms, tok/sec:157352.96
step 2549, loss: 3.465097, norm:0.2725, lr:5.7909e-04 dt: 3332.40ms, tok/sec:157330.53
HellaSwag accuracy:-4591750049002486711/-2=2295875024501243392.0000
rank 1 sample 0: Hello, I'm a language model, the first step is to create a file, as shown in “The New York Times: the New York Times�
rank 1 sample 1: Hello, I'm a language model, which I'm here to help you get started.
When you create a program that requires 3 bytes to run, you
rank 1 sample 2: Hello, I'm a language model, it gets to say that the language is not a language that people have been used to. I don't think of a
rank 1 sample 3: Hello, I'm a language model, and I'm very proud of how that supports the whole philosophy of social communication because it covers grammar, grammar, and grammar
rank 0 sample 0: Hello, I'm a language model, and I'm a teacher with some training about working with your students who come in to teach for my first semester. This
rank 0 sample 1: Hello, I'm a language model, but for sure, it's just this one: one has a different object in the image (or some other object in
rank 0 sample 2: Hello, I'm a language model, but I could only refer to the actual language itself.
This article is a brief summary of the two-dimensional relationships
rank 0 sample 3: Hello, I'm a language model, and an API is a simple and easy language builder.
From above, we think of any language or API, any
step 2550, loss: 3.516535, norm:0.2804, lr:5.7907e-04 dt: 48514.42ms, tok/sec:10806.85
step 2551, loss: 3.533212, norm:0.2911, lr:5.7904e-04 dt: 3332.53ms, tok/sec:157324.43
step 2552, loss: 3.462303, norm:0.2804, lr:5.7902e-04 dt: 3332.04ms, tok/sec:157347.37
step 2553, loss: 3.446844, norm:0.2730, lr:5.7900e-04 dt: 3331.78ms, tok/sec:157359.56
step 2554, loss: 3.624133, norm:0.2592, lr:5.7898e-04 dt: 3332.18ms, tok/sec:157340.85
step 2555, loss: 3.756766, norm:0.2694, lr:5.7895e-04 dt: 3332.05ms, tok/sec:157346.84
step 2556, loss: 3.716755, norm:0.3005, lr:5.7893e-04 dt: 3332.07ms, tok/sec:157346.06
step 2557, loss: 3.728949, norm:0.2886, lr:5.7891e-04 dt: 3331.90ms, tok/sec:157353.99
step 2558, loss: 3.751611, norm:0.3441, lr:5.7889e-04 dt: 3332.06ms, tok/sec:157346.65
step 2559, loss: 3.765465, norm:0.3581, lr:5.7886e-04 dt: 3332.26ms, tok/sec:157336.89
step 2560, loss: 3.732273, norm:0.3273, lr:5.7884e-04 dt: 3332.63ms, tok/sec:157319.56
step 2561, loss: 3.673190, norm:0.2992, lr:5.7882e-04 dt: 3331.99ms, tok/sec:157349.64
step 2562, loss: 3.671020, norm:0.2993, lr:5.7880e-04 dt: 3332.07ms, tok/sec:157346.25
step 2563, loss: 3.720432, norm:0.2726, lr:5.7877e-04 dt: 3332.06ms, tok/sec:157346.38
step 2564, loss: 3.718533, norm:0.2875, lr:5.7875e-04 dt: 3332.22ms, tok/sec:157338.73
step 2565, loss: 3.761916, norm:0.3019, lr:5.7873e-04 dt: 3332.20ms, tok/sec:157340.12
step 2566, loss: 3.671176, norm:0.3251, lr:5.7870e-04 dt: 3332.08ms, tok/sec:157345.38
step 2567, loss: 3.707627, norm:0.3087, lr:5.7868e-04 dt: 3332.24ms, tok/sec:157338.19
step 2568, loss: 3.713171, norm:0.2972, lr:5.7866e-04 dt: 3332.34ms, tok/sec:157333.11
step 2569, loss: 3.687552, norm:0.2797, lr:5.7864e-04 dt: 3332.05ms, tok/sec:157347.04
step 2570, loss: 3.674026, norm:0.2833, lr:5.7861e-04 dt: 3332.26ms, tok/sec:157337.25
step 2571, loss: 3.735082, norm:0.2803, lr:5.7859e-04 dt: 3332.08ms, tok/sec:157345.39
step 2572, loss: 3.667792, norm:0.2927, lr:5.7857e-04 dt: 3332.05ms, tok/sec:157346.82
step 2573, loss: 3.725216, norm:0.2741, lr:5.7855e-04 dt: 3332.09ms, tok/sec:157345.31
step 2574, loss: 3.729340, norm:0.2827, lr:5.7852e-04 dt: 3332.03ms, tok/sec:157347.70
step 2575, loss: 3.683628, norm:0.2681, lr:5.7850e-04 dt: 3332.13ms, tok/sec:157343.20
step 2576, loss: 3.624672, norm:0.2987, lr:5.7848e-04 dt: 3332.33ms, tok/sec:157333.72
step 2577, loss: 3.691453, norm:0.2732, lr:5.7845e-04 dt: 3332.36ms, tok/sec:157332.16
step 2578, loss: 3.682355, norm:0.3100, lr:5.7843e-04 dt: 3332.33ms, tok/sec:157333.93
step 2579, loss: 3.634393, norm:0.3126, lr:5.7841e-04 dt: 3332.08ms, tok/sec:157345.42
step 2580, loss: 3.619822, norm:0.2988, lr:5.7839e-04 dt: 3332.03ms, tok/sec:157347.90
step 2581, loss: 3.609520, norm:0.3008, lr:5.7836e-04 dt: 3332.04ms, tok/sec:157347.29
step 2582, loss: 3.621377, norm:0.2892, lr:5.7834e-04 dt: 3331.92ms, tok/sec:157353.28
step 2583, loss: 3.609119, norm:0.2965, lr:5.7832e-04 dt: 3332.10ms, tok/sec:157344.39
step 2584, loss: 3.609309, norm:0.3018, lr:5.7829e-04 dt: 3331.93ms, tok/sec:157352.75
step 2585, loss: 3.612089, norm:0.2866, lr:5.7827e-04 dt: 3332.07ms, tok/sec:157345.98
step 2586, loss: 3.618030, norm:0.2706, lr:5.7825e-04 dt: 3332.73ms, tok/sec:157314.68
step 2587, loss: 3.628134, norm:0.2653, lr:5.7823e-04 dt: 3332.16ms, tok/sec:157341.80
step 2588, loss: 3.625268, norm:0.2664, lr:5.7820e-04 dt: 3332.47ms, tok/sec:157327.17
step 2589, loss: 3.542207, norm:0.2628, lr:5.7818e-04 dt: 3332.12ms, tok/sec:157343.65
step 2590, loss: 3.510076, norm:0.2754, lr:5.7816e-04 dt: 3331.74ms, tok/sec:157361.43
step 2591, loss: 3.524802, norm:0.2770, lr:5.7813e-04 dt: 3331.97ms, tok/sec:157350.55
step 2592, loss: 3.485101, norm:0.2602, lr:5.7811e-04 dt: 3332.06ms, tok/sec:157346.49
step 2593, loss: 3.638732, norm:0.2785, lr:5.7809e-04 dt: 3332.20ms, tok/sec:157340.12
step 2594, loss: 3.440176, norm:0.3097, lr:5.7806e-04 dt: 3332.12ms, tok/sec:157343.89
step 2595, loss: 3.426929, norm:0.2997, lr:5.7804e-04 dt: 3332.18ms, tok/sec:157340.85
step 2596, loss: 3.435246, norm:0.2642, lr:5.7802e-04 dt: 3332.35ms, tok/sec:157333.00
step 2597, loss: 3.486988, norm:0.2521, lr:5.7800e-04 dt: 3332.13ms, tok/sec:157343.30
step 2598, loss: 3.453181, norm:0.2842, lr:5.7797e-04 dt: 3331.98ms, tok/sec:157350.42
step 2599, loss: 3.477109, norm:0.2524, lr:5.7795e-04 dt: 3332.34ms, tok/sec:157333.29
validation loss: 3.6933
Model and optimizer state saved.
HellaSwag accuracy:-4591750046854970295/-2=2295875023427485184.0000
rank 1 sample 0: Hello, I'm a language model,
so what is the best way when it happens when you learn to write things? How many pages of books are there
rank 1 sample 1: Hello, I'm a language model, it is very exciting to see a great diversity of ideas you can draw on as a researcher and be a researcher.

rank 1 sample 2: Hello, I'm a language model, a great resource for learning about the language of the house of Jesus, a church, and a family in a country where
rank 1 sample 3: Hello, I'm a language model, and I'm interested to create a neural engineer. I feel that's important enough to come up with a neural engineer.
rank 0 sample 0: Hello, I'm a language model, and I really want a free version of things with more than just grammar you've got here. But what's the right
rank 0 sample 1: Hello, I'm a language model, I really want to know what you need to understand a program, so try to think through some examples if you want to
rank 0 sample 2: Hello, I'm a language model, I'm doing my best to ensure it's relevant to me so we can make more informed decisions.
I think for
rank 0 sample 3: Hello, I'm a language model, and how to use it. I’m a language model. I think this is about the context, how an
step 2600, loss: 3.478891, norm:0.2493, lr:5.7793e-04 dt: 56166.17ms, tok/sec:9334.59
step 2601, loss: 3.599214, norm:0.2726, lr:5.7790e-04 dt: 3332.26ms, tok/sec:157337.14
step 2602, loss: 3.690978, norm:0.2657, lr:5.7788e-04 dt: 3332.36ms, tok/sec:157332.29
step 2603, loss: 3.642911, norm:0.2483, lr:5.7786e-04 dt: 3332.06ms, tok/sec:157346.38
step 2604, loss: 3.777839, norm:0.2813, lr:5.7783e-04 dt: 3332.06ms, tok/sec:157346.73
step 2605, loss: 3.705448, norm:0.3308, lr:5.7781e-04 dt: 3332.12ms, tok/sec:157343.53
step 2606, loss: 3.805644, norm:0.2939, lr:5.7779e-04 dt: 3332.23ms, tok/sec:157338.35
step 2607, loss: 3.713124, norm:0.3038, lr:5.7776e-04 dt: 3331.93ms, tok/sec:157352.64
step 2608, loss: 3.698200, norm:0.3112, lr:5.7774e-04 dt: 3331.97ms, tok/sec:157350.61
step 2609, loss: 3.704842, norm:0.3143, lr:5.7772e-04 dt: 3332.34ms, tok/sec:157333.39
step 2610, loss: 3.699428, norm:0.3428, lr:5.7769e-04 dt: 3332.14ms, tok/sec:157342.93
step 2611, loss: 3.705394, norm:0.2978, lr:5.7767e-04 dt: 3332.49ms, tok/sec:157326.22
step 2612, loss: 3.697239, norm:0.2914, lr:5.7765e-04 dt: 3332.17ms, tok/sec:157341.29
step 2613, loss: 3.736625, norm:0.3122, lr:5.7762e-04 dt: 3332.17ms, tok/sec:157341.40
step 2614, loss: 3.732448, norm:0.3268, lr:5.7760e-04 dt: 3331.94ms, tok/sec:157352.11
step 2615, loss: 3.746978, norm:0.2984, lr:5.7758e-04 dt: 3332.27ms, tok/sec:157336.44
step 2616, loss: 3.702276, norm:0.3192, lr:5.7756e-04 dt: 3332.00ms, tok/sec:157349.46
step 2617, loss: 3.652771, norm:0.2993, lr:5.7753e-04 dt: 3332.10ms, tok/sec:157344.53
step 2618, loss: 3.682500, norm:0.3041, lr:5.7751e-04 dt: 3332.22ms, tok/sec:157339.07
step 2619, loss: 3.705125, norm:0.3118, lr:5.7749e-04 dt: 3331.91ms, tok/sec:157353.59
step 2620, loss: 3.768916, norm:0.3269, lr:5.7746e-04 dt: 3332.47ms, tok/sec:157327.14
step 2621, loss: 3.733292, norm:0.4077, lr:5.7744e-04 dt: 3332.01ms, tok/sec:157349.06
step 2622, loss: 3.691358, norm:0.2852, lr:5.7742e-04 dt: 3332.02ms, tok/sec:157348.61
step 2623, loss: 3.753497, norm:0.2858, lr:5.7739e-04 dt: 3332.19ms, tok/sec:157340.40
step 2624, loss: 3.656776, norm:0.2757, lr:5.7737e-04 dt: 3332.00ms, tok/sec:157349.15
step 2625, loss: 3.658831, norm:0.2433, lr:5.7735e-04 dt: 3332.15ms, tok/sec:157342.32
step 2626, loss: 3.664526, norm:0.2734, lr:5.7732e-04 dt: 3332.23ms, tok/sec:157338.46
step 2627, loss: 3.619949, norm:0.2496, lr:5.7730e-04 dt: 3332.07ms, tok/sec:157345.92
step 2628, loss: 3.609759, norm:0.2575, lr:5.7727e-04 dt: 3333.35ms, tok/sec:157285.79
step 2629, loss: 3.604948, norm:0.2591, lr:5.7725e-04 dt: 3332.85ms, tok/sec:157309.40
step 2630, loss: 3.710260, norm:0.3445, lr:5.7723e-04 dt: 3332.38ms, tok/sec:157331.57
step 2631, loss: 3.635442, norm:0.2704, lr:5.7720e-04 dt: 3332.09ms, tok/sec:157345.20
step 2632, loss: 3.606451, norm:0.2546, lr:5.7718e-04 dt: 3332.09ms, tok/sec:157344.88
step 2633, loss: 3.723912, norm:0.2932, lr:5.7716e-04 dt: 3331.98ms, tok/sec:157350.07
step 2634, loss: 3.619193, norm:0.2837, lr:5.7713e-04 dt: 3332.04ms, tok/sec:157347.36
step 2635, loss: 3.717697, norm:0.3114, lr:5.7711e-04 dt: 3331.99ms, tok/sec:157349.77
step 2636, loss: 3.586858, norm:0.3238, lr:5.7709e-04 dt: 3332.17ms, tok/sec:157341.39
step 2637, loss: 3.580511, norm:0.2895, lr:5.7706e-04 dt: 3332.29ms, tok/sec:157335.42
step 2638, loss: 3.407500, norm:0.3672, lr:5.7704e-04 dt: 3331.85ms, tok/sec:157356.24
step 2639, loss: 3.536052, norm:0.3482, lr:5.7702e-04 dt: 3332.50ms, tok/sec:157325.60
step 2640, loss: 3.525949, norm:0.2835, lr:5.7699e-04 dt: 3332.20ms, tok/sec:157340.07
step 2641, loss: 3.468488, norm:0.3086, lr:5.7697e-04 dt: 3332.05ms, tok/sec:157346.99
step 2642, loss: 3.528651, norm:0.3127, lr:5.7695e-04 dt: 3331.93ms, tok/sec:157352.76
step 2643, loss: 3.436308, norm:0.2702, lr:5.7692e-04 dt: 3331.91ms, tok/sec:157353.57
step 2644, loss: 3.440285, norm:0.2792, lr:5.7690e-04 dt: 3332.06ms, tok/sec:157346.48
step 2645, loss: 3.458164, norm:0.2600, lr:5.7688e-04 dt: 3332.07ms, tok/sec:157346.21
step 2646, loss: 3.481827, norm:0.2539, lr:5.7685e-04 dt: 3332.30ms, tok/sec:157335.05
step 2647, loss: 3.503493, norm:0.2665, lr:5.7683e-04 dt: 3332.16ms, tok/sec:157342.00
step 2648, loss: 3.529106, norm:0.2512, lr:5.7680e-04 dt: 3332.14ms, tok/sec:157342.78
step 2649, loss: 3.499515, norm:0.2465, lr:5.7678e-04 dt: 3332.02ms, tok/sec:157348.26
HellaSwag accuracy:4623178874569868361/-2=-2311589437284934144.0000
rank 1 sample 0: Hello, I'm a language model, right? Do you mean it's cute or interesting? There are a lot of different types of animals, like dogs,
rank 1 sample 1: Hello, I'm a language model, a lot of research is done, especially a lot of our research shows that this data needs to be processed.
I
rank 1 sample 2: Hello, I'm a language model, I suppose I want to model the time. I'm just gonna be in the middle of it if the data and data
rank 1 sample 3: Hello, I'm a language model, a couple of different things...
We try to talk from other languages (it's your opinion).
Some of the
rank 0 sample 0: Hello, I'm a language model, and I don't know a good reason not to have any other input as input. So, this can work. So
rank 0 sample 1: Hello, I'm a language model, but with a lot of technical jargon available, you find things that can potentially cause you trouble getting your brain running.

rank 0 sample 2: Hello, I'm a language model, but I used the “How is” type. So let's say this:
- Well, let�
rank 0 sample 3: Hello, I'm a language model, not currently a computer.
I started working that way, but in fact it got quite easy to say that the best
step 2650, loss: 3.687846, norm:0.2632, lr:5.7676e-04 dt: 48518.86ms, tok/sec:10805.86
step 2651, loss: 3.678241, norm:0.2701, lr:5.7673e-04 dt: 3332.52ms, tok/sec:157324.62
step 2652, loss: 3.695071, norm:0.2596, lr:5.7671e-04 dt: 3332.41ms, tok/sec:157329.87
step 2653, loss: 3.743858, norm:0.2822, lr:5.7669e-04 dt: 3332.04ms, tok/sec:157347.54
step 2654, loss: 3.666250, norm:0.2609, lr:5.7666e-04 dt: 3331.98ms, tok/sec:157350.49
step 2655, loss: 3.687884, norm:0.2642, lr:5.7664e-04 dt: 3332.22ms, tok/sec:157339.04
step 2656, loss: 3.667658, norm:0.2914, lr:5.7661e-04 dt: 3332.37ms, tok/sec:157331.88
step 2657, loss: 3.696778, norm:0.2569, lr:5.7659e-04 dt: 3331.94ms, tok/sec:157352.11
step 2658, loss: 3.661865, norm:0.2562, lr:5.7657e-04 dt: 3332.35ms, tok/sec:157332.87
step 2659, loss: 3.753973, norm:0.2599, lr:5.7654e-04 dt: 3332.25ms, tok/sec:157337.43
step 2660, loss: 3.677879, norm:0.2319, lr:5.7652e-04 dt: 3332.59ms, tok/sec:157321.66
step 2661, loss: 3.727921, norm:0.2603, lr:5.7650e-04 dt: 3332.09ms, tok/sec:157345.26
step 2662, loss: 3.684644, norm:0.3281, lr:5.7647e-04 dt: 3331.87ms, tok/sec:157355.66
step 2663, loss: 3.622176, norm:0.2872, lr:5.7645e-04 dt: 3332.07ms, tok/sec:157346.01
step 2664, loss: 3.673765, norm:0.3090, lr:5.7642e-04 dt: 3332.29ms, tok/sec:157335.55
step 2665, loss: 3.713594, norm:0.2626, lr:5.7640e-04 dt: 3331.99ms, tok/sec:157349.63
step 2666, loss: 3.619978, norm:0.2819, lr:5.7638e-04 dt: 3334.12ms, tok/sec:157249.31
step 2667, loss: 3.759389, norm:0.2720, lr:5.7635e-04 dt: 3332.42ms, tok/sec:157329.37
step 2668, loss: 3.703001, norm:0.2735, lr:5.7633e-04 dt: 3332.17ms, tok/sec:157341.54
step 2669, loss: 3.680501, norm:0.2629, lr:5.7630e-04 dt: 3332.20ms, tok/sec:157340.04
step 2670, loss: 3.719360, norm:0.3029, lr:5.7628e-04 dt: 3332.06ms, tok/sec:157346.73
step 2671, loss: 3.631516, norm:0.3357, lr:5.7626e-04 dt: 3332.17ms, tok/sec:157341.12
step 2672, loss: 3.732162, norm:0.2793, lr:5.7623e-04 dt: 3332.28ms, tok/sec:157336.17
step 2673, loss: 3.658616, norm:0.2879, lr:5.7621e-04 dt: 3331.82ms, tok/sec:157357.82
step 2674, loss: 3.631075, norm:0.2844, lr:5.7619e-04 dt: 3332.13ms, tok/sec:157343.28
step 2675, loss: 3.661852, norm:0.2607, lr:5.7616e-04 dt: 3332.20ms, tok/sec:157339.89
step 2676, loss: 3.758607, norm:0.3278, lr:5.7614e-04 dt: 3332.35ms, tok/sec:157332.77
step 2677, loss: 3.669043, norm:0.2795, lr:5.7611e-04 dt: 3331.93ms, tok/sec:157352.55
step 2678, loss: 3.630817, norm:0.2863, lr:5.7609e-04 dt: 3332.15ms, tok/sec:157342.12
step 2679, loss: 3.605377, norm:0.2707, lr:5.7607e-04 dt: 3332.35ms, tok/sec:157332.61
step 2680, loss: 3.605212, norm:0.2806, lr:5.7604e-04 dt: 3332.58ms, tok/sec:157322.03
step 2681, loss: 3.616441, norm:0.2555, lr:5.7602e-04 dt: 3332.02ms, tok/sec:157348.46
step 2682, loss: 3.637703, norm:0.2621, lr:5.7599e-04 dt: 3332.08ms, tok/sec:157345.59
step 2683, loss: 3.613607, norm:0.2676, lr:5.7597e-04 dt: 3332.12ms, tok/sec:157343.45
step 2684, loss: 3.613632, norm:0.2374, lr:5.7595e-04 dt: 3332.21ms, tok/sec:157339.60
step 2685, loss: 3.627052, norm:0.2554, lr:5.7592e-04 dt: 3332.22ms, tok/sec:157338.98
step 2686, loss: 3.484102, norm:0.2624, lr:5.7590e-04 dt: 3331.94ms, tok/sec:157351.98
step 2687, loss: 3.512709, norm:0.2570, lr:5.7587e-04 dt: 3331.99ms, tok/sec:157349.85
step 2688, loss: 3.490446, norm:0.2752, lr:5.7585e-04 dt: 3332.72ms, tok/sec:157315.22
step 2689, loss: 3.408895, norm:0.2515, lr:5.7582e-04 dt: 3331.92ms, tok/sec:157353.01
step 2690, loss: 3.425302, norm:0.2659, lr:5.7580e-04 dt: 3331.90ms, tok/sec:157353.92
step 2691, loss: 3.496010, norm:0.2894, lr:5.7578e-04 dt: 3332.21ms, tok/sec:157339.57
step 2692, loss: 3.466506, norm:0.3021, lr:5.7575e-04 dt: 3332.19ms, tok/sec:157340.31
step 2693, loss: 3.499218, norm:0.2947, lr:5.7573e-04 dt: 3331.84ms, tok/sec:157357.11
step 2694, loss: 3.499488, norm:0.2808, lr:5.7570e-04 dt: 3332.00ms, tok/sec:157349.42
step 2695, loss: 3.460336, norm:0.2675, lr:5.7568e-04 dt: 3332.23ms, tok/sec:157338.51
step 2696, loss: 3.471768, norm:0.2726, lr:5.7566e-04 dt: 3332.35ms, tok/sec:157332.96
step 2697, loss: 3.592202, norm:0.2631, lr:5.7563e-04 dt: 3332.04ms, tok/sec:157347.51
step 2698, loss: 3.693551, norm:0.2691, lr:5.7561e-04 dt: 3332.04ms, tok/sec:157347.64
step 2699, loss: 3.734854, norm:0.2644, lr:5.7558e-04 dt: 3332.16ms, tok/sec:157341.71
validation loss: 3.6751
Model and optimizer state saved.
HellaSwag accuracy:-4591326702666021815/-2=2295663351333010944.0000
rank 1 sample 0: Hello, I'm a language model, and the whole thing is the same number, you have to consider the number of words, you're going to need to
rank 1 sample 1: Hello, I'm a language model, which I have built for my own purpose. I want him to take control of my vocabulary, from my own experience.
rank 0 sample 0: Hello, I'm a language model, and I know that it's an open process. If you want to check this out by setting your language into a single
rank 1 sample 2: Hello, I'm a language model, so every day I'm waiting for my next day. And for me it's a lot. It's not about music
rank 0 sample 1: Hello, I'm a language model, for many people, it's not your own, with lots of fun, I've seen people in some parts of therank 1 sample 3: Hello, I'm a language model, and I'm really interested in this and "I'm using a C, you're asking: 'What is my name

rank 0 sample 2: Hello, I'm a language model, but I won't discuss about it at this point.
This was done in the context of the SAGE, we
rank 0 sample 3: Hello, I'm a language model, and here's my list of the rules for me.
C: This was once called My Bible Translation. I guess
step 2700, loss: 3.647010, norm:0.2744, lr:5.7556e-04 dt: 56203.85ms, tok/sec:9328.33
step 2701, loss: 3.668765, norm:0.2549, lr:5.7553e-04 dt: 3332.23ms, tok/sec:157338.50
step 2702, loss: 3.719432, norm:0.3030, lr:5.7551e-04 dt: 3332.24ms, tok/sec:157337.97
step 2703, loss: 3.685435, norm:0.2986, lr:5.7549e-04 dt: 3332.23ms, tok/sec:157338.50
step 2704, loss: 3.665513, norm:0.2794, lr:5.7546e-04 dt: 3332.13ms, tok/sec:157343.11
step 2705, loss: 3.642666, norm:0.2877, lr:5.7544e-04 dt: 3332.01ms, tok/sec:157348.67
step 2706, loss: 3.725595, norm:0.2946, lr:5.7541e-04 dt: 3332.32ms, tok/sec:157334.25
step 2707, loss: 3.645366, norm:0.2896, lr:5.7539e-04 dt: 3332.14ms, tok/sec:157342.76
step 2708, loss: 3.643399, norm:0.3194, lr:5.7536e-04 dt: 3332.54ms, tok/sec:157323.80
step 2709, loss: 3.678945, norm:0.2623, lr:5.7534e-04 dt: 3332.19ms, tok/sec:157340.25
step 2710, loss: 3.656940, norm:0.2975, lr:5.7532e-04 dt: 3332.10ms, tok/sec:157344.75
step 2711, loss: 3.634349, norm:0.2703, lr:5.7529e-04 dt: 3332.09ms, tok/sec:157344.86
step 2712, loss: 3.692122, norm:0.2671, lr:5.7527e-04 dt: 3332.07ms, tok/sec:157346.12
step 2713, loss: 3.659765, norm:0.2616, lr:5.7524e-04 dt: 3331.81ms, tok/sec:157358.20
step 2714, loss: 3.651232, norm:0.2994, lr:5.7522e-04 dt: 3332.03ms, tok/sec:157347.90
step 2715, loss: 3.705096, norm:0.3007, lr:5.7519e-04 dt: 3332.49ms, tok/sec:157326.16
step 2716, loss: 3.694862, norm:0.3143, lr:5.7517e-04 dt: 3332.12ms, tok/sec:157343.54
step 2717, loss: 3.708985, norm:0.3361, lr:5.7514e-04 dt: 3331.89ms, tok/sec:157354.53
step 2718, loss: 3.676770, norm:0.2606, lr:5.7512e-04 dt: 3332.06ms, tok/sec:157346.45
step 2719, loss: 3.661854, norm:0.3051, lr:5.7510e-04 dt: 3332.13ms, tok/sec:157343.12
step 2720, loss: 3.697429, norm:0.3106, lr:5.7507e-04 dt: 3331.96ms, tok/sec:157351.32
step 2721, loss: 3.659865, norm:0.2844, lr:5.7505e-04 dt: 3332.18ms, tok/sec:157340.85
step 2722, loss: 3.625367, norm:0.3066, lr:5.7502e-04 dt: 3332.27ms, tok/sec:157336.37
step 2723, loss: 3.651931, norm:0.2677, lr:5.7500e-04 dt: 3332.13ms, tok/sec:157343.18
step 2724, loss: 3.713478, norm:0.2921, lr:5.7497e-04 dt: 3332.45ms, tok/sec:157328.23
step 2725, loss: 3.629022, norm:0.2690, lr:5.7495e-04 dt: 3332.34ms, tok/sec:157333.36
step 2726, loss: 3.577720, norm:0.2806, lr:5.7492e-04 dt: 3332.00ms, tok/sec:157349.33
step 2727, loss: 3.624520, norm:0.2496, lr:5.7490e-04 dt: 3332.36ms, tok/sec:157332.53
step 2728, loss: 3.594130, norm:0.2648, lr:5.7488e-04 dt: 3332.11ms, tok/sec:157344.10
step 2729, loss: 3.631364, norm:0.2876, lr:5.7485e-04 dt: 3332.10ms, tok/sec:157344.70
step 2730, loss: 3.623245, norm:0.2878, lr:5.7483e-04 dt: 3332.12ms, tok/sec:157343.48
step 2731, loss: 3.600946, norm:0.3002, lr:5.7480e-04 dt: 3332.49ms, tok/sec:157326.15
step 2732, loss: 3.679310, norm:0.2639, lr:5.7478e-04 dt: 3331.94ms, tok/sec:157352.33
step 2733, loss: 3.595121, norm:0.2781, lr:5.7475e-04 dt: 3332.21ms, tok/sec:157339.24
step 2734, loss: 3.540793, norm:0.3015, lr:5.7473e-04 dt: 3332.05ms, tok/sec:157347.08
step 2735, loss: 3.456504, norm:0.2516, lr:5.7470e-04 dt: 3332.03ms, tok/sec:157347.77
step 2736, loss: 3.479130, norm:0.2948, lr:5.7468e-04 dt: 3332.05ms, tok/sec:157346.95
step 2737, loss: 3.421829, norm:0.2556, lr:5.7465e-04 dt: 3332.06ms, tok/sec:157346.56
step 2738, loss: 3.487362, norm:0.2782, lr:5.7463e-04 dt: 3331.85ms, tok/sec:157356.34
step 2739, loss: 3.439910, norm:0.2672, lr:5.7460e-04 dt: 3332.28ms, tok/sec:157336.10
step 2740, loss: 3.419453, norm:0.2615, lr:5.7458e-04 dt: 3332.17ms, tok/sec:157341.17
step 2741, loss: 3.470374, norm:0.2675, lr:5.7456e-04 dt: 3331.96ms, tok/sec:157351.33
step 2742, loss: 3.495004, norm:0.2760, lr:5.7453e-04 dt: 3332.22ms, tok/sec:157338.99
step 2743, loss: 3.488283, norm:0.2676, lr:5.7451e-04 dt: 3332.24ms, tok/sec:157337.92
step 2744, loss: 3.478108, norm:0.2776, lr:5.7448e-04 dt: 3332.07ms, tok/sec:157345.88
step 2745, loss: 3.461434, norm:0.2623, lr:5.7446e-04 dt: 3331.95ms, tok/sec:157351.60
step 2746, loss: 3.724272, norm:0.3119, lr:5.7443e-04 dt: 3332.28ms, tok/sec:157336.01
step 2747, loss: 3.658580, norm:0.2915, lr:5.7441e-04 dt: 3332.54ms, tok/sec:157323.68
step 2748, loss: 3.722706, norm:0.2953, lr:5.7438e-04 dt: 3332.16ms, tok/sec:157341.96
step 2749, loss: 3.672293, norm:0.3600, lr:5.7436e-04 dt: 3332.27ms, tok/sec:157336.56
HellaSwag accuracy:-4591185971620117423/-2=2295592985810058752.0000
rank 1 sample 0: Hello, I'm a language model, right? Absolutely, yes, I learned about all of the major languages that I've heard of before. But I'm
rank 1 sample 1: Hello, I'm a language model, a model, and a model in that style.
|Jensen, H., Tölden, A.,
rank 1 sample 2: Hello, I'm a language model, but was really intrigued by the idea of a language that I couldn't find out about. The question of what it meant
rank 1 sample 3: Hello, I'm a language model, and I'm the first two years of course, I still has several theories around the human consciousness that I've got to
rank 0 sample 0: Hello, I'm a language model, and I was a computer programer. After three years, I went to an MIT lab and then worked with the language
rank 0 sample 1: Hello, I'm a language model, so a student can write a sequence in the same grammar to write in a similar language to it, with different sounds,
rank 0 sample 2: Hello, I'm a language model, but I also a program I should also be trying to teach you the language. This is the first step in my plan
rank 0 sample 3: Hello, I'm a language model, and when I say "I don't have no idea what the first words in that sentence looks like I don't make
step 2750, loss: 3.647877, norm:0.3251, lr:5.7433e-04 dt: 48518.84ms, tok/sec:10805.86
step 2751, loss: 3.678843, norm:0.2950, lr:5.7431e-04 dt: 3331.90ms, tok/sec:157354.20
step 2752, loss: 3.847635, norm:0.3881, lr:5.7428e-04 dt: 3332.01ms, tok/sec:157348.70
step 2753, loss: 3.686936, norm:0.3363, lr:5.7426e-04 dt: 3332.09ms, tok/sec:157344.87
step 2754, loss: 3.767383, norm:0.3150, lr:5.7423e-04 dt: 3332.12ms, tok/sec:157343.54
step 2755, loss: 3.702389, norm:0.2856, lr:5.7421e-04 dt: 3332.16ms, tok/sec:157341.87
step 2756, loss: 3.666556, norm:0.3158, lr:5.7418e-04 dt: 3332.21ms, tok/sec:157339.26
step 2757, loss: 3.704984, norm:0.2876, lr:5.7416e-04 dt: 3332.54ms, tok/sec:157323.94
step 2758, loss: 3.637736, norm:0.2953, lr:5.7413e-04 dt: 3332.12ms, tok/sec:157343.54
step 2759, loss: 3.671409, norm:0.2741, lr:5.7411e-04 dt: 3332.04ms, tok/sec:157347.40
step 2760, loss: 3.676482, norm:0.2738, lr:5.7408e-04 dt: 3332.19ms, tok/sec:157340.14
step 2761, loss: 3.692490, norm:0.2707, lr:5.7406e-04 dt: 3332.28ms, tok/sec:157336.27
step 2762, loss: 3.652875, norm:0.2671, lr:5.7403e-04 dt: 3332.03ms, tok/sec:157347.93
step 2763, loss: 3.611705, norm:0.2919, lr:5.7401e-04 dt: 3332.11ms, tok/sec:157344.35
step 2764, loss: 3.688858, norm:0.2855, lr:5.7398e-04 dt: 3332.21ms, tok/sec:157339.29
step 2765, loss: 3.649069, norm:0.2657, lr:5.7396e-04 dt: 3332.23ms, tok/sec:157338.43
step 2766, loss: 3.634467, norm:0.2540, lr:5.7393e-04 dt: 3331.93ms, tok/sec:157352.67
step 2767, loss: 3.644761, norm:0.2698, lr:5.7391e-04 dt: 3332.02ms, tok/sec:157348.49
step 2768, loss: 3.685866, norm:0.2684, lr:5.7388e-04 dt: 3332.19ms, tok/sec:157340.41
step 2769, loss: 3.674717, norm:0.2641, lr:5.7386e-04 dt: 3332.19ms, tok/sec:157340.35
step 2770, loss: 3.670275, norm:0.2533, lr:5.7383e-04 dt: 3332.18ms, tok/sec:157340.84
step 2771, loss: 3.586832, norm:0.2596, lr:5.7381e-04 dt: 3332.28ms, tok/sec:157336.28
step 2772, loss: 3.597960, norm:0.2648, lr:5.7378e-04 dt: 3332.10ms, tok/sec:157344.46
step 2773, loss: 3.623341, norm:0.2576, lr:5.7376e-04 dt: 3332.44ms, tok/sec:157328.61
step 2774, loss: 3.563997, norm:0.2611, lr:5.7373e-04 dt: 3332.43ms, tok/sec:157329.13
step 2775, loss: 3.612170, norm:0.2536, lr:5.7371e-04 dt: 3332.24ms, tok/sec:157337.99
step 2776, loss: 3.647361, norm:0.2827, lr:5.7368e-04 dt: 3332.03ms, tok/sec:157347.82
step 2777, loss: 3.636006, norm:0.2934, lr:5.7366e-04 dt: 3332.29ms, tok/sec:157335.82
step 2778, loss: 3.645877, norm:0.2795, lr:5.7363e-04 dt: 3332.22ms, tok/sec:157338.97
step 2779, loss: 3.605342, norm:0.2609, lr:5.7361e-04 dt: 3332.05ms, tok/sec:157346.79
step 2780, loss: 3.630004, norm:0.2406, lr:5.7358e-04 dt: 3332.35ms, tok/sec:157332.64
step 2781, loss: 3.540998, norm:0.2738, lr:5.7356e-04 dt: 3331.95ms, tok/sec:157351.89
step 2782, loss: 3.487099, norm:0.2562, lr:5.7353e-04 dt: 3332.51ms, tok/sec:157325.43
step 2783, loss: 3.492331, norm:0.2771, lr:5.7351e-04 dt: 3331.97ms, tok/sec:157350.88
step 2784, loss: 3.483045, norm:0.2720, lr:5.7348e-04 dt: 3331.81ms, tok/sec:157358.44
step 2785, loss: 3.454439, norm:0.2915, lr:5.7346e-04 dt: 3332.19ms, tok/sec:157340.15
step 2786, loss: 3.440189, norm:0.2692, lr:5.7343e-04 dt: 3332.13ms, tok/sec:157343.18
step 2787, loss: 3.429621, norm:0.2478, lr:5.7341e-04 dt: 3332.00ms, tok/sec:157349.50
step 2788, loss: 3.484906, norm:0.2723, lr:5.7338e-04 dt: 3332.03ms, tok/sec:157347.81
step 2789, loss: 3.559735, norm:0.2742, lr:5.7336e-04 dt: 3332.28ms, tok/sec:157335.99
step 2790, loss: 3.481399, norm:0.2623, lr:5.7333e-04 dt: 3332.42ms, tok/sec:157329.35
step 2791, loss: 3.467243, norm:0.2718, lr:5.7330e-04 dt: 3332.01ms, tok/sec:157349.05
step 2792, loss: 3.417428, norm:0.2759, lr:5.7328e-04 dt: 3332.09ms, tok/sec:157345.31
step 2793, loss: 3.556254, norm:0.3038, lr:5.7325e-04 dt: 3332.12ms, tok/sec:157343.51
step 2794, loss: 3.673202, norm:0.2889, lr:5.7323e-04 dt: 3331.97ms, tok/sec:157350.94
step 2795, loss: 3.605968, norm:0.3198, lr:5.7320e-04 dt: 3332.09ms, tok/sec:157345.11
step 2796, loss: 3.704715, norm:0.3040, lr:5.7318e-04 dt: 3332.06ms, tok/sec:157346.37
step 2797, loss: 3.689876, norm:0.2919, lr:5.7315e-04 dt: 3332.24ms, tok/sec:157338.24
step 2798, loss: 3.716754, norm:0.3123, lr:5.7313e-04 dt: 3332.45ms, tok/sec:157328.32
step 2799, loss: 3.655407, norm:0.3056, lr:5.7310e-04 dt: 3332.52ms, tok/sec:157324.76
validation loss: 3.6596
Model and optimizer state saved.
HellaSwag accuracy:4632184974850098257/-2=-2316092487425049088.0000
rank 1 sample 0: Hello, I'm a language model, the second is a class of terms [of languages,] .
A lot of languages can do this with a single
rank 1 sample 1: Hello, I'm a language model, that is very popular in the US and English language. They are written in many different levels and one is a language model
rank 1 sample 2: Hello, I'm a language model, so every other day I'm going to talk about it in "I've learned a lot from this one," which I
rank 1 sample 3: Hello, I'm a language model, and I'm interested to think that, exactly, how far in each country that I will come in.
It's
rank 0 sample 0: Hello, I'm a language model, and I know that it's just so difficult to tell if you understand where a language exists. The reason why I find
rank 0 sample 1: Hello, I'm a language model, I really want to know what the structure of this object works. So. Let's get an idea what this object works
rank 0 sample 2: Hello, I'm a language model, I'm used to see you it has a word which I use, so I'm a language model.
The key
rank 0 sample 3: Hello, I'm a language model, but no one knows what it's and how far it's allowed to be, depending on its context.
- 1
step 2800, loss: 3.667474, norm:0.2692, lr:5.7308e-04 dt: 56183.07ms, tok/sec:9331.78
step 2801, loss: 3.653028, norm:0.3104, lr:5.7305e-04 dt: 3332.23ms, tok/sec:157338.41
step 2802, loss: 3.731459, norm:0.3142, lr:5.7303e-04 dt: 3332.01ms, tok/sec:157348.87
step 2803, loss: 3.773723, norm:0.3263, lr:5.7300e-04 dt: 3332.48ms, tok/sec:157326.82
step 2804, loss: 3.683297, norm:0.2821, lr:5.7298e-04 dt: 3332.20ms, tok/sec:157339.72
step 2805, loss: 3.669329, norm:0.2842, lr:5.7295e-04 dt: 3332.21ms, tok/sec:157339.52
step 2806, loss: 3.655318, norm:0.2925, lr:5.7292e-04 dt: 3332.14ms, tok/sec:157342.81
step 2807, loss: 3.652151, norm:0.3194, lr:5.7290e-04 dt: 3331.99ms, tok/sec:157349.59
step 2808, loss: 3.651671, norm:0.2881, lr:5.7287e-04 dt: 3331.87ms, tok/sec:157355.69
step 2809, loss: 3.617386, norm:0.2902, lr:5.7285e-04 dt: 3332.34ms, tok/sec:157333.11
step 2810, loss: 3.600045, norm:0.2848, lr:5.7282e-04 dt: 3331.97ms, tok/sec:157350.83
step 2811, loss: 3.706691, norm:0.2780, lr:5.7280e-04 dt: 3332.52ms, tok/sec:157324.81
step 2812, loss: 3.630761, norm:0.2662, lr:5.7277e-04 dt: 3332.01ms, tok/sec:157348.84
step 2813, loss: 3.688289, norm:0.2578, lr:5.7275e-04 dt: 3332.11ms, tok/sec:157344.01
step 2814, loss: 3.634699, norm:0.2614, lr:5.7272e-04 dt: 3332.20ms, tok/sec:157340.01
step 2815, loss: 3.657628, norm:0.2770, lr:5.7269e-04 dt: 3331.94ms, tok/sec:157352.24
step 2816, loss: 3.608324, norm:0.2740, lr:5.7267e-04 dt: 3332.29ms, tok/sec:157335.59
step 2817, loss: 3.671968, norm:0.2676, lr:5.7264e-04 dt: 3332.11ms, tok/sec:157343.91
step 2818, loss: 3.605520, norm:0.3107, lr:5.7262e-04 dt: 3332.39ms, tok/sec:157330.97
step 2819, loss: 3.566678, norm:0.2880, lr:5.7259e-04 dt: 3332.06ms, tok/sec:157346.49
step 2820, loss: 3.643431, norm:0.2965, lr:5.7257e-04 dt: 3332.04ms, tok/sec:157347.54
step 2821, loss: 3.630085, norm:0.2847, lr:5.7254e-04 dt: 3332.20ms, tok/sec:157339.93
step 2822, loss: 3.644937, norm:0.2548, lr:5.7252e-04 dt: 3332.28ms, tok/sec:157336.03
step 2823, loss: 3.644783, norm:0.2779, lr:5.7249e-04 dt: 3332.06ms, tok/sec:157346.33
step 2824, loss: 3.606056, norm:0.2587, lr:5.7246e-04 dt: 3332.10ms, tok/sec:157344.81
step 2825, loss: 3.600586, norm:0.2358, lr:5.7244e-04 dt: 3332.25ms, tok/sec:157337.36
step 2826, loss: 3.584586, norm:0.2469, lr:5.7241e-04 dt: 3332.06ms, tok/sec:157346.43
step 2827, loss: 3.583699, norm:0.2513, lr:5.7239e-04 dt: 3332.54ms, tok/sec:157323.99
step 2828, loss: 3.599303, norm:0.2616, lr:5.7236e-04 dt: 3332.09ms, tok/sec:157345.13
step 2829, loss: 3.540147, norm:0.2512, lr:5.7234e-04 dt: 3332.12ms, tok/sec:157343.53
step 2830, loss: 3.441279, norm:0.2660, lr:5.7231e-04 dt: 3332.13ms, tok/sec:157343.14
step 2831, loss: 3.408086, norm:0.2710, lr:5.7228e-04 dt: 3331.95ms, tok/sec:157351.85
step 2832, loss: 3.465725, norm:0.2871, lr:5.7226e-04 dt: 3332.00ms, tok/sec:157349.25
step 2833, loss: 3.407078, norm:0.2882, lr:5.7223e-04 dt: 3332.11ms, tok/sec:157344.25
step 2834, loss: 3.459182, norm:0.2655, lr:5.7221e-04 dt: 3332.35ms, tok/sec:157332.81
step 2835, loss: 3.490145, norm:0.2586, lr:5.7218e-04 dt: 3332.06ms, tok/sec:157346.73
step 2836, loss: 3.432606, norm:0.2654, lr:5.7216e-04 dt: 3331.88ms, tok/sec:157355.24
step 2837, loss: 3.454482, norm:0.2659, lr:5.7213e-04 dt: 3332.16ms, tok/sec:157341.92
step 2838, loss: 3.432247, norm:0.2538, lr:5.7210e-04 dt: 3332.18ms, tok/sec:157340.81
step 2839, loss: 3.404258, norm:0.2655, lr:5.7208e-04 dt: 3332.01ms, tok/sec:157348.64
step 2840, loss: 3.410681, norm:0.2696, lr:5.7205e-04 dt: 3332.37ms, tok/sec:157331.66
step 2841, loss: 3.467577, norm:0.2651, lr:5.7203e-04 dt: 3333.57ms, tok/sec:157275.03
step 2842, loss: 3.685904, norm:0.2791, lr:5.7200e-04 dt: 3332.53ms, tok/sec:157324.12
step 2843, loss: 3.656075, norm:0.2860, lr:5.7197e-04 dt: 3332.02ms, tok/sec:157348.19
step 2844, loss: 3.697695, norm:0.2694, lr:5.7195e-04 dt: 3332.06ms, tok/sec:157346.43
step 2845, loss: 3.648262, norm:0.2738, lr:5.7192e-04 dt: 3332.30ms, tok/sec:157335.37
step 2846, loss: 3.616224, norm:0.2912, lr:5.7190e-04 dt: 3331.92ms, tok/sec:157353.01
step 2847, loss: 3.709543, norm:0.2790, lr:5.7187e-04 dt: 3332.28ms, tok/sec:157335.99
step 2848, loss: 3.652227, norm:0.2697, lr:5.7185e-04 dt: 3331.94ms, tok/sec:157352.27
step 2849, loss: 3.629731, norm:0.2977, lr:5.7182e-04 dt: 3332.75ms, tok/sec:157313.94
HellaSwag accuracy:4623041401256658001/-2=-2311520700628328960.0000
rank 1 sample 0: Hello, I'm a language model, an interface, a method of making complex decisions; a design theory, a way of creating and testing complex decision-making
rank 1 sample 1: Hello, I'm a language model, which I've read.
If you read the article carefully, you can learn the HTML article, which is a good
rank 1 sample 2: Hello, I'm a language model, so have the words and the words that I want to make out the sentences.
I hope the word "crowd
rank 1 sample 3: Hello, I'm a language model, and I'm pretty excited enough to do much of it: (It's no longer easy to learn how to program.)
rank 0 sample 0: Hello, I'm a language model, and I don't think they all know I's being right here now. And I just found it here, and then
rank 0 sample 1: Hello, I'm a language model, but my two-language model can create a language template so I can implement my code, including a one-language model
rank 0 sample 2: Hello, I'm a language model, but I thought it just can't explain it properly.
A few years ago, I had a long time in my
rank 0 sample 3: Hello, I'm a language model, and for the language, I've looked up Google's search engine, and then moved down to Google's search engine "
step 2850, loss: 3.659029, norm:0.2721, lr:5.7179e-04 dt: 48603.44ms, tok/sec:10787.06
step 2851, loss: 3.667933, norm:0.2583, lr:5.7177e-04 dt: 3332.16ms, tok/sec:157341.67
step 2852, loss: 3.611949, norm:0.2757, lr:5.7174e-04 dt: 3332.42ms, tok/sec:157329.59
step 2853, loss: 3.663624, norm:0.2674, lr:5.7172e-04 dt: 3331.92ms, tok/sec:157353.31
step 2854, loss: 3.655206, norm:0.2549, lr:5.7169e-04 dt: 3332.00ms, tok/sec:157349.18
step 2855, loss: 3.667606, norm:0.2543, lr:5.7166e-04 dt: 3332.43ms, tok/sec:157329.15
step 2856, loss: 3.678496, norm:0.2519, lr:5.7164e-04 dt: 3332.34ms, tok/sec:157333.22
step 2857, loss: 3.640489, norm:0.2690, lr:5.7161e-04 dt: 3334.77ms, tok/sec:157218.44
step 2858, loss: 3.645651, norm:0.2587, lr:5.7159e-04 dt: 3332.22ms, tok/sec:157339.18
step 2859, loss: 3.677007, norm:0.2901, lr:5.7156e-04 dt: 3332.03ms, tok/sec:157348.13
step 2860, loss: 3.641571, norm:0.2677, lr:5.7153e-04 dt: 3331.98ms, tok/sec:157350.32
step 2861, loss: 3.658790, norm:0.2807, lr:5.7151e-04 dt: 3331.93ms, tok/sec:157352.42
step 2862, loss: 3.661129, norm:0.2788, lr:5.7148e-04 dt: 3332.06ms, tok/sec:157346.37
step 2863, loss: 3.665395, norm:0.2577, lr:5.7146e-04 dt: 3331.97ms, tok/sec:157350.53
step 2864, loss: 3.552654, norm:0.2581, lr:5.7143e-04 dt: 3332.04ms, tok/sec:157347.45
step 2865, loss: 3.585134, norm:0.2766, lr:5.7140e-04 dt: 3331.97ms, tok/sec:157350.92
step 2866, loss: 3.575713, norm:0.2616, lr:5.7138e-04 dt: 3332.16ms, tok/sec:157341.99
step 2867, loss: 3.647945, norm:0.3038, lr:5.7135e-04 dt: 3332.28ms, tok/sec:157336.22
step 2868, loss: 3.603273, norm:0.2828, lr:5.7132e-04 dt: 3332.35ms, tok/sec:157332.68
step 2869, loss: 3.574091, norm:0.2707, lr:5.7130e-04 dt: 3332.03ms, tok/sec:157348.12
step 2870, loss: 3.603880, norm:0.2530, lr:5.7127e-04 dt: 3332.34ms, tok/sec:157333.14
step 2871, loss: 3.625317, norm:0.2811, lr:5.7125e-04 dt: 3331.97ms, tok/sec:157350.68
step 2872, loss: 3.588036, norm:0.2633, lr:5.7122e-04 dt: 3331.94ms, tok/sec:157352.18
step 2873, loss: 3.588567, norm:0.2901, lr:5.7119e-04 dt: 3332.00ms, tok/sec:157349.34
step 2874, loss: 3.619086, norm:0.2468, lr:5.7117e-04 dt: 3332.13ms, tok/sec:157343.21
step 2875, loss: 3.567956, norm:0.2744, lr:5.7114e-04 dt: 3332.02ms, tok/sec:157348.52
step 2876, loss: 3.573837, norm:0.2653, lr:5.7111e-04 dt: 3332.19ms, tok/sec:157340.33
step 2877, loss: 3.453794, norm:0.2583, lr:5.7109e-04 dt: 3332.12ms, tok/sec:157343.82
step 2878, loss: 3.392781, norm:0.2727, lr:5.7106e-04 dt: 3332.04ms, tok/sec:157347.66
step 2879, loss: 3.413647, norm:0.2576, lr:5.7104e-04 dt: 3332.24ms, tok/sec:157338.22
step 2880, loss: 3.412657, norm:0.2552, lr:5.7101e-04 dt: 3332.17ms, tok/sec:157341.50
step 2881, loss: 3.436417, norm:0.2838, lr:5.7098e-04 dt: 3332.06ms, tok/sec:157346.60
step 2882, loss: 3.419827, norm:0.2645, lr:5.7096e-04 dt: 3331.95ms, tok/sec:157351.62
step 2883, loss: 3.493255, norm:0.2660, lr:5.7093e-04 dt: 3332.02ms, tok/sec:157348.40
step 2884, loss: 3.438410, norm:0.2702, lr:5.7090e-04 dt: 3331.97ms, tok/sec:157350.71
step 2885, loss: 3.457068, norm:0.2786, lr:5.7088e-04 dt: 3332.03ms, tok/sec:157348.15
step 2886, loss: 3.502878, norm:0.2900, lr:5.7085e-04 dt: 3332.10ms, tok/sec:157344.39
step 2887, loss: 3.474174, norm:0.2794, lr:5.7083e-04 dt: 3332.34ms, tok/sec:157333.24
step 2888, loss: 3.509122, norm:0.2555, lr:5.7080e-04 dt: 3332.10ms, tok/sec:157344.61
step 2889, loss: 3.635693, norm:0.2633, lr:5.7077e-04 dt: 3332.14ms, tok/sec:157342.83
step 2890, loss: 3.683174, norm:0.2801, lr:5.7075e-04 dt: 3332.13ms, tok/sec:157343.35
step 2891, loss: 3.693847, norm:0.2699, lr:5.7072e-04 dt: 3332.27ms, tok/sec:157336.54
step 2892, loss: 3.664018, norm:0.2573, lr:5.7069e-04 dt: 3332.05ms, tok/sec:157346.86
step 2893, loss: 3.628631, norm:0.2928, lr:5.7067e-04 dt: 3332.10ms, tok/sec:157344.57
step 2894, loss: 3.675921, norm:0.2716, lr:5.7064e-04 dt: 3332.10ms, tok/sec:157344.58
step 2895, loss: 3.687623, norm:0.2826, lr:5.7061e-04 dt: 3331.89ms, tok/sec:157354.44
step 2896, loss: 3.633173, norm:0.3203, lr:5.7059e-04 dt: 3332.41ms, tok/sec:157329.96
step 2897, loss: 3.662693, norm:0.3255, lr:5.7056e-04 dt: 3332.16ms, tok/sec:157342.00
step 2898, loss: 3.686080, norm:0.2862, lr:5.7053e-04 dt: 3332.07ms, tok/sec:157346.09
step 2899, loss: 3.681054, norm:0.2812, lr:5.7051e-04 dt: 3332.27ms, tok/sec:157336.56
validation loss: 3.6450
Model and optimizer state saved.
HellaSwag accuracy:2317318271022892113/-2=-1158659135511446016.0000
rank 1 sample 0: Hello, I'm a language model, just the big thing: I'm building an architecture that takes code and uses it to create an interactive environment, and I
rank 1 sample 1: Hello, I'm a language model, you know that is a language model. For example, that is just a word, there's the same word, and
rank 1 sample 2: Hello, I'm a language model, but on this level, I'm pretty sure that I know. It has a lot of things you have learned over time
rank 1 sample 3: Hello, I'm a language model, and I'm using the web, in conjunction with the video chat software in Windows. Also, I'm using the web
rank 0 sample 0: Hello, I'm a language model, and I want to know a bunch of little "murd" in any one of them. That way I'm getting
rank 0 sample 1: Hello, I'm a language model, but we're not talking about that that. We might see it's there as well:
I was thinking about a
rank 0 sample 2: Hello, I'm a language model, but I wanted to change them. Why? They're not so sure about the same thing, but they're just.
rank 0 sample 3: Hello, I'm a language model, a group of people who have been to be interminable and the world has worked very hard. A group of others
step 2900, loss: 3.673474, norm:0.2603, lr:5.7048e-04 dt: 56343.07ms, tok/sec:9305.28
step 2901, loss: 3.638487, norm:0.2555, lr:5.7046e-04 dt: 3332.01ms, tok/sec:157348.80
step 2902, loss: 3.639998, norm:0.2827, lr:5.7043e-04 dt: 3332.55ms, tok/sec:157323.14
step 2903, loss: 3.662778, norm:0.3002, lr:5.7040e-04 dt: 3332.09ms, tok/sec:157344.97
step 2904, loss: 3.685473, norm:0.2705, lr:5.7038e-04 dt: 3331.93ms, tok/sec:157352.42
step 2905, loss: 3.671799, norm:0.3101, lr:5.7035e-04 dt: 3332.02ms, tok/sec:157348.57
step 2906, loss: 3.668448, norm:0.2673, lr:5.7032e-04 dt: 3332.23ms, tok/sec:157338.41
step 2907, loss: 3.675602, norm:0.2446, lr:5.7030e-04 dt: 3332.18ms, tok/sec:157340.74
step 2908, loss: 3.679499, norm:0.2482, lr:5.7027e-04 dt: 3331.94ms, tok/sec:157352.39
step 2909, loss: 3.605752, norm:0.2454, lr:5.7024e-04 dt: 3332.22ms, tok/sec:157339.16
step 2910, loss: 3.589518, norm:0.2429, lr:5.7022e-04 dt: 3332.24ms, tok/sec:157337.99
step 2911, loss: 3.636854, norm:0.2598, lr:5.7019e-04 dt: 3332.28ms, tok/sec:157336.21
step 2912, loss: 3.578300, norm:0.2497, lr:5.7016e-04 dt: 3332.02ms, tok/sec:157348.39
step 2913, loss: 3.553016, norm:0.2598, lr:5.7014e-04 dt: 3331.93ms, tok/sec:157352.60
step 2914, loss: 3.609397, norm:0.3438, lr:5.7011e-04 dt: 3331.79ms, tok/sec:157359.43
step 2915, loss: 3.566273, norm:0.3163, lr:5.7008e-04 dt: 3332.10ms, tok/sec:157344.52
step 2916, loss: 3.589022, norm:0.2812, lr:5.7006e-04 dt: 3332.09ms, tok/sec:157345.24
step 2917, loss: 3.642380, norm:0.3036, lr:5.7003e-04 dt: 3332.11ms, tok/sec:157343.91
step 2918, loss: 3.643200, norm:0.3258, lr:5.7000e-04 dt: 3331.83ms, tok/sec:157357.33
step 2919, loss: 3.602988, norm:0.2842, lr:5.6998e-04 dt: 3332.19ms, tok/sec:157340.20
step 2920, loss: 3.599845, norm:0.2645, lr:5.6995e-04 dt: 3332.18ms, tok/sec:157340.61
step 2921, loss: 3.575274, norm:0.2790, lr:5.6992e-04 dt: 3332.44ms, tok/sec:157328.66
step 2922, loss: 3.512874, norm:0.3441, lr:5.6990e-04 dt: 3331.96ms, tok/sec:157351.29
step 2923, loss: 3.611399, norm:0.2989, lr:5.6987e-04 dt: 3331.90ms, tok/sec:157354.26
step 2924, loss: 3.519922, norm:0.2748, lr:5.6984e-04 dt: 3331.98ms, tok/sec:157350.23
step 2925, loss: 3.497891, norm:0.2637, lr:5.6982e-04 dt: 3332.02ms, tok/sec:157348.33
step 2926, loss: 3.440696, norm:0.2796, lr:5.6979e-04 dt: 3332.00ms, tok/sec:157349.44
step 2927, loss: 3.431341, norm:0.2744, lr:5.6976e-04 dt: 3332.04ms, tok/sec:157347.55
step 2928, loss: 3.385823, norm:0.2640, lr:5.6973e-04 dt: 3332.10ms, tok/sec:157344.71
step 2929, loss: 3.431875, norm:0.2613, lr:5.6971e-04 dt: 3331.88ms, tok/sec:157354.90
step 2930, loss: 3.448425, norm:0.2596, lr:5.6968e-04 dt: 3332.43ms, tok/sec:157328.93
step 2931, loss: 3.449857, norm:0.2770, lr:5.6965e-04 dt: 3332.11ms, tok/sec:157344.25
step 2932, loss: 3.454715, norm:0.2605, lr:5.6963e-04 dt: 3332.02ms, tok/sec:157348.45
step 2933, loss: 3.397113, norm:0.2463, lr:5.6960e-04 dt: 3331.96ms, tok/sec:157351.25
step 2934, loss: 3.438531, norm:0.2703, lr:5.6957e-04 dt: 3332.04ms, tok/sec:157347.29
step 2935, loss: 3.492333, norm:0.2505, lr:5.6955e-04 dt: 3331.96ms, tok/sec:157350.99
step 2936, loss: 3.604196, norm:0.2825, lr:5.6952e-04 dt: 3332.04ms, tok/sec:157347.29
step 2937, loss: 3.688562, norm:0.2640, lr:5.6949e-04 dt: 3332.29ms, tok/sec:157335.74
step 2938, loss: 3.690360, norm:0.2922, lr:5.6947e-04 dt: 3332.40ms, tok/sec:157330.38
step 2939, loss: 3.629871, norm:0.2770, lr:5.6944e-04 dt: 3332.14ms, tok/sec:157342.71
step 2940, loss: 3.630002, norm:0.2664, lr:5.6941e-04 dt: 3332.06ms, tok/sec:157346.39
step 2941, loss: 3.695169, norm:0.3124, lr:5.6939e-04 dt: 3332.36ms, tok/sec:157332.13
step 2942, loss: 3.682630, norm:0.2685, lr:5.6936e-04 dt: 3332.33ms, tok/sec:157333.89
step 2943, loss: 3.676674, norm:0.2943, lr:5.6933e-04 dt: 3332.27ms, tok/sec:157336.38
step 2944, loss: 3.618459, norm:0.2997, lr:5.6930e-04 dt: 3332.09ms, tok/sec:157344.90
step 2945, loss: 3.662555, norm:0.2774, lr:5.6928e-04 dt: 3332.12ms, tok/sec:157343.79
step 2946, loss: 3.676838, norm:0.2659, lr:5.6925e-04 dt: 3332.43ms, tok/sec:157329.02
step 2947, loss: 3.678249, norm:0.2895, lr:5.6922e-04 dt: 3332.60ms, tok/sec:157321.22
step 2948, loss: 3.609316, norm:0.2422, lr:5.6920e-04 dt: 3331.99ms, tok/sec:157350.00
step 2949, loss: 3.623797, norm:0.2831, lr:5.6917e-04 dt: 3332.16ms, tok/sec:157341.71
HellaSwag accuracy:4632167379979699281/-2=-2316083689989849600.0000
rank 1 sample 0: Hello, I'm a language model, and have been reading it. I could be very confident that one of the rules is a little difficult to implement.

rank 1 sample 1: Hello, I'm a language model, which I've got the most popular with.
It should be understood that if I run into classes I will be able
rank 1 sample 2: Hello, I'm a language model, but after the initial steps, I'm going to build an implementation of this exercise.
- Start a simple game you
rank 1 sample 3: Hello, I'm a language model, and I'm happy that everyone is saying (I'm pretty nice this one isn't just too long, I'm not
rank 0 sample 0: Hello, I'm a language model, but I still have to look forward to these important skills:
* Writing, writing and others in English.
*
rank 0 sample 1: Hello, I'm a language model, and what I'm saying is what you're going to understand, you were right, but we are here.
What
rank 0 sample 2: Hello, I'm a language model, I'm thinking, What?
How do you think of how we are doing something that's not about us? So
rank 0 sample 3: Hello, I'm a language model, you say, 'I'm not an 'official' language. We need to change things as well.
The model
step 2950, loss: 3.658569, norm:0.2486, lr:5.6914e-04 dt: 48516.45ms, tok/sec:10806.40
step 2951, loss: 3.605353, norm:0.2859, lr:5.6911e-04 dt: 3332.63ms, tok/sec:157319.65
step 2952, loss: 3.664689, norm:0.2782, lr:5.6909e-04 dt: 3331.99ms, tok/sec:157349.98
step 2953, loss: 3.606348, norm:0.2462, lr:5.6906e-04 dt: 3332.03ms, tok/sec:157348.03
step 2954, loss: 3.620796, norm:0.2703, lr:5.6903e-04 dt: 3332.12ms, tok/sec:157343.61
step 2955, loss: 3.636764, norm:0.2927, lr:5.6901e-04 dt: 3332.03ms, tok/sec:157347.82
step 2956, loss: 3.618735, norm:0.2726, lr:5.6898e-04 dt: 3332.14ms, tok/sec:157342.54
step 2957, loss: 3.712124, norm:0.2471, lr:5.6895e-04 dt: 3332.11ms, tok/sec:157344.17
step 2958, loss: 3.664849, norm:0.3055, lr:5.6892e-04 dt: 3331.99ms, tok/sec:157349.80
step 2959, loss: 3.664123, norm:0.2826, lr:5.6890e-04 dt: 3332.29ms, tok/sec:157335.54
step 2960, loss: 3.534312, norm:0.2730, lr:5.6887e-04 dt: 3331.98ms, tok/sec:157350.24
step 2961, loss: 3.545409, norm:0.2656, lr:5.6884e-04 dt: 3332.53ms, tok/sec:157324.46
step 2962, loss: 3.586213, norm:0.2672, lr:5.6882e-04 dt: 3332.19ms, tok/sec:157340.33
step 2963, loss: 3.561083, norm:0.2572, lr:5.6879e-04 dt: 3331.97ms, tok/sec:157350.96
step 2964, loss: 3.553675, norm:0.2779, lr:5.6876e-04 dt: 3332.05ms, tok/sec:157347.19
step 2965, loss: 3.624216, norm:0.2880, lr:5.6873e-04 dt: 3332.33ms, tok/sec:157333.94
step 2966, loss: 3.538711, norm:0.2435, lr:5.6871e-04 dt: 3331.70ms, tok/sec:157363.47
step 2967, loss: 3.607055, norm:0.2846, lr:5.6868e-04 dt: 3331.90ms, tok/sec:157354.25
step 2968, loss: 3.557538, norm:0.2835, lr:5.6865e-04 dt: 3332.22ms, tok/sec:157339.02
step 2969, loss: 3.574101, norm:0.2510, lr:5.6863e-04 dt: 3332.03ms, tok/sec:157348.00
step 2970, loss: 3.591336, norm:0.2700, lr:5.6860e-04 dt: 3332.66ms, tok/sec:157318.14
step 2971, loss: 3.527535, norm:0.2744, lr:5.6857e-04 dt: 3331.75ms, tok/sec:157361.14
step 2972, loss: 3.425217, norm:0.2587, lr:5.6854e-04 dt: 3331.85ms, tok/sec:157356.38
step 2973, loss: 3.427467, norm:0.2994, lr:5.6852e-04 dt: 3331.93ms, tok/sec:157352.53
step 2974, loss: 3.430879, norm:0.2673, lr:5.6849e-04 dt: 3332.20ms, tok/sec:157339.67
step 2975, loss: 3.421416, norm:0.2667, lr:5.6846e-04 dt: 3331.96ms, tok/sec:157351.38
step 2976, loss: 3.469963, norm:0.2517, lr:5.6843e-04 dt: 3331.99ms, tok/sec:157349.82
step 2977, loss: 3.492370, norm:0.2671, lr:5.6841e-04 dt: 3332.23ms, tok/sec:157338.59
step 2978, loss: 3.447306, norm:0.2704, lr:5.6838e-04 dt: 3332.07ms, tok/sec:157345.95
step 2979, loss: 3.468467, norm:0.2434, lr:5.6835e-04 dt: 3332.31ms, tok/sec:157334.65
step 2980, loss: 3.386119, norm:0.2699, lr:5.6832e-04 dt: 3331.84ms, tok/sec:157357.13
step 2981, loss: 3.403502, norm:0.2627, lr:5.6830e-04 dt: 3332.07ms, tok/sec:157345.83
step 2982, loss: 3.435780, norm:0.2814, lr:5.6827e-04 dt: 3331.87ms, tok/sec:157355.62
step 2983, loss: 3.705614, norm:0.2922, lr:5.6824e-04 dt: 3332.04ms, tok/sec:157347.24
step 2984, loss: 3.703542, norm:0.3008, lr:5.6821e-04 dt: 3332.07ms, tok/sec:157346.10
step 2985, loss: 3.600316, norm:0.3006, lr:5.6819e-04 dt: 3331.99ms, tok/sec:157349.79
step 2986, loss: 3.611720, norm:0.3038, lr:5.6816e-04 dt: 3332.25ms, tok/sec:157337.33
step 2987, loss: 3.647016, norm:0.2918, lr:5.6813e-04 dt: 3332.35ms, tok/sec:157332.77
step 2988, loss: 3.671551, norm:0.3067, lr:5.6810e-04 dt: 3332.11ms, tok/sec:157344.19
step 2989, loss: 3.612566, norm:0.2809, lr:5.6808e-04 dt: 3331.93ms, tok/sec:157352.62
step 2990, loss: 3.681623, norm:0.2841, lr:5.6805e-04 dt: 3332.22ms, tok/sec:157339.11
step 2991, loss: 3.645750, norm:0.2585, lr:5.6802e-04 dt: 3332.00ms, tok/sec:157349.17
step 2992, loss: 3.604644, norm:0.3206, lr:5.6799e-04 dt: 3332.03ms, tok/sec:157347.80
step 2993, loss: 3.651131, norm:0.3397, lr:5.6797e-04 dt: 3332.03ms, tok/sec:157347.91
step 2994, loss: 3.630819, norm:0.2924, lr:5.6794e-04 dt: 3332.10ms, tok/sec:157344.39
step 2995, loss: 3.595506, norm:0.2569, lr:5.6791e-04 dt: 3332.12ms, tok/sec:157343.90
step 2996, loss: 3.614495, norm:0.2498, lr:5.6788e-04 dt: 3332.21ms, tok/sec:157339.40
step 2997, loss: 3.668632, norm:0.2555, lr:5.6786e-04 dt: 3332.20ms, tok/sec:157340.12
step 2998, loss: 3.646681, norm:0.2483, lr:5.6783e-04 dt: 3332.11ms, tok/sec:157344.00
step 2999, loss: 3.618338, norm:0.2726, lr:5.6780e-04 dt: 3332.13ms, tok/sec:157343.14
validation loss: 3.6318
Model and optimizer state saved.
HellaSwag accuracy:-6906055415099571119/-2=3453027707549785600.0000
rank 1 sample 0: Hello, I'm a language model,
It is a software that will handle a set of mathematical variables.
Now, you're getting a basic understanding of
rank 1 sample 1: Hello, I'm a language model, it is an iterative process in which there is no explicit entity which is to say or by accident, and it is
rank 1 sample 2: Hello, I'm a language model, but why do I need to know it?
How does C/V print out the data to create the object

rank 1 sample 3: Hello, I'm a language model, and I'm just starting working on getting as much code as I can for, and am more and more testing out.
rank 0 sample 0: Hello, I'm a language model, and I don't think the whole picture about programming is clear, especially because there's an abstract. It's a way
rank 0 sample 1: Hello, I'm a language model, but if you're looking for ways to use them there it is possible to create a better sense of one another.

rank 0 sample 2: Hello, I'm a language model, but I could learn different languages independently (I was surprised to know the word "covid" is a word;
rank 0 sample 3: Hello, I'm a language model, but, I'm a language teacher and a child. I'm really working in any language school. As a child is
step 3000, loss: 3.621978, norm:0.2496, lr:5.6777e-04 dt: 56213.82ms, tok/sec:9326.67
step 3001, loss: 3.658099, norm:0.2705, lr:5.6775e-04 dt: 3332.14ms, tok/sec:157342.84
step 3002, loss: 3.643130, norm:0.2501, lr:5.6772e-04 dt: 3332.50ms, tok/sec:157325.90
step 3003, loss: 3.656215, norm:0.2533, lr:5.6769e-04 dt: 3331.95ms, tok/sec:157351.58
step 3004, loss: 3.640656, norm:0.2650, lr:5.6766e-04 dt: 3332.31ms, tok/sec:157334.73
step 3005, loss: 3.766124, norm:0.3733, lr:5.6764e-04 dt: 3332.05ms, tok/sec:157346.91
step 3006, loss: 3.641133, norm:0.4353, lr:5.6761e-04 dt: 3331.88ms, tok/sec:157355.14
step 3007, loss: 3.559520, norm:0.3113, lr:5.6758e-04 dt: 3332.14ms, tok/sec:157342.61
step 3008, loss: 3.604045, norm:0.3062, lr:5.6755e-04 dt: 3331.98ms, tok/sec:157350.06
step 3009, loss: 3.644709, norm:0.3185, lr:5.6753e-04 dt: 3332.14ms, tok/sec:157342.87
step 3010, loss: 3.602075, norm:0.2571, lr:5.6750e-04 dt: 3332.28ms, tok/sec:157335.93
step 3011, loss: 3.603497, norm:0.2890, lr:5.6747e-04 dt: 3332.07ms, tok/sec:157346.10
step 3012, loss: 3.516435, norm:0.2635, lr:5.6744e-04 dt: 3331.88ms, tok/sec:157354.80
step 3013, loss: 3.543204, norm:0.2436, lr:5.6741e-04 dt: 3332.15ms, tok/sec:157342.07
step 3014, loss: 3.602252, norm:0.2489, lr:5.6739e-04 dt: 3331.80ms, tok/sec:157358.88
step 3015, loss: 3.583856, norm:0.2433, lr:5.6736e-04 dt: 3332.13ms, tok/sec:157343.17
step 3016, loss: 3.541975, norm:0.2654, lr:5.6733e-04 dt: 3332.16ms, tok/sec:157341.87
step 3017, loss: 3.530758, norm:0.2647, lr:5.6730e-04 dt: 3332.02ms, tok/sec:157348.62
step 3018, loss: 3.528419, norm:0.2799, lr:5.6728e-04 dt: 3332.34ms, tok/sec:157333.06
step 3019, loss: 3.462819, norm:0.2777, lr:5.6725e-04 dt: 3332.23ms, tok/sec:157338.29
step 3020, loss: 3.382462, norm:0.2641, lr:5.6722e-04 dt: 3331.93ms, tok/sec:157352.44
step 3021, loss: 3.446991, norm:0.2759, lr:5.6719e-04 dt: 3332.24ms, tok/sec:157338.01
step 3022, loss: 3.372973, norm:0.2845, lr:5.6716e-04 dt: 3331.80ms, tok/sec:157358.62
step 3023, loss: 3.448256, norm:0.2694, lr:5.6714e-04 dt: 3332.04ms, tok/sec:157347.46
step 3024, loss: 3.434214, norm:0.2623, lr:5.6711e-04 dt: 3332.23ms, tok/sec:157338.69
step 3025, loss: 3.381814, norm:0.2780, lr:5.6708e-04 dt: 3332.15ms, tok/sec:157342.13
step 3026, loss: 3.380038, norm:0.2753, lr:5.6705e-04 dt: 3331.94ms, tok/sec:157352.29
step 3027, loss: 3.412469, norm:0.2544, lr:5.6702e-04 dt: 3332.09ms, tok/sec:157345.12
step 3028, loss: 3.403275, norm:0.2362, lr:5.6700e-04 dt: 3332.22ms, tok/sec:157339.05
step 3029, loss: 3.386494, norm:0.2845, lr:5.6697e-04 dt: 3332.09ms, tok/sec:157345.31
step 3030, loss: 3.530344, norm:0.2732, lr:5.6694e-04 dt: 3332.01ms, tok/sec:157348.78
step 3031, loss: 3.673897, norm:0.2431, lr:5.6691e-04 dt: 3332.06ms, tok/sec:157346.58
step 3032, loss: 3.625261, norm:0.2600, lr:5.6688e-04 dt: 3331.95ms, tok/sec:157351.52
step 3033, loss: 3.607050, norm:0.2865, lr:5.6686e-04 dt: 3332.22ms, tok/sec:157339.06
step 3034, loss: 3.659157, norm:0.2739, lr:5.6683e-04 dt: 3332.07ms, tok/sec:157346.16
step 3035, loss: 3.660987, norm:0.2922, lr:5.6680e-04 dt: 3332.22ms, tok/sec:157338.72
step 3036, loss: 3.641701, norm:0.2624, lr:5.6677e-04 dt: 3332.15ms, tok/sec:157342.23
step 3037, loss: 3.638436, norm:0.2630, lr:5.6674e-04 dt: 3332.64ms, tok/sec:157319.07
step 3038, loss: 3.639571, norm:0.2615, lr:5.6672e-04 dt: 3332.16ms, tok/sec:157341.81
step 3039, loss: 3.670267, norm:0.2872, lr:5.6669e-04 dt: 3331.98ms, tok/sec:157350.51
step 3040, loss: 3.676481, norm:0.2771, lr:5.6666e-04 dt: 3332.09ms, tok/sec:157345.17
step 3041, loss: 3.694875, norm:0.2968, lr:5.6663e-04 dt: 3332.40ms, tok/sec:157330.43
step 3042, loss: 3.622558, norm:0.2630, lr:5.6660e-04 dt: 3332.02ms, tok/sec:157348.21
step 3043, loss: 3.646895, norm:0.2953, lr:5.6658e-04 dt: 3331.91ms, tok/sec:157353.79
step 3044, loss: 3.599971, norm:0.2758, lr:5.6655e-04 dt: 3332.19ms, tok/sec:157340.25
step 3045, loss: 3.579592, norm:0.2975, lr:5.6652e-04 dt: 3332.39ms, tok/sec:157331.15
step 3046, loss: 3.658648, norm:0.2548, lr:5.6649e-04 dt: 3332.45ms, tok/sec:157328.07
step 3047, loss: 3.668518, norm:0.2811, lr:5.6646e-04 dt: 3334.02ms, tok/sec:157254.11
step 3048, loss: 3.665706, norm:0.2922, lr:5.6644e-04 dt: 3332.14ms, tok/sec:157342.52
step 3049, loss: 3.588202, norm:0.2554, lr:5.6641e-04 dt: 3332.15ms, tok/sec:157342.09
HellaSwag accuracy:2326341965099549777/-2=-1163170982549774848.0000
rank 1 sample 0: Hello, I'm a language model, with some slight differences, but I certainly do believe I won't be able to explain that to others. But I'm
rank 1 sample 1: Hello, I'm a language model, it is very dynamic. I know this is a very fast process, and we continue studying linguistics and language as a
rank 1 sample 2: Hello, I'm a language model, but first of all, I'm going to use the machine (the "hello" machine). It was the one that
rank 1 sample 3: Hello, I'm a language model, and I'm writing my PhD in Java without a reference copy. After finishing each of these blog posts, I was able
rank 0 sample 0: Hello, I'm a language model, and I'm a teacher! It's important to keep up to date with all the changes that occur during each year to
rank 0 sample 1: Hello, I'm a language model, so my children are not sure where the language comes from. I have done it in a class, then the children are
rank 0 sample 2: Hello, I'm a language model, but I guess what role are they, and then I'm interested to share some of their experiences with you.<|endoftext|>�
rank 0 sample 3: Hello, I'm a language model, and for the last time I've come back an article from the past. It means "You can only speak one person
step 3050, loss: 3.608378, norm:0.2757, lr:5.6638e-04 dt: 48525.87ms, tok/sec:10804.30
step 3051, loss: 3.587628, norm:0.2835, lr:5.6635e-04 dt: 3332.27ms, tok/sec:157336.53
step 3052, loss: 3.623009, norm:0.2632, lr:5.6632e-04 dt: 3332.51ms, tok/sec:157325.30
step 3053, loss: 3.570186, norm:0.2416, lr:5.6629e-04 dt: 3332.09ms, tok/sec:157344.91
step 3054, loss: 3.596090, norm:0.2514, lr:5.6627e-04 dt: 3332.18ms, tok/sec:157340.91
step 3055, loss: 3.524458, norm:0.2339, lr:5.6624e-04 dt: 3332.23ms, tok/sec:157338.45
step 3056, loss: 3.513235, norm:0.2467, lr:5.6621e-04 dt: 3332.14ms, tok/sec:157342.54
step 3057, loss: 3.588750, norm:0.2411, lr:5.6618e-04 dt: 3331.86ms, tok/sec:157355.77
step 3058, loss: 3.592573, norm:0.2474, lr:5.6615e-04 dt: 3332.22ms, tok/sec:157338.74
step 3059, loss: 3.549189, norm:0.2729, lr:5.6612e-04 dt: 3332.20ms, tok/sec:157339.85
step 3060, loss: 3.566042, norm:0.2621, lr:5.6610e-04 dt: 3332.23ms, tok/sec:157338.52
step 3061, loss: 3.571793, norm:0.2670, lr:5.6607e-04 dt: 3332.00ms, tok/sec:157349.13
step 3062, loss: 3.533580, norm:0.2589, lr:5.6604e-04 dt: 3332.16ms, tok/sec:157341.86
step 3063, loss: 3.586500, norm:0.2414, lr:5.6601e-04 dt: 3331.97ms, tok/sec:157350.60
step 3064, loss: 3.436771, norm:0.2748, lr:5.6598e-04 dt: 3332.05ms, tok/sec:157346.77
step 3065, loss: 3.399041, norm:0.2449, lr:5.6595e-04 dt: 3332.12ms, tok/sec:157343.68
step 3066, loss: 3.413532, norm:0.2449, lr:5.6593e-04 dt: 3332.02ms, tok/sec:157348.58
step 3067, loss: 3.448720, norm:0.2416, lr:5.6590e-04 dt: 3332.37ms, tok/sec:157332.05
step 3068, loss: 3.433396, norm:0.2548, lr:5.6587e-04 dt: 3332.14ms, tok/sec:157342.49
step 3069, loss: 3.357790, norm:0.2546, lr:5.6584e-04 dt: 3331.99ms, tok/sec:157349.70
step 3070, loss: 3.499224, norm:0.2842, lr:5.6581e-04 dt: 3332.03ms, tok/sec:157347.89
step 3071, loss: 3.416920, norm:0.2762, lr:5.6578e-04 dt: 3332.14ms, tok/sec:157342.56
step 3072, loss: 3.436879, norm:0.2755, lr:5.6576e-04 dt: 3332.09ms, tok/sec:157345.14
step 3073, loss: 3.385681, norm:0.2616, lr:5.6573e-04 dt: 3332.00ms, tok/sec:157349.32
step 3074, loss: 3.339972, norm:0.2703, lr:5.6570e-04 dt: 3332.26ms, tok/sec:157337.14
step 3075, loss: 3.360603, norm:0.2724, lr:5.6567e-04 dt: 3332.10ms, tok/sec:157344.76
step 3076, loss: 3.589962, norm:0.2861, lr:5.6564e-04 dt: 3332.42ms, tok/sec:157329.51
step 3077, loss: 3.667279, norm:0.2717, lr:5.6561e-04 dt: 3332.14ms, tok/sec:157342.53
step 3078, loss: 3.577846, norm:0.2660, lr:5.6559e-04 dt: 3331.99ms, tok/sec:157349.85
step 3079, loss: 3.660325, norm:0.2644, lr:5.6556e-04 dt: 3332.23ms, tok/sec:157338.70
step 3080, loss: 3.582625, norm:0.2771, lr:5.6553e-04 dt: 3332.02ms, tok/sec:157348.33
step 3081, loss: 3.694480, norm:0.2741, lr:5.6550e-04 dt: 3332.06ms, tok/sec:157346.72
step 3082, loss: 3.613554, norm:0.2685, lr:5.6547e-04 dt: 3332.25ms, tok/sec:157337.51
step 3083, loss: 3.614816, norm:0.2491, lr:5.6544e-04 dt: 3332.57ms, tok/sec:157322.24
step 3084, loss: 3.616448, norm:0.2475, lr:5.6541e-04 dt: 3332.10ms, tok/sec:157344.49
step 3085, loss: 3.646142, norm:0.2530, lr:5.6539e-04 dt: 3331.87ms, tok/sec:157355.28
step 3086, loss: 3.591796, norm:0.2626, lr:5.6536e-04 dt: 3332.12ms, tok/sec:157343.72
step 3087, loss: 3.634221, norm:0.2492, lr:5.6533e-04 dt: 3332.09ms, tok/sec:157345.15
step 3088, loss: 3.670433, norm:0.2641, lr:5.6530e-04 dt: 3332.14ms, tok/sec:157342.71
step 3089, loss: 3.590957, norm:0.2752, lr:5.6527e-04 dt: 3332.17ms, tok/sec:157341.14
step 3090, loss: 3.653032, norm:0.2559, lr:5.6524e-04 dt: 3332.38ms, tok/sec:157331.41
step 3091, loss: 3.606067, norm:0.2486, lr:5.6521e-04 dt: 3332.07ms, tok/sec:157346.09
step 3092, loss: 3.570382, norm:0.2854, lr:5.6519e-04 dt: 3332.39ms, tok/sec:157331.12
step 3093, loss: 3.619249, norm:0.2866, lr:5.6516e-04 dt: 3331.93ms, tok/sec:157352.58
step 3094, loss: 3.594219, norm:0.2767, lr:5.6513e-04 dt: 3331.83ms, tok/sec:157357.37
step 3095, loss: 3.622414, norm:0.2594, lr:5.6510e-04 dt: 3331.99ms, tok/sec:157349.61
step 3096, loss: 3.637672, norm:0.2725, lr:5.6507e-04 dt: 3332.32ms, tok/sec:157333.99
step 3097, loss: 3.672213, norm:0.2828, lr:5.6504e-04 dt: 3331.97ms, tok/sec:157350.84
step 3098, loss: 3.672049, norm:0.2768, lr:5.6501e-04 dt: 3332.11ms, tok/sec:157344.27
step 3099, loss: 3.513586, norm:0.2675, lr:5.6498e-04 dt: 3332.33ms, tok/sec:157333.54
validation loss: 3.6220
Model and optimizer state saved.
HellaSwag accuracy:4632184422409684049/-2=-2316092211204841984.0000
rank 1 sample 0: Hello, I'm a language model, as well as a web-based implementation that helps to explain your own language with a short video that explains why you should
rank 1 sample 1: Hello, I'm a language model, which I've started out with. So when I think how do we really use semantic reasoning in class?
I'm
rank 1 sample 2: Hello, I'm a language model, but its context is not there. It's a huge tool I can do things with. I am going to write my
rank 1 sample 3: Hello, I'm a language model, and I'm just going to work on many of the people that made a comment. A certain language model is more than
rank 0 sample 0: Hello, I'm a language model, and I'd like a book that says these topics as a guide too. So I want to give a look at my
rank 0 sample 1: Hello, I'm a language model, but we're not a language expert -- I'll focus on a topic just a little while. I haven't got an
rank 0 sample 2: Hello, I'm a language model, but I wish I'd say a good reason before I go about that.
We've got a lot of great programming
rank 0 sample 3: Hello, I'm a language model, and one of the most effective tools to teach languages. I'm teaching about a common language such as Bilingualism

step 3100, loss: 3.530342, norm:0.2588, lr:5.6496e-04 dt: 56145.32ms, tok/sec:9338.05
step 3101, loss: 3.572446, norm:0.2477, lr:5.6493e-04 dt: 3331.90ms, tok/sec:157353.89
step 3102, loss: 3.491783, norm:0.2631, lr:5.6490e-04 dt: 3332.05ms, tok/sec:157346.86
step 3103, loss: 3.663478, norm:0.2814, lr:5.6487e-04 dt: 3332.42ms, tok/sec:157329.53
step 3104, loss: 3.557151, norm:0.2755, lr:5.6484e-04 dt: 3332.16ms, tok/sec:157341.80
step 3105, loss: 3.567655, norm:0.2504, lr:5.6481e-04 dt: 3331.94ms, tok/sec:157352.33
step 3106, loss: 3.655336, norm:0.2660, lr:5.6478e-04 dt: 3332.03ms, tok/sec:157347.85
step 3107, loss: 3.577138, norm:0.2840, lr:5.6475e-04 dt: 3332.25ms, tok/sec:157337.57
step 3108, loss: 3.576647, norm:0.2729, lr:5.6473e-04 dt: 3331.87ms, tok/sec:157355.51
step 3109, loss: 3.564134, norm:0.2626, lr:5.6470e-04 dt: 3332.22ms, tok/sec:157338.90
step 3110, loss: 3.565091, norm:0.2646, lr:5.6467e-04 dt: 3332.39ms, tok/sec:157330.85
step 3111, loss: 3.538469, norm:0.2477, lr:5.6464e-04 dt: 3331.97ms, tok/sec:157350.61
step 3112, loss: 3.430556, norm:0.2751, lr:5.6461e-04 dt: 3332.04ms, tok/sec:157347.29
step 3113, loss: 3.360229, norm:0.2642, lr:5.6458e-04 dt: 3332.04ms, tok/sec:157347.66
step 3114, loss: 3.413111, norm:0.2935, lr:5.6455e-04 dt: 3332.10ms, tok/sec:157344.85
step 3115, loss: 3.399869, norm:0.2574, lr:5.6452e-04 dt: 3331.98ms, tok/sec:157350.07
step 3116, loss: 3.448528, norm:0.2472, lr:5.6449e-04 dt: 3332.03ms, tok/sec:157347.83
step 3117, loss: 3.402041, norm:0.2878, lr:5.6447e-04 dt: 3331.81ms, tok/sec:157358.20
step 3118, loss: 3.384413, norm:0.2604, lr:5.6444e-04 dt: 3332.20ms, tok/sec:157339.74
step 3119, loss: 3.445431, norm:0.2613, lr:5.6441e-04 dt: 3332.45ms, tok/sec:157328.07
step 3120, loss: 3.442296, norm:0.2907, lr:5.6438e-04 dt: 3332.06ms, tok/sec:157346.30
step 3121, loss: 3.391808, norm:0.2958, lr:5.6435e-04 dt: 3332.02ms, tok/sec:157348.63
step 3122, loss: 3.388528, norm:0.2759, lr:5.6432e-04 dt: 3332.19ms, tok/sec:157340.26
step 3123, loss: 3.469514, norm:0.3015, lr:5.6429e-04 dt: 3332.08ms, tok/sec:157345.39
step 3124, loss: 3.560297, norm:0.3249, lr:5.6426e-04 dt: 3331.90ms, tok/sec:157353.87
step 3125, loss: 3.701009, norm:0.2655, lr:5.6423e-04 dt: 3332.32ms, tok/sec:157334.24
step 3126, loss: 3.552489, norm:0.3285, lr:5.6421e-04 dt: 3332.04ms, tok/sec:157347.62
step 3127, loss: 3.595263, norm:0.2884, lr:5.6418e-04 dt: 3332.50ms, tok/sec:157325.50
step 3128, loss: 3.561223, norm:0.3150, lr:5.6415e-04 dt: 3332.05ms, tok/sec:157347.15
step 3129, loss: 3.615186, norm:0.2773, lr:5.6412e-04 dt: 3331.97ms, tok/sec:157350.79
step 3130, loss: 3.601883, norm:0.2901, lr:5.6409e-04 dt: 3332.14ms, tok/sec:157342.64
step 3131, loss: 3.589643, norm:0.2868, lr:5.6406e-04 dt: 3332.39ms, tok/sec:157331.16
step 3132, loss: 3.572521, norm:0.2724, lr:5.6403e-04 dt: 3332.00ms, tok/sec:157349.28
step 3133, loss: 3.547602, norm:0.2775, lr:5.6400e-04 dt: 3331.99ms, tok/sec:157349.83
step 3134, loss: 3.602127, norm:0.2743, lr:5.6397e-04 dt: 3332.29ms, tok/sec:157335.60
step 3135, loss: 3.548311, norm:0.3081, lr:5.6394e-04 dt: 3332.48ms, tok/sec:157326.72
step 3136, loss: 3.670507, norm:0.3225, lr:5.6391e-04 dt: 3331.86ms, tok/sec:157356.16
step 3137, loss: 3.648518, norm:0.3082, lr:5.6389e-04 dt: 3332.03ms, tok/sec:157347.92
step 3138, loss: 3.683876, norm:0.2976, lr:5.6386e-04 dt: 3332.21ms, tok/sec:157339.45
step 3139, loss: 3.649076, norm:0.2665, lr:5.6383e-04 dt: 3332.18ms, tok/sec:157340.94
step 3140, loss: 3.686491, norm:0.3025, lr:5.6380e-04 dt: 3331.87ms, tok/sec:157355.36
step 3141, loss: 3.661116, norm:0.2953, lr:5.6377e-04 dt: 3332.39ms, tok/sec:157330.97
step 3142, loss: 3.656224, norm:0.2706, lr:5.6374e-04 dt: 3332.11ms, tok/sec:157344.20
step 3143, loss: 3.672216, norm:0.2956, lr:5.6371e-04 dt: 3332.75ms, tok/sec:157314.00
step 3144, loss: 3.647888, norm:0.2523, lr:5.6368e-04 dt: 3331.99ms, tok/sec:157349.92
step 3145, loss: 3.622731, norm:0.2921, lr:5.6365e-04 dt: 3331.92ms, tok/sec:157352.90
step 3146, loss: 3.606881, norm:0.2843, lr:5.6362e-04 dt: 3332.03ms, tok/sec:157348.08
step 3147, loss: 3.581146, norm:0.2660, lr:5.6359e-04 dt: 3332.21ms, tok/sec:157339.42
step 3148, loss: 3.599326, norm:0.2548, lr:5.6356e-04 dt: 3331.97ms, tok/sec:157350.89
step 3149, loss: 3.555925, norm:0.2531, lr:5.6353e-04 dt: 3331.90ms, tok/sec:157354.08
HellaSwag accuracy:4631622022212322385/-2=-2315811011106161152.0000
rank 1 sample 0: Hello, I'm a language model, the next time I was a big data modeler and just took a look at the output and its output, I was
rank 1 sample 1: Hello, I'm a language model, which I am aware of. This is all about the "Word.Word Model". My new Language model is a model
rank 1 sample 2: Hello, I'm a language model, I wish to go to the next post.
There was nothing wrong. For example, it wasn't really easy.
rank 1 sample 3: Hello, I'm a language model, and I'm very interested in the subject where I teach people what you call Python. Most articles will be published in the
rank 0 sample 0: Hello, I'm a language model, and I really want you to see some similarities amongst a single language."
In the beginning, "The Languages of Australia
rank 0 sample 1: Hello, I'm a language model, I find it fascinating. I have, for the language training I teach, there is nothing wrong in our teaching. It
rank 0 sample 2: Hello, I'm a language model, but I prefer that. We learn from the data, and this isn't the case. We are constantly learning to apply
rank 0 sample 3: Hello, I'm a language model, I find it useful to have an effective model program and I am doing so for several years.
How to Model Out
step 3150, loss: 3.530431, norm:0.2489, lr:5.6351e-04 dt: 48515.63ms, tok/sec:10806.58
step 3151, loss: 3.589849, norm:0.2649, lr:5.6348e-04 dt: 3332.10ms, tok/sec:157344.52
step 3152, loss: 3.588176, norm:0.2417, lr:5.6345e-04 dt: 3332.08ms, tok/sec:157345.75
step 3153, loss: 3.554265, norm:0.2475, lr:5.6342e-04 dt: 3332.25ms, tok/sec:157337.50
step 3154, loss: 3.555036, norm:0.2458, lr:5.6339e-04 dt: 3332.18ms, tok/sec:157340.68
step 3155, loss: 3.498960, norm:0.2604, lr:5.6336e-04 dt: 3331.81ms, tok/sec:157358.17
step 3156, loss: 3.570408, norm:0.2717, lr:5.6333e-04 dt: 3332.17ms, tok/sec:157341.31
step 3157, loss: 3.573357, norm:0.2718, lr:5.6330e-04 dt: 3331.92ms, tok/sec:157353.20
step 3158, loss: 3.546814, norm:0.2618, lr:5.6327e-04 dt: 3332.27ms, tok/sec:157336.40
step 3159, loss: 3.364217, norm:0.2957, lr:5.6324e-04 dt: 3332.16ms, tok/sec:157341.69
step 3160, loss: 3.412393, norm:0.2640, lr:5.6321e-04 dt: 3332.05ms, tok/sec:157346.85
step 3161, loss: 3.487032, norm:0.2628, lr:5.6318e-04 dt: 3332.43ms, tok/sec:157328.84
step 3162, loss: 3.401523, norm:0.2872, lr:5.6315e-04 dt: 3332.06ms, tok/sec:157346.39
step 3163, loss: 3.407601, norm:0.2646, lr:5.6312e-04 dt: 3331.88ms, tok/sec:157354.98
step 3164, loss: 3.402521, norm:0.2826, lr:5.6309e-04 dt: 3331.97ms, tok/sec:157350.74
step 3165, loss: 3.339930, norm:0.2527, lr:5.6306e-04 dt: 3332.06ms, tok/sec:157346.74
step 3166, loss: 3.396644, norm:0.2529, lr:5.6304e-04 dt: 3332.13ms, tok/sec:157343.11
step 3167, loss: 3.408325, norm:0.2782, lr:5.6301e-04 dt: 3331.96ms, tok/sec:157351.46
step 3168, loss: 3.371203, norm:0.2737, lr:5.6298e-04 dt: 3332.28ms, tok/sec:157336.07
step 3169, loss: 3.381174, norm:0.2532, lr:5.6295e-04 dt: 3332.05ms, tok/sec:157347.10
step 3170, loss: 3.491272, norm:0.2592, lr:5.6292e-04 dt: 3332.02ms, tok/sec:157348.30
step 3171, loss: 3.586385, norm:0.2693, lr:5.6289e-04 dt: 3332.12ms, tok/sec:157343.77
step 3172, loss: 3.593476, norm:0.2794, lr:5.6286e-04 dt: 3332.02ms, tok/sec:157348.38
step 3173, loss: 3.582512, norm:0.2727, lr:5.6283e-04 dt: 3331.93ms, tok/sec:157352.66
step 3174, loss: 3.609365, norm:0.2894, lr:5.6280e-04 dt: 3332.15ms, tok/sec:157342.04
step 3175, loss: 3.569498, norm:0.2756, lr:5.6277e-04 dt: 3332.16ms, tok/sec:157341.73
step 3176, loss: 3.588233, norm:0.3078, lr:5.6274e-04 dt: 3332.05ms, tok/sec:157347.19
step 3177, loss: 3.610050, norm:0.2878, lr:5.6271e-04 dt: 3332.49ms, tok/sec:157326.01
step 3178, loss: 3.545896, norm:0.2578, lr:5.6268e-04 dt: 3332.08ms, tok/sec:157345.35
step 3179, loss: 3.608267, norm:0.2991, lr:5.6265e-04 dt: 3332.20ms, tok/sec:157340.01
step 3180, loss: 3.645315, norm:0.2481, lr:5.6262e-04 dt: 3332.23ms, tok/sec:157338.40
step 3181, loss: 3.574542, norm:0.2566, lr:5.6259e-04 dt: 3332.05ms, tok/sec:157347.20
step 3182, loss: 3.575513, norm:0.2895, lr:5.6256e-04 dt: 3332.20ms, tok/sec:157339.86
step 3183, loss: 3.611484, norm:0.3272, lr:5.6253e-04 dt: 3332.31ms, tok/sec:157334.80
step 3184, loss: 3.626655, norm:0.2658, lr:5.6250e-04 dt: 3332.34ms, tok/sec:157333.11
step 3185, loss: 3.584213, norm:0.2600, lr:5.6247e-04 dt: 3332.24ms, tok/sec:157337.98
step 3186, loss: 3.540337, norm:0.2782, lr:5.6244e-04 dt: 3332.08ms, tok/sec:157345.34
step 3187, loss: 3.651653, norm:0.3583, lr:5.6241e-04 dt: 3331.86ms, tok/sec:157356.11
step 3188, loss: 3.611276, norm:0.3461, lr:5.6238e-04 dt: 3331.96ms, tok/sec:157351.33
step 3189, loss: 3.581005, norm:0.2854, lr:5.6236e-04 dt: 3332.07ms, tok/sec:157346.03
step 3190, loss: 3.577837, norm:0.3249, lr:5.6233e-04 dt: 3332.10ms, tok/sec:157344.46
step 3191, loss: 3.592230, norm:0.2851, lr:5.6230e-04 dt: 3332.11ms, tok/sec:157344.30
step 3192, loss: 3.610601, norm:0.2697, lr:5.6227e-04 dt: 3332.09ms, tok/sec:157344.99
step 3193, loss: 3.583465, norm:0.2576, lr:5.6224e-04 dt: 3332.41ms, tok/sec:157330.11
step 3194, loss: 3.525041, norm:0.2462, lr:5.6221e-04 dt: 3332.08ms, tok/sec:157345.77
step 3195, loss: 3.540769, norm:0.2465, lr:5.6218e-04 dt: 3332.11ms, tok/sec:157343.97
step 3196, loss: 3.599068, norm:0.2539, lr:5.6215e-04 dt: 3332.05ms, tok/sec:157346.84
step 3197, loss: 3.545248, norm:0.2587, lr:5.6212e-04 dt: 3332.04ms, tok/sec:157347.58
step 3198, loss: 3.556859, norm:0.2504, lr:5.6209e-04 dt: 3332.07ms, tok/sec:157345.93
step 3199, loss: 3.543026, norm:0.2839, lr:5.6206e-04 dt: 3332.32ms, tok/sec:157334.42
validation loss: 3.6115
Model and optimizer state saved.
HellaSwag accuracy:-6906600257470626807/-2=3453300128735313408.0000
rank 1 sample 0: Hello, I'm a language model,
What's the heck is the right answer here: How we can do this? Here, it's pretty easy to
rank 1 sample 1: Hello, I'm a language model, which I have not been able to understand enough about what these words are. If I cannot understand enough, I can't
rank 1 sample 2: Hello, I'm a language model, I could just be a language model.
I also like having to create a new language, so it is not something
rank 0 sample 0: Hello, I'm a language model, and I like to say my first languages first hand. I'll leave behind all the rest of our vocabulary list. Itrank 1 sample 3: Hello, I'm a language model, and I'm always excited about this and got a look across this subject! When I found this to you, I thought

rank 0 sample 1: Hello, I'm a language model, I will teach you a number. Since I have read everything, I'm using it in my classes now, but I
rank 0 sample 2: Hello, I'm a language model, I'm pretty sure now all of these are, and I will say, "Oh, I'm a language model."
rank 0 sample 3: Hello, I'm a language model, which isn't very good at the other hand the language model is based on. Any language like that one can be defined
step 3200, loss: 3.543447, norm:0.2651, lr:5.6203e-04 dt: 56253.84ms, tok/sec:9320.04
step 3201, loss: 3.547894, norm:0.2670, lr:5.6200e-04 dt: 3332.17ms, tok/sec:157341.35
step 3202, loss: 3.494654, norm:0.2585, lr:5.6197e-04 dt: 3332.00ms, tok/sec:157349.53
step 3203, loss: 3.637869, norm:0.2471, lr:5.6194e-04 dt: 3332.32ms, tok/sec:157334.46
step 3204, loss: 3.577418, norm:0.2809, lr:5.6191e-04 dt: 3332.21ms, tok/sec:157339.51
step 3205, loss: 3.487782, norm:0.2709, lr:5.6188e-04 dt: 3331.96ms, tok/sec:157351.38
step 3206, loss: 3.432806, norm:0.2796, lr:5.6185e-04 dt: 3332.08ms, tok/sec:157345.47
step 3207, loss: 3.466767, norm:0.2728, lr:5.6182e-04 dt: 3332.02ms, tok/sec:157348.36
step 3208, loss: 3.385791, norm:0.2913, lr:5.6179e-04 dt: 3331.96ms, tok/sec:157351.24
step 3209, loss: 3.371473, norm:0.2413, lr:5.6176e-04 dt: 3332.35ms, tok/sec:157333.04
step 3210, loss: 3.327615, norm:0.2774, lr:5.6173e-04 dt: 3332.08ms, tok/sec:157345.78
step 3211, loss: 3.400158, norm:0.2544, lr:5.6170e-04 dt: 3331.92ms, tok/sec:157353.07
step 3212, loss: 3.391932, norm:0.2706, lr:5.6167e-04 dt: 3331.88ms, tok/sec:157354.96
step 3213, loss: 3.401206, norm:0.2670, lr:5.6164e-04 dt: 3332.20ms, tok/sec:157339.92
step 3214, loss: 3.453180, norm:0.2538, lr:5.6161e-04 dt: 3331.81ms, tok/sec:157358.12
step 3215, loss: 3.384446, norm:0.2740, lr:5.6158e-04 dt: 3332.19ms, tok/sec:157340.31
step 3216, loss: 3.361627, norm:0.2829, lr:5.6155e-04 dt: 3332.20ms, tok/sec:157339.85
step 3217, loss: 3.583439, norm:0.2755, lr:5.6152e-04 dt: 3332.34ms, tok/sec:157333.26
step 3218, loss: 3.632107, norm:0.2692, lr:5.6149e-04 dt: 3332.41ms, tok/sec:157330.14
step 3219, loss: 3.639735, norm:0.2758, lr:5.6146e-04 dt: 3332.16ms, tok/sec:157341.74
step 3220, loss: 3.626532, norm:0.2757, lr:5.6143e-04 dt: 3332.12ms, tok/sec:157343.59
step 3221, loss: 3.585550, norm:0.2738, lr:5.6140e-04 dt: 3332.69ms, tok/sec:157316.65
step 3222, loss: 3.615666, norm:0.3035, lr:5.6137e-04 dt: 3332.15ms, tok/sec:157342.12
step 3223, loss: 3.599319, norm:0.2665, lr:5.6134e-04 dt: 3332.12ms, tok/sec:157343.77
step 3224, loss: 3.625893, norm:0.2736, lr:5.6131e-04 dt: 3332.14ms, tok/sec:157342.96
step 3225, loss: 3.618681, norm:0.2853, lr:5.6128e-04 dt: 3332.47ms, tok/sec:157327.14
step 3226, loss: 3.543917, norm:0.2707, lr:5.6125e-04 dt: 3332.26ms, tok/sec:157337.23
step 3227, loss: 3.666977, norm:0.2793, lr:5.6122e-04 dt: 3331.95ms, tok/sec:157351.69
step 3228, loss: 3.599093, norm:0.2763, lr:5.6119e-04 dt: 3332.20ms, tok/sec:157339.70
step 3229, loss: 3.585833, norm:0.2574, lr:5.6116e-04 dt: 3332.21ms, tok/sec:157339.56
step 3230, loss: 3.581420, norm:0.2665, lr:5.6113e-04 dt: 3332.07ms, tok/sec:157345.94
step 3231, loss: 3.725029, norm:0.2517, lr:5.6110e-04 dt: 3332.36ms, tok/sec:157332.51
step 3232, loss: 3.622332, norm:0.2524, lr:5.6107e-04 dt: 3332.39ms, tok/sec:157331.02
step 3233, loss: 3.570563, norm:0.2481, lr:5.6104e-04 dt: 3332.19ms, tok/sec:157340.51
step 3234, loss: 3.617951, norm:0.2558, lr:5.6101e-04 dt: 3331.95ms, tok/sec:157351.67
step 3235, loss: 3.626642, norm:0.2695, lr:5.6098e-04 dt: 3332.17ms, tok/sec:157341.26
step 3236, loss: 3.662015, norm:0.2670, lr:5.6095e-04 dt: 3332.05ms, tok/sec:157347.11
step 3237, loss: 3.596165, norm:0.2564, lr:5.6092e-04 dt: 3332.22ms, tok/sec:157338.86
step 3238, loss: 3.821720, norm:0.4254, lr:5.6089e-04 dt: 3334.64ms, tok/sec:157224.92
step 3239, loss: 3.596812, norm:0.3603, lr:5.6086e-04 dt: 3332.11ms, tok/sec:157344.27
step 3240, loss: 3.578990, norm:0.2892, lr:5.6083e-04 dt: 3332.42ms, tok/sec:157329.58
step 3241, loss: 3.541183, norm:0.2928, lr:5.6080e-04 dt: 3332.31ms, tok/sec:157334.68
step 3242, loss: 3.548360, norm:0.3175, lr:5.6077e-04 dt: 3332.05ms, tok/sec:157347.00
step 3243, loss: 3.498829, norm:0.2616, lr:5.6074e-04 dt: 3331.84ms, tok/sec:157356.67
step 3244, loss: 3.568441, norm:0.3222, lr:5.6071e-04 dt: 3332.13ms, tok/sec:157343.24
step 3245, loss: 3.513304, norm:0.2678, lr:5.6067e-04 dt: 3332.11ms, tok/sec:157344.20
step 3246, loss: 3.564138, norm:0.2703, lr:5.6064e-04 dt: 3332.09ms, tok/sec:157344.89
step 3247, loss: 3.516912, norm:0.2605, lr:5.6061e-04 dt: 3332.13ms, tok/sec:157343.00
step 3248, loss: 3.522884, norm:0.2540, lr:5.6058e-04 dt: 3332.07ms, tok/sec:157346.03
step 3249, loss: 3.569303, norm:0.2697, lr:5.6055e-04 dt: 3332.26ms, tok/sec:157337.03
HellaSwag accuracy:2326482700440372297/-2=-1163241350220186112.0000
rank 1 sample 0: Hello, I'm a language model, or if I'm a computer scientist ... or, as of 2011, I will be the first and only one to be
rank 1 sample 1: Hello, I'm a language model, which I've taken in the past week of my life that I never talked about, when I finally came to the idea
rank 1 sample 2: Hello, I'm a language model, I also know about the internet, but I'm not quite certain about how to use it. In my opinion about how
rank 1 sample 3: Hello, I'm a language model, and I'm looking for people who haven't been through traditional syntax when using PHP.”
“It'srank 0 sample 0: Hello, I'm a language model, and I like to say. What makes grammar exactly what it's worth remembering for me.
But what do I want

rank 0 sample 1: Hello, I'm a language model, but how do I use it to change my language-concept?
That's the reason we've talked about the idea
rank 0 sample 2: Hello, I'm a language model, but I had a difficult time explaining why I got that. What was going on? What is the difference between a standard
rank 0 sample 3: Hello, I'm a language model, not really a language model. I like the story of the time: when you find it, it'll be the real
step 3250, loss: 3.550420, norm:0.2426, lr:5.6052e-04 dt: 48520.41ms, tok/sec:10805.51
step 3251, loss: 3.436532, norm:0.2532, lr:5.6049e-04 dt: 3332.02ms, tok/sec:157348.37
step 3252, loss: 3.462795, norm:0.2628, lr:5.6046e-04 dt: 3332.15ms, tok/sec:157342.39
step 3253, loss: 3.359323, norm:0.2549, lr:5.6043e-04 dt: 3333.10ms, tok/sec:157297.59
step 3254, loss: 3.336666, norm:0.2518, lr:5.6040e-04 dt: 3332.70ms, tok/sec:157316.26
step 3255, loss: 3.416603, norm:0.2335, lr:5.6037e-04 dt: 3332.55ms, tok/sec:157323.19
step 3256, loss: 3.389428, norm:0.2562, lr:5.6034e-04 dt: 3332.06ms, tok/sec:157346.61
step 3257, loss: 3.349321, norm:0.2543, lr:5.6031e-04 dt: 3331.75ms, tok/sec:157361.09
step 3258, loss: 3.406604, norm:0.2501, lr:5.6028e-04 dt: 3332.00ms, tok/sec:157349.46
step 3259, loss: 3.378918, norm:0.2398, lr:5.6025e-04 dt: 3331.96ms, tok/sec:157351.40
step 3260, loss: 3.356587, norm:0.2481, lr:5.6022e-04 dt: 3331.96ms, tok/sec:157351.26
step 3261, loss: 3.395265, norm:0.2456, lr:5.6019e-04 dt: 3331.97ms, tok/sec:157350.92
step 3262, loss: 3.337458, norm:0.2631, lr:5.6016e-04 dt: 3332.26ms, tok/sec:157337.23
step 3263, loss: 3.584809, norm:0.2643, lr:5.6013e-04 dt: 3332.09ms, tok/sec:157345.26
step 3264, loss: 3.578630, norm:0.2935, lr:5.6010e-04 dt: 3332.56ms, tok/sec:157322.82
step 3265, loss: 3.628461, norm:0.2690, lr:5.6007e-04 dt: 3332.11ms, tok/sec:157344.06
step 3266, loss: 3.568136, norm:0.2848, lr:5.6004e-04 dt: 3331.86ms, tok/sec:157355.85
step 3267, loss: 3.630140, norm:0.2579, lr:5.6001e-04 dt: 3331.93ms, tok/sec:157352.72
step 3268, loss: 3.648161, norm:0.2662, lr:5.5997e-04 dt: 3332.23ms, tok/sec:157338.41
step 3269, loss: 3.659313, norm:0.2667, lr:5.5994e-04 dt: 3331.89ms, tok/sec:157354.72
step 3270, loss: 3.571719, norm:0.3199, lr:5.5991e-04 dt: 3331.79ms, tok/sec:157359.46
step 3271, loss: 3.591013, norm:0.3014, lr:5.5988e-04 dt: 3332.21ms, tok/sec:157339.40
step 3272, loss: 3.607080, norm:0.2688, lr:5.5985e-04 dt: 3332.35ms, tok/sec:157332.67
step 3273, loss: 3.619685, norm:0.2724, lr:5.5982e-04 dt: 3332.17ms, tok/sec:157341.20
step 3274, loss: 3.659804, norm:0.2876, lr:5.5979e-04 dt: 3332.25ms, tok/sec:157337.70
step 3275, loss: 3.594642, norm:0.2672, lr:5.5976e-04 dt: 3331.91ms, tok/sec:157353.80
step 3276, loss: 3.552449, norm:0.2869, lr:5.5973e-04 dt: 3332.31ms, tok/sec:157334.53
step 3277, loss: 3.589011, norm:0.2727, lr:5.5970e-04 dt: 3332.05ms, tok/sec:157346.84
step 3278, loss: 3.591016, norm:0.2602, lr:5.5967e-04 dt: 3331.94ms, tok/sec:157352.38
step 3279, loss: 3.619177, norm:0.2617, lr:5.5964e-04 dt: 3332.23ms, tok/sec:157338.64
step 3280, loss: 3.629361, norm:0.2862, lr:5.5961e-04 dt: 3332.47ms, tok/sec:157327.27
step 3281, loss: 3.561974, norm:0.2425, lr:5.5958e-04 dt: 3332.47ms, tok/sec:157327.07
step 3282, loss: 3.596078, norm:0.2612, lr:5.5955e-04 dt: 3332.03ms, tok/sec:157348.08
step 3283, loss: 3.573800, norm:0.2540, lr:5.5952e-04 dt: 3332.03ms, tok/sec:157347.79
step 3284, loss: 3.606112, norm:0.2736, lr:5.5948e-04 dt: 3332.18ms, tok/sec:157340.86
step 3285, loss: 3.585374, norm:0.2572, lr:5.5945e-04 dt: 3332.09ms, tok/sec:157345.09
step 3286, loss: 3.499156, norm:0.2619, lr:5.5942e-04 dt: 3331.89ms, tok/sec:157354.57
step 3287, loss: 3.541131, norm:0.2541, lr:5.5939e-04 dt: 3332.27ms, tok/sec:157336.63
step 3288, loss: 3.562323, norm:0.2755, lr:5.5936e-04 dt: 3331.96ms, tok/sec:157351.11
step 3289, loss: 3.576410, norm:0.2723, lr:5.5933e-04 dt: 3331.96ms, tok/sec:157351.30
step 3290, loss: 3.598702, norm:0.2685, lr:5.5930e-04 dt: 3332.32ms, tok/sec:157334.30
step 3291, loss: 3.547595, norm:0.2701, lr:5.5927e-04 dt: 3332.01ms, tok/sec:157348.90
step 3292, loss: 3.545233, norm:0.2517, lr:5.5924e-04 dt: 3332.19ms, tok/sec:157340.51
step 3293, loss: 3.548527, norm:0.2448, lr:5.5921e-04 dt: 3331.87ms, tok/sec:157355.69
step 3294, loss: 3.538127, norm:0.2534, lr:5.5918e-04 dt: 3331.78ms, tok/sec:157359.65
step 3295, loss: 3.567593, norm:0.2649, lr:5.5915e-04 dt: 3332.17ms, tok/sec:157341.20
step 3296, loss: 3.545547, norm:0.2494, lr:5.5912e-04 dt: 3332.18ms, tok/sec:157340.62
step 3297, loss: 3.460188, norm:0.2346, lr:5.5908e-04 dt: 3331.96ms, tok/sec:157351.39
step 3298, loss: 3.343378, norm:0.2591, lr:5.5905e-04 dt: 3331.91ms, tok/sec:157353.82
step 3299, loss: 3.337833, norm:0.2376, lr:5.5902e-04 dt: 3332.66ms, tok/sec:157318.08
validation loss: 3.6016
Model and optimizer state saved.
HellaSwag accuracy:2317335863208938569/-2=-1158667931604469248.0000
rank 1 sample 0: Hello, I'm a language model, what are you going to do? Read this out:
Hello, I'm a language model in the middle of the
rank 1 sample 1: Hello, I'm a language model, which I think should be a bit weird but I'm writing a course on my future work, which is a good way
rank 1 sample 2: Hello, I'm a language model, so a model like this is a model of the language, or language model, that is used for this language is a
rank 1 sample 3: Hello, I'm a language model, and I'm writing about using a "main" that might be familiar to everybody. Most of what I'm reading is
rank 0 sample 0: Hello, I'm a language model, and I know that's pretty darn useful today. I'm not in love with the concept of being an author, you
rank 0 sample 1: Hello, I'm a language model, I was given a copy of this story. It reads my story about their parents, who worked in other schools, and
rank 0 sample 2: Hello, I'm a language model, I'm happy to take your information on the job and get started!
I'm a professional, I'm an internet
rank 0 sample 3: Hello, I'm a language model, it gets a lot of fun and makes it suitable for students. However, if you've used it at home and is
step 3300, loss: 3.441347, norm:0.2529, lr:5.5899e-04 dt: 56138.84ms, tok/sec:9339.13
step 3301, loss: 3.380964, norm:0.2309, lr:5.5896e-04 dt: 3332.03ms, tok/sec:157348.13
step 3302, loss: 3.457933, norm:0.2492, lr:5.5893e-04 dt: 3332.02ms, tok/sec:157348.25
step 3303, loss: 3.353593, norm:0.2507, lr:5.5890e-04 dt: 3332.33ms, tok/sec:157333.98
step 3304, loss: 3.372178, norm:0.2504, lr:5.5887e-04 dt: 3332.32ms, tok/sec:157334.08
step 3305, loss: 3.382574, norm:0.2716, lr:5.5884e-04 dt: 3331.96ms, tok/sec:157351.44
step 3306, loss: 3.377646, norm:0.3333, lr:5.5881e-04 dt: 3331.81ms, tok/sec:157358.22
step 3307, loss: 3.384489, norm:0.3057, lr:5.5877e-04 dt: 3332.04ms, tok/sec:157347.66
step 3308, loss: 3.363803, norm:0.2656, lr:5.5874e-04 dt: 3331.89ms, tok/sec:157354.55
step 3309, loss: 3.547689, norm:0.2885, lr:5.5871e-04 dt: 3332.17ms, tok/sec:157341.21
step 3310, loss: 3.604075, norm:0.3001, lr:5.5868e-04 dt: 3332.11ms, tok/sec:157343.96
step 3311, loss: 3.595291, norm:0.2821, lr:5.5865e-04 dt: 3332.24ms, tok/sec:157337.82
step 3312, loss: 3.624515, norm:0.2910, lr:5.5862e-04 dt: 3332.05ms, tok/sec:157347.09
step 3313, loss: 3.582281, norm:0.2780, lr:5.5859e-04 dt: 3332.46ms, tok/sec:157327.77
step 3314, loss: 3.604142, norm:0.2702, lr:5.5856e-04 dt: 3332.08ms, tok/sec:157345.77
step 3315, loss: 3.547989, norm:0.2787, lr:5.5853e-04 dt: 3332.25ms, tok/sec:157337.33
step 3316, loss: 3.579947, norm:0.2850, lr:5.5850e-04 dt: 3331.84ms, tok/sec:157357.01
step 3317, loss: 3.591012, norm:0.2585, lr:5.5846e-04 dt: 3332.15ms, tok/sec:157342.32
step 3318, loss: 3.692083, norm:0.2703, lr:5.5843e-04 dt: 3332.09ms, tok/sec:157344.93
step 3319, loss: 3.605988, norm:0.3010, lr:5.5840e-04 dt: 3332.12ms, tok/sec:157343.87
step 3320, loss: 3.577570, norm:0.2816, lr:5.5837e-04 dt: 3331.84ms, tok/sec:157356.95
step 3321, loss: 3.543774, norm:0.2420, lr:5.5834e-04 dt: 3332.29ms, tok/sec:157335.63
step 3322, loss: 3.531867, norm:0.2666, lr:5.5831e-04 dt: 3332.51ms, tok/sec:157325.26
step 3323, loss: 3.606225, norm:0.2784, lr:5.5828e-04 dt: 3332.06ms, tok/sec:157346.58
step 3324, loss: 3.580491, norm:0.2892, lr:5.5825e-04 dt: 3332.02ms, tok/sec:157348.21
step 3325, loss: 3.591263, norm:0.2768, lr:5.5822e-04 dt: 3332.01ms, tok/sec:157348.80
step 3326, loss: 3.575552, norm:0.3107, lr:5.5818e-04 dt: 3331.97ms, tok/sec:157350.67
step 3327, loss: 3.581561, norm:0.2673, lr:5.5815e-04 dt: 3332.27ms, tok/sec:157336.40
step 3328, loss: 3.587791, norm:0.2542, lr:5.5812e-04 dt: 3332.14ms, tok/sec:157342.60
step 3329, loss: 3.603677, norm:0.2480, lr:5.5809e-04 dt: 3332.25ms, tok/sec:157337.57
step 3330, loss: 3.585874, norm:0.2643, lr:5.5806e-04 dt: 3332.09ms, tok/sec:157345.04
step 3331, loss: 3.603733, norm:0.2596, lr:5.5803e-04 dt: 3332.77ms, tok/sec:157312.86
step 3332, loss: 3.558142, norm:0.2718, lr:5.5800e-04 dt: 3331.98ms, tok/sec:157350.16
step 3333, loss: 3.605928, norm:0.3646, lr:5.5797e-04 dt: 3331.94ms, tok/sec:157352.09
step 3334, loss: 3.528123, norm:0.4012, lr:5.5793e-04 dt: 3332.18ms, tok/sec:157340.90
step 3335, loss: 3.511452, norm:0.4212, lr:5.5790e-04 dt: 3332.16ms, tok/sec:157341.67
step 3336, loss: 3.546780, norm:0.2991, lr:5.5787e-04 dt: 3331.87ms, tok/sec:157355.50
step 3337, loss: 3.566507, norm:0.3171, lr:5.5784e-04 dt: 3332.09ms, tok/sec:157345.32
step 3338, loss: 3.519625, norm:0.2964, lr:5.5781e-04 dt: 3332.17ms, tok/sec:157341.20
step 3339, loss: 3.526726, norm:0.2846, lr:5.5778e-04 dt: 3332.10ms, tok/sec:157344.85
step 3340, loss: 3.564972, norm:0.2669, lr:5.5775e-04 dt: 3332.41ms, tok/sec:157330.04
step 3341, loss: 3.557627, norm:0.2596, lr:5.5772e-04 dt: 3332.05ms, tok/sec:157346.93
step 3342, loss: 3.529130, norm:0.2516, lr:5.5768e-04 dt: 3332.09ms, tok/sec:157344.96
step 3343, loss: 3.444550, norm:0.2525, lr:5.5765e-04 dt: 3332.06ms, tok/sec:157346.55
step 3344, loss: 3.359212, norm:0.2724, lr:5.5762e-04 dt: 3332.03ms, tok/sec:157347.82
step 3345, loss: 3.398190, norm:0.3001, lr:5.5759e-04 dt: 3331.99ms, tok/sec:157349.83
step 3346, loss: 3.367451, norm:0.2426, lr:5.5756e-04 dt: 3331.99ms, tok/sec:157349.89
step 3347, loss: 3.357986, norm:0.2743, lr:5.5753e-04 dt: 3332.07ms, tok/sec:157346.03
step 3348, loss: 3.316734, norm:0.2843, lr:5.5750e-04 dt: 3332.06ms, tok/sec:157346.50
step 3349, loss: 3.363405, norm:0.2633, lr:5.5746e-04 dt: 3331.88ms, tok/sec:157355.06
HellaSwag accuracy:2316208314034390089/-2=-1158104157017195008.0000
rank 1 sample 0: Hello, I'm a language model, and is useful for business, business organization or management. When choosing a language model, one needs to understand the skills and
rank 1 sample 1: Hello, I'm a language model, but it has got a lot of interest with the use environment. When I write this example, I can see that the
rank 1 sample 2: Hello, I'm a language model, so to get you started, you need to understand the concepts being studied and written in a logical and logical manner in a
rank 1 sample 3: Hello, I'm a language model, and I'm just a couple of students using the language 'text' or an 'media based' language. We're
rank 0 sample 0: Hello, I'm a language model, but I think it is pretty good that any time you let me create this page, you'll know how well you have
rank 0 sample 1: Hello, I'm a language model, and what I'm saying is how an object can act as an object’s own object, the only thing that
rank 0 sample 2: Hello, I'm a language model, and I learned about both topics. My first post is about this:
- I love this language.
- Can
rank 0 sample 3: Hello, I'm a language model, and have a few questions. I really enjoyed teaching, and I can’t wait to see it.”
step 3350, loss: 3.483979, norm:0.2795, lr:5.5743e-04 dt: 48522.27ms, tok/sec:10805.10
step 3351, loss: 3.415339, norm:0.2631, lr:5.5740e-04 dt: 3332.14ms, tok/sec:157342.85
step 3352, loss: 3.408723, norm:0.2352, lr:5.5737e-04 dt: 3332.28ms, tok/sec:157335.98
step 3353, loss: 3.395592, norm:0.2809, lr:5.5734e-04 dt: 3332.04ms, tok/sec:157347.29
step 3354, loss: 3.427625, norm:0.2620, lr:5.5731e-04 dt: 3332.05ms, tok/sec:157346.94
step 3355, loss: 3.616621, norm:0.2864, lr:5.5728e-04 dt: 3331.95ms, tok/sec:157351.61
step 3356, loss: 3.674859, norm:0.2887, lr:5.5724e-04 dt: 3332.09ms, tok/sec:157344.96
step 3357, loss: 3.657357, norm:0.3083, lr:5.5721e-04 dt: 3332.00ms, tok/sec:157349.49
step 3358, loss: 3.548494, norm:0.3004, lr:5.5718e-04 dt: 3331.96ms, tok/sec:157351.35
step 3359, loss: 3.759639, norm:0.2978, lr:5.5715e-04 dt: 3332.13ms, tok/sec:157343.01
step 3360, loss: 3.606635, norm:0.2883, lr:5.5712e-04 dt: 3332.01ms, tok/sec:157348.83
step 3361, loss: 3.564285, norm:0.2666, lr:5.5709e-04 dt: 3332.53ms, tok/sec:157324.51
step 3362, loss: 3.581960, norm:0.2802, lr:5.5706e-04 dt: 3332.20ms, tok/sec:157339.79
step 3363, loss: 3.588861, norm:0.2601, lr:5.5702e-04 dt: 3331.92ms, tok/sec:157353.01
step 3364, loss: 3.549701, norm:0.2934, lr:5.5699e-04 dt: 3332.01ms, tok/sec:157348.88
step 3365, loss: 3.581872, norm:0.2902, lr:5.5696e-04 dt: 3332.04ms, tok/sec:157347.48
step 3366, loss: 3.631353, norm:0.2742, lr:5.5693e-04 dt: 3331.99ms, tok/sec:157349.69
step 3367, loss: 3.703909, norm:0.3330, lr:5.5690e-04 dt: 3331.95ms, tok/sec:157351.84
step 3368, loss: 3.623555, norm:0.3426, lr:5.5687e-04 dt: 3332.14ms, tok/sec:157342.96
step 3369, loss: 3.598641, norm:0.2987, lr:5.5683e-04 dt: 3332.23ms, tok/sec:157338.64
step 3370, loss: 3.574828, norm:0.3280, lr:5.5680e-04 dt: 3332.47ms, tok/sec:157326.94
step 3371, loss: 3.601823, norm:0.3501, lr:5.5677e-04 dt: 3331.92ms, tok/sec:157353.32
step 3372, loss: 3.579381, norm:0.2951, lr:5.5674e-04 dt: 3332.06ms, tok/sec:157346.38
step 3373, loss: 3.662874, norm:0.2820, lr:5.5671e-04 dt: 3331.99ms, tok/sec:157349.89
step 3374, loss: 3.631174, norm:0.2949, lr:5.5668e-04 dt: 3332.15ms, tok/sec:157342.12
step 3375, loss: 3.597236, norm:0.2827, lr:5.5664e-04 dt: 3332.07ms, tok/sec:157346.19
step 3376, loss: 3.648996, norm:0.3023, lr:5.5661e-04 dt: 3331.99ms, tok/sec:157349.89
step 3377, loss: 3.586502, norm:0.3412, lr:5.5658e-04 dt: 3332.16ms, tok/sec:157342.00
step 3378, loss: 3.594509, norm:0.2945, lr:5.5655e-04 dt: 3332.57ms, tok/sec:157322.57
step 3379, loss: 3.560675, norm:0.2871, lr:5.5652e-04 dt: 3331.98ms, tok/sec:157350.41
step 3380, loss: 3.521431, norm:0.2723, lr:5.5649e-04 dt: 3332.05ms, tok/sec:157346.82
step 3381, loss: 3.515461, norm:0.2790, lr:5.5645e-04 dt: 3332.03ms, tok/sec:157348.04
step 3382, loss: 3.572910, norm:0.2603, lr:5.5642e-04 dt: 3332.08ms, tok/sec:157345.63
step 3383, loss: 3.527959, norm:0.2765, lr:5.5639e-04 dt: 3332.01ms, tok/sec:157348.72
step 3384, loss: 3.531857, norm:0.2747, lr:5.5636e-04 dt: 3331.87ms, tok/sec:157355.50
step 3385, loss: 3.604345, norm:0.2811, lr:5.5633e-04 dt: 3332.05ms, tok/sec:157346.79
step 3386, loss: 3.545933, norm:0.2451, lr:5.5629e-04 dt: 3332.27ms, tok/sec:157336.76
step 3387, loss: 3.511163, norm:0.2675, lr:5.5626e-04 dt: 3332.29ms, tok/sec:157335.71
step 3388, loss: 3.539605, norm:0.2514, lr:5.5623e-04 dt: 3331.92ms, tok/sec:157353.12
step 3389, loss: 3.557998, norm:0.2656, lr:5.5620e-04 dt: 3331.97ms, tok/sec:157350.79
step 3390, loss: 3.381114, norm:0.2745, lr:5.5617e-04 dt: 3332.08ms, tok/sec:157345.71
step 3391, loss: 3.415048, norm:0.2417, lr:5.5614e-04 dt: 3332.17ms, tok/sec:157341.55
step 3392, loss: 3.344279, norm:0.2778, lr:5.5610e-04 dt: 3332.00ms, tok/sec:157349.31
step 3393, loss: 3.361109, norm:0.2766, lr:5.5607e-04 dt: 3331.82ms, tok/sec:157357.67
step 3394, loss: 3.470732, norm:0.2379, lr:5.5604e-04 dt: 3332.02ms, tok/sec:157348.53
step 3395, loss: 3.346670, norm:0.2608, lr:5.5601e-04 dt: 3331.92ms, tok/sec:157353.18
step 3396, loss: 3.361841, norm:0.2878, lr:5.5598e-04 dt: 3332.36ms, tok/sec:157332.52
step 3397, loss: 3.437723, norm:0.2638, lr:5.5594e-04 dt: 3332.06ms, tok/sec:157346.67
step 3398, loss: 3.317603, norm:0.2622, lr:5.5591e-04 dt: 3332.32ms, tok/sec:157334.07
step 3399, loss: 3.518567, norm:0.2815, lr:5.5588e-04 dt: 3332.04ms, tok/sec:157347.67
validation loss: 3.5967
Model and optimizer state saved.
HellaSwag accuracy:-6897170809243581359/-2=3448585404621790720.0000
rank 1 sample 0: Hello, I'm a language model, who I am, my class is this: http://www.youtube.com/watch?src=n+k
rank 1 sample 1: Hello, I'm a language model, which I have just done with my computer. I'm so excited about the potential for languages, which are the future of
rank 1 sample 2: Hello, I'm a language model, so after reading a few of the articles I've gotten, in this blog post, I've decided to write this post
rank 1 sample 3: Hello, I'm a language model, and I'm the author of this project — and I recently published a few examples of things I have done with kids.
rank 0 sample 0: Hello, I'm a language model, and I like to use some language in its natural language, which sounds confusing because it can be easily read from a book
rank 0 sample 1: Hello, I'm a language model, so here's a quick tip for reading this: There are a lot of words in a language that do not have words
rank 0 sample 2: Hello, I'm a language model, but I guess it's probably what the language will look like: a language that is just a language that is just an
rank 0 sample 3: Hello, I'm a language model, and then I'm a language expert and you feel like I'm using something different! This is the difference between languages when
step 3400, loss: 3.406879, norm:0.2607, lr:5.5585e-04 dt: 56193.45ms, tok/sec:9330.06
step 3401, loss: 3.409943, norm:0.2532, lr:5.5582e-04 dt: 3332.32ms, tok/sec:157334.21
step 3402, loss: 3.568477, norm:0.3021, lr:5.5578e-04 dt: 3331.92ms, tok/sec:157353.05
step 3403, loss: 3.664077, norm:0.3304, lr:5.5575e-04 dt: 3332.01ms, tok/sec:157348.93
step 3404, loss: 3.615999, norm:0.2641, lr:5.5572e-04 dt: 3332.10ms, tok/sec:157344.48
step 3405, loss: 3.633519, norm:0.2997, lr:5.5569e-04 dt: 3332.10ms, tok/sec:157344.43
step 3406, loss: 3.611799, norm:0.2695, lr:5.5566e-04 dt: 3332.34ms, tok/sec:157333.20
step 3407, loss: 3.612432, norm:0.2763, lr:5.5562e-04 dt: 3332.49ms, tok/sec:157326.06
step 3408, loss: 3.618406, norm:0.2799, lr:5.5559e-04 dt: 3332.06ms, tok/sec:157346.55
step 3409, loss: 3.615075, norm:0.2552, lr:5.5556e-04 dt: 3332.03ms, tok/sec:157347.81
step 3410, loss: 3.649740, norm:0.2796, lr:5.5553e-04 dt: 3332.09ms, tok/sec:157345.04
step 3411, loss: 3.581184, norm:0.2570, lr:5.5550e-04 dt: 3332.09ms, tok/sec:157345.14
step 3412, loss: 3.533866, norm:0.2702, lr:5.5546e-04 dt: 3332.18ms, tok/sec:157340.92
step 3413, loss: 3.629989, norm:0.2762, lr:5.5543e-04 dt: 3332.04ms, tok/sec:157347.39
step 3414, loss: 3.569303, norm:0.2660, lr:5.5540e-04 dt: 3331.89ms, tok/sec:157354.44
step 3415, loss: 3.592435, norm:0.2604, lr:5.5537e-04 dt: 3332.09ms, tok/sec:157345.25
step 3416, loss: 3.648254, norm:0.2617, lr:5.5534e-04 dt: 3332.34ms, tok/sec:157333.27
step 3417, loss: 3.572760, norm:0.3348, lr:5.5530e-04 dt: 3331.98ms, tok/sec:157350.40
step 3418, loss: 3.573366, norm:0.2747, lr:5.5527e-04 dt: 3331.89ms, tok/sec:157354.50
step 3419, loss: 3.576483, norm:0.2608, lr:5.5524e-04 dt: 3332.04ms, tok/sec:157347.56
step 3420, loss: 3.584224, norm:0.3363, lr:5.5521e-04 dt: 3332.00ms, tok/sec:157349.55
step 3421, loss: 3.570600, norm:0.3252, lr:5.5517e-04 dt: 3332.12ms, tok/sec:157343.62
step 3422, loss: 3.604085, norm:0.2675, lr:5.5514e-04 dt: 3332.02ms, tok/sec:157348.48
step 3423, loss: 3.592132, norm:0.3350, lr:5.5511e-04 dt: 3332.00ms, tok/sec:157349.41
step 3424, loss: 3.550195, norm:0.2812, lr:5.5508e-04 dt: 3332.20ms, tok/sec:157339.85
step 3425, loss: 3.595132, norm:0.2686, lr:5.5505e-04 dt: 3332.37ms, tok/sec:157331.97
step 3426, loss: 3.540549, norm:0.2916, lr:5.5501e-04 dt: 3331.98ms, tok/sec:157350.23
step 3427, loss: 3.558535, norm:0.2539, lr:5.5498e-04 dt: 3331.93ms, tok/sec:157352.72
step 3428, loss: 3.558564, norm:0.2923, lr:5.5495e-04 dt: 3333.99ms, tok/sec:157255.30
step 3429, loss: 3.545131, norm:0.2699, lr:5.5492e-04 dt: 3332.29ms, tok/sec:157335.84
step 3430, loss: 3.528404, norm:0.2653, lr:5.5488e-04 dt: 3332.10ms, tok/sec:157344.48
step 3431, loss: 3.498783, norm:0.2508, lr:5.5485e-04 dt: 3331.92ms, tok/sec:157353.21
step 3432, loss: 3.515644, norm:0.2685, lr:5.5482e-04 dt: 3332.14ms, tok/sec:157342.61
step 3433, loss: 3.611841, norm:0.2570, lr:5.5479e-04 dt: 3332.08ms, tok/sec:157345.36
step 3434, loss: 3.491174, norm:0.2970, lr:5.5476e-04 dt: 3332.26ms, tok/sec:157337.16
step 3435, loss: 3.523096, norm:0.2775, lr:5.5472e-04 dt: 3331.75ms, tok/sec:157361.06
step 3436, loss: 3.569404, norm:0.2715, lr:5.5469e-04 dt: 3331.76ms, tok/sec:157360.45
step 3437, loss: 3.529088, norm:0.2818, lr:5.5466e-04 dt: 3332.36ms, tok/sec:157332.46
step 3438, loss: 3.376478, norm:0.2760, lr:5.5463e-04 dt: 3331.85ms, tok/sec:157356.43
step 3439, loss: 3.389813, norm:0.2986, lr:5.5459e-04 dt: 3331.96ms, tok/sec:157351.21
step 3440, loss: 3.290917, norm:0.3033, lr:5.5456e-04 dt: 3331.97ms, tok/sec:157350.96
step 3441, loss: 3.313236, norm:0.2798, lr:5.5453e-04 dt: 3331.92ms, tok/sec:157353.23
step 3442, loss: 3.404451, norm:0.2708, lr:5.5450e-04 dt: 3332.06ms, tok/sec:157346.33
step 3443, loss: 3.315976, norm:0.2613, lr:5.5446e-04 dt: 3332.08ms, tok/sec:157345.61
step 3444, loss: 3.343150, norm:0.2663, lr:5.5443e-04 dt: 3332.16ms, tok/sec:157341.75
step 3445, loss: 3.324607, norm:0.2522, lr:5.5440e-04 dt: 3331.94ms, tok/sec:157352.13
step 3446, loss: 3.338806, norm:0.2526, lr:5.5437e-04 dt: 3332.25ms, tok/sec:157337.77
step 3447, loss: 3.330571, norm:0.2597, lr:5.5433e-04 dt: 3331.89ms, tok/sec:157354.77
step 3448, loss: 3.427189, norm:0.2834, lr:5.5430e-04 dt: 3332.12ms, tok/sec:157343.62
step 3449, loss: 3.440093, norm:0.2508, lr:5.5427e-04 dt: 3331.96ms, tok/sec:157351.42
HellaSwag accuracy:-4600194264010881975/-2=2300097132005441024.0000
rank 1 sample 0: Hello, I'm a language model, and the purpose of this project is making a positive contribution to people's lives.
This project will be supported by the
rank 1 sample 1: Hello, I'm a language model, so you're starting to understand why language arts is important enough to enhance your reading experience through coding!<|endoftext|>The American Academy
rank 1 sample 2: Hello, I'm a language model, which features language that is already used in the classroom. It does have the same language as a second language. This is
rank 1 sample 3: Hello, I'm a language model, and I'm just going to do one when I'm already out of print books. Since I am a language model,
rank 0 sample 0: Hello, I'm a language model, and I like to use it with any length classes. I have lots of fun with my projects!<|endoftext|>We all hear
rank 0 sample 1: Hello, I'm a language model, I understand that it's a lot like a lot of software, but even I can write something that does something good,
rank 0 sample 2: Hello, I'm a language model, but I do know that we can call it using a simple word function.
But you can use the function as an
rank 0 sample 3: Hello, I'm a language model, I hope you've got a better idea. With my help, I'll get some good reviews.
I've done
step 3450, loss: 3.617642, norm:0.2692, lr:5.5424e-04 dt: 48512.86ms, tok/sec:10807.20
step 3451, loss: 3.506390, norm:0.6322, lr:5.5420e-04 dt: 3332.07ms, tok/sec:157345.89
step 3452, loss: 3.582768, norm:0.2816, lr:5.5417e-04 dt: 3332.08ms, tok/sec:157345.47
step 3453, loss: 3.594841, norm:0.3663, lr:5.5414e-04 dt: 3332.18ms, tok/sec:157340.90
step 3454, loss: 3.557388, norm:0.2981, lr:5.5411e-04 dt: 3331.94ms, tok/sec:157352.31
step 3455, loss: 3.535340, norm:0.2683, lr:5.5407e-04 dt: 3332.41ms, tok/sec:157330.21
step 3456, loss: 3.589218, norm:0.2981, lr:5.5404e-04 dt: 3332.23ms, tok/sec:157338.26
step 3457, loss: 3.576283, norm:0.2713, lr:5.5401e-04 dt: 3332.05ms, tok/sec:157347.12
step 3458, loss: 3.638575, norm:0.2759, lr:5.5398e-04 dt: 3332.26ms, tok/sec:157336.90
step 3459, loss: 3.567509, norm:0.2634, lr:5.5394e-04 dt: 3332.03ms, tok/sec:157348.11
step 3460, loss: 3.632144, norm:0.2531, lr:5.5391e-04 dt: 3332.16ms, tok/sec:157341.64
step 3461, loss: 3.607871, norm:0.2905, lr:5.5388e-04 dt: 3331.97ms, tok/sec:157350.63
step 3462, loss: 3.573989, norm:0.2716, lr:5.5385e-04 dt: 3332.16ms, tok/sec:157341.92
step 3463, loss: 3.549359, norm:0.2595, lr:5.5381e-04 dt: 3332.02ms, tok/sec:157348.42
step 3464, loss: 3.602699, norm:0.2839, lr:5.5378e-04 dt: 3331.91ms, tok/sec:157353.39
step 3465, loss: 3.575899, norm:0.2572, lr:5.5375e-04 dt: 3332.05ms, tok/sec:157347.02
step 3466, loss: 3.634823, norm:0.2998, lr:5.5372e-04 dt: 3333.15ms, tok/sec:157295.26
step 3467, loss: 3.544379, norm:0.3833, lr:5.5368e-04 dt: 3332.78ms, tok/sec:157312.46
step 3468, loss: 3.586341, norm:0.2686, lr:5.5365e-04 dt: 3332.13ms, tok/sec:157343.26
step 3469, loss: 3.556069, norm:0.2747, lr:5.5362e-04 dt: 3332.19ms, tok/sec:157340.56
step 3470, loss: 3.601438, norm:0.2824, lr:5.5359e-04 dt: 3332.15ms, tok/sec:157342.09
step 3471, loss: 3.550473, norm:0.2751, lr:5.5355e-04 dt: 3332.21ms, tok/sec:157339.60
step 3472, loss: 3.527353, norm:0.2826, lr:5.5352e-04 dt: 3331.98ms, tok/sec:157350.13
step 3473, loss: 3.565722, norm:0.2594, lr:5.5349e-04 dt: 3331.91ms, tok/sec:157353.44
step 3474, loss: 3.544610, norm:0.2970, lr:5.5345e-04 dt: 3332.08ms, tok/sec:157345.67
step 3475, loss: 3.514099, norm:0.2735, lr:5.5342e-04 dt: 3332.03ms, tok/sec:157348.00
step 3476, loss: 3.519742, norm:0.2395, lr:5.5339e-04 dt: 3332.04ms, tok/sec:157347.57
step 3477, loss: 3.590981, norm:0.3043, lr:5.5336e-04 dt: 3331.94ms, tok/sec:157352.29
step 3478, loss: 3.516905, norm:0.2854, lr:5.5332e-04 dt: 3332.24ms, tok/sec:157337.93
step 3479, loss: 3.546921, norm:0.3017, lr:5.5329e-04 dt: 3332.10ms, tok/sec:157344.57
step 3480, loss: 3.499735, norm:0.2758, lr:5.5326e-04 dt: 3332.52ms, tok/sec:157324.75
step 3481, loss: 3.574244, norm:0.2717, lr:5.5323e-04 dt: 3331.94ms, tok/sec:157352.07
step 3482, loss: 3.521787, norm:0.2714, lr:5.5319e-04 dt: 3331.86ms, tok/sec:157356.16
step 3483, loss: 3.623948, norm:0.3160, lr:5.5316e-04 dt: 3331.79ms, tok/sec:157359.27
step 3484, loss: 3.410136, norm:0.2935, lr:5.5313e-04 dt: 3332.09ms, tok/sec:157345.06
step 3485, loss: 3.286569, norm:0.2776, lr:5.5309e-04 dt: 3332.04ms, tok/sec:157347.23
step 3486, loss: 3.329349, norm:0.2879, lr:5.5306e-04 dt: 3332.01ms, tok/sec:157348.88
step 3487, loss: 3.371333, norm:0.2711, lr:5.5303e-04 dt: 3332.16ms, tok/sec:157341.78
step 3488, loss: 3.347672, norm:0.2996, lr:5.5300e-04 dt: 3332.19ms, tok/sec:157340.38
step 3489, loss: 3.365138, norm:0.3198, lr:5.5296e-04 dt: 3332.32ms, tok/sec:157334.08
step 3490, loss: 3.394881, norm:0.2660, lr:5.5293e-04 dt: 3331.93ms, tok/sec:157352.44
step 3491, loss: 3.305552, norm:0.2663, lr:5.5290e-04 dt: 3331.87ms, tok/sec:157355.35
step 3492, loss: 3.395586, norm:0.2567, lr:5.5286e-04 dt: 3332.02ms, tok/sec:157348.54
step 3493, loss: 3.346001, norm:0.2675, lr:5.5283e-04 dt: 3331.94ms, tok/sec:157352.26
step 3494, loss: 3.347159, norm:0.2919, lr:5.5280e-04 dt: 3331.90ms, tok/sec:157354.21
step 3495, loss: 3.403795, norm:0.2452, lr:5.5276e-04 dt: 3331.85ms, tok/sec:157356.64
step 3496, loss: 3.673125, norm:0.2634, lr:5.5273e-04 dt: 3332.16ms, tok/sec:157341.75
step 3497, loss: 3.565961, norm:0.2599, lr:5.5270e-04 dt: 3332.28ms, tok/sec:157336.17
step 3498, loss: 3.668986, norm:0.2868, lr:5.5267e-04 dt: 3331.99ms, tok/sec:157350.04
step 3499, loss: 3.578077, norm:0.3388, lr:5.5263e-04 dt: 3332.07ms, tok/sec:157345.81
validation loss: 3.5816
Model and optimizer state saved.
HellaSwag accuracy:-6897030108262710255/-2=3448515054131355136.0000
rank 1 sample 0: Hello, I'm a language model, meaning "on the network".
Hermank and Nitsen, 2008, ISBN 97805204723.
rank 1 sample 1: Hello, I'm a language model, so you'll notice that the "numpy" part does not always use the same notation as one.
The "
rank 1 sample 2: Hello, I'm a language model, it took the course of a year to learn the language and use that knowledge to make a meaningful conversation, but what happens
rank 1 sample 3: Hello, I'm a language model, and I'm an English mathematician. But most of the more than 1000 mathematical algorithms are "right" to the idea of
rank 0 sample 0: Hello, I'm a language model, and I really want you to think how things relate to the real world so as to give you all that I can learn
rank 0 sample 1: Hello, I'm a language model, but have you ever heard of a child who's on stage, so he could have the language. If you ask him
rank 0 sample 2: Hello, I'm a language model, but I found the whole tutorial useless because I could put it off to me.
If you want to learn how to
rank 0 sample 3: Hello, I'm a language model, but also a model of the universe that has built a universe that has the potential to solve puzzles and is good at answering
step 3500, loss: 3.568500, norm:0.2664, lr:5.5260e-04 dt: 56184.73ms, tok/sec:9331.50
step 3501, loss: 3.608205, norm:0.2992, lr:5.5257e-04 dt: 3332.26ms, tok/sec:157336.88
step 3502, loss: 3.619158, norm:0.3191, lr:5.5253e-04 dt: 3332.13ms, tok/sec:157343.01
step 3503, loss: 3.567988, norm:0.3096, lr:5.5250e-04 dt: 3332.44ms, tok/sec:157328.43
step 3504, loss: 3.588401, norm:0.2443, lr:5.5247e-04 dt: 3332.03ms, tok/sec:157347.91
step 3505, loss: 3.578916, norm:0.2633, lr:5.5243e-04 dt: 3332.10ms, tok/sec:157344.79
step 3506, loss: 3.664183, norm:0.2468, lr:5.5240e-04 dt: 3332.07ms, tok/sec:157346.05
step 3507, loss: 3.603793, norm:0.2490, lr:5.5237e-04 dt: 3332.32ms, tok/sec:157334.07
step 3508, loss: 3.544525, norm:0.2497, lr:5.5234e-04 dt: 3331.83ms, tok/sec:157357.20
step 3509, loss: 3.524769, norm:0.2522, lr:5.5230e-04 dt: 3332.11ms, tok/sec:157344.32
step 3510, loss: 3.548593, norm:0.2587, lr:5.5227e-04 dt: 3332.25ms, tok/sec:157337.66
step 3511, loss: 3.592465, norm:0.2415, lr:5.5224e-04 dt: 3332.21ms, tok/sec:157339.65
step 3512, loss: 3.594556, norm:0.2670, lr:5.5220e-04 dt: 3332.39ms, tok/sec:157330.71
step 3513, loss: 3.579118, norm:0.2380, lr:5.5217e-04 dt: 3332.05ms, tok/sec:157346.77
step 3514, loss: 3.554872, norm:0.2608, lr:5.5214e-04 dt: 3332.12ms, tok/sec:157343.81
step 3515, loss: 3.500824, norm:0.2725, lr:5.5210e-04 dt: 3332.27ms, tok/sec:157336.77
step 3516, loss: 3.510663, norm:0.2401, lr:5.5207e-04 dt: 3332.16ms, tok/sec:157341.84
step 3517, loss: 3.503196, norm:0.2711, lr:5.5204e-04 dt: 3331.89ms, tok/sec:157354.66
step 3518, loss: 3.607646, norm:0.2680, lr:5.5200e-04 dt: 3332.22ms, tok/sec:157338.90
step 3519, loss: 3.525529, norm:0.2721, lr:5.5197e-04 dt: 3332.36ms, tok/sec:157332.20
step 3520, loss: 3.569688, norm:0.2542, lr:5.5194e-04 dt: 3331.96ms, tok/sec:157351.22
step 3521, loss: 3.532143, norm:0.2650, lr:5.5190e-04 dt: 3331.96ms, tok/sec:157351.38
step 3522, loss: 3.478054, norm:0.2625, lr:5.5187e-04 dt: 3332.10ms, tok/sec:157344.81
step 3523, loss: 3.489888, norm:0.2466, lr:5.5184e-04 dt: 3332.26ms, tok/sec:157337.02
step 3524, loss: 3.496487, norm:0.2792, lr:5.5180e-04 dt: 3331.88ms, tok/sec:157354.79
step 3525, loss: 3.454132, norm:0.2733, lr:5.5177e-04 dt: 3331.79ms, tok/sec:157359.07
step 3526, loss: 3.489552, norm:0.2615, lr:5.5174e-04 dt: 3332.09ms, tok/sec:157344.86
step 3527, loss: 3.491384, norm:0.2681, lr:5.5170e-04 dt: 3331.98ms, tok/sec:157350.06
step 3528, loss: 3.569666, norm:0.2648, lr:5.5167e-04 dt: 3332.44ms, tok/sec:157328.50
step 3529, loss: 3.530168, norm:0.2532, lr:5.5164e-04 dt: 3332.26ms, tok/sec:157336.95
step 3530, loss: 3.540377, norm:0.2601, lr:5.5161e-04 dt: 3332.23ms, tok/sec:157338.46
step 3531, loss: 3.440193, norm:0.2463, lr:5.5157e-04 dt: 3332.21ms, tok/sec:157339.25
step 3532, loss: 3.411841, norm:0.2706, lr:5.5154e-04 dt: 3332.01ms, tok/sec:157348.90
step 3533, loss: 3.393918, norm:0.2620, lr:5.5151e-04 dt: 3331.95ms, tok/sec:157351.86
step 3534, loss: 3.338153, norm:0.3085, lr:5.5147e-04 dt: 3332.14ms, tok/sec:157342.74
step 3535, loss: 3.334946, norm:0.2693, lr:5.5144e-04 dt: 3332.48ms, tok/sec:157326.75
step 3536, loss: 3.389874, norm:0.2602, lr:5.5140e-04 dt: 3332.02ms, tok/sec:157348.58
step 3537, loss: 3.372872, norm:0.3076, lr:5.5137e-04 dt: 3331.87ms, tok/sec:157355.60
step 3538, loss: 3.322044, norm:0.2711, lr:5.5134e-04 dt: 3331.94ms, tok/sec:157352.28
step 3539, loss: 3.367503, norm:0.2829, lr:5.5130e-04 dt: 3332.29ms, tok/sec:157335.66
step 3540, loss: 3.330251, norm:0.2597, lr:5.5127e-04 dt: 3332.11ms, tok/sec:157344.08
step 3541, loss: 3.389512, norm:0.2612, lr:5.5124e-04 dt: 3332.05ms, tok/sec:157347.20
step 3542, loss: 3.396936, norm:0.2612, lr:5.5120e-04 dt: 3331.93ms, tok/sec:157352.41
step 3543, loss: 3.492010, norm:0.2688, lr:5.5117e-04 dt: 3332.22ms, tok/sec:157338.80
step 3544, loss: 3.552140, norm:0.2714, lr:5.5114e-04 dt: 3332.26ms, tok/sec:157337.16
step 3545, loss: 3.533098, norm:0.2823, lr:5.5110e-04 dt: 3332.28ms, tok/sec:157335.90
step 3546, loss: 3.537452, norm:0.2810, lr:5.5107e-04 dt: 3332.25ms, tok/sec:157337.51
step 3547, loss: 3.625742, norm:0.2722, lr:5.5104e-04 dt: 3332.36ms, tok/sec:157332.18
step 3548, loss: 3.681347, norm:0.3117, lr:5.5100e-04 dt: 3332.02ms, tok/sec:157348.43
step 3549, loss: 3.698744, norm:0.2759, lr:5.5097e-04 dt: 3331.99ms, tok/sec:157349.62
HellaSwag accuracy:-4600757213427465199/-2=2300378606713732608.0000
rank 1 sample 0: Hello, I'm a language model, and the goal is to get as strong and efficient the code as possible. By the end of last week, I'll
rank 1 sample 1: Hello, I'm a language model, so you know it's not. The word "tú" refers to how you and your learners are going to use
rank 1 sample 2: Hello, I'm a language model, but some other words are more difficult to understand.
In that respect the differences between the different cultures have changed little on
rank 1 sample 3: Hello, I'm a language model, and I'm using it too. In spite of the numerous articles I spoke about, those same language models are just the
rank 0 sample 0: Hello, I'm a language model, and I know that it isn't an extension of the truth about human creativity in the field of psychology and linguistics (
rank 0 sample 1: Hello, I'm a language model, but a program that's written for most people and requires me to do anything else, so I'll probably have to deal
rank 0 sample 2: Hello, I'm a language model, but I just love playing!
Let's try this: A few lines of this article were taken from the article titled
rank 0 sample 3: Hello, I'm a language model, and, as the name suggests, the language makes you think of some interesting and simple sentences which, after all, show
step 3550, loss: 3.573005, norm:0.2952, lr:5.5094e-04 dt: 48511.11ms, tok/sec:10807.59
step 3551, loss: 3.594695, norm:0.2863, lr:5.5090e-04 dt: 3332.22ms, tok/sec:157339.03
step 3552, loss: 3.521898, norm:0.3809, lr:5.5087e-04 dt: 3332.03ms, tok/sec:157348.09
step 3553, loss: 3.569370, norm:0.3071, lr:5.5084e-04 dt: 3332.33ms, tok/sec:157333.79
step 3554, loss: 3.641019, norm:0.2891, lr:5.5080e-04 dt: 3332.20ms, tok/sec:157339.67
step 3555, loss: 3.607113, norm:0.2530, lr:5.5077e-04 dt: 3332.07ms, tok/sec:157346.04
step 3556, loss: 3.544612, norm:0.2654, lr:5.5074e-04 dt: 3332.12ms, tok/sec:157343.46
step 3557, loss: 3.671489, norm:0.2865, lr:5.5070e-04 dt: 3331.94ms, tok/sec:157352.03
step 3558, loss: 3.569009, norm:0.2823, lr:5.5067e-04 dt: 3332.04ms, tok/sec:157347.35
step 3559, loss: 3.548872, norm:0.2741, lr:5.5063e-04 dt: 3332.06ms, tok/sec:157346.48
step 3560, loss: 3.603743, norm:0.5279, lr:5.5060e-04 dt: 3332.01ms, tok/sec:157349.05
step 3561, loss: 3.588898, norm:0.3685, lr:5.5057e-04 dt: 3332.67ms, tok/sec:157317.50
step 3562, loss: 3.563015, norm:0.3054, lr:5.5053e-04 dt: 3331.92ms, tok/sec:157353.23
step 3563, loss: 3.590337, norm:0.3480, lr:5.5050e-04 dt: 3331.91ms, tok/sec:157353.37
step 3564, loss: 3.615294, norm:0.3532, lr:5.5047e-04 dt: 3332.13ms, tok/sec:157343.15
step 3565, loss: 3.571882, norm:0.2771, lr:5.5043e-04 dt: 3331.96ms, tok/sec:157351.06
step 3566, loss: 3.533792, norm:0.2804, lr:5.5040e-04 dt: 3332.32ms, tok/sec:157334.26
step 3567, loss: 3.451867, norm:0.2602, lr:5.5037e-04 dt: 3331.92ms, tok/sec:157353.26
step 3568, loss: 3.504940, norm:0.2387, lr:5.5033e-04 dt: 3332.31ms, tok/sec:157334.58
step 3569, loss: 3.512487, norm:0.2503, lr:5.5030e-04 dt: 3332.21ms, tok/sec:157339.34
step 3570, loss: 3.463948, norm:0.2262, lr:5.5026e-04 dt: 3332.08ms, tok/sec:157345.78
step 3571, loss: 3.518600, norm:0.2426, lr:5.5023e-04 dt: 3332.06ms, tok/sec:157346.56
step 3572, loss: 3.562166, norm:0.2326, lr:5.5020e-04 dt: 3331.97ms, tok/sec:157350.89
step 3573, loss: 3.520981, norm:0.2537, lr:5.5016e-04 dt: 3331.92ms, tok/sec:157352.93
step 3574, loss: 3.457871, norm:0.2466, lr:5.5013e-04 dt: 3331.95ms, tok/sec:157351.49
step 3575, loss: 3.512442, norm:0.2594, lr:5.5010e-04 dt: 3332.05ms, tok/sec:157347.02
step 3576, loss: 3.536796, norm:0.2316, lr:5.5006e-04 dt: 3331.94ms, tok/sec:157352.21
step 3577, loss: 3.562513, norm:0.2679, lr:5.5003e-04 dt: 3332.42ms, tok/sec:157329.57
step 3578, loss: 3.414012, norm:0.2506, lr:5.4999e-04 dt: 3332.41ms, tok/sec:157329.90
step 3579, loss: 3.372901, norm:0.2476, lr:5.4996e-04 dt: 3332.02ms, tok/sec:157348.51
step 3580, loss: 3.387923, norm:0.2614, lr:5.4993e-04 dt: 3332.03ms, tok/sec:157348.15
step 3581, loss: 3.345289, norm:0.2672, lr:5.4989e-04 dt: 3331.86ms, tok/sec:157355.72
step 3582, loss: 3.381819, norm:0.2640, lr:5.4986e-04 dt: 3332.35ms, tok/sec:157332.82
step 3583, loss: 3.407600, norm:0.2594, lr:5.4982e-04 dt: 3332.07ms, tok/sec:157346.10
step 3584, loss: 3.378943, norm:0.2390, lr:5.4979e-04 dt: 3332.38ms, tok/sec:157331.45
step 3585, loss: 3.343596, norm:0.2491, lr:5.4976e-04 dt: 3332.04ms, tok/sec:157347.63
step 3586, loss: 3.375024, norm:0.2638, lr:5.4972e-04 dt: 3332.04ms, tok/sec:157347.22
step 3587, loss: 3.408249, norm:0.2624, lr:5.4969e-04 dt: 3332.02ms, tok/sec:157348.61
step 3588, loss: 3.397880, norm:0.2772, lr:5.4966e-04 dt: 3331.94ms, tok/sec:157352.02
step 3589, loss: 3.325727, norm:0.3085, lr:5.4962e-04 dt: 3331.97ms, tok/sec:157350.97
step 3590, loss: 3.574407, norm:0.2805, lr:5.4959e-04 dt: 3332.36ms, tok/sec:157332.55
step 3591, loss: 3.597643, norm:0.2801, lr:5.4955e-04 dt: 3332.15ms, tok/sec:157342.26
step 3592, loss: 3.618257, norm:0.3143, lr:5.4952e-04 dt: 3331.97ms, tok/sec:157350.76
step 3593, loss: 3.640808, norm:0.2769, lr:5.4949e-04 dt: 3332.34ms, tok/sec:157333.27
step 3594, loss: 3.601154, norm:0.2966, lr:5.4945e-04 dt: 3332.12ms, tok/sec:157343.79
step 3595, loss: 3.561335, norm:0.2711, lr:5.4942e-04 dt: 3332.16ms, tok/sec:157341.82
step 3596, loss: 3.605564, norm:0.2752, lr:5.4938e-04 dt: 3332.09ms, tok/sec:157345.27
step 3597, loss: 3.588264, norm:0.2618, lr:5.4935e-04 dt: 3332.04ms, tok/sec:157347.31
step 3598, loss: 3.596117, norm:0.2533, lr:5.4932e-04 dt: 3332.08ms, tok/sec:157345.42
step 3599, loss: 3.637648, norm:0.2622, lr:5.4928e-04 dt: 3332.40ms, tok/sec:157330.64
validation loss: 3.5717
Model and optimizer state saved.
HellaSwag accuracy:2326201226000270345/-2=-1163100613000135168.0000
rank 1 sample 0: Hello, I'm a language model, meaning you're a language. We try it on your own.
If you're not using one language, you can
rank 1 sample 1: Hello, I'm a language model, which I have already seen. This is in the course syllabus, so this is to be what you need to know
rank 1 sample 2: Hello, I'm a language model, but really not quite as good as I'm. I have found a way to use the word to mean something and when
rank 1 sample 3: Hello, I'm a language model, which is a "program language" language designed for high-level-level text-book libraries.
This project is
rank 0 sample 0: Hello, I'm a language model, and I'd like you to write in-class names, and this really works. So, this one gets a really
rank 0 sample 1: Hello, I'm a language model, so be sure to look at it using the 'jumped' language test.
The reason that Google does not provide
rank 0 sample 2: Hello, I'm a language model, but I got the answer "not so much..." if I could find any other language, I'd be wrong, unless
rank 0 sample 3: Hello, I'm a language model, so when I run my Java app, it only takes a few minutes to complete this assignment. I recommend this article from
step 3600, loss: 3.612794, norm:0.2564, lr:5.4925e-04 dt: 56234.65ms, tok/sec:9323.22
step 3601, loss: 3.571734, norm:0.2614, lr:5.4921e-04 dt: 3332.21ms, tok/sec:157339.54
step 3602, loss: 3.579523, norm:0.2638, lr:5.4918e-04 dt: 3331.93ms, tok/sec:157352.50
step 3603, loss: 3.634432, norm:0.2565, lr:5.4915e-04 dt: 3332.38ms, tok/sec:157331.35
step 3604, loss: 3.568716, norm:0.2678, lr:5.4911e-04 dt: 3331.93ms, tok/sec:157352.82
step 3605, loss: 3.532238, norm:0.2884, lr:5.4908e-04 dt: 3332.20ms, tok/sec:157339.81
step 3606, loss: 3.584484, norm:0.2553, lr:5.4904e-04 dt: 3331.96ms, tok/sec:157351.01
step 3607, loss: 3.573639, norm:0.2599, lr:5.4901e-04 dt: 3332.16ms, tok/sec:157341.75
step 3608, loss: 3.593903, norm:0.2685, lr:5.4897e-04 dt: 3331.86ms, tok/sec:157356.13
step 3609, loss: 3.548728, norm:0.2777, lr:5.4894e-04 dt: 3332.44ms, tok/sec:157328.66
step 3610, loss: 3.610076, norm:0.2682, lr:5.4891e-04 dt: 3332.28ms, tok/sec:157336.16
step 3611, loss: 3.562243, norm:0.2846, lr:5.4887e-04 dt: 3332.03ms, tok/sec:157347.76
step 3612, loss: 3.526122, norm:0.2921, lr:5.4884e-04 dt: 3332.28ms, tok/sec:157336.20
step 3613, loss: 3.515728, norm:0.2503, lr:5.4880e-04 dt: 3332.00ms, tok/sec:157349.34
step 3614, loss: 3.549351, norm:0.2644, lr:5.4877e-04 dt: 3331.98ms, tok/sec:157350.13
step 3615, loss: 3.544000, norm:0.2573, lr:5.4874e-04 dt: 3332.09ms, tok/sec:157345.02
step 3616, loss: 3.618703, norm:0.2853, lr:5.4870e-04 dt: 3332.30ms, tok/sec:157335.35
step 3617, loss: 3.456358, norm:0.2780, lr:5.4867e-04 dt: 3332.37ms, tok/sec:157331.82
step 3618, loss: 3.553443, norm:0.2460, lr:5.4863e-04 dt: 3332.14ms, tok/sec:157342.85
step 3619, loss: 3.483815, norm:0.2588, lr:5.4860e-04 dt: 3334.55ms, tok/sec:157229.09
step 3620, loss: 3.541400, norm:0.2543, lr:5.4856e-04 dt: 3332.19ms, tok/sec:157340.29
step 3621, loss: 3.533630, norm:0.2553, lr:5.4853e-04 dt: 3332.07ms, tok/sec:157345.91
step 3622, loss: 3.545188, norm:0.2429, lr:5.4850e-04 dt: 3332.01ms, tok/sec:157348.80
step 3623, loss: 3.499208, norm:0.2467, lr:5.4846e-04 dt: 3332.10ms, tok/sec:157344.48
step 3624, loss: 3.403275, norm:0.2560, lr:5.4843e-04 dt: 3331.86ms, tok/sec:157355.96
step 3625, loss: 3.416002, norm:0.2605, lr:5.4839e-04 dt: 3332.10ms, tok/sec:157344.64
step 3626, loss: 3.339801, norm:0.2534, lr:5.4836e-04 dt: 3332.00ms, tok/sec:157349.38
step 3627, loss: 3.349126, norm:0.2510, lr:5.4832e-04 dt: 3331.94ms, tok/sec:157352.13
step 3628, loss: 3.338890, norm:0.2478, lr:5.4829e-04 dt: 3331.86ms, tok/sec:157355.94
step 3629, loss: 3.348873, norm:0.2470, lr:5.4826e-04 dt: 3332.15ms, tok/sec:157342.03
step 3630, loss: 3.359501, norm:0.2502, lr:5.4822e-04 dt: 3331.72ms, tok/sec:157362.58
step 3631, loss: 3.350363, norm:0.2423, lr:5.4819e-04 dt: 3332.50ms, tok/sec:157325.77
step 3632, loss: 3.402679, norm:0.2552, lr:5.4815e-04 dt: 3332.03ms, tok/sec:157348.15
step 3633, loss: 3.296390, norm:0.2457, lr:5.4812e-04 dt: 3332.20ms, tok/sec:157339.86
step 3634, loss: 3.329437, norm:0.2671, lr:5.4808e-04 dt: 3331.96ms, tok/sec:157351.05
step 3635, loss: 3.414322, norm:0.2586, lr:5.4805e-04 dt: 3331.99ms, tok/sec:157349.69
step 3636, loss: 3.550467, norm:0.2443, lr:5.4802e-04 dt: 3331.92ms, tok/sec:157353.25
step 3637, loss: 3.629099, norm:0.2790, lr:5.4798e-04 dt: 3332.13ms, tok/sec:157343.01
step 3638, loss: 3.563113, norm:0.2752, lr:5.4795e-04 dt: 3332.43ms, tok/sec:157329.06
step 3639, loss: 3.586454, norm:0.2992, lr:5.4791e-04 dt: 3332.03ms, tok/sec:157347.85
step 3640, loss: 3.684443, norm:0.3056, lr:5.4788e-04 dt: 3332.47ms, tok/sec:157326.91
step 3641, loss: 3.571860, norm:0.3099, lr:5.4784e-04 dt: 3332.35ms, tok/sec:157332.83
step 3642, loss: 3.562153, norm:0.2898, lr:5.4781e-04 dt: 3332.00ms, tok/sec:157349.50
step 3643, loss: 3.603791, norm:0.2579, lr:5.4777e-04 dt: 3331.86ms, tok/sec:157355.93
step 3644, loss: 3.587218, norm:0.2982, lr:5.4774e-04 dt: 3332.22ms, tok/sec:157338.87
step 3645, loss: 3.623098, norm:0.2503, lr:5.4771e-04 dt: 3332.01ms, tok/sec:157348.87
step 3646, loss: 3.578991, norm:0.2896, lr:5.4767e-04 dt: 3332.12ms, tok/sec:157343.52
step 3647, loss: 3.559805, norm:0.2890, lr:5.4764e-04 dt: 3332.35ms, tok/sec:157332.82
step 3648, loss: 3.555889, norm:0.2552, lr:5.4760e-04 dt: 3332.46ms, tok/sec:157327.59
step 3649, loss: 3.591922, norm:0.2712, lr:5.4757e-04 dt: 3332.25ms, tok/sec:157337.71
HellaSwag accuracy:2317194028289049617/-2=-1158597014144524800.0000
rank 1 sample 0: Hello, I'm a language model, if I make a decision to buy such things from me,
I want to make some changes that I have made.
rank 1 sample 1: Hello, I'm a language model, you can see everything.
In your world, if that's okay, the big world of multisects is
rank 1 sample 2: Hello, I'm a language model, so look up it.
- The first thing I'll take to get to the end of the lesson, so think
rank 1 sample 3: Hello, I'm a language model, and I'm writing this and I'm starting to ask us for this topic so I look forward to the next question.
rank 0 sample 0: Hello, I'm a language model, and I was writing about one day about which you would most like this tutorial on how to develop my first version of an
rank 0 sample 1: Hello, I'm a language model, but as I say to you in a language you won't have to hear any of these kinds of people talking about them
rank 0 sample 2: Hello, I'm a language model, but I like that there will be lots of discussion in the conversation and conversation.
Here are some hints to get me
rank 0 sample 3: Hello, I'm a language model, and what I'm doing is the question I are trying to answer. I've gone on and on all of these years
step 3650, loss: 3.518042, norm:0.2437, lr:5.4753e-04 dt: 48512.45ms, tok/sec:10807.29
step 3651, loss: 3.535998, norm:0.2389, lr:5.4750e-04 dt: 3332.08ms, tok/sec:157345.57
step 3652, loss: 3.534591, norm:0.2639, lr:5.4746e-04 dt: 3332.13ms, tok/sec:157343.16
step 3653, loss: 3.569021, norm:0.2654, lr:5.4743e-04 dt: 3332.00ms, tok/sec:157349.28
step 3654, loss: 3.543977, norm:0.2860, lr:5.4739e-04 dt: 3331.85ms, tok/sec:157356.47
step 3655, loss: 3.575996, norm:0.2475, lr:5.4736e-04 dt: 3332.15ms, tok/sec:157342.13
step 3656, loss: 3.515719, norm:0.3047, lr:5.4733e-04 dt: 3331.89ms, tok/sec:157354.47
step 3657, loss: 3.590597, norm:0.2831, lr:5.4729e-04 dt: 3331.97ms, tok/sec:157350.75
step 3658, loss: 3.543040, norm:0.2785, lr:5.4726e-04 dt: 3331.93ms, tok/sec:157352.78
step 3659, loss: 3.544884, norm:0.2514, lr:5.4722e-04 dt: 3332.37ms, tok/sec:157331.73
step 3660, loss: 3.509451, norm:0.2409, lr:5.4719e-04 dt: 3332.13ms, tok/sec:157343.27
step 3661, loss: 3.494071, norm:0.2502, lr:5.4715e-04 dt: 3332.07ms, tok/sec:157346.12
step 3662, loss: 3.544228, norm:0.2653, lr:5.4712e-04 dt: 3332.13ms, tok/sec:157343.10
step 3663, loss: 3.516877, norm:0.2685, lr:5.4708e-04 dt: 3332.19ms, tok/sec:157340.38
step 3664, loss: 3.520078, norm:0.2530, lr:5.4705e-04 dt: 3332.07ms, tok/sec:157346.04
step 3665, loss: 3.464257, norm:0.2465, lr:5.4701e-04 dt: 3331.93ms, tok/sec:157352.55
step 3666, loss: 3.504870, norm:0.2511, lr:5.4698e-04 dt: 3332.09ms, tok/sec:157344.89
step 3667, loss: 3.542197, norm:0.2441, lr:5.4694e-04 dt: 3331.94ms, tok/sec:157352.07
step 3668, loss: 3.474703, norm:0.2633, lr:5.4691e-04 dt: 3332.60ms, tok/sec:157321.05
step 3669, loss: 3.448206, norm:0.2418, lr:5.4687e-04 dt: 3331.84ms, tok/sec:157356.91
step 3670, loss: 3.476509, norm:0.2646, lr:5.4684e-04 dt: 3331.93ms, tok/sec:157352.66
step 3671, loss: 3.424157, norm:0.2660, lr:5.4681e-04 dt: 3332.03ms, tok/sec:157347.70
step 3672, loss: 3.366341, norm:0.2489, lr:5.4677e-04 dt: 3332.15ms, tok/sec:157342.22
step 3673, loss: 3.331228, norm:0.2370, lr:5.4674e-04 dt: 3332.06ms, tok/sec:157346.54
step 3674, loss: 3.281918, norm:0.2692, lr:5.4670e-04 dt: 3332.24ms, tok/sec:157338.09
step 3675, loss: 3.340671, norm:0.2581, lr:5.4667e-04 dt: 3331.98ms, tok/sec:157350.35
step 3676, loss: 3.359868, norm:0.2694, lr:5.4663e-04 dt: 3332.00ms, tok/sec:157349.36
step 3677, loss: 3.409997, norm:0.2327, lr:5.4660e-04 dt: 3332.14ms, tok/sec:157342.55
step 3678, loss: 3.345391, norm:0.2654, lr:5.4656e-04 dt: 3332.48ms, tok/sec:157326.75
step 3679, loss: 3.378766, norm:0.2725, lr:5.4653e-04 dt: 3333.33ms, tok/sec:157286.34
step 3680, loss: 3.411601, norm:0.2929, lr:5.4649e-04 dt: 3332.57ms, tok/sec:157322.57
step 3681, loss: 3.393501, norm:0.2567, lr:5.4646e-04 dt: 3332.13ms, tok/sec:157343.41
step 3682, loss: 3.351405, norm:0.2754, lr:5.4642e-04 dt: 3331.98ms, tok/sec:157350.10
step 3683, loss: 3.501604, norm:0.2726, lr:5.4639e-04 dt: 3331.88ms, tok/sec:157354.87
step 3684, loss: 3.614109, norm:0.2762, lr:5.4635e-04 dt: 3332.29ms, tok/sec:157335.83
step 3685, loss: 3.566725, norm:0.2648, lr:5.4632e-04 dt: 3331.86ms, tok/sec:157355.72
step 3686, loss: 3.542903, norm:0.2741, lr:5.4628e-04 dt: 3332.12ms, tok/sec:157343.72
step 3687, loss: 3.602391, norm:0.2823, lr:5.4625e-04 dt: 3332.36ms, tok/sec:157332.17
step 3688, loss: 3.506700, norm:0.2890, lr:5.4621e-04 dt: 3332.15ms, tok/sec:157342.21
step 3689, loss: 3.586008, norm:0.3397, lr:5.4618e-04 dt: 3331.88ms, tok/sec:157354.98
step 3690, loss: 3.568027, norm:0.3024, lr:5.4614e-04 dt: 3331.93ms, tok/sec:157352.47
step 3691, loss: 3.645697, norm:0.2653, lr:5.4611e-04 dt: 3332.44ms, tok/sec:157328.61
step 3692, loss: 3.561506, norm:0.3512, lr:5.4607e-04 dt: 3332.13ms, tok/sec:157343.02
step 3693, loss: 3.586687, norm:0.2875, lr:5.4604e-04 dt: 3331.99ms, tok/sec:157349.89
step 3694, loss: 3.556129, norm:0.2677, lr:5.4600e-04 dt: 3332.33ms, tok/sec:157333.89
step 3695, loss: 3.539845, norm:0.3188, lr:5.4597e-04 dt: 3332.04ms, tok/sec:157347.28
step 3696, loss: 3.471612, norm:0.7740, lr:5.4593e-04 dt: 3332.68ms, tok/sec:157317.23
step 3697, loss: 3.501671, norm:0.2931, lr:5.4590e-04 dt: 3331.92ms, tok/sec:157353.01
step 3698, loss: 3.559878, norm:0.2731, lr:5.4586e-04 dt: 3332.17ms, tok/sec:157341.45
step 3699, loss: 3.554717, norm:0.2640, lr:5.4583e-04 dt: 3332.07ms, tok/sec:157346.03
validation loss: 3.5647
Model and optimizer state saved.
HellaSwag accuracy:-6870008473991298039/-2=3435004236995649024.0000
rank 1 sample 0: Hello, I'm a language model, using a form of speech, in Spanish and other words. How do you interpret the form of this language?
I
rank 1 sample 1: Hello, I'm a language model, which I think helps me understand my theory and I think what I thought is really important in the study. I think it
rank 1 sample 2: Hello, I'm a language model, I actually use some of the language models. I'm going to say I'm going to talk to people, so people
rank 1 sample 3: Hello, I'm a language model, and I'm just going to have that sentence. Then look at the sentences they are calling this one.
"I
rank 0 sample 0: Hello, I'm a language model, and I'd like you to write more comments...
|A, "B, B, and G respectively. (
rank 0 sample 1: Hello, I'm a language model, but now I'm not sure why because I have something similar. My only wish is for this to become more abstract.
rank 0 sample 2: Hello, I'm a language model, so I get my real, wonderful thing. Thank you so much to the people who are you.
I am in
rank 0 sample 3: Hello, I'm a language model, and not a linguist.
And it made me very happy to be here for one month, or so. A
step 3700, loss: 3.537711, norm:0.2642, lr:5.4579e-04 dt: 56236.63ms, tok/sec:9322.89
step 3701, loss: 3.553768, norm:0.2596, lr:5.4576e-04 dt: 3332.26ms, tok/sec:157337.12
step 3702, loss: 3.560554, norm:0.2627, lr:5.4572e-04 dt: 3332.03ms, tok/sec:157347.89
step 3703, loss: 3.600630, norm:0.2518, lr:5.4569e-04 dt: 3332.15ms, tok/sec:157342.03
step 3704, loss: 3.578060, norm:0.2462, lr:5.4565e-04 dt: 3332.04ms, tok/sec:157347.41
step 3705, loss: 3.605135, norm:0.2702, lr:5.4562e-04 dt: 3331.98ms, tok/sec:157350.41
step 3706, loss: 3.500082, norm:0.2431, lr:5.4558e-04 dt: 3332.06ms, tok/sec:157346.51
step 3707, loss: 3.521257, norm:0.2697, lr:5.4555e-04 dt: 3332.20ms, tok/sec:157339.67
step 3708, loss: 3.465833, norm:0.2606, lr:5.4551e-04 dt: 3331.84ms, tok/sec:157356.79
step 3709, loss: 3.513881, norm:0.2483, lr:5.4548e-04 dt: 3332.19ms, tok/sec:157340.47
step 3710, loss: 3.459087, norm:0.2444, lr:5.4544e-04 dt: 3332.30ms, tok/sec:157335.12
step 3711, loss: 3.493067, norm:0.2643, lr:5.4541e-04 dt: 3332.05ms, tok/sec:157347.10
step 3712, loss: 3.497966, norm:0.2748, lr:5.4537e-04 dt: 3331.99ms, tok/sec:157349.79
step 3713, loss: 3.492248, norm:0.2419, lr:5.4534e-04 dt: 3331.90ms, tok/sec:157353.89
step 3714, loss: 3.495691, norm:0.2702, lr:5.4530e-04 dt: 3332.08ms, tok/sec:157345.76
step 3715, loss: 3.482632, norm:0.2419, lr:5.4527e-04 dt: 3332.08ms, tok/sec:157345.63
step 3716, loss: 3.566642, norm:0.2770, lr:5.4523e-04 dt: 3332.17ms, tok/sec:157341.23
step 3717, loss: 3.530678, norm:0.2535, lr:5.4520e-04 dt: 3332.40ms, tok/sec:157330.66
step 3718, loss: 3.351548, norm:0.2632, lr:5.4516e-04 dt: 3331.91ms, tok/sec:157353.76
step 3719, loss: 3.428130, norm:0.2754, lr:5.4513e-04 dt: 3332.55ms, tok/sec:157323.49
step 3720, loss: 3.382041, norm:0.2612, lr:5.4509e-04 dt: 3331.92ms, tok/sec:157353.19
step 3721, loss: 3.365716, norm:0.2742, lr:5.4505e-04 dt: 3331.92ms, tok/sec:157352.96
step 3722, loss: 3.348378, norm:0.3060, lr:5.4502e-04 dt: 3331.93ms, tok/sec:157352.87
step 3723, loss: 3.402705, norm:0.3165, lr:5.4498e-04 dt: 3332.08ms, tok/sec:157345.62
step 3724, loss: 3.439141, norm:0.2763, lr:5.4495e-04 dt: 3331.96ms, tok/sec:157351.11
step 3725, loss: 3.372920, norm:0.2612, lr:5.4491e-04 dt: 3332.12ms, tok/sec:157343.44
step 3726, loss: 3.415310, norm:0.2645, lr:5.4488e-04 dt: 3332.03ms, tok/sec:157348.08
step 3727, loss: 3.341533, norm:0.2669, lr:5.4484e-04 dt: 3331.99ms, tok/sec:157350.00
step 3728, loss: 3.304404, norm:0.2562, lr:5.4481e-04 dt: 3331.98ms, tok/sec:157350.35
step 3729, loss: 3.332081, norm:0.2713, lr:5.4477e-04 dt: 3332.19ms, tok/sec:157340.43
step 3730, loss: 3.531005, norm:0.2791, lr:5.4474e-04 dt: 3332.08ms, tok/sec:157345.35
step 3731, loss: 3.646231, norm:0.2799, lr:5.4470e-04 dt: 3332.28ms, tok/sec:157335.93
step 3732, loss: 3.558268, norm:0.2623, lr:5.4467e-04 dt: 3332.01ms, tok/sec:157349.07
step 3733, loss: 3.611962, norm:0.2690, lr:5.4463e-04 dt: 3332.11ms, tok/sec:157343.95
step 3734, loss: 3.667019, norm:0.2560, lr:5.4460e-04 dt: 3332.15ms, tok/sec:157342.36
step 3735, loss: 3.644480, norm:0.2702, lr:5.4456e-04 dt: 3332.45ms, tok/sec:157328.31
step 3736, loss: 3.620323, norm:0.2667, lr:5.4452e-04 dt: 3331.88ms, tok/sec:157354.89
step 3737, loss: 3.615997, norm:0.2703, lr:5.4449e-04 dt: 3332.31ms, tok/sec:157334.86
step 3738, loss: 3.550347, norm:0.2770, lr:5.4445e-04 dt: 3332.43ms, tok/sec:157329.06
step 3739, loss: 3.600393, norm:0.3136, lr:5.4442e-04 dt: 3332.11ms, tok/sec:157343.99
step 3740, loss: 3.528773, norm:0.2699, lr:5.4438e-04 dt: 3331.99ms, tok/sec:157349.90
step 3741, loss: 3.542459, norm:0.2819, lr:5.4435e-04 dt: 3332.15ms, tok/sec:157342.28
step 3742, loss: 3.549779, norm:0.2754, lr:5.4431e-04 dt: 3332.16ms, tok/sec:157341.76
step 3743, loss: 3.568330, norm:0.2904, lr:5.4428e-04 dt: 3332.14ms, tok/sec:157342.76
step 3744, loss: 3.513154, norm:0.2939, lr:5.4424e-04 dt: 3332.00ms, tok/sec:157349.17
step 3745, loss: 3.559456, norm:0.2615, lr:5.4421e-04 dt: 3332.21ms, tok/sec:157339.53
step 3746, loss: 3.492081, norm:0.2664, lr:5.4417e-04 dt: 3332.20ms, tok/sec:157339.94
step 3747, loss: 3.550863, norm:0.2776, lr:5.4413e-04 dt: 3332.51ms, tok/sec:157325.25
step 3748, loss: 3.547389, norm:0.2677, lr:5.4410e-04 dt: 3332.07ms, tok/sec:157345.86
step 3749, loss: 3.537678, norm:0.2565, lr:5.4406e-04 dt: 3332.11ms, tok/sec:157343.97
HellaSwag accuracy:-6897170845751081903/-2=3448585422875540992.0000
rank 1 sample 0: Hello, I'm a language model, i need a language model for the purpose, as the "dynamic" environment is very much different from the "d
rank 1 sample 1: Hello, I'm a language model, you're working on a language model of something that you see in your everyday life, the very stuff that you see in
rank 0 sample 0: Hello, I'm a language model, and I want to write, to see for something in a book called What You Can Write My Essays in the first
rank 1 sample 2: Hello, I'm a language model, which represents an example of a language model.
And, like in a language model, we use an algorithm. How
rank 1 sample 3: Hello, I'm a language model, and I'm interested to explain what a
language model involves. My point...
Let's take a look at the
rank 0 sample 1: Hello, I'm a language model, so when I was talking about "definitions", like for example, which we can see here, this is a complex
rank 0 sample 2: Hello, I'm a language model, so I do feel an interdisciplinary group of about to do that because I'm very similar to the International Language Institute (
rank 0 sample 3: Hello, I'm a language model, not "C++" but a subset of java.lang.lang. This term defines some kind of language as object
step 3750, loss: 3.575751, norm:0.2531, lr:5.4403e-04 dt: 48511.21ms, tok/sec:10807.56
step 3751, loss: 3.613608, norm:0.2399, lr:5.4399e-04 dt: 3332.05ms, tok/sec:157347.09
step 3752, loss: 3.512558, norm:0.2640, lr:5.4396e-04 dt: 3332.20ms, tok/sec:157340.03
step 3753, loss: 3.494897, norm:0.2446, lr:5.4392e-04 dt: 3332.01ms, tok/sec:157348.70
step 3754, loss: 3.504158, norm:0.2542, lr:5.4389e-04 dt: 3332.24ms, tok/sec:157338.01
step 3755, loss: 3.473387, norm:0.2648, lr:5.4385e-04 dt: 3332.34ms, tok/sec:157333.16
step 3756, loss: 3.503484, norm:0.2797, lr:5.4381e-04 dt: 3332.14ms, tok/sec:157342.53
step 3757, loss: 3.513340, norm:0.2697, lr:5.4378e-04 dt: 3332.00ms, tok/sec:157349.14
step 3758, loss: 3.458816, norm:0.2535, lr:5.4374e-04 dt: 3332.29ms, tok/sec:157335.78
step 3759, loss: 3.511664, norm:0.2590, lr:5.4371e-04 dt: 3332.25ms, tok/sec:157337.74
step 3760, loss: 3.517379, norm:0.2603, lr:5.4367e-04 dt: 3331.96ms, tok/sec:157351.40
step 3761, loss: 3.509899, norm:0.2513, lr:5.4364e-04 dt: 3332.02ms, tok/sec:157348.34
step 3762, loss: 3.464360, norm:0.2505, lr:5.4360e-04 dt: 3331.84ms, tok/sec:157356.69
step 3763, loss: 3.509648, norm:0.2406, lr:5.4356e-04 dt: 3332.02ms, tok/sec:157348.28
step 3764, loss: 3.406363, norm:0.3136, lr:5.4353e-04 dt: 3332.00ms, tok/sec:157349.42
step 3765, loss: 3.328348, norm:0.2911, lr:5.4349e-04 dt: 3332.02ms, tok/sec:157348.36
step 3766, loss: 3.395629, norm:0.2934, lr:5.4346e-04 dt: 3332.37ms, tok/sec:157332.00
step 3767, loss: 3.378695, norm:0.2646, lr:5.4342e-04 dt: 3331.93ms, tok/sec:157352.58
step 3768, loss: 3.344714, norm:0.3153, lr:5.4339e-04 dt: 3332.23ms, tok/sec:157338.62
step 3769, loss: 3.335951, norm:0.2809, lr:5.4335e-04 dt: 3331.94ms, tok/sec:157352.01
step 3770, loss: 3.416768, norm:0.2586, lr:5.4331e-04 dt: 3331.93ms, tok/sec:157352.77
step 3771, loss: 3.351032, norm:0.2843, lr:5.4328e-04 dt: 3331.84ms, tok/sec:157356.76
step 3772, loss: 3.383394, norm:0.2459, lr:5.4324e-04 dt: 3332.12ms, tok/sec:157343.63
step 3773, loss: 3.316632, norm:0.2655, lr:5.4321e-04 dt: 3332.02ms, tok/sec:157348.40
step 3774, loss: 3.324387, norm:0.2517, lr:5.4317e-04 dt: 3332.01ms, tok/sec:157349.10
step 3775, loss: 3.395742, norm:0.2568, lr:5.4314e-04 dt: 3332.04ms, tok/sec:157347.31
step 3776, loss: 3.610934, norm:0.2419, lr:5.4310e-04 dt: 3332.30ms, tok/sec:157334.97
step 3777, loss: 3.559070, norm:0.2587, lr:5.4306e-04 dt: 3332.63ms, tok/sec:157319.64
step 3778, loss: 3.579304, norm:0.2714, lr:5.4303e-04 dt: 3332.00ms, tok/sec:157349.22
step 3779, loss: 3.565304, norm:0.2667, lr:5.4299e-04 dt: 3331.97ms, tok/sec:157350.96
step 3780, loss: 3.555019, norm:0.2771, lr:5.4296e-04 dt: 3332.10ms, tok/sec:157344.57
step 3781, loss: 3.544615, norm:0.2630, lr:5.4292e-04 dt: 3332.24ms, tok/sec:157338.22
step 3782, loss: 3.523435, norm:0.2744, lr:5.4288e-04 dt: 3332.08ms, tok/sec:157345.53
step 3783, loss: 3.588307, norm:0.2496, lr:5.4285e-04 dt: 3332.30ms, tok/sec:157335.04
step 3784, loss: 3.503875, norm:0.2902, lr:5.4281e-04 dt: 3332.41ms, tok/sec:157330.19
step 3785, loss: 3.549336, norm:0.2767, lr:5.4278e-04 dt: 3332.15ms, tok/sec:157342.10
step 3786, loss: 3.605968, norm:0.2375, lr:5.4274e-04 dt: 3332.18ms, tok/sec:157340.87
step 3787, loss: 3.533257, norm:0.2845, lr:5.4271e-04 dt: 3332.18ms, tok/sec:157340.87
step 3788, loss: 3.563574, norm:0.2519, lr:5.4267e-04 dt: 3331.89ms, tok/sec:157354.50
step 3789, loss: 3.554485, norm:0.2709, lr:5.4263e-04 dt: 3332.03ms, tok/sec:157347.70
step 3790, loss: 3.500587, norm:0.2412, lr:5.4260e-04 dt: 3332.21ms, tok/sec:157339.42
step 3791, loss: 3.576336, norm:0.2809, lr:5.4256e-04 dt: 3332.35ms, tok/sec:157332.87
step 3792, loss: 3.557691, norm:0.3155, lr:5.4253e-04 dt: 3332.01ms, tok/sec:157348.75
step 3793, loss: 3.537110, norm:0.2878, lr:5.4249e-04 dt: 3331.92ms, tok/sec:157353.27
step 3794, loss: 3.541117, norm:0.2724, lr:5.4245e-04 dt: 3332.08ms, tok/sec:157345.74
step 3795, loss: 3.555869, norm:0.3078, lr:5.4242e-04 dt: 3332.61ms, tok/sec:157320.37
step 3796, loss: 3.517164, norm:0.2598, lr:5.4238e-04 dt: 3331.96ms, tok/sec:157351.17
step 3797, loss: 3.547792, norm:0.2665, lr:5.4235e-04 dt: 3332.44ms, tok/sec:157328.77
step 3798, loss: 3.460053, norm:0.2558, lr:5.4231e-04 dt: 3332.11ms, tok/sec:157344.14
step 3799, loss: 3.553526, norm:0.2621, lr:5.4227e-04 dt: 3332.56ms, tok/sec:157323.13
validation loss: 3.5538
Model and optimizer state saved.
HellaSwag accuracy:20358216249738257/-2=-10179108124869128.0000
rank 1 sample 0: Hello, I'm a language model, and they are pretty well written. Don's keep it open up, and write it for yourself:
"I'm
rank 1 sample 1: Hello, I'm a language model, which I've found that the same number of words per topic is found in this model
> How do you write a
rank 1 sample 2: Hello, I'm a language model, I haven't found it. I'm a language model for writing and reading. I'm writing code for a particular keyword
rank 1 sample 3: Hello, I'm a language model, and I'm interested to create a game - I know many games don't always work fine -- you can use one of
rank 0 sample 0: Hello, I'm a language model, and I'll be working with other languages over the years.
And remember the first word of God is not the only
rank 0 sample 1: Hello, I'm a language model, so to speak, I'm using as a tool: how do I code and what I am trying to do? What
rank 0 sample 2: Hello, I'm a language model, so I like it really to start at the language point. The point at which I want to start at the point is
rank 0 sample 3: Hello, I'm a language model, and is open to you. I think it just takes a little learning to learn new words.
To get you the
step 3800, loss: 3.523562, norm:0.2696, lr:5.4224e-04 dt: 56160.55ms, tok/sec:9335.52
step 3801, loss: 3.458681, norm:0.2642, lr:5.4220e-04 dt: 3332.35ms, tok/sec:157332.64
step 3802, loss: 3.545838, norm:0.2617, lr:5.4217e-04 dt: 3331.99ms, tok/sec:157349.80
step 3803, loss: 3.488291, norm:0.2418, lr:5.4213e-04 dt: 3332.38ms, tok/sec:157331.45
step 3804, loss: 3.498952, norm:0.2352, lr:5.4209e-04 dt: 3332.07ms, tok/sec:157346.05
step 3805, loss: 3.492059, norm:0.2512, lr:5.4206e-04 dt: 3332.14ms, tok/sec:157342.87
step 3806, loss: 3.532156, norm:0.2503, lr:5.4202e-04 dt: 3331.94ms, tok/sec:157352.11
step 3807, loss: 3.524654, norm:0.2435, lr:5.4199e-04 dt: 3332.09ms, tok/sec:157345.17
step 3808, loss: 3.499835, norm:0.2456, lr:5.4195e-04 dt: 3331.97ms, tok/sec:157350.72
step 3809, loss: 3.441736, norm:0.2491, lr:5.4191e-04 dt: 3334.05ms, tok/sec:157252.47
step 3810, loss: 3.407983, norm:0.2677, lr:5.4188e-04 dt: 3332.29ms, tok/sec:157335.48
step 3811, loss: 3.310369, norm:0.2673, lr:5.4184e-04 dt: 3331.97ms, tok/sec:157350.83
step 3812, loss: 3.381445, norm:0.3037, lr:5.4180e-04 dt: 3332.65ms, tok/sec:157318.51
step 3813, loss: 3.324859, norm:0.2824, lr:5.4177e-04 dt: 3332.26ms, tok/sec:157336.91
step 3814, loss: 3.320121, norm:0.2689, lr:5.4173e-04 dt: 3331.99ms, tok/sec:157350.01
step 3815, loss: 3.311523, norm:0.2910, lr:5.4170e-04 dt: 3332.05ms, tok/sec:157347.12
step 3816, loss: 3.387561, norm:0.2593, lr:5.4166e-04 dt: 3332.12ms, tok/sec:157343.61
step 3817, loss: 3.330539, norm:0.2685, lr:5.4162e-04 dt: 3332.04ms, tok/sec:157347.26
step 3818, loss: 3.368170, norm:0.2661, lr:5.4159e-04 dt: 3332.01ms, tok/sec:157349.09
step 3819, loss: 3.310349, norm:0.2666, lr:5.4155e-04 dt: 3331.89ms, tok/sec:157354.43
step 3820, loss: 3.347670, norm:0.2654, lr:5.4151e-04 dt: 3331.87ms, tok/sec:157355.43
step 3821, loss: 3.354847, norm:0.2674, lr:5.4148e-04 dt: 3331.84ms, tok/sec:157356.80
step 3822, loss: 3.503975, norm:0.3133, lr:5.4144e-04 dt: 3332.22ms, tok/sec:157338.89
step 3823, loss: 3.530979, norm:0.2547, lr:5.4141e-04 dt: 3332.51ms, tok/sec:157325.27
step 3824, loss: 3.505358, norm:0.2623, lr:5.4137e-04 dt: 3332.10ms, tok/sec:157344.72
step 3825, loss: 3.539791, norm:0.2592, lr:5.4133e-04 dt: 3331.98ms, tok/sec:157350.34
step 3826, loss: 3.547708, norm:0.2963, lr:5.4130e-04 dt: 3332.00ms, tok/sec:157349.52
step 3827, loss: 3.566137, norm:0.2655, lr:5.4126e-04 dt: 3332.08ms, tok/sec:157345.58
step 3828, loss: 3.571472, norm:0.2445, lr:5.4122e-04 dt: 3332.14ms, tok/sec:157342.56
step 3829, loss: 3.538962, norm:0.2523, lr:5.4119e-04 dt: 3332.11ms, tok/sec:157344.18
step 3830, loss: 3.527591, norm:0.2523, lr:5.4115e-04 dt: 3331.89ms, tok/sec:157354.77
step 3831, loss: 3.523612, norm:0.2665, lr:5.4112e-04 dt: 3331.81ms, tok/sec:157358.53
step 3832, loss: 3.535504, norm:0.2481, lr:5.4108e-04 dt: 3332.31ms, tok/sec:157334.84
step 3833, loss: 3.522090, norm:0.2580, lr:5.4104e-04 dt: 3332.26ms, tok/sec:157337.27
step 3834, loss: 3.507025, norm:0.2689, lr:5.4101e-04 dt: 3332.03ms, tok/sec:157347.74
step 3835, loss: 3.617332, norm:0.2439, lr:5.4097e-04 dt: 3331.99ms, tok/sec:157349.61
step 3836, loss: 3.495157, norm:0.3125, lr:5.4093e-04 dt: 3332.02ms, tok/sec:157348.44
step 3837, loss: 3.566154, norm:0.2632, lr:5.4090e-04 dt: 3332.18ms, tok/sec:157341.08
step 3838, loss: 3.575386, norm:0.2893, lr:5.4086e-04 dt: 3332.07ms, tok/sec:157345.83
step 3839, loss: 3.590555, norm:0.2661, lr:5.4082e-04 dt: 3331.93ms, tok/sec:157352.84
step 3840, loss: 3.553516, norm:0.2684, lr:5.4079e-04 dt: 3332.08ms, tok/sec:157345.47
step 3841, loss: 3.548891, norm:0.2715, lr:5.4075e-04 dt: 3332.35ms, tok/sec:157332.63
step 3842, loss: 3.498516, norm:0.2820, lr:5.4071e-04 dt: 3331.86ms, tok/sec:157355.78
step 3843, loss: 3.571446, norm:0.2995, lr:5.4068e-04 dt: 3332.08ms, tok/sec:157345.70
step 3844, loss: 3.557065, norm:0.2649, lr:5.4064e-04 dt: 3331.97ms, tok/sec:157350.55
step 3845, loss: 3.469819, norm:0.2739, lr:5.4061e-04 dt: 3332.19ms, tok/sec:157340.22
step 3846, loss: 3.494581, norm:0.2748, lr:5.4057e-04 dt: 3331.94ms, tok/sec:157352.00
step 3847, loss: 3.504285, norm:0.2637, lr:5.4053e-04 dt: 3332.01ms, tok/sec:157348.79
step 3848, loss: 3.464484, norm:0.2639, lr:5.4050e-04 dt: 3332.26ms, tok/sec:157336.97
step 3849, loss: 3.511925, norm:0.2653, lr:5.4046e-04 dt: 3332.01ms, tok/sec:157348.65
HellaSwag accuracy:-4601320198277708727/-2=2300660099138854400.0000
rank 1 sample 0: Hello, I'm a language model, and you'll be able to read to the people of mine because I'm one of my peers on the subject.

rank 1 sample 1: Hello, I'm a language model, which I can read as a function for example. In essence, every function should have unique parts and unique properties.

rank 1 sample 2: Hello, I'm a language model, I won't do it. I'm a language model and is the only language model I'm currently writing for you,
rank 1 sample 3: Hello, I'm a language model, and I'm in a special area of what I'm having with those students now. All their names are in parentheses.
rank 0 sample 0: Hello, I'm a language model, but I haven't had this one on as there are certain languages out there which are different from what you do. If
rank 0 sample 1: Hello, I'm a language model, I use JPL and JPL [Java], "How to make learning easier," but I don't want to go
rank 0 sample 2: Hello, I'm a language model, and I haven't already started and this is very useful. We can also use the "Java" form to get instructions
rank 0 sample 3: Hello, I'm a language model, I wanted to learn how to write more of characters. I made many mistakes when getting me.
A lot of time
step 3850, loss: 3.545597, norm:0.2719, lr:5.4042e-04 dt: 48513.94ms, tok/sec:10806.96
step 3851, loss: 3.495866, norm:0.2414, lr:5.4039e-04 dt: 3332.29ms, tok/sec:157335.78
step 3852, loss: 3.411206, norm:0.2806, lr:5.4035e-04 dt: 3332.19ms, tok/sec:157340.59
step 3853, loss: 3.673018, norm:0.2560, lr:5.4031e-04 dt: 3332.14ms, tok/sec:157342.56
step 3854, loss: 3.481406, norm:0.2952, lr:5.4028e-04 dt: 3332.10ms, tok/sec:157344.63
step 3855, loss: 3.509978, norm:0.2862, lr:5.4024e-04 dt: 3331.98ms, tok/sec:157350.50
step 3856, loss: 3.443202, norm:0.2805, lr:5.4020e-04 dt: 3332.36ms, tok/sec:157332.13
step 3857, loss: 3.407950, norm:0.6124, lr:5.4017e-04 dt: 3332.13ms, tok/sec:157343.37
step 3858, loss: 3.276570, norm:0.3154, lr:5.4013e-04 dt: 3332.53ms, tok/sec:157324.16
step 3859, loss: 3.394595, norm:0.3505, lr:5.4009e-04 dt: 3332.21ms, tok/sec:157339.48
step 3860, loss: 3.385299, norm:0.2753, lr:5.4006e-04 dt: 3332.02ms, tok/sec:157348.53
step 3861, loss: 3.368396, norm:0.3232, lr:5.4002e-04 dt: 3331.82ms, tok/sec:157357.63
step 3862, loss: 3.384025, norm:0.2638, lr:5.3998e-04 dt: 3331.84ms, tok/sec:157356.96
step 3863, loss: 3.426512, norm:0.3071, lr:5.3995e-04 dt: 3331.80ms, tok/sec:157358.98
step 3864, loss: 3.387933, norm:0.3105, lr:5.3991e-04 dt: 3332.06ms, tok/sec:157346.59
step 3865, loss: 3.369588, norm:0.3194, lr:5.3987e-04 dt: 3332.12ms, tok/sec:157343.73
step 3866, loss: 3.354488, norm:0.2844, lr:5.3984e-04 dt: 3332.26ms, tok/sec:157337.27
step 3867, loss: 3.389215, norm:0.2701, lr:5.3980e-04 dt: 3332.03ms, tok/sec:157348.09
step 3868, loss: 3.293343, norm:0.2659, lr:5.3976e-04 dt: 3332.06ms, tok/sec:157346.34
step 3869, loss: 3.496844, norm:0.2685, lr:5.3973e-04 dt: 3332.05ms, tok/sec:157346.82
step 3870, loss: 3.522691, norm:0.2443, lr:5.3969e-04 dt: 3332.09ms, tok/sec:157345.00
step 3871, loss: 3.572973, norm:0.2898, lr:5.3965e-04 dt: 3331.85ms, tok/sec:157356.59
step 3872, loss: 3.573061, norm:0.2830, lr:5.3962e-04 dt: 3332.43ms, tok/sec:157329.11
step 3873, loss: 3.476674, norm:0.2619, lr:5.3958e-04 dt: 3331.91ms, tok/sec:157353.58
step 3874, loss: 3.546697, norm:0.2757, lr:5.3954e-04 dt: 3332.57ms, tok/sec:157322.38
step 3875, loss: 3.578510, norm:0.2671, lr:5.3951e-04 dt: 3332.08ms, tok/sec:157345.71
step 3876, loss: 3.544004, norm:0.2585, lr:5.3947e-04 dt: 3332.10ms, tok/sec:157344.46
step 3877, loss: 3.483319, norm:0.2686, lr:5.3943e-04 dt: 3332.03ms, tok/sec:157347.84
step 3878, loss: 3.587317, norm:0.2569, lr:5.3940e-04 dt: 3332.26ms, tok/sec:157336.86
step 3879, loss: 3.554930, norm:0.2581, lr:5.3936e-04 dt: 3331.96ms, tok/sec:157351.25
step 3880, loss: 3.529366, norm:0.2536, lr:5.3932e-04 dt: 3332.22ms, tok/sec:157338.99
step 3881, loss: 3.491800, norm:0.2378, lr:5.3929e-04 dt: 3332.14ms, tok/sec:157342.65
step 3882, loss: 3.565804, norm:0.2682, lr:5.3925e-04 dt: 3332.01ms, tok/sec:157348.76
step 3883, loss: 3.558333, norm:0.2509, lr:5.3921e-04 dt: 3332.42ms, tok/sec:157329.47
step 3884, loss: 3.546926, norm:0.2555, lr:5.3917e-04 dt: 3332.12ms, tok/sec:157343.81
step 3885, loss: 3.535027, norm:0.2577, lr:5.3914e-04 dt: 3332.00ms, tok/sec:157349.20
step 3886, loss: 3.517347, norm:0.2528, lr:5.3910e-04 dt: 3332.11ms, tok/sec:157343.95
step 3887, loss: 3.497673, norm:0.2682, lr:5.3906e-04 dt: 3331.97ms, tok/sec:157350.84
step 3888, loss: 3.552431, norm:0.2495, lr:5.3903e-04 dt: 3332.39ms, tok/sec:157331.02
step 3889, loss: 3.540865, norm:0.2638, lr:5.3899e-04 dt: 3331.93ms, tok/sec:157352.84
step 3890, loss: 3.518693, norm:0.3679, lr:5.3895e-04 dt: 3332.50ms, tok/sec:157325.66
step 3891, loss: 3.576057, norm:0.2520, lr:5.3892e-04 dt: 3332.33ms, tok/sec:157333.69
step 3892, loss: 3.552431, norm:0.2790, lr:5.3888e-04 dt: 3333.26ms, tok/sec:157289.95
step 3893, loss: 3.489342, norm:0.2477, lr:5.3884e-04 dt: 3332.44ms, tok/sec:157328.47
step 3894, loss: 3.508832, norm:0.3038, lr:5.3881e-04 dt: 3332.20ms, tok/sec:157339.85
step 3895, loss: 3.468963, norm:0.2509, lr:5.3877e-04 dt: 3332.15ms, tok/sec:157342.16
step 3896, loss: 3.536342, norm:0.2544, lr:5.3873e-04 dt: 3332.19ms, tok/sec:157340.37
step 3897, loss: 3.488949, norm:0.2529, lr:5.3869e-04 dt: 3332.33ms, tok/sec:157333.72
step 3898, loss: 3.497145, norm:0.2479, lr:5.3866e-04 dt: 3332.05ms, tok/sec:157347.02
step 3899, loss: 3.536053, norm:0.2578, lr:5.3862e-04 dt: 3332.69ms, tok/sec:157316.69
validation loss: 3.5451
Model and optimizer state saved.
HellaSwag accuracy:2325216028618114057/-2=-1162608014309057024.0000
rank 1 sample 0: Hello, I'm a language model, in that it's actually a lot like that's what actually drives the process.
In fact, I'd like to
rank 1 sample 1: Hello, I'm a language model, which I've come to understand by this exercise.
1. Introduction to Python: Overview of python.
2.
rank 1 sample 2: Hello, I'm a language model, I thought about using the language model. I think it is quite interesting.
I think the idea was that it is
rank 1 sample 3: Hello, I'm a language model, and I'm interested to give you a call to do everything for it. Now, get through your first comment about the
rank 0 sample 0: Hello, I'm a language model, and I would like you to understand them on Linux. I would do any kind of Python scripting on the Arduino board and
rank 0 sample 1: Hello, I'm a language model, I like to understand what I am trying to understand - for instance, using an object. If you try to understand how
rank 0 sample 2: Hello, I'm a language model, so I get it when I click it. My problem is you don't know what you're doing. I can�
rank 0 sample 3: Hello, I'm a language model, which isn't it?
I hope that whatever you call it, this one won't really work unless you know someone
step 3900, loss: 3.498950, norm:0.2467, lr:5.3858e-04 dt: 56253.94ms, tok/sec:9320.02
step 3901, loss: 3.458149, norm:0.2400, lr:5.3855e-04 dt: 3331.90ms, tok/sec:157354.23
step 3902, loss: 3.470554, norm:0.2648, lr:5.3851e-04 dt: 3331.88ms, tok/sec:157355.08
step 3903, loss: 3.570193, norm:0.2881, lr:5.3847e-04 dt: 3332.02ms, tok/sec:157348.47
step 3904, loss: 3.479971, norm:0.2840, lr:5.3844e-04 dt: 3332.37ms, tok/sec:157331.64
step 3905, loss: 3.317349, norm:0.2749, lr:5.3840e-04 dt: 3332.10ms, tok/sec:157344.60
step 3906, loss: 3.401643, norm:0.2983, lr:5.3836e-04 dt: 3331.94ms, tok/sec:157352.32
step 3907, loss: 3.391879, norm:0.2908, lr:5.3832e-04 dt: 3332.26ms, tok/sec:157337.00
step 3908, loss: 3.341807, norm:0.3024, lr:5.3829e-04 dt: 3332.12ms, tok/sec:157343.73
step 3909, loss: 3.336630, norm:0.2635, lr:5.3825e-04 dt: 3331.98ms, tok/sec:157350.45
step 3910, loss: 3.338931, norm:0.2680, lr:5.3821e-04 dt: 3332.25ms, tok/sec:157337.57
step 3911, loss: 3.357704, norm:0.2882, lr:5.3818e-04 dt: 3332.25ms, tok/sec:157337.50
step 3912, loss: 3.337220, norm:0.2747, lr:5.3814e-04 dt: 3332.19ms, tok/sec:157340.34
step 3913, loss: 3.352905, norm:0.2791, lr:5.3810e-04 dt: 3331.89ms, tok/sec:157354.35
step 3914, loss: 3.344528, norm:0.2811, lr:5.3806e-04 dt: 3332.00ms, tok/sec:157349.22
step 3915, loss: 3.323708, norm:0.2739, lr:5.3803e-04 dt: 3332.26ms, tok/sec:157337.28
step 3916, loss: 3.384570, norm:0.2639, lr:5.3799e-04 dt: 3332.10ms, tok/sec:157344.61
step 3917, loss: 3.515279, norm:0.2847, lr:5.3795e-04 dt: 3332.15ms, tok/sec:157342.21
step 3918, loss: 3.530623, norm:0.2752, lr:5.3792e-04 dt: 3332.08ms, tok/sec:157345.49
step 3919, loss: 3.533898, norm:0.2784, lr:5.3788e-04 dt: 3332.42ms, tok/sec:157329.71
step 3920, loss: 3.551876, norm:0.2807, lr:5.3784e-04 dt: 3332.16ms, tok/sec:157341.67
step 3921, loss: 3.532460, norm:0.2537, lr:5.3780e-04 dt: 3332.01ms, tok/sec:157349.05
step 3922, loss: 3.543919, norm:0.2690, lr:5.3777e-04 dt: 3332.11ms, tok/sec:157343.93
step 3923, loss: 3.565187, norm:0.2789, lr:5.3773e-04 dt: 3332.13ms, tok/sec:157343.16
step 3924, loss: 3.529680, norm:0.2499, lr:5.3769e-04 dt: 3331.97ms, tok/sec:157350.71
step 3925, loss: 3.545613, norm:0.2753, lr:5.3765e-04 dt: 3332.02ms, tok/sec:157348.42
step 3926, loss: 3.514843, norm:0.2551, lr:5.3762e-04 dt: 3332.40ms, tok/sec:157330.22
step 3927, loss: 3.491620, norm:0.2868, lr:5.3758e-04 dt: 3331.92ms, tok/sec:157353.13
step 3928, loss: 3.591660, norm:0.2679, lr:5.3754e-04 dt: 3332.41ms, tok/sec:157329.80
step 3929, loss: 3.569123, norm:0.2857, lr:5.3751e-04 dt: 3332.17ms, tok/sec:157341.35
step 3930, loss: 3.534338, norm:0.2809, lr:5.3747e-04 dt: 3331.97ms, tok/sec:157350.84
step 3931, loss: 3.518116, norm:0.2663, lr:5.3743e-04 dt: 3332.29ms, tok/sec:157335.60
step 3932, loss: 3.472479, norm:0.2772, lr:5.3739e-04 dt: 3332.07ms, tok/sec:157345.93
step 3933, loss: 3.521609, norm:0.2702, lr:5.3736e-04 dt: 3332.28ms, tok/sec:157335.89
step 3934, loss: 3.524379, norm:0.2677, lr:5.3732e-04 dt: 3332.03ms, tok/sec:157347.72
step 3935, loss: 3.521760, norm:0.2623, lr:5.3728e-04 dt: 3332.38ms, tok/sec:157331.48
step 3936, loss: 3.588859, norm:0.2713, lr:5.3724e-04 dt: 3331.92ms, tok/sec:157352.95
step 3937, loss: 3.554487, norm:0.2618, lr:5.3721e-04 dt: 3332.03ms, tok/sec:157347.90
step 3938, loss: 3.542247, norm:0.2422, lr:5.3717e-04 dt: 3331.92ms, tok/sec:157353.28
step 3939, loss: 3.477645, norm:0.2796, lr:5.3713e-04 dt: 3332.08ms, tok/sec:157345.78
step 3940, loss: 3.488375, norm:0.2700, lr:5.3709e-04 dt: 3331.84ms, tok/sec:157356.78
step 3941, loss: 3.455811, norm:0.2526, lr:5.3706e-04 dt: 3332.20ms, tok/sec:157339.92
step 3942, loss: 3.472888, norm:0.2633, lr:5.3702e-04 dt: 3332.43ms, tok/sec:157329.12
step 3943, loss: 3.484133, norm:0.2472, lr:5.3698e-04 dt: 3332.11ms, tok/sec:157344.30
step 3944, loss: 3.485117, norm:0.2553, lr:5.3694e-04 dt: 3332.40ms, tok/sec:157330.48
step 3945, loss: 3.504965, norm:0.2410, lr:5.3691e-04 dt: 3332.00ms, tok/sec:157349.23
step 3946, loss: 3.488120, norm:0.2377, lr:5.3687e-04 dt: 3331.83ms, tok/sec:157357.59
step 3947, loss: 3.507187, norm:0.2394, lr:5.3683e-04 dt: 3332.09ms, tok/sec:157345.24
step 3948, loss: 3.522439, norm:0.2330, lr:5.3679e-04 dt: 3332.15ms, tok/sec:157342.04
step 3949, loss: 3.492950, norm:0.2384, lr:5.3676e-04 dt: 3331.88ms, tok/sec:157354.95
HellaSwag accuracy:4632343301772772425/-2=-2316171650886386176.0000
rank 0 sample 0: Hello, I'm a language model, and I don't think anyone should say about it. I think of many different things. Here I have some of my
rank 0 sample 1: Hello, I'm a language model, but as I'm writing, the problem is, when all of our interactions are done correctly, it gets tricky. It
rank 0 sample 2: Hello, I'm a language model, so I hope that our goal is to help all of us develop language, and not only to help us understand and solve
rank 0 sample 3: Hello, I'm a language model, and its not just a language. I am aware of the fact that this has worked for several decades, and the ability
rank 1 sample 0: Hello, I'm a language model, the data flow is defined in a C, then I create the data flow structure. This way of doing the data flow
rank 1 sample 1: Hello, I'm a language model, which I used to explain the different levels of the verb tenses.
If I look at the example above, I
rank 1 sample 2: Hello, I'm a language model, so hopefully the rest of the code is in the same state I want you to know. But there's another language that
rank 1 sample 3: Hello, I'm a language model, and I'm in a middle school English band. When finished, I can describe the entire system in a different way.
step 3950, loss: 3.479187, norm:0.2541, lr:5.3672e-04 dt: 48516.01ms, tok/sec:10806.49
step 3951, loss: 3.487352, norm:0.2648, lr:5.3668e-04 dt: 3331.98ms, tok/sec:157350.39
step 3952, loss: 3.384880, norm:0.2724, lr:5.3664e-04 dt: 3332.16ms, tok/sec:157341.57
step 3953, loss: 3.355602, norm:0.2950, lr:5.3661e-04 dt: 3332.43ms, tok/sec:157328.99
step 3954, loss: 3.371756, norm:0.2655, lr:5.3657e-04 dt: 3332.06ms, tok/sec:157346.33
step 3955, loss: 3.294996, norm:0.2697, lr:5.3653e-04 dt: 3331.95ms, tok/sec:157351.83
step 3956, loss: 3.293615, norm:0.2582, lr:5.3649e-04 dt: 3332.02ms, tok/sec:157348.54
step 3957, loss: 3.369135, norm:0.2512, lr:5.3646e-04 dt: 3331.96ms, tok/sec:157351.13
step 3958, loss: 3.298855, norm:0.2517, lr:5.3642e-04 dt: 3332.06ms, tok/sec:157346.37
step 3959, loss: 3.336054, norm:0.2747, lr:5.3638e-04 dt: 3332.12ms, tok/sec:157343.73
step 3960, loss: 3.353585, norm:0.2766, lr:5.3634e-04 dt: 3332.26ms, tok/sec:157337.29
step 3961, loss: 3.364513, norm:0.2622, lr:5.3631e-04 dt: 3332.07ms, tok/sec:157346.09
step 3962, loss: 3.287572, norm:0.2658, lr:5.3627e-04 dt: 3332.10ms, tok/sec:157344.63
step 3963, loss: 3.450348, norm:0.2649, lr:5.3623e-04 dt: 3332.06ms, tok/sec:157346.47
step 3964, loss: 3.524586, norm:0.2699, lr:5.3619e-04 dt: 3331.97ms, tok/sec:157350.95
step 3965, loss: 3.539925, norm:0.2957, lr:5.3616e-04 dt: 3332.13ms, tok/sec:157343.39
step 3966, loss: 3.548481, norm:0.2695, lr:5.3612e-04 dt: 3332.13ms, tok/sec:157343.08
step 3967, loss: 3.533773, norm:0.2769, lr:5.3608e-04 dt: 3332.10ms, tok/sec:157344.75
step 3968, loss: 3.547439, norm:0.2845, lr:5.3604e-04 dt: 3332.17ms, tok/sec:157341.53
step 3969, loss: 3.558192, norm:0.2675, lr:5.3600e-04 dt: 3332.28ms, tok/sec:157336.23
step 3970, loss: 3.507561, norm:0.2649, lr:5.3597e-04 dt: 3332.10ms, tok/sec:157344.66
step 3971, loss: 3.540882, norm:0.2756, lr:5.3593e-04 dt: 3332.08ms, tok/sec:157345.43
step 3972, loss: 3.521966, norm:0.2964, lr:5.3589e-04 dt: 3332.26ms, tok/sec:157337.16
step 3973, loss: 3.464106, norm:0.2917, lr:5.3585e-04 dt: 3331.90ms, tok/sec:157353.85
step 3974, loss: 3.521943, norm:0.2803, lr:5.3582e-04 dt: 3332.15ms, tok/sec:157342.13
step 3975, loss: 3.557247, norm:0.2604, lr:5.3578e-04 dt: 3332.28ms, tok/sec:157335.99
step 3976, loss: 3.546893, norm:0.2880, lr:5.3574e-04 dt: 3332.13ms, tok/sec:157343.28
step 3977, loss: 3.555707, norm:0.2927, lr:5.3570e-04 dt: 3332.35ms, tok/sec:157332.72
step 3978, loss: 3.529779, norm:0.2824, lr:5.3567e-04 dt: 3332.12ms, tok/sec:157343.53
step 3979, loss: 3.486766, norm:0.2552, lr:5.3563e-04 dt: 3332.19ms, tok/sec:157340.39
step 3980, loss: 3.580702, norm:0.2591, lr:5.3559e-04 dt: 3332.29ms, tok/sec:157335.86
step 3981, loss: 3.611883, norm:0.3240, lr:5.3555e-04 dt: 3332.14ms, tok/sec:157342.67
step 3982, loss: 3.553653, norm:0.3186, lr:5.3551e-04 dt: 3332.31ms, tok/sec:157334.71
step 3983, loss: 3.573386, norm:0.2848, lr:5.3548e-04 dt: 3331.85ms, tok/sec:157356.38
step 3984, loss: 3.528041, norm:0.2941, lr:5.3544e-04 dt: 3332.39ms, tok/sec:157330.92
step 3985, loss: 3.493092, norm:0.2742, lr:5.3540e-04 dt: 3332.20ms, tok/sec:157339.92
step 3986, loss: 3.489470, norm:0.2439, lr:5.3536e-04 dt: 3332.14ms, tok/sec:157342.54
step 3987, loss: 3.486277, norm:0.2625, lr:5.3532e-04 dt: 3332.07ms, tok/sec:157345.87
step 3988, loss: 3.520585, norm:0.2679, lr:5.3529e-04 dt: 3332.02ms, tok/sec:157348.35
step 3989, loss: 3.541549, norm:0.2853, lr:5.3525e-04 dt: 3332.19ms, tok/sec:157340.15
step 3990, loss: 3.501843, norm:0.2503, lr:5.3521e-04 dt: 3332.30ms, tok/sec:157335.31
step 3991, loss: 3.449612, norm:0.2609, lr:5.3517e-04 dt: 3332.41ms, tok/sec:157329.85
step 3992, loss: 3.522501, norm:0.2546, lr:5.3514e-04 dt: 3332.02ms, tok/sec:157348.40
step 3993, loss: 3.512067, norm:0.2495, lr:5.3510e-04 dt: 3332.14ms, tok/sec:157342.72
step 3994, loss: 3.507040, norm:0.2516, lr:5.3506e-04 dt: 3332.00ms, tok/sec:157349.15
step 3995, loss: 3.457855, norm:0.2247, lr:5.3502e-04 dt: 3332.02ms, tok/sec:157348.45
step 3996, loss: 3.470644, norm:0.2637, lr:5.3498e-04 dt: 3332.05ms, tok/sec:157347.00
step 3997, loss: 3.492713, norm:0.2317, lr:5.3495e-04 dt: 3332.28ms, tok/sec:157336.10
step 3998, loss: 3.410027, norm:0.2601, lr:5.3491e-04 dt: 3332.11ms, tok/sec:157344.04
step 3999, loss: 3.330970, norm:0.2366, lr:5.3487e-04 dt: 3332.49ms, tok/sec:157326.13
validation loss: 3.5381
Model and optimizer state saved.
HellaSwag accuracy:4631060171770283081/-2=-2315530085885141504.0000
rank 1 sample 0: Hello, I'm a language model, and now you're reading a lot right here you've got a lot of choices. Let's see how we can use
rank 1 sample 1: Hello, I'm a language model, I've had that question, which is I'm just asking a question. This is I'm simply saying that I'm
rank 1 sample 2: Hello, I'm a language model, which could be very useful for the integration of the software, or a tool for the integration of other tools, to add
rank 1 sample 3: Hello, I'm a language model, and I'm just a group of you out there. After finishing out our second language study, I'm going to be
rank 0 sample 0: Hello, I'm a language model, and I would like you to do an extended exercise.
If you prefer using the exercise, and if you prefer a
rank 0 sample 1: Hello, I'm a language model, so when you're trying to create images, the objects should look as though they are in your background. In this case
rank 0 sample 2: Hello, I'm a language model, but I found this really intriguing. And I hope to see a more in-depth discussion about the value of this kind
rank 0 sample 3: Hello, I'm a language model, and what I learned from this experience was the perfect tool for teachers. My first exposure to my core classes was a short
step 4000, loss: 3.365387, norm:0.2410, lr:5.3483e-04 dt: 56134.42ms, tok/sec:9339.87
step 4001, loss: 3.278762, norm:0.3534, lr:5.3479e-04 dt: 3332.22ms, tok/sec:157338.89
step 4002, loss: 3.368953, norm:0.2977, lr:5.3476e-04 dt: 3331.73ms, tok/sec:157362.21
step 4003, loss: 3.365036, norm:0.2636, lr:5.3472e-04 dt: 3332.50ms, tok/sec:157325.84
step 4004, loss: 3.322263, norm:0.2339, lr:5.3468e-04 dt: 3332.15ms, tok/sec:157342.19
step 4005, loss: 3.257778, norm:0.3075, lr:5.3464e-04 dt: 3332.00ms, tok/sec:157349.28
step 4006, loss: 3.341412, norm:0.2882, lr:5.3460e-04 dt: 3332.04ms, tok/sec:157347.48
step 4007, loss: 3.356934, norm:0.2776, lr:5.3457e-04 dt: 3331.89ms, tok/sec:157354.32
step 4008, loss: 3.316711, norm:0.2717, lr:5.3453e-04 dt: 3331.98ms, tok/sec:157350.19
step 4009, loss: 3.325569, norm:0.2857, lr:5.3449e-04 dt: 3332.00ms, tok/sec:157349.23
step 4010, loss: 3.545298, norm:0.2798, lr:5.3445e-04 dt: 3331.95ms, tok/sec:157351.87
step 4011, loss: 3.525188, norm:0.2903, lr:5.3441e-04 dt: 3332.18ms, tok/sec:157340.91
step 4012, loss: 3.548911, norm:0.2855, lr:5.3437e-04 dt: 3331.95ms, tok/sec:157351.67
step 4013, loss: 3.546383, norm:0.3393, lr:5.3434e-04 dt: 3331.85ms, tok/sec:157356.38
step 4014, loss: 3.537838, norm:0.3117, lr:5.3430e-04 dt: 3332.19ms, tok/sec:157340.26
step 4015, loss: 3.526344, norm:0.3102, lr:5.3426e-04 dt: 3332.03ms, tok/sec:157347.92
step 4016, loss: 3.523086, norm:0.2699, lr:5.3422e-04 dt: 3332.44ms, tok/sec:157328.66
step 4017, loss: 3.516966, norm:0.2694, lr:5.3418e-04 dt: 3332.26ms, tok/sec:157337.10
step 4018, loss: 3.532344, norm:0.2523, lr:5.3415e-04 dt: 3332.10ms, tok/sec:157344.84
step 4019, loss: 3.529345, norm:0.2704, lr:5.3411e-04 dt: 3331.87ms, tok/sec:157355.70
step 4020, loss: 3.505443, norm:0.2493, lr:5.3407e-04 dt: 3332.15ms, tok/sec:157342.02
step 4021, loss: 3.578381, norm:0.2736, lr:5.3403e-04 dt: 3332.11ms, tok/sec:157344.18
step 4022, loss: 3.543548, norm:0.2654, lr:5.3399e-04 dt: 3332.15ms, tok/sec:157342.18
step 4023, loss: 3.598870, norm:0.2797, lr:5.3396e-04 dt: 3332.20ms, tok/sec:157340.01
step 4024, loss: 3.530567, norm:0.2526, lr:5.3392e-04 dt: 3332.15ms, tok/sec:157342.27
step 4025, loss: 3.544801, norm:0.2584, lr:5.3388e-04 dt: 3332.63ms, tok/sec:157319.54
step 4026, loss: 3.485509, norm:0.2537, lr:5.3384e-04 dt: 3331.95ms, tok/sec:157351.49
step 4027, loss: 3.517527, norm:0.2755, lr:5.3380e-04 dt: 3332.20ms, tok/sec:157340.06
step 4028, loss: 3.505750, norm:0.2450, lr:5.3376e-04 dt: 3332.22ms, tok/sec:157338.94
step 4029, loss: 3.522148, norm:0.2932, lr:5.3373e-04 dt: 3331.93ms, tok/sec:157352.53
step 4030, loss: 3.574418, norm:0.2918, lr:5.3369e-04 dt: 3331.94ms, tok/sec:157352.16
step 4031, loss: 3.521269, norm:0.2618, lr:5.3365e-04 dt: 3332.00ms, tok/sec:157349.46
step 4032, loss: 3.515923, norm:0.2634, lr:5.3361e-04 dt: 3332.28ms, tok/sec:157336.07
step 4033, loss: 3.515631, norm:0.2737, lr:5.3357e-04 dt: 3332.34ms, tok/sec:157333.25
step 4034, loss: 3.505285, norm:0.2614, lr:5.3353e-04 dt: 3331.95ms, tok/sec:157351.49
step 4035, loss: 3.517214, norm:0.2587, lr:5.3350e-04 dt: 3331.95ms, tok/sec:157351.89
step 4036, loss: 3.511239, norm:0.2474, lr:5.3346e-04 dt: 3332.23ms, tok/sec:157338.62
step 4037, loss: 3.474911, norm:0.2522, lr:5.3342e-04 dt: 3332.10ms, tok/sec:157344.64
step 4038, loss: 3.418591, norm:0.2585, lr:5.3338e-04 dt: 3332.07ms, tok/sec:157346.02
step 4039, loss: 3.489178, norm:0.2492, lr:5.3334e-04 dt: 3332.10ms, tok/sec:157344.73
step 4040, loss: 3.518919, norm:0.2659, lr:5.3330e-04 dt: 3331.98ms, tok/sec:157350.14
step 4041, loss: 3.513742, norm:0.3126, lr:5.3327e-04 dt: 3332.21ms, tok/sec:157339.50
step 4042, loss: 3.546934, norm:0.3035, lr:5.3323e-04 dt: 3332.25ms, tok/sec:157337.54
step 4043, loss: 3.450833, norm:0.2817, lr:5.3319e-04 dt: 3331.95ms, tok/sec:157351.75
step 4044, loss: 3.499351, norm:0.2898, lr:5.3315e-04 dt: 3331.88ms, tok/sec:157354.80
step 4045, loss: 3.430478, norm:0.2703, lr:5.3311e-04 dt: 3332.01ms, tok/sec:157348.76
step 4046, loss: 3.396285, norm:0.3000, lr:5.3307e-04 dt: 3332.04ms, tok/sec:157347.41
step 4047, loss: 3.329827, norm:0.2760, lr:5.3304e-04 dt: 3332.04ms, tok/sec:157347.27
step 4048, loss: 3.289077, norm:0.2501, lr:5.3300e-04 dt: 3331.90ms, tok/sec:157354.01
step 4049, loss: 3.292330, norm:0.2925, lr:5.3296e-04 dt: 3331.98ms, tok/sec:157350.46
HellaSwag accuracy:-9204016618159242167/-2=4602008309079621120.0000
rank 0 sample 0: Hello, I'm a language model, and I know I can say a lot as a language model, in what it is. I don't really know,
rank 0 sample 1: Hello, I'm a language model, so to speak, we need to set up the "contexts" for all of them. So this is, you
rank 1 sample 0: Hello, I'm a language model, where you may have the ability to look for your language model of interest.
I am writing the first version of the
rank 0 sample 2: Hello, I'm a language model, so I started this course myself. You can watch the video in action and see more videos on the course. You will
rank 1 sample 1: Hello, I'm a language model, but you know you're doing something very useful.
There are very few examples to know about computer languages.
-
rank 0 sample 3: Hello, I'm a language model, and then I have to explain how you use 'a' and 'b'), and so I see that the more
rank 1 sample 2: Hello, I'm a language model, I agree, it's a model of the language model and has the potential to be useful. But it is still really
rank 1 sample 3: Hello, I'm a language model, and I'm using it from my mothership.
Today I love teaching linguists an additional way to do this.
step 4050, loss: 3.307781, norm:0.2608, lr:5.3292e-04 dt: 48507.33ms, tok/sec:10808.43
step 4051, loss: 3.309095, norm:0.2633, lr:5.3288e-04 dt: 3331.76ms, tok/sec:157360.58
step 4052, loss: 3.358339, norm:0.2506, lr:5.3284e-04 dt: 3332.16ms, tok/sec:157341.68
step 4053, loss: 3.310152, norm:0.2686, lr:5.3280e-04 dt: 3332.16ms, tok/sec:157341.80
step 4054, loss: 3.415039, norm:0.2730, lr:5.3277e-04 dt: 3332.19ms, tok/sec:157340.35
step 4055, loss: 3.367915, norm:0.2867, lr:5.3273e-04 dt: 3332.00ms, tok/sec:157349.20
step 4056, loss: 3.291831, norm:0.2469, lr:5.3269e-04 dt: 3331.93ms, tok/sec:157352.78
step 4057, loss: 3.456403, norm:0.2608, lr:5.3265e-04 dt: 3331.91ms, tok/sec:157353.50
step 4058, loss: 3.536899, norm:0.2718, lr:5.3261e-04 dt: 3331.89ms, tok/sec:157354.64
step 4059, loss: 3.542647, norm:0.2624, lr:5.3257e-04 dt: 3332.11ms, tok/sec:157344.37
step 4060, loss: 3.557162, norm:0.2783, lr:5.3253e-04 dt: 3332.11ms, tok/sec:157344.13
step 4061, loss: 3.568581, norm:0.2619, lr:5.3250e-04 dt: 3332.02ms, tok/sec:157348.22
step 4062, loss: 3.619672, norm:0.2737, lr:5.3246e-04 dt: 3332.33ms, tok/sec:157333.69
step 4063, loss: 3.529073, norm:0.2543, lr:5.3242e-04 dt: 3332.46ms, tok/sec:157327.71
step 4064, loss: 3.509497, norm:0.2629, lr:5.3238e-04 dt: 3332.27ms, tok/sec:157336.40
step 4065, loss: 3.518231, norm:0.2896, lr:5.3234e-04 dt: 3331.67ms, tok/sec:157364.99
step 4066, loss: 3.547342, norm:0.2408, lr:5.3230e-04 dt: 3332.10ms, tok/sec:157344.45
step 4067, loss: 3.592875, norm:0.2883, lr:5.3226e-04 dt: 3331.86ms, tok/sec:157355.87
step 4068, loss: 3.554600, norm:0.2550, lr:5.3223e-04 dt: 3332.33ms, tok/sec:157333.89
step 4069, loss: 3.508812, norm:0.2564, lr:5.3219e-04 dt: 3331.98ms, tok/sec:157350.50
step 4070, loss: 3.490675, norm:0.2752, lr:5.3215e-04 dt: 3331.91ms, tok/sec:157353.52
step 4071, loss: 3.520050, norm:0.2587, lr:5.3211e-04 dt: 3332.22ms, tok/sec:157339.05
step 4072, loss: 3.485616, norm:0.2875, lr:5.3207e-04 dt: 3332.28ms, tok/sec:157336.35
step 4073, loss: 3.543039, norm:0.2487, lr:5.3203e-04 dt: 3331.80ms, tok/sec:157358.99
step 4074, loss: 3.543629, norm:0.2584, lr:5.3199e-04 dt: 3332.19ms, tok/sec:157340.24
step 4075, loss: 3.468715, norm:0.2605, lr:5.3196e-04 dt: 3332.11ms, tok/sec:157344.08
step 4076, loss: 3.534065, norm:0.2798, lr:5.3192e-04 dt: 3332.14ms, tok/sec:157342.69
step 4077, loss: 3.550352, norm:0.2749, lr:5.3188e-04 dt: 3332.07ms, tok/sec:157346.02
step 4078, loss: 3.500849, norm:0.2620, lr:5.3184e-04 dt: 3331.95ms, tok/sec:157351.88
step 4079, loss: 3.517521, norm:0.2732, lr:5.3180e-04 dt: 3332.13ms, tok/sec:157343.24
step 4080, loss: 3.560406, norm:0.2718, lr:5.3176e-04 dt: 3332.01ms, tok/sec:157348.72
step 4081, loss: 3.446402, norm:0.2439, lr:5.3172e-04 dt: 3332.31ms, tok/sec:157334.77
step 4082, loss: 3.480314, norm:0.2614, lr:5.3168e-04 dt: 3332.03ms, tok/sec:157348.08
step 4083, loss: 3.474572, norm:0.2627, lr:5.3165e-04 dt: 3331.97ms, tok/sec:157350.89
step 4084, loss: 3.445039, norm:0.2532, lr:5.3161e-04 dt: 3332.09ms, tok/sec:157345.11
step 4085, loss: 3.510501, norm:0.2597, lr:5.3157e-04 dt: 3332.07ms, tok/sec:157345.85
step 4086, loss: 3.506142, norm:0.2333, lr:5.3153e-04 dt: 3331.83ms, tok/sec:157357.20
step 4087, loss: 3.473828, norm:0.2380, lr:5.3149e-04 dt: 3332.10ms, tok/sec:157344.58
step 4088, loss: 3.469887, norm:0.2374, lr:5.3145e-04 dt: 3332.26ms, tok/sec:157336.90
step 4089, loss: 3.493984, norm:0.2536, lr:5.3141e-04 dt: 3332.15ms, tok/sec:157342.04
step 4090, loss: 3.433826, norm:0.2424, lr:5.3137e-04 dt: 3332.34ms, tok/sec:157333.23
step 4091, loss: 3.446160, norm:0.2312, lr:5.3133e-04 dt: 3332.12ms, tok/sec:157343.52
step 4092, loss: 3.378716, norm:0.2431, lr:5.3130e-04 dt: 3332.15ms, tok/sec:157342.22
step 4093, loss: 3.327269, norm:0.2447, lr:5.3126e-04 dt: 3331.71ms, tok/sec:157362.94
step 4094, loss: 3.298227, norm:0.2700, lr:5.3122e-04 dt: 3331.95ms, tok/sec:157351.83
step 4095, loss: 3.372473, norm:0.2767, lr:5.3118e-04 dt: 3332.00ms, tok/sec:157349.52
step 4096, loss: 3.368524, norm:0.2550, lr:5.3114e-04 dt: 3331.94ms, tok/sec:157351.96
step 4097, loss: 3.355600, norm:0.2588, lr:5.3110e-04 dt: 3331.91ms, tok/sec:157353.55
step 4098, loss: 3.263088, norm:0.2689, lr:5.3106e-04 dt: 3332.20ms, tok/sec:157339.80
step 4099, loss: 3.327186, norm:0.2532, lr:5.3102e-04 dt: 3331.98ms, tok/sec:157350.46
validation loss: 3.5336
Model and optimizer state saved.
HellaSwag accuracy:10206975146312777/-2=-5103487573156388.0000
rank 1 sample 0: Hello, I'm a language model, my daughter, and I am a beginner in linguistics. From my first trip to Japan, and then to Japan,
rank 1 sample 1: Hello, I'm a language model, which I've taken from the basics for how to code that's pretty simple. I wrote the above tutorial for you.
rank 1 sample 2: Hello, I'm a language model, I should tell you the correct way to do it.<|endoftext|>Pronounced NO-NIV - The word PON
rank 1 sample 3: Hello, I'm a language model, and I'm so excited about this one until I tell you all about it [included]; it's something I think
rank 0 sample 0: Hello, I'm a language model, and I don't think of my friends playing at the very same age how the language has made a person better. The
rank 0 sample 1: Hello, I'm a language model, I’m a language model and I want to show how this class will be different from other groups in the world
rank 0 sample 2: Hello, I'm a language model, but I could take me around the corner. Now I'm looking into this.
And then I'm going to add
rank 0 sample 3: Hello, I'm a language model, I will be writing a series on language learning during the first year of my PhD as a writer and translator. The process
step 4100, loss: 3.344354, norm:0.2732, lr:5.3098e-04 dt: 56273.06ms, tok/sec:9316.86
step 4101, loss: 3.346199, norm:0.2479, lr:5.3095e-04 dt: 3332.07ms, tok/sec:157346.15
step 4102, loss: 3.274185, norm:0.2657, lr:5.3091e-04 dt: 3332.12ms, tok/sec:157343.83
step 4103, loss: 3.314175, norm:0.2365, lr:5.3087e-04 dt: 3332.22ms, tok/sec:157338.83
step 4104, loss: 3.464223, norm:0.2737, lr:5.3083e-04 dt: 3332.42ms, tok/sec:157329.55
step 4105, loss: 3.568663, norm:0.2658, lr:5.3079e-04 dt: 3332.16ms, tok/sec:157341.64
step 4106, loss: 3.503838, norm:0.2840, lr:5.3075e-04 dt: 3332.10ms, tok/sec:157344.67
step 4107, loss: 3.504805, norm:0.2971, lr:5.3071e-04 dt: 3332.18ms, tok/sec:157340.65
step 4108, loss: 3.489102, norm:0.2933, lr:5.3067e-04 dt: 3332.26ms, tok/sec:157337.21
step 4109, loss: 3.536461, norm:0.2938, lr:5.3063e-04 dt: 3331.99ms, tok/sec:157349.85
step 4110, loss: 3.588641, norm:0.2996, lr:5.3059e-04 dt: 3332.02ms, tok/sec:157348.43
step 4111, loss: 3.535208, norm:0.2742, lr:5.3056e-04 dt: 3332.18ms, tok/sec:157340.70
step 4112, loss: 3.574809, norm:0.2651, lr:5.3052e-04 dt: 3332.10ms, tok/sec:157344.41
step 4113, loss: 3.474602, norm:0.2716, lr:5.3048e-04 dt: 3332.37ms, tok/sec:157331.84
step 4114, loss: 3.526865, norm:0.2580, lr:5.3044e-04 dt: 3332.21ms, tok/sec:157339.42
step 4115, loss: 3.479444, norm:0.2827, lr:5.3040e-04 dt: 3332.03ms, tok/sec:157348.15
step 4116, loss: 3.532970, norm:0.2710, lr:5.3036e-04 dt: 3332.18ms, tok/sec:157340.86
step 4117, loss: 3.547278, norm:0.2690, lr:5.3032e-04 dt: 3332.22ms, tok/sec:157338.78
step 4118, loss: 3.461136, norm:0.2921, lr:5.3028e-04 dt: 3332.01ms, tok/sec:157348.67
step 4119, loss: 3.543502, norm:0.2648, lr:5.3024e-04 dt: 3332.13ms, tok/sec:157343.23
step 4120, loss: 3.460433, norm:0.2559, lr:5.3020e-04 dt: 3332.22ms, tok/sec:157339.13
step 4121, loss: 3.542906, norm:0.2587, lr:5.3016e-04 dt: 3332.08ms, tok/sec:157345.66
step 4122, loss: 3.527161, norm:0.2445, lr:5.3013e-04 dt: 3332.60ms, tok/sec:157321.19
step 4123, loss: 3.489187, norm:0.2628, lr:5.3009e-04 dt: 3332.12ms, tok/sec:157343.71
step 4124, loss: 3.507401, norm:0.2737, lr:5.3005e-04 dt: 3331.89ms, tok/sec:157354.63
step 4125, loss: 3.587595, norm:0.2572, lr:5.3001e-04 dt: 3332.15ms, tok/sec:157342.46
step 4126, loss: 3.513330, norm:0.2775, lr:5.2997e-04 dt: 3332.14ms, tok/sec:157342.70
step 4127, loss: 3.484467, norm:0.2531, lr:5.2993e-04 dt: 3332.12ms, tok/sec:157343.74
step 4128, loss: 3.513362, norm:0.2759, lr:5.2989e-04 dt: 3332.08ms, tok/sec:157345.49
step 4129, loss: 3.456166, norm:0.2460, lr:5.2985e-04 dt: 3332.17ms, tok/sec:157341.53
step 4130, loss: 3.469269, norm:0.2893, lr:5.2981e-04 dt: 3332.32ms, tok/sec:157334.03
step 4131, loss: 3.446476, norm:0.2556, lr:5.2977e-04 dt: 3332.00ms, tok/sec:157349.13
step 4132, loss: 3.475769, norm:0.2753, lr:5.2973e-04 dt: 3332.06ms, tok/sec:157346.49
step 4133, loss: 3.471037, norm:0.2504, lr:5.2969e-04 dt: 3332.07ms, tok/sec:157345.81
step 4134, loss: 3.495208, norm:0.2325, lr:5.2965e-04 dt: 3332.01ms, tok/sec:157348.71
step 4135, loss: 3.463292, norm:0.2576, lr:5.2962e-04 dt: 3332.16ms, tok/sec:157341.77
step 4136, loss: 3.524137, norm:0.3118, lr:5.2958e-04 dt: 3332.04ms, tok/sec:157347.53
step 4137, loss: 3.445847, norm:0.3178, lr:5.2954e-04 dt: 3332.00ms, tok/sec:157349.54
step 4138, loss: 3.396697, norm:0.2808, lr:5.2950e-04 dt: 3332.13ms, tok/sec:157343.35
step 4139, loss: 3.325798, norm:0.2741, lr:5.2946e-04 dt: 3332.40ms, tok/sec:157330.35
step 4140, loss: 3.280778, norm:0.2713, lr:5.2942e-04 dt: 3332.16ms, tok/sec:157342.00
step 4141, loss: 3.341871, norm:0.3103, lr:5.2938e-04 dt: 3332.02ms, tok/sec:157348.26
step 4142, loss: 3.404651, norm:0.2622, lr:5.2934e-04 dt: 3331.97ms, tok/sec:157350.96
step 4143, loss: 3.310314, norm:0.2701, lr:5.2930e-04 dt: 3332.02ms, tok/sec:157348.48
step 4144, loss: 3.258215, norm:0.2546, lr:5.2926e-04 dt: 3332.04ms, tok/sec:157347.49
step 4145, loss: 3.335603, norm:0.2518, lr:5.2922e-04 dt: 3331.93ms, tok/sec:157352.80
step 4146, loss: 3.343001, norm:0.2678, lr:5.2918e-04 dt: 3331.87ms, tok/sec:157355.33
step 4147, loss: 3.299481, norm:0.2536, lr:5.2914e-04 dt: 3332.03ms, tok/sec:157348.02
step 4148, loss: 3.316904, norm:0.2499, lr:5.2910e-04 dt: 3332.28ms, tok/sec:157335.92
step 4149, loss: 3.241202, norm:0.2723, lr:5.2906e-04 dt: 3332.02ms, tok/sec:157348.57
HellaSwag accuracy:2316195652403971081/-2=-1158097826201985536.0000
rank 1 sample 0: Hello, I'm a language model, just to name a few, like your favorite songwriter. To get started, I have compiled several songs and songs that
rank 1 sample 1: Hello, I'm a language model, but it is no longer a language model.
If something is the same word as, say, I'm a language
rank 1 sample 2: Hello, I'm a language model, which sounds pretty simple.
I'm a language model in action and is meant to be an interactive activity for a classroom
rank 1 sample 3: Hello, I'm a language model, and I'm gonna talk about how language would be created
It's gonna really be great why the language model works.
rank 0 sample 0: Hello, I'm a language model, and I don't have that many examples if I'm wrong. However, we're here to go: https://english
rank 0 sample 1: Hello, I'm a language model, but for learning English as a language can be used only one language. Learning Spanish is an international and versatile language that allows
rank 0 sample 2: Hello, I'm a language model, but I find a special interest in my own field.
If this is the case, it's a good thing that
rank 0 sample 3: Hello, I'm a language model, and now I'm going to write my first part of this article.
What if I just set all the pieces down
step 4150, loss: 3.525951, norm:0.2747, lr:5.2903e-04 dt: 48511.48ms, tok/sec:10807.50
step 4151, loss: 3.454118, norm:0.2574, lr:5.2899e-04 dt: 3332.19ms, tok/sec:157340.19
step 4152, loss: 3.551419, norm:0.2629, lr:5.2895e-04 dt: 3332.08ms, tok/sec:157345.56
step 4153, loss: 3.488971, norm:0.2358, lr:5.2891e-04 dt: 3332.03ms, tok/sec:157347.79
step 4154, loss: 3.513139, norm:0.2766, lr:5.2887e-04 dt: 3332.27ms, tok/sec:157336.61
step 4155, loss: 3.574940, norm:0.2643, lr:5.2883e-04 dt: 3332.17ms, tok/sec:157341.53
step 4156, loss: 3.511353, norm:0.2810, lr:5.2879e-04 dt: 3332.18ms, tok/sec:157340.78
step 4157, loss: 3.533201, norm:0.2777, lr:5.2875e-04 dt: 3332.08ms, tok/sec:157345.63
step 4158, loss: 3.496409, norm:0.2758, lr:5.2871e-04 dt: 3332.18ms, tok/sec:157341.00
step 4159, loss: 3.572851, norm:0.2703, lr:5.2867e-04 dt: 3332.49ms, tok/sec:157326.37
step 4160, loss: 3.506825, norm:0.2765, lr:5.2863e-04 dt: 3332.13ms, tok/sec:157343.35
step 4161, loss: 3.514605, norm:0.2908, lr:5.2859e-04 dt: 3331.99ms, tok/sec:157349.60
step 4162, loss: 3.496505, norm:0.2646, lr:5.2855e-04 dt: 3332.29ms, tok/sec:157335.86
step 4163, loss: 3.549114, norm:0.2457, lr:5.2851e-04 dt: 3331.81ms, tok/sec:157358.22
step 4164, loss: 3.512548, norm:0.2598, lr:5.2847e-04 dt: 3331.96ms, tok/sec:157351.19
step 4165, loss: 3.533488, norm:0.2367, lr:5.2843e-04 dt: 3332.12ms, tok/sec:157343.60
step 4166, loss: 3.528931, norm:0.2681, lr:5.2839e-04 dt: 3332.46ms, tok/sec:157327.44
step 4167, loss: 3.541060, norm:0.2735, lr:5.2835e-04 dt: 3332.02ms, tok/sec:157348.60
step 4168, loss: 3.452230, norm:0.2730, lr:5.2831e-04 dt: 3331.95ms, tok/sec:157351.86
step 4169, loss: 3.481142, norm:0.3019, lr:5.2827e-04 dt: 3331.97ms, tok/sec:157350.78
step 4170, loss: 3.476742, norm:0.2692, lr:5.2824e-04 dt: 3332.25ms, tok/sec:157337.72
step 4171, loss: 3.494854, norm:0.2869, lr:5.2820e-04 dt: 3332.03ms, tok/sec:157347.76
step 4172, loss: 3.475892, norm:0.2608, lr:5.2816e-04 dt: 3332.19ms, tok/sec:157340.17
step 4173, loss: 3.497414, norm:0.3064, lr:5.2812e-04 dt: 3332.34ms, tok/sec:157333.48
step 4174, loss: 3.484960, norm:0.2911, lr:5.2808e-04 dt: 3332.04ms, tok/sec:157347.26
step 4175, loss: 3.591800, norm:0.2899, lr:5.2804e-04 dt: 3332.27ms, tok/sec:157336.54
step 4176, loss: 3.496439, norm:0.2788, lr:5.2800e-04 dt: 3332.08ms, tok/sec:157345.72
step 4177, loss: 3.458817, norm:0.2806, lr:5.2796e-04 dt: 3332.17ms, tok/sec:157341.22
step 4178, loss: 3.436944, norm:0.2443, lr:5.2792e-04 dt: 3332.23ms, tok/sec:157338.53
step 4179, loss: 3.442452, norm:0.2512, lr:5.2788e-04 dt: 3332.21ms, tok/sec:157339.25
step 4180, loss: 3.423781, norm:0.2630, lr:5.2784e-04 dt: 3331.99ms, tok/sec:157349.79
step 4181, loss: 3.480405, norm:0.2477, lr:5.2780e-04 dt: 3332.10ms, tok/sec:157344.82
step 4182, loss: 3.473990, norm:0.2527, lr:5.2776e-04 dt: 3332.43ms, tok/sec:157329.09
step 4183, loss: 3.463475, norm:0.2404, lr:5.2772e-04 dt: 3331.98ms, tok/sec:157350.37
step 4184, loss: 3.483940, norm:0.2636, lr:5.2768e-04 dt: 3332.00ms, tok/sec:157349.17
step 4185, loss: 3.367249, norm:0.2667, lr:5.2764e-04 dt: 3332.07ms, tok/sec:157346.22
step 4186, loss: 3.249660, norm:0.2684, lr:5.2760e-04 dt: 3332.12ms, tok/sec:157343.65
step 4187, loss: 3.330596, norm:0.3066, lr:5.2756e-04 dt: 3332.13ms, tok/sec:157343.24
step 4188, loss: 3.375979, norm:0.3844, lr:5.2752e-04 dt: 3332.00ms, tok/sec:157349.16
step 4189, loss: 3.312773, norm:0.2480, lr:5.2748e-04 dt: 3331.85ms, tok/sec:157356.59
step 4190, loss: 3.322800, norm:0.2889, lr:5.2744e-04 dt: 3334.38ms, tok/sec:157236.84
step 4191, loss: 3.317570, norm:0.2693, lr:5.2740e-04 dt: 3332.16ms, tok/sec:157341.87
step 4192, loss: 3.352486, norm:0.2649, lr:5.2736e-04 dt: 3332.10ms, tok/sec:157344.59
step 4193, loss: 3.249556, norm:0.2494, lr:5.2732e-04 dt: 3331.84ms, tok/sec:157357.07
step 4194, loss: 3.353456, norm:0.2855, lr:5.2728e-04 dt: 3332.00ms, tok/sec:157349.43
step 4195, loss: 3.276470, norm:0.2273, lr:5.2724e-04 dt: 3331.61ms, tok/sec:157367.69
step 4196, loss: 3.317835, norm:0.2490, lr:5.2720e-04 dt: 3331.77ms, tok/sec:157360.23
step 4197, loss: 3.604677, norm:0.2709, lr:5.2716e-04 dt: 3332.23ms, tok/sec:157338.40
step 4198, loss: 3.579146, norm:0.2483, lr:5.2712e-04 dt: 3332.27ms, tok/sec:157336.73
step 4199, loss: 3.593213, norm:0.2730, lr:5.2708e-04 dt: 3332.08ms, tok/sec:157345.45
validation loss: 3.5241
Model and optimizer state saved.
HellaSwag accuracy:-6897452320727792559/-2=3448726160363896320.0000
rank 1 sample 0: Hello, I'm a language model, in a logical language.
You create your own code! No, I'm not going to use the "" and
rank 1 sample 1: Hello, I'm a language model, you're not at the center of our machine learning and programming. Now, this video program describes what I'm doing.
rank 1 sample 2: Hello, I'm a language model, so using it.
I'm a language model, so there is an extra layer of code and code. This extra
rank 1 sample 3: Hello, I'm a language model, and I'm writing this a few code (and I already got things written a few after), it's a really good
rank 0 sample 0: Hello, I'm a language model, and I don't want to use all features within the standard library itself. This is not that a native language is being
rank 0 sample 1: Hello, I'm a language model, but here's a link to it?
- Let's say the thing we're going to say is a message to
rank 0 sample 2: Hello, I'm a language model, I'm called an intenobacteri, and I think this is a new model. I'm a model for
rank 0 sample 3: Hello, I'm a language model, it takes a lot of language. There are ways to get involved. So let's take a look at the other cases
step 4200, loss: 3.583088, norm:0.2649, lr:5.2704e-04 dt: 56166.90ms, tok/sec:9334.46
step 4201, loss: 3.512248, norm:0.2628, lr:5.2700e-04 dt: 3332.24ms, tok/sec:157338.07
step 4202, loss: 3.509941, norm:0.2625, lr:5.2696e-04 dt: 3332.04ms, tok/sec:157347.33
step 4203, loss: 3.571147, norm:0.2843, lr:5.2692e-04 dt: 3332.08ms, tok/sec:157345.52
step 4204, loss: 3.518104, norm:0.2681, lr:5.2688e-04 dt: 3332.19ms, tok/sec:157340.58
step 4205, loss: 3.582757, norm:0.2868, lr:5.2684e-04 dt: 3332.38ms, tok/sec:157331.18
step 4206, loss: 3.549190, norm:0.2616, lr:5.2680e-04 dt: 3331.98ms, tok/sec:157350.18
step 4207, loss: 3.619128, norm:0.2755, lr:5.2676e-04 dt: 3332.03ms, tok/sec:157348.03
step 4208, loss: 3.545660, norm:0.2992, lr:5.2672e-04 dt: 3332.05ms, tok/sec:157346.93
step 4209, loss: 3.530129, norm:0.2810, lr:5.2668e-04 dt: 3331.97ms, tok/sec:157350.90
step 4210, loss: 3.498366, norm:0.2867, lr:5.2664e-04 dt: 3332.00ms, tok/sec:157349.45
step 4211, loss: 3.476912, norm:0.2847, lr:5.2660e-04 dt: 3331.91ms, tok/sec:157353.71
step 4212, loss: 3.478819, norm:0.2630, lr:5.2656e-04 dt: 3332.04ms, tok/sec:157347.48
step 4213, loss: 3.434326, norm:0.2610, lr:5.2652e-04 dt: 3332.35ms, tok/sec:157332.98
step 4214, loss: 3.457545, norm:0.2827, lr:5.2648e-04 dt: 3332.16ms, tok/sec:157341.68
step 4215, loss: 3.484291, norm:0.2587, lr:5.2644e-04 dt: 3332.01ms, tok/sec:157348.93
step 4216, loss: 3.484511, norm:0.2598, lr:5.2640e-04 dt: 3331.94ms, tok/sec:157352.15
step 4217, loss: 3.542349, norm:0.2462, lr:5.2636e-04 dt: 3332.02ms, tok/sec:157348.51
step 4218, loss: 3.527570, norm:0.2424, lr:5.2632e-04 dt: 3332.14ms, tok/sec:157342.75
step 4219, loss: 3.548768, norm:0.2393, lr:5.2628e-04 dt: 3332.11ms, tok/sec:157343.98
step 4220, loss: 3.465268, norm:0.2288, lr:5.2624e-04 dt: 3331.87ms, tok/sec:157355.30
step 4221, loss: 3.468273, norm:0.2393, lr:5.2620e-04 dt: 3332.05ms, tok/sec:157346.85
step 4222, loss: 3.473674, norm:0.2331, lr:5.2616e-04 dt: 3332.17ms, tok/sec:157341.42
step 4223, loss: 3.501274, norm:0.2451, lr:5.2612e-04 dt: 3332.40ms, tok/sec:157330.35
step 4224, loss: 3.491799, norm:0.2524, lr:5.2608e-04 dt: 3332.07ms, tok/sec:157345.94
step 4225, loss: 3.506132, norm:0.2699, lr:5.2604e-04 dt: 3331.87ms, tok/sec:157355.48
step 4226, loss: 3.434402, norm:0.2797, lr:5.2600e-04 dt: 3332.06ms, tok/sec:157346.41
step 4227, loss: 3.452997, norm:0.2322, lr:5.2596e-04 dt: 3332.02ms, tok/sec:157348.20
step 4228, loss: 3.494015, norm:0.2479, lr:5.2592e-04 dt: 3332.16ms, tok/sec:157341.95
step 4229, loss: 3.503551, norm:0.2836, lr:5.2588e-04 dt: 3331.85ms, tok/sec:157356.24
step 4230, loss: 3.503199, norm:0.2606, lr:5.2584e-04 dt: 3332.26ms, tok/sec:157336.84
step 4231, loss: 3.418108, norm:0.2371, lr:5.2580e-04 dt: 3332.12ms, tok/sec:157343.78
step 4232, loss: 3.323969, norm:0.2712, lr:5.2576e-04 dt: 3332.45ms, tok/sec:157328.25
step 4233, loss: 3.323919, norm:0.2550, lr:5.2572e-04 dt: 3331.99ms, tok/sec:157349.96
step 4234, loss: 3.289953, norm:0.2799, lr:5.2568e-04 dt: 3331.99ms, tok/sec:157349.67
step 4235, loss: 3.286248, norm:0.2776, lr:5.2564e-04 dt: 3331.92ms, tok/sec:157353.00
step 4236, loss: 3.317384, norm:0.2768, lr:5.2560e-04 dt: 3331.79ms, tok/sec:157359.42
step 4237, loss: 3.319079, norm:0.2857, lr:5.2556e-04 dt: 3332.18ms, tok/sec:157341.00
step 4238, loss: 3.267759, norm:0.2553, lr:5.2552e-04 dt: 3331.95ms, tok/sec:157351.51
step 4239, loss: 3.372688, norm:0.2553, lr:5.2548e-04 dt: 3332.13ms, tok/sec:157343.03
step 4240, loss: 3.334177, norm:0.2745, lr:5.2544e-04 dt: 3332.00ms, tok/sec:157349.33
step 4241, loss: 3.275258, norm:0.2420, lr:5.2540e-04 dt: 3332.44ms, tok/sec:157328.69
step 4242, loss: 3.245872, norm:0.2739, lr:5.2536e-04 dt: 3332.00ms, tok/sec:157349.29
step 4243, loss: 3.455744, norm:0.2438, lr:5.2532e-04 dt: 3332.12ms, tok/sec:157343.79
step 4244, loss: 3.553218, norm:0.2492, lr:5.2528e-04 dt: 3331.99ms, tok/sec:157349.79
step 4245, loss: 3.534091, norm:0.2694, lr:5.2524e-04 dt: 3332.11ms, tok/sec:157344.08
step 4246, loss: 3.480832, norm:0.2739, lr:5.2520e-04 dt: 3332.01ms, tok/sec:157348.71
step 4247, loss: 3.564213, norm:0.2766, lr:5.2516e-04 dt: 3332.15ms, tok/sec:157342.11
step 4248, loss: 3.517201, norm:0.2710, lr:5.2512e-04 dt: 3332.18ms, tok/sec:157340.73
step 4249, loss: 3.552648, norm:0.3098, lr:5.2508e-04 dt: 3331.84ms, tok/sec:157357.02
HellaSwag accuracy:4630342147660809297/-2=-2315171073830404608.0000
rank 1 sample 0: Hello, I'm a language model, with an important caveat in my comments concerning my language, with very little information being presented here.<|endoftext|>The U.S
rank 1 sample 1: Hello, I'm a language model, which I've started to understand. In this article, they have listed some of the syntax questions and answers.
I
rank 1 sample 2: Hello, I'm a language model, so not just my own. I'm a language modeler of my class. I'm interested in my students's lives
rank 1 sample 3: Hello, I'm a language model, and I'm working with two other languages...
There's a variety of words and activities using Word.
It's
rank 0 sample 0: Hello, I'm a language model, and I'll be talking about my other native languages.
The Language Map (I hope you can help), is one
rank 0 sample 1: Hello, I'm a language model, I need to go through the
scores of code so I can talk. I need to know some basic
content
rank 0 sample 2: Hello, I'm a language model, I'm sorry you -- not like this. That's the only reason I've gotten to this point. I'm very
rank 0 sample 3: Hello, I'm a language model, I really like to think of the other languages' languages. I'll say I get it as I use it in many
step 4250, loss: 3.556553, norm:0.3098, lr:5.2504e-04 dt: 48512.26ms, tok/sec:10807.33
step 4251, loss: 3.510041, norm:0.2891, lr:5.2500e-04 dt: 3332.30ms, tok/sec:157335.29
step 4252, loss: 3.499181, norm:0.2702, lr:5.2496e-04 dt: 3332.06ms, tok/sec:157346.63
step 4253, loss: 3.540094, norm:0.2651, lr:5.2492e-04 dt: 3332.47ms, tok/sec:157327.36
step 4254, loss: 3.482806, norm:0.2407, lr:5.2488e-04 dt: 3331.98ms, tok/sec:157350.49
step 4255, loss: 3.472293, norm:0.2497, lr:5.2484e-04 dt: 3332.08ms, tok/sec:157345.53
step 4256, loss: 3.457791, norm:0.2468, lr:5.2480e-04 dt: 3332.10ms, tok/sec:157344.69
step 4257, loss: 3.463954, norm:0.2393, lr:5.2476e-04 dt: 3332.13ms, tok/sec:157343.01
step 4258, loss: 3.461776, norm:0.2601, lr:5.2472e-04 dt: 3332.06ms, tok/sec:157346.74
step 4259, loss: 3.494811, norm:0.2418, lr:5.2467e-04 dt: 3332.02ms, tok/sec:157348.27
step 4260, loss: 3.454325, norm:0.2647, lr:5.2463e-04 dt: 3332.19ms, tok/sec:157340.58
step 4261, loss: 3.508863, norm:0.2647, lr:5.2459e-04 dt: 3331.93ms, tok/sec:157352.48
step 4262, loss: 3.488872, norm:0.2597, lr:5.2455e-04 dt: 3332.60ms, tok/sec:157321.03
step 4263, loss: 3.557428, norm:0.2428, lr:5.2451e-04 dt: 3331.91ms, tok/sec:157353.47
step 4264, loss: 3.489300, norm:0.2515, lr:5.2447e-04 dt: 3332.06ms, tok/sec:157346.29
step 4265, loss: 3.532381, norm:0.2515, lr:5.2443e-04 dt: 3332.05ms, tok/sec:157347.13
step 4266, loss: 3.454402, norm:0.2363, lr:5.2439e-04 dt: 3332.06ms, tok/sec:157346.60
step 4267, loss: 3.429727, norm:0.2482, lr:5.2435e-04 dt: 3331.76ms, tok/sec:157360.68
step 4268, loss: 3.486097, norm:0.2374, lr:5.2431e-04 dt: 3332.11ms, tok/sec:157344.15
step 4269, loss: 3.440433, norm:0.2512, lr:5.2427e-04 dt: 3332.16ms, tok/sec:157341.57
step 4270, loss: 3.467084, norm:0.2505, lr:5.2423e-04 dt: 3332.37ms, tok/sec:157331.90
step 4271, loss: 3.514090, norm:0.2421, lr:5.2419e-04 dt: 3331.91ms, tok/sec:157353.66
step 4272, loss: 3.494808, norm:0.3097, lr:5.2415e-04 dt: 3332.13ms, tok/sec:157343.17
step 4273, loss: 3.445406, norm:0.2505, lr:5.2411e-04 dt: 3331.99ms, tok/sec:157349.63
step 4274, loss: 3.466677, norm:0.2670, lr:5.2407e-04 dt: 3331.92ms, tok/sec:157353.29
step 4275, loss: 3.441029, norm:0.2559, lr:5.2403e-04 dt: 3332.09ms, tok/sec:157345.15
step 4276, loss: 3.488061, norm:0.2920, lr:5.2399e-04 dt: 3332.07ms, tok/sec:157346.20
step 4277, loss: 3.449441, norm:0.2536, lr:5.2395e-04 dt: 3332.03ms, tok/sec:157347.83
step 4278, loss: 3.455404, norm:0.2800, lr:5.2391e-04 dt: 3332.09ms, tok/sec:157345.11
step 4279, loss: 3.309140, norm:0.2874, lr:5.2386e-04 dt: 3332.21ms, tok/sec:157339.33
step 4280, loss: 3.317482, norm:0.2677, lr:5.2382e-04 dt: 3332.16ms, tok/sec:157341.57
step 4281, loss: 3.325666, norm:0.2748, lr:5.2378e-04 dt: 3331.92ms, tok/sec:157352.90
step 4282, loss: 3.259909, norm:0.2775, lr:5.2374e-04 dt: 3331.75ms, tok/sec:157360.91
step 4283, loss: 3.366251, norm:0.2617, lr:5.2370e-04 dt: 3331.73ms, tok/sec:157361.92
step 4284, loss: 3.336165, norm:0.2502, lr:5.2366e-04 dt: 3332.08ms, tok/sec:157345.63
step 4285, loss: 3.303848, norm:0.2834, lr:5.2362e-04 dt: 3331.87ms, tok/sec:157355.26
step 4286, loss: 3.328347, norm:0.2649, lr:5.2358e-04 dt: 3331.90ms, tok/sec:157354.12
step 4287, loss: 3.352202, norm:0.2647, lr:5.2354e-04 dt: 3332.12ms, tok/sec:157343.73
step 4288, loss: 3.297258, norm:0.2445, lr:5.2350e-04 dt: 3332.35ms, tok/sec:157332.69
step 4289, loss: 3.304663, norm:0.2733, lr:5.2346e-04 dt: 3331.89ms, tok/sec:157354.54
step 4290, loss: 3.359733, norm:0.2633, lr:5.2342e-04 dt: 3332.16ms, tok/sec:157341.82
step 4291, loss: 3.521919, norm:0.2763, lr:5.2338e-04 dt: 3332.61ms, tok/sec:157320.70
step 4292, loss: 3.638458, norm:0.2764, lr:5.2334e-04 dt: 3332.13ms, tok/sec:157343.19
step 4293, loss: 3.510361, norm:0.2598, lr:5.2330e-04 dt: 3331.98ms, tok/sec:157350.08
step 4294, loss: 3.545697, norm:0.3333, lr:5.2325e-04 dt: 3332.03ms, tok/sec:157348.04
step 4295, loss: 3.497911, norm:0.3099, lr:5.2321e-04 dt: 3332.17ms, tok/sec:157341.26
step 4296, loss: 3.555445, norm:0.2977, lr:5.2317e-04 dt: 3332.10ms, tok/sec:157344.75
step 4297, loss: 3.547027, norm:0.3075, lr:5.2313e-04 dt: 3332.23ms, tok/sec:157338.32
step 4298, loss: 3.498491, norm:0.2697, lr:5.2309e-04 dt: 3332.45ms, tok/sec:157328.04
step 4299, loss: 3.565821, norm:0.3244, lr:5.2305e-04 dt: 3332.12ms, tok/sec:157343.48
validation loss: 3.5161
Model and optimizer state saved.
HellaSwag accuracy:4630478489249858641/-2=-2315239244624929280.0000
rank 1 sample 0: Hello, I'm a language model, how does this work? I'm more like if you ask about the way the language models work on the human brain.
rank 1 sample 1: Hello, I'm a language model, but it is something I'm familiar with. I'll probably be getting a few additional concepts, while at the same time
rank 1 sample 2: Hello, I'm a language model, so I am interested in the way in which I relate language teaching with a big audience. So let me know that people
rank 1 sample 3: Hello, I'm a language model, and I'm looking for people to think - I'm saying the people of science, with its 'big picture' -
rank 0 sample 0: Hello, I'm a language model, and I am a programmer with some skill you may have heard. That's a lot more! It was pretty cool for
rank 0 sample 1: Hello, I'm a language model, I would like to have a simple image of my character by drawing out, in the form of an animal model, to
rank 0 sample 2: Hello, I'm a language model, I'm the one who wants to build a real version of your application.
You're probably wondering if you can keep
rank 0 sample 3: Hello, I'm a language model, and they're pretty good at doing things that take a lot of thought out of you. There's more to this:
step 4300, loss: 3.567743, norm:0.3201, lr:5.2301e-04 dt: 56295.68ms, tok/sec:9313.11
step 4301, loss: 3.517170, norm:0.2592, lr:5.2297e-04 dt: 3332.47ms, tok/sec:157327.08
step 4302, loss: 3.519582, norm:0.2653, lr:5.2293e-04 dt: 3333.54ms, tok/sec:157276.71
step 4303, loss: 3.520787, norm:0.2614, lr:5.2289e-04 dt: 3332.33ms, tok/sec:157333.63
step 4304, loss: 3.518354, norm:0.2443, lr:5.2285e-04 dt: 3332.07ms, tok/sec:157346.25
step 4305, loss: 3.502877, norm:0.2667, lr:5.2281e-04 dt: 3331.90ms, tok/sec:157354.10
step 4306, loss: 3.522630, norm:0.2560, lr:5.2277e-04 dt: 3331.99ms, tok/sec:157349.74
step 4307, loss: 3.567299, norm:0.2551, lr:5.2272e-04 dt: 3332.21ms, tok/sec:157339.65
step 4308, loss: 3.465063, norm:0.2826, lr:5.2268e-04 dt: 3331.98ms, tok/sec:157350.24
step 4309, loss: 3.562882, norm:0.2730, lr:5.2264e-04 dt: 3332.03ms, tok/sec:157347.77
step 4310, loss: 3.522145, norm:0.2525, lr:5.2260e-04 dt: 3332.06ms, tok/sec:157346.58
step 4311, loss: 3.457640, norm:0.2620, lr:5.2256e-04 dt: 3332.47ms, tok/sec:157326.99
step 4312, loss: 3.524863, norm:0.2736, lr:5.2252e-04 dt: 3332.10ms, tok/sec:157344.82
step 4313, loss: 3.508041, norm:0.2734, lr:5.2248e-04 dt: 3331.97ms, tok/sec:157350.74
step 4314, loss: 3.424916, norm:0.2597, lr:5.2244e-04 dt: 3331.95ms, tok/sec:157351.53
step 4315, loss: 3.450668, norm:0.2762, lr:5.2240e-04 dt: 3332.08ms, tok/sec:157345.35
step 4316, loss: 3.479286, norm:0.2637, lr:5.2236e-04 dt: 3332.07ms, tok/sec:157345.95
step 4317, loss: 3.497172, norm:0.2509, lr:5.2232e-04 dt: 3332.01ms, tok/sec:157349.05
step 4318, loss: 3.477170, norm:0.2426, lr:5.2227e-04 dt: 3331.95ms, tok/sec:157351.55
step 4319, loss: 3.445375, norm:0.2265, lr:5.2223e-04 dt: 3332.28ms, tok/sec:157336.18
step 4320, loss: 3.515957, norm:0.2643, lr:5.2219e-04 dt: 3332.22ms, tok/sec:157338.76
step 4321, loss: 3.398893, norm:0.2691, lr:5.2215e-04 dt: 3332.02ms, tok/sec:157348.24
step 4322, loss: 3.421422, norm:0.2385, lr:5.2211e-04 dt: 3332.08ms, tok/sec:157345.76
step 4323, loss: 3.434428, norm:0.2634, lr:5.2207e-04 dt: 3332.04ms, tok/sec:157347.47
step 4324, loss: 3.429634, norm:0.2315, lr:5.2203e-04 dt: 3332.09ms, tok/sec:157344.88
step 4325, loss: 3.413304, norm:0.2638, lr:5.2199e-04 dt: 3331.97ms, tok/sec:157350.55
step 4326, loss: 3.282983, norm:0.2707, lr:5.2195e-04 dt: 3331.94ms, tok/sec:157352.21
step 4327, loss: 3.324417, norm:0.2694, lr:5.2191e-04 dt: 3332.38ms, tok/sec:157331.23
step 4328, loss: 3.285238, norm:0.2687, lr:5.2186e-04 dt: 3331.76ms, tok/sec:157360.59
step 4329, loss: 3.329823, norm:0.2470, lr:5.2182e-04 dt: 3332.23ms, tok/sec:157338.38
step 4330, loss: 3.341698, norm:0.2696, lr:5.2178e-04 dt: 3331.99ms, tok/sec:157349.94
step 4331, loss: 3.265587, norm:0.2550, lr:5.2174e-04 dt: 3331.91ms, tok/sec:157353.46
step 4332, loss: 3.354422, norm:0.2554, lr:5.2170e-04 dt: 3331.94ms, tok/sec:157352.05
step 4333, loss: 3.334526, norm:0.2466, lr:5.2166e-04 dt: 3332.00ms, tok/sec:157349.13
step 4334, loss: 3.274246, norm:0.2536, lr:5.2162e-04 dt: 3331.84ms, tok/sec:157356.80
step 4335, loss: 3.301129, norm:0.2511, lr:5.2158e-04 dt: 3332.06ms, tok/sec:157346.34
step 4336, loss: 3.293433, norm:0.2742, lr:5.2154e-04 dt: 3332.12ms, tok/sec:157343.61
step 4337, loss: 3.382150, norm:0.2431, lr:5.2150e-04 dt: 3332.07ms, tok/sec:157345.86
step 4338, loss: 3.527450, norm:0.2734, lr:5.2145e-04 dt: 3332.40ms, tok/sec:157330.54
step 4339, loss: 3.512000, norm:0.2769, lr:5.2141e-04 dt: 3332.22ms, tok/sec:157338.80
step 4340, loss: 3.567444, norm:0.2572, lr:5.2137e-04 dt: 3331.92ms, tok/sec:157353.10
step 4341, loss: 3.530743, norm:0.2616, lr:5.2133e-04 dt: 3331.82ms, tok/sec:157357.84
step 4342, loss: 3.477143, norm:0.2803, lr:5.2129e-04 dt: 3332.23ms, tok/sec:157338.33
step 4343, loss: 3.518433, norm:0.2547, lr:5.2125e-04 dt: 3332.00ms, tok/sec:157349.52
step 4344, loss: 3.517661, norm:0.2597, lr:5.2121e-04 dt: 3332.10ms, tok/sec:157344.76
step 4345, loss: 3.554788, norm:0.2807, lr:5.2117e-04 dt: 3332.13ms, tok/sec:157343.41
step 4346, loss: 3.528507, norm:0.2811, lr:5.2112e-04 dt: 3332.49ms, tok/sec:157326.15
step 4347, loss: 3.535849, norm:0.2517, lr:5.2108e-04 dt: 3332.30ms, tok/sec:157334.94
step 4348, loss: 3.516974, norm:0.2581, lr:5.2104e-04 dt: 3332.00ms, tok/sec:157349.26
step 4349, loss: 3.483635, norm:0.2553, lr:5.2100e-04 dt: 3332.08ms, tok/sec:157345.33
HellaSwag accuracy:2325779006488790097/-2=-1162889503244395008.0000
rank 1 sample 0: Hello, I'm a language model, right? I'm so excited to contribute! Thank you! 🙂
Thank you for joining. Please join and share your
rank 1 sample 1: Hello, I'm a language model, you know it's not a language that understands the language or the culture. And your thinking, well you know it's
rank 1 sample 2: Hello, I'm a language model, so your program has to be able to do it. It takes some effort to do it and some good things for it
rank 1 sample 3: Hello, I'm a language model, and I'm trying to say hello to God. I did. He wanted me to follow God in the program. I
rank 0 sample 0: Hello, I'm a language model, and I don't understand its structure for its entirety. I've used semantic, semantic and epistemological approaches to define
rank 0 sample 1: Hello, I'm a language model, I need to be fluent in English," said Roshilin.
Roshilin's native English speakers are
rank 0 sample 2: Hello, I'm a language model, I'm having an issue with it, I haven't had that to go into writing, I'm having an issue with
rank 0 sample 3: Hello, I'm a language model, a game design approach, and I have been following the same approach that I have given in my video course. It allows
step 4350, loss: 3.522532, norm:0.2664, lr:5.2096e-04 dt: 48513.98ms, tok/sec:10806.95
step 4351, loss: 3.511305, norm:0.2455, lr:5.2092e-04 dt: 3332.08ms, tok/sec:157345.35
step 4352, loss: 3.542677, norm:0.2407, lr:5.2088e-04 dt: 3331.88ms, tok/sec:157355.19
step 4353, loss: 3.505653, norm:0.2610, lr:5.2084e-04 dt: 3332.45ms, tok/sec:157327.98
step 4354, loss: 3.510746, norm:0.3271, lr:5.2080e-04 dt: 3332.39ms, tok/sec:157331.11
step 4355, loss: 3.474274, norm:0.2602, lr:5.2075e-04 dt: 3332.03ms, tok/sec:157347.85
step 4356, loss: 3.528317, norm:0.2568, lr:5.2071e-04 dt: 3332.37ms, tok/sec:157331.65
step 4357, loss: 3.522450, norm:0.2752, lr:5.2067e-04 dt: 3331.82ms, tok/sec:157357.78
step 4358, loss: 3.487447, norm:0.2549, lr:5.2063e-04 dt: 3332.09ms, tok/sec:157344.98
step 4359, loss: 3.537061, norm:0.2507, lr:5.2059e-04 dt: 3331.86ms, tok/sec:157356.00
step 4360, loss: 3.543278, norm:0.2601, lr:5.2055e-04 dt: 3332.15ms, tok/sec:157342.02
step 4361, loss: 3.467215, norm:0.2478, lr:5.2051e-04 dt: 3332.02ms, tok/sec:157348.38
step 4362, loss: 3.413268, norm:0.2523, lr:5.2046e-04 dt: 3332.08ms, tok/sec:157345.77
step 4363, loss: 3.479727, norm:0.2304, lr:5.2042e-04 dt: 3332.27ms, tok/sec:157336.54
step 4364, loss: 3.508934, norm:0.2624, lr:5.2038e-04 dt: 3332.14ms, tok/sec:157342.71
step 4365, loss: 3.484581, norm:0.2347, lr:5.2034e-04 dt: 3332.37ms, tok/sec:157331.87
step 4366, loss: 3.474643, norm:0.2486, lr:5.2030e-04 dt: 3332.10ms, tok/sec:157344.59
step 4367, loss: 3.489917, norm:0.2878, lr:5.2026e-04 dt: 3331.98ms, tok/sec:157350.22
step 4368, loss: 3.453656, norm:0.2239, lr:5.2022e-04 dt: 3332.05ms, tok/sec:157346.76
step 4369, loss: 3.418786, norm:0.2453, lr:5.2018e-04 dt: 3332.21ms, tok/sec:157339.31
step 4370, loss: 3.460489, norm:0.2361, lr:5.2013e-04 dt: 3332.12ms, tok/sec:157343.69
step 4371, loss: 3.431620, norm:0.2377, lr:5.2009e-04 dt: 3331.98ms, tok/sec:157350.16
step 4372, loss: 3.394440, norm:0.2440, lr:5.2005e-04 dt: 3332.43ms, tok/sec:157328.91
step 4373, loss: 3.367333, norm:0.2414, lr:5.2001e-04 dt: 3331.94ms, tok/sec:157352.10
step 4374, loss: 3.271816, norm:0.2557, lr:5.1997e-04 dt: 3332.02ms, tok/sec:157348.28
step 4375, loss: 3.344925, norm:0.2424, lr:5.1993e-04 dt: 3332.04ms, tok/sec:157347.62
step 4376, loss: 3.302001, norm:0.2738, lr:5.1989e-04 dt: 3331.93ms, tok/sec:157352.51
step 4377, loss: 3.277676, norm:0.2730, lr:5.1984e-04 dt: 3331.85ms, tok/sec:157356.34
step 4378, loss: 3.260190, norm:0.2780, lr:5.1980e-04 dt: 3332.02ms, tok/sec:157348.26
step 4379, loss: 3.340595, norm:0.2770, lr:5.1976e-04 dt: 3332.19ms, tok/sec:157340.52
step 4380, loss: 3.258694, norm:0.2671, lr:5.1972e-04 dt: 3331.97ms, tok/sec:157350.93
step 4381, loss: 3.327138, norm:0.2515, lr:5.1968e-04 dt: 3334.44ms, tok/sec:157234.07
step 4382, loss: 3.390913, norm:0.2831, lr:5.1964e-04 dt: 3332.24ms, tok/sec:157338.14
step 4383, loss: 3.304563, norm:0.2616, lr:5.1959e-04 dt: 3331.93ms, tok/sec:157352.81
step 4384, loss: 3.352813, norm:0.2836, lr:5.1955e-04 dt: 3332.10ms, tok/sec:157344.58
step 4385, loss: 3.536597, norm:0.2984, lr:5.1951e-04 dt: 3332.30ms, tok/sec:157334.99
step 4386, loss: 3.531333, norm:0.2663, lr:5.1947e-04 dt: 3331.99ms, tok/sec:157349.61
step 4387, loss: 3.554502, norm:0.2975, lr:5.1943e-04 dt: 3332.61ms, tok/sec:157320.68
step 4388, loss: 3.537668, norm:0.2665, lr:5.1939e-04 dt: 3332.03ms, tok/sec:157347.99
step 4389, loss: 3.549892, norm:0.2910, lr:5.1935e-04 dt: 3332.15ms, tok/sec:157342.03
step 4390, loss: 3.535210, norm:0.2664, lr:5.1930e-04 dt: 3332.28ms, tok/sec:157335.90
step 4391, loss: 3.491078, norm:0.2907, lr:5.1926e-04 dt: 3331.96ms, tok/sec:157351.29
step 4392, loss: 3.515037, norm:0.2877, lr:5.1922e-04 dt: 3332.30ms, tok/sec:157335.02
step 4393, loss: 3.540435, norm:0.3063, lr:5.1918e-04 dt: 3332.27ms, tok/sec:157336.48
step 4394, loss: 3.509389, norm:0.2789, lr:5.1914e-04 dt: 3332.30ms, tok/sec:157335.15
step 4395, loss: 3.463189, norm:0.2802, lr:5.1910e-04 dt: 3332.24ms, tok/sec:157337.81
step 4396, loss: 3.521001, norm:0.2769, lr:5.1905e-04 dt: 3331.99ms, tok/sec:157349.91
step 4397, loss: 3.470068, norm:0.2794, lr:5.1901e-04 dt: 3332.35ms, tok/sec:157332.61
step 4398, loss: 3.491428, norm:0.2489, lr:5.1897e-04 dt: 3332.57ms, tok/sec:157322.38
step 4399, loss: 3.445761, norm:0.2371, lr:5.1893e-04 dt: 3332.10ms, tok/sec:157344.49
validation loss: 3.5081
Model and optimizer state saved.
HellaSwag accuracy:4632184939953243217/-2=-2316092469976621568.0000
rank 1 sample 0: Hello, I'm a language model, what do you think?
We first need to look at data. We could look at it. We could look at
rank 1 sample 1: Hello, I'm a language model, which I am used to. This is actually a bit weird, in that there is much more power than I can get
rank 1 sample 2: Hello, I'm a language model, so have an easy-to-use interface.
In terms of how to use the tool I am going to need
rank 1 sample 3: Hello, I'm a language model, and I'm really excited--what kind of language that everybody just started up was that some of them were all the same
rank 0 sample 0: Hello, I'm a language model, and I'd like a bit more explanation because now I'd like to put all of these down here in class. The
rank 0 sample 1: Hello, I'm a language model, but how do you use it? Use it. Now this week, I've got an update. There's a lot
rank 0 sample 2: Hello, I'm a language model, but I did say that if you give me enough time, he can say that this is the best way to do that
rank 0 sample 3: Hello, I'm a language model, and now I know how to do it, they're going to go and see there's nothing to fear. So with
step 4400, loss: 3.511874, norm:0.2445, lr:5.1889e-04 dt: 56277.08ms, tok/sec:9316.19
step 4401, loss: 3.486671, norm:0.2412, lr:5.1885e-04 dt: 3332.22ms, tok/sec:157339.15
step 4402, loss: 3.480362, norm:0.2487, lr:5.1880e-04 dt: 3332.13ms, tok/sec:157343.30
step 4403, loss: 3.548833, norm:0.2464, lr:5.1876e-04 dt: 3332.02ms, tok/sec:157348.36
step 4404, loss: 3.433569, norm:0.3018, lr:5.1872e-04 dt: 3332.48ms, tok/sec:157326.82
step 4405, loss: 3.480328, norm:0.2571, lr:5.1868e-04 dt: 3332.17ms, tok/sec:157341.27
step 4406, loss: 3.512802, norm:0.2930, lr:5.1864e-04 dt: 3332.04ms, tok/sec:157347.28
step 4407, loss: 3.484642, norm:0.2649, lr:5.1860e-04 dt: 3332.10ms, tok/sec:157344.84
step 4408, loss: 3.476639, norm:0.2436, lr:5.1855e-04 dt: 3332.38ms, tok/sec:157331.50
step 4409, loss: 3.475319, norm:0.2386, lr:5.1851e-04 dt: 3332.19ms, tok/sec:157340.31
step 4410, loss: 3.509170, norm:0.2520, lr:5.1847e-04 dt: 3332.19ms, tok/sec:157340.21
step 4411, loss: 3.447061, norm:0.2202, lr:5.1843e-04 dt: 3332.24ms, tok/sec:157337.97
step 4412, loss: 3.536104, norm:0.2402, lr:5.1839e-04 dt: 3331.93ms, tok/sec:157352.78
step 4413, loss: 3.532287, norm:0.2492, lr:5.1834e-04 dt: 3332.45ms, tok/sec:157328.21
step 4414, loss: 3.449524, norm:0.2511, lr:5.1830e-04 dt: 3331.92ms, tok/sec:157353.07
step 4415, loss: 3.431004, norm:0.2483, lr:5.1826e-04 dt: 3331.88ms, tok/sec:157354.82
step 4416, loss: 3.488410, norm:0.2326, lr:5.1822e-04 dt: 3331.90ms, tok/sec:157353.94
step 4417, loss: 3.409524, norm:0.2683, lr:5.1818e-04 dt: 3332.02ms, tok/sec:157348.27
step 4418, loss: 3.461297, norm:0.2511, lr:5.1814e-04 dt: 3331.90ms, tok/sec:157354.07
step 4419, loss: 3.459770, norm:0.2609, lr:5.1809e-04 dt: 3332.02ms, tok/sec:157348.58
step 4420, loss: 3.306367, norm:0.3014, lr:5.1805e-04 dt: 3332.33ms, tok/sec:157333.88
step 4421, loss: 3.321685, norm:0.3163, lr:5.1801e-04 dt: 3332.07ms, tok/sec:157345.93
step 4422, loss: 3.335855, norm:0.2458, lr:5.1797e-04 dt: 3332.62ms, tok/sec:157320.21
step 4423, loss: 3.349904, norm:0.3075, lr:5.1793e-04 dt: 3331.92ms, tok/sec:157353.00
step 4424, loss: 3.275750, norm:0.2895, lr:5.1788e-04 dt: 3331.99ms, tok/sec:157349.61
step 4425, loss: 3.311467, norm:0.2840, lr:5.1784e-04 dt: 3332.02ms, tok/sec:157348.22
step 4426, loss: 3.391523, norm:0.2430, lr:5.1780e-04 dt: 3332.07ms, tok/sec:157346.25
step 4427, loss: 3.292120, norm:0.2807, lr:5.1776e-04 dt: 3332.02ms, tok/sec:157348.49
step 4428, loss: 3.280828, norm:0.2438, lr:5.1772e-04 dt: 3331.96ms, tok/sec:157351.01
step 4429, loss: 3.342909, norm:0.2620, lr:5.1768e-04 dt: 3332.55ms, tok/sec:157323.48
step 4430, loss: 3.275833, norm:0.2564, lr:5.1763e-04 dt: 3332.33ms, tok/sec:157333.59
step 4431, loss: 3.342848, norm:0.2452, lr:5.1759e-04 dt: 3332.05ms, tok/sec:157346.83
step 4432, loss: 3.541307, norm:0.2618, lr:5.1755e-04 dt: 3332.21ms, tok/sec:157339.20
step 4433, loss: 3.515730, norm:0.2678, lr:5.1751e-04 dt: 3332.21ms, tok/sec:157339.58
step 4434, loss: 3.528834, norm:0.2579, lr:5.1747e-04 dt: 3331.97ms, tok/sec:157350.80
step 4435, loss: 3.536388, norm:0.2758, lr:5.1742e-04 dt: 3332.17ms, tok/sec:157341.49
step 4436, loss: 3.568061, norm:0.2598, lr:5.1738e-04 dt: 3332.17ms, tok/sec:157341.29
step 4437, loss: 3.482247, norm:0.2809, lr:5.1734e-04 dt: 3332.10ms, tok/sec:157344.44
step 4438, loss: 3.518348, norm:0.2789, lr:5.1730e-04 dt: 3332.10ms, tok/sec:157344.78
step 4439, loss: 3.507810, norm:0.9708, lr:5.1726e-04 dt: 3332.35ms, tok/sec:157332.87
step 4440, loss: 3.531000, norm:0.3376, lr:5.1721e-04 dt: 3332.19ms, tok/sec:157340.15
step 4441, loss: 3.525856, norm:0.2640, lr:5.1717e-04 dt: 3332.79ms, tok/sec:157311.87
step 4442, loss: 3.502592, norm:0.2777, lr:5.1713e-04 dt: 3332.08ms, tok/sec:157345.76
step 4443, loss: 3.454099, norm:0.3012, lr:5.1709e-04 dt: 3331.91ms, tok/sec:157353.50
step 4444, loss: 3.495146, norm:0.2543, lr:5.1705e-04 dt: 3331.97ms, tok/sec:157350.63
step 4445, loss: 3.553213, norm:0.2835, lr:5.1700e-04 dt: 3332.28ms, tok/sec:157336.30
step 4446, loss: 3.521765, norm:0.2741, lr:5.1696e-04 dt: 3332.13ms, tok/sec:157343.34
step 4447, loss: 3.542071, norm:0.2553, lr:5.1692e-04 dt: 3332.04ms, tok/sec:157347.28
step 4448, loss: 3.487105, norm:0.2522, lr:5.1688e-04 dt: 3332.10ms, tok/sec:157344.71
step 4449, loss: 3.514167, norm:0.2817, lr:5.1683e-04 dt: 3332.21ms, tok/sec:157339.58
HellaSwag accuracy:2326341922082505801/-2=-1163170961041252864.0000
rank 1 sample 0: Hello, I'm a language model, and a source of some interesting statistics as a matter of fact to me. A lot of work was done to define the
rank 1 sample 1: Hello, I'm a language model, you know what's next.
But for now, your biggest one is an alternative speech. Don’t be
rank 1 sample 2: Hello, I'm a language model, I tried my best to give you a good idea of where our model was for the next couple of months, as well
rank 1 sample 3: Hello, I'm a language model, and I'm interested to explore how these networks work. Hopefully, there will one or two (or more) of these
rank 0 sample 0: Hello, I'm a language model, but I didn't have one because this person called me too. But she called me and so it is not a question
rank 0 sample 1: Hello, I'm a language model, and when I start working with this (and the languages which are still running on my site), it takes me a long
rank 0 sample 2: Hello, I'm a language model, and I had a time line with an object:
I don't know what a language model is, but it says
rank 0 sample 3: Hello, I'm a language model, and when I do I think it should help at all. I just need some questions that need to be answered in your
step 4450, loss: 3.495438, norm:0.2590, lr:5.1679e-04 dt: 48515.68ms, tok/sec:10806.57
step 4451, loss: 3.509970, norm:0.2673, lr:5.1675e-04 dt: 3332.64ms, tok/sec:157319.26
step 4452, loss: 3.502801, norm:0.2524, lr:5.1671e-04 dt: 3332.00ms, tok/sec:157349.31
step 4453, loss: 3.489030, norm:0.2461, lr:5.1667e-04 dt: 3332.20ms, tok/sec:157339.81
step 4454, loss: 3.429200, norm:0.2582, lr:5.1662e-04 dt: 3332.21ms, tok/sec:157339.48
step 4455, loss: 3.407342, norm:0.2541, lr:5.1658e-04 dt: 3332.26ms, tok/sec:157337.29
step 4456, loss: 3.427697, norm:0.2556, lr:5.1654e-04 dt: 3332.17ms, tok/sec:157341.12
step 4457, loss: 3.432097, norm:0.2694, lr:5.1650e-04 dt: 3331.96ms, tok/sec:157351.21
step 4458, loss: 3.436914, norm:0.2605, lr:5.1646e-04 dt: 3331.90ms, tok/sec:157354.03
step 4459, loss: 3.450831, norm:0.2658, lr:5.1641e-04 dt: 3332.20ms, tok/sec:157339.87
step 4460, loss: 3.425359, norm:0.2558, lr:5.1637e-04 dt: 3332.39ms, tok/sec:157331.12
step 4461, loss: 3.444634, norm:0.2687, lr:5.1633e-04 dt: 3332.04ms, tok/sec:157347.58
step 4462, loss: 3.449411, norm:0.2679, lr:5.1629e-04 dt: 3331.96ms, tok/sec:157351.37
step 4463, loss: 3.460807, norm:0.2432, lr:5.1624e-04 dt: 3332.18ms, tok/sec:157340.99
step 4464, loss: 3.455621, norm:0.2596, lr:5.1620e-04 dt: 3332.02ms, tok/sec:157348.51
step 4465, loss: 3.451771, norm:0.2493, lr:5.1616e-04 dt: 3332.22ms, tok/sec:157338.94
step 4466, loss: 3.267984, norm:0.2525, lr:5.1612e-04 dt: 3331.85ms, tok/sec:157356.58
step 4467, loss: 3.291954, norm:0.2579, lr:5.1608e-04 dt: 3332.05ms, tok/sec:157346.95
step 4468, loss: 3.219875, norm:0.2438, lr:5.1603e-04 dt: 3332.21ms, tok/sec:157339.58
step 4469, loss: 3.322406, norm:0.2669, lr:5.1599e-04 dt: 3332.29ms, tok/sec:157335.81
step 4470, loss: 3.283802, norm:0.2353, lr:5.1595e-04 dt: 3332.19ms, tok/sec:157340.42
step 4471, loss: 3.235132, norm:0.2498, lr:5.1591e-04 dt: 3332.01ms, tok/sec:157349.06
step 4472, loss: 3.304983, norm:0.2482, lr:5.1586e-04 dt: 3332.26ms, tok/sec:157337.16
step 4473, loss: 3.287948, norm:0.2539, lr:5.1582e-04 dt: 3332.19ms, tok/sec:157340.53
step 4474, loss: 3.440959, norm:0.2962, lr:5.1578e-04 dt: 3332.00ms, tok/sec:157349.34
step 4475, loss: 3.316243, norm:0.2681, lr:5.1574e-04 dt: 3332.11ms, tok/sec:157344.01
step 4476, loss: 3.278954, norm:0.2716, lr:5.1569e-04 dt: 3332.09ms, tok/sec:157344.90
step 4477, loss: 3.402981, norm:0.2659, lr:5.1565e-04 dt: 3331.98ms, tok/sec:157350.51
step 4478, loss: 3.537945, norm:0.2712, lr:5.1561e-04 dt: 3332.42ms, tok/sec:157329.67
step 4479, loss: 3.506705, norm:0.2586, lr:5.1557e-04 dt: 3331.98ms, tok/sec:157350.23
step 4480, loss: 3.531227, norm:0.2847, lr:5.1553e-04 dt: 3332.01ms, tok/sec:157348.84
step 4481, loss: 3.567748, norm:0.2932, lr:5.1548e-04 dt: 3332.01ms, tok/sec:157348.98
step 4482, loss: 3.546917, norm:0.3737, lr:5.1544e-04 dt: 3331.98ms, tok/sec:157350.15
step 4483, loss: 3.558545, norm:0.3720, lr:5.1540e-04 dt: 3332.00ms, tok/sec:157349.14
step 4484, loss: 3.478146, norm:0.2939, lr:5.1536e-04 dt: 3332.25ms, tok/sec:157337.36
step 4485, loss: 3.560606, norm:0.2974, lr:5.1531e-04 dt: 3332.09ms, tok/sec:157344.94
step 4486, loss: 3.591390, norm:0.2902, lr:5.1527e-04 dt: 3331.84ms, tok/sec:157357.09
step 4487, loss: 3.488934, norm:0.2816, lr:5.1523e-04 dt: 3332.22ms, tok/sec:157339.12
step 4488, loss: 3.590127, norm:0.2817, lr:5.1519e-04 dt: 3332.67ms, tok/sec:157317.81
step 4489, loss: 3.521994, norm:0.2681, lr:5.1514e-04 dt: 3332.08ms, tok/sec:157345.54
step 4490, loss: 3.473502, norm:0.2707, lr:5.1510e-04 dt: 3332.03ms, tok/sec:157347.98
step 4491, loss: 3.497685, norm:0.2593, lr:5.1506e-04 dt: 3332.27ms, tok/sec:157336.45
step 4492, loss: 3.477266, norm:0.2572, lr:5.1502e-04 dt: 3332.10ms, tok/sec:157344.77
step 4493, loss: 3.453289, norm:0.2455, lr:5.1497e-04 dt: 3331.89ms, tok/sec:157354.69
step 4494, loss: 3.539596, norm:0.3124, lr:5.1493e-04 dt: 3332.01ms, tok/sec:157349.08
step 4495, loss: 3.490532, norm:0.2861, lr:5.1489e-04 dt: 3332.30ms, tok/sec:157334.96
step 4496, loss: 3.483053, norm:0.2976, lr:5.1485e-04 dt: 3332.09ms, tok/sec:157344.91
step 4497, loss: 3.505115, norm:0.2520, lr:5.1480e-04 dt: 3332.37ms, tok/sec:157331.79
step 4498, loss: 3.519228, norm:0.2694, lr:5.1476e-04 dt: 3332.09ms, tok/sec:157344.88
step 4499, loss: 3.578859, norm:0.2508, lr:5.1472e-04 dt: 3332.08ms, tok/sec:157345.34
validation loss: 3.5026
Model and optimizer state saved.
HellaSwag accuracy:19377419598414929/-2=-9688709799207464.0000
rank 1 sample 0: Hello, I'm a language model, and then we're done.
They have three classes : "Hello, Welcome, I'm just a model, and
rank 1 sample 1: Hello, I'm a language model, which means I need to know my grammar code. I was at the height of my success. Since then I've been
rank 1 sample 2: Hello, I'm a language model, I still want to understand the difference between the two. But you're still out there, you don't have an English
rank 1 sample 3: Hello, I'm a language model, and I'm using the first of our basic classes. At the end of this post, I want to teach you how
rank 0 sample 0: Hello, I'm a language model, and I don't have it to show some information about the environment when building my own software. This will help me make
rank 0 sample 1: Hello, I'm a language model, but a
little bit of the world. So i use
that (as in my example). (In my example
rank 0 sample 2: Hello, I'm a language model, but I find the "best" side of your application. And if you're working on a language model, it isn
rank 0 sample 3: Hello, I'm a language model, and will be teaching a language. My goal is to teach a language as an instrument in a classroom based on my particular
step 4500, loss: 3.480700, norm:0.2813, lr:5.1468e-04 dt: 56163.72ms, tok/sec:9334.99
step 4501, loss: 3.421803, norm:0.2458, lr:5.1463e-04 dt: 3332.04ms, tok/sec:157347.26
step 4502, loss: 3.476471, norm:0.2431, lr:5.1459e-04 dt: 3332.03ms, tok/sec:157347.99
step 4503, loss: 3.412813, norm:0.2554, lr:5.1455e-04 dt: 3332.08ms, tok/sec:157345.49
step 4504, loss: 3.459857, norm:0.2624, lr:5.1451e-04 dt: 3331.94ms, tok/sec:157352.20
step 4505, loss: 3.458279, norm:0.2593, lr:5.1446e-04 dt: 3332.03ms, tok/sec:157347.99
step 4506, loss: 3.432323, norm:0.2411, lr:5.1442e-04 dt: 3332.30ms, tok/sec:157335.01
step 4507, loss: 3.468989, norm:0.2491, lr:5.1438e-04 dt: 3332.03ms, tok/sec:157347.94
step 4508, loss: 3.442566, norm:0.2414, lr:5.1434e-04 dt: 3332.06ms, tok/sec:157346.63
step 4509, loss: 3.475984, norm:0.2550, lr:5.1429e-04 dt: 3332.42ms, tok/sec:157329.42
step 4510, loss: 3.416758, norm:0.2418, lr:5.1425e-04 dt: 3332.16ms, tok/sec:157341.72
step 4511, loss: 3.428726, norm:0.2357, lr:5.1421e-04 dt: 3332.14ms, tok/sec:157342.58
step 4512, loss: 3.321052, norm:0.2312, lr:5.1417e-04 dt: 3332.04ms, tok/sec:157347.29
step 4513, loss: 3.261534, norm:0.2430, lr:5.1412e-04 dt: 3331.97ms, tok/sec:157350.90
step 4514, loss: 3.295226, norm:0.2599, lr:5.1408e-04 dt: 3332.17ms, tok/sec:157341.49
step 4515, loss: 3.247932, norm:0.2510, lr:5.1404e-04 dt: 3333.13ms, tok/sec:157296.18
step 4516, loss: 3.274058, norm:0.2735, lr:5.1399e-04 dt: 3332.02ms, tok/sec:157348.52
step 4517, loss: 3.293193, norm:0.3131, lr:5.1395e-04 dt: 3332.26ms, tok/sec:157337.08
step 4518, loss: 3.290344, norm:0.2799, lr:5.1391e-04 dt: 3332.51ms, tok/sec:157325.21
step 4519, loss: 3.338874, norm:0.2833, lr:5.1387e-04 dt: 3331.92ms, tok/sec:157353.02
step 4520, loss: 3.287683, norm:0.2615, lr:5.1382e-04 dt: 3331.85ms, tok/sec:157356.50
step 4521, loss: 3.404005, norm:0.2738, lr:5.1378e-04 dt: 3331.92ms, tok/sec:157353.13
step 4522, loss: 3.338578, norm:0.2975, lr:5.1374e-04 dt: 3332.03ms, tok/sec:157348.15
step 4523, loss: 3.333447, norm:0.2561, lr:5.1370e-04 dt: 3332.07ms, tok/sec:157345.85
step 4524, loss: 3.417524, norm:0.2871, lr:5.1365e-04 dt: 3332.04ms, tok/sec:157347.57
step 4525, loss: 3.473809, norm:0.2589, lr:5.1361e-04 dt: 3332.17ms, tok/sec:157341.55
step 4526, loss: 3.463361, norm:0.2811, lr:5.1357e-04 dt: 3332.06ms, tok/sec:157346.60
step 4527, loss: 3.581823, norm:0.2938, lr:5.1352e-04 dt: 3332.57ms, tok/sec:157322.30
step 4528, loss: 3.492975, norm:0.2939, lr:5.1348e-04 dt: 3332.10ms, tok/sec:157344.48
step 4529, loss: 3.505222, norm:0.2786, lr:5.1344e-04 dt: 3332.18ms, tok/sec:157341.06
step 4530, loss: 3.497434, norm:0.2635, lr:5.1340e-04 dt: 3332.13ms, tok/sec:157342.98
step 4531, loss: 3.453937, norm:0.2819, lr:5.1335e-04 dt: 3332.16ms, tok/sec:157341.99
step 4532, loss: 3.552772, norm:0.2913, lr:5.1331e-04 dt: 3332.12ms, tok/sec:157343.51
step 4533, loss: 3.456982, norm:0.3259, lr:5.1327e-04 dt: 3332.11ms, tok/sec:157344.04
step 4534, loss: 3.531095, norm:0.2977, lr:5.1322e-04 dt: 3332.15ms, tok/sec:157342.44
step 4535, loss: 3.499122, norm:0.2642, lr:5.1318e-04 dt: 3332.18ms, tok/sec:157340.84
step 4536, loss: 3.518313, norm:0.2941, lr:5.1314e-04 dt: 3332.47ms, tok/sec:157327.06
step 4537, loss: 3.500006, norm:0.2743, lr:5.1310e-04 dt: 3331.76ms, tok/sec:157360.81
step 4538, loss: 3.470846, norm:0.2558, lr:5.1305e-04 dt: 3332.08ms, tok/sec:157345.79
step 4539, loss: 3.476304, norm:0.2581, lr:5.1301e-04 dt: 3332.07ms, tok/sec:157346.27
step 4540, loss: 3.500051, norm:0.2572, lr:5.1297e-04 dt: 3332.08ms, tok/sec:157345.61
step 4541, loss: 3.560680, norm:0.2805, lr:5.1292e-04 dt: 3332.12ms, tok/sec:157343.61
step 4542, loss: 3.513824, norm:0.2678, lr:5.1288e-04 dt: 3332.10ms, tok/sec:157344.55
step 4543, loss: 3.449567, norm:0.2535, lr:5.1284e-04 dt: 3332.47ms, tok/sec:157327.12
step 4544, loss: 3.488695, norm:0.2644, lr:5.1280e-04 dt: 3332.17ms, tok/sec:157341.40
step 4545, loss: 3.477506, norm:0.2363, lr:5.1275e-04 dt: 3331.90ms, tok/sec:157353.88
step 4546, loss: 3.488988, norm:0.2362, lr:5.1271e-04 dt: 3332.30ms, tok/sec:157335.30
step 4547, loss: 3.504159, norm:0.2395, lr:5.1267e-04 dt: 3332.06ms, tok/sec:157346.72
step 4548, loss: 3.521079, norm:0.2559, lr:5.1262e-04 dt: 3331.94ms, tok/sec:157352.20
step 4549, loss: 3.447400, norm:0.2392, lr:5.1258e-04 dt: 3332.14ms, tok/sec:157342.70
HellaSwag accuracy:2325057699010806801/-2=-1162528849505403392.0000
rank 1 sample 0: Hello, I'm a language model,” said the presenter.
Mr Smith has also published their book, Not to get excited when he was asked to
rank 1 sample 1: Hello, I'm a language model, which I can find in the library for your computer. There are thousands of computer programs. Many different types of programs are
rank 1 sample 2: Hello, I'm a language model, I guess you'll be able to show you how to create new versions, build a new one in your code so you
rank 1 sample 3: Hello, I'm a language model, and I'm using the concept of object communication.
Then we’ll be putting another piece of the paper into
rank 0 sample 0: Hello, I'm a language model, and I know that it's been pretty darn simple. I've worked that, and now I know what I mean,
rank 0 sample 1: Hello, I'm a language model, but for today I have to work full time and care over my life"<|endoftext|>A recent study published in the journal Science
rank 0 sample 2: Hello, I'm a language model, but I will continue to build this object. There's a bit more to it, and I'm going to have a
rank 0 sample 3: Hello, I'm a language model, and will be a very powerful language for people coming to the online world.<|endoftext|>(N1) How to Write to
step 4550, loss: 3.424899, norm:0.2513, lr:5.1254e-04 dt: 48519.66ms, tok/sec:10805.68
step 4551, loss: 3.424890, norm:0.2503, lr:5.1250e-04 dt: 3332.08ms, tok/sec:157345.34
step 4552, loss: 3.463875, norm:0.2442, lr:5.1245e-04 dt: 3332.40ms, tok/sec:157330.41
step 4553, loss: 3.455551, norm:0.2734, lr:5.1241e-04 dt: 3332.01ms, tok/sec:157348.76
step 4554, loss: 3.474338, norm:0.2633, lr:5.1237e-04 dt: 3332.13ms, tok/sec:157343.09
step 4555, loss: 3.455402, norm:0.2369, lr:5.1232e-04 dt: 3331.91ms, tok/sec:157353.70
step 4556, loss: 3.490339, norm:0.2676, lr:5.1228e-04 dt: 3331.89ms, tok/sec:157354.60
step 4557, loss: 3.445959, norm:0.2543, lr:5.1224e-04 dt: 3332.09ms, tok/sec:157345.02
step 4558, loss: 3.422936, norm:0.2392, lr:5.1219e-04 dt: 3332.08ms, tok/sec:157345.65
step 4559, loss: 3.459926, norm:0.3010, lr:5.1215e-04 dt: 3331.82ms, tok/sec:157357.99
step 4560, loss: 3.244730, norm:0.3005, lr:5.1211e-04 dt: 3332.04ms, tok/sec:157347.46
step 4561, loss: 3.351422, norm:0.3030, lr:5.1207e-04 dt: 3332.42ms, tok/sec:157329.65
step 4562, loss: 3.330483, norm:0.3066, lr:5.1202e-04 dt: 3332.32ms, tok/sec:157334.42
step 4563, loss: 3.311861, norm:0.2495, lr:5.1198e-04 dt: 3331.81ms, tok/sec:157358.46
step 4564, loss: 3.219158, norm:0.2722, lr:5.1194e-04 dt: 3331.83ms, tok/sec:157357.16
step 4565, loss: 3.236730, norm:0.2706, lr:5.1189e-04 dt: 3332.16ms, tok/sec:157341.58
step 4566, loss: 3.252757, norm:0.2764, lr:5.1185e-04 dt: 3331.84ms, tok/sec:157357.05
step 4567, loss: 3.275343, norm:0.2741, lr:5.1181e-04 dt: 3332.07ms, tok/sec:157345.95
step 4568, loss: 3.286528, norm:0.2810, lr:5.1176e-04 dt: 3332.05ms, tok/sec:157347.00
step 4569, loss: 3.232271, norm:0.2628, lr:5.1172e-04 dt: 3331.99ms, tok/sec:157349.65
step 4570, loss: 3.235195, norm:0.2533, lr:5.1168e-04 dt: 3332.63ms, tok/sec:157319.79
step 4571, loss: 3.366668, norm:0.2465, lr:5.1163e-04 dt: 3334.05ms, tok/sec:157252.59
step 4572, loss: 3.488690, norm:0.2705, lr:5.1159e-04 dt: 3332.24ms, tok/sec:157337.97
step 4573, loss: 3.469526, norm:0.2683, lr:5.1155e-04 dt: 3332.15ms, tok/sec:157342.37
step 4574, loss: 3.441389, norm:0.2518, lr:5.1150e-04 dt: 3332.23ms, tok/sec:157338.69
step 4575, loss: 3.487467, norm:0.2771, lr:5.1146e-04 dt: 3332.04ms, tok/sec:157347.51
step 4576, loss: 3.572586, norm:0.2921, lr:5.1142e-04 dt: 3332.44ms, tok/sec:157328.52
step 4577, loss: 3.485995, norm:0.2727, lr:5.1137e-04 dt: 3332.26ms, tok/sec:157337.14
step 4578, loss: 3.486858, norm:0.2839, lr:5.1133e-04 dt: 3332.16ms, tok/sec:157341.69
step 4579, loss: 3.499742, norm:0.2788, lr:5.1129e-04 dt: 3331.81ms, tok/sec:157358.27
step 4580, loss: 3.447041, norm:0.3049, lr:5.1125e-04 dt: 3332.08ms, tok/sec:157345.69
step 4581, loss: 3.529268, norm:0.2927, lr:5.1120e-04 dt: 3332.18ms, tok/sec:157341.04
step 4582, loss: 3.454108, norm:0.2550, lr:5.1116e-04 dt: 3331.93ms, tok/sec:157352.67
step 4583, loss: 3.512050, norm:0.2802, lr:5.1112e-04 dt: 3332.16ms, tok/sec:157341.67
step 4584, loss: 3.500910, norm:0.2898, lr:5.1107e-04 dt: 3332.03ms, tok/sec:157347.93
step 4585, loss: 3.505527, norm:0.3007, lr:5.1103e-04 dt: 3332.01ms, tok/sec:157348.94
step 4586, loss: 3.478906, norm:0.2671, lr:5.1099e-04 dt: 3332.41ms, tok/sec:157330.21
step 4587, loss: 3.451271, norm:0.2661, lr:5.1094e-04 dt: 3331.76ms, tok/sec:157360.58
step 4588, loss: 3.534126, norm:0.2727, lr:5.1090e-04 dt: 3332.46ms, tok/sec:157327.43
step 4589, loss: 3.548409, norm:0.2612, lr:5.1086e-04 dt: 3332.14ms, tok/sec:157342.60
step 4590, loss: 3.524210, norm:0.2926, lr:5.1081e-04 dt: 3332.30ms, tok/sec:157335.37
step 4591, loss: 3.413657, norm:0.2763, lr:5.1077e-04 dt: 3331.70ms, tok/sec:157363.57
step 4592, loss: 3.539326, norm:0.2822, lr:5.1073e-04 dt: 3332.06ms, tok/sec:157346.46
step 4593, loss: 3.490851, norm:0.2697, lr:5.1068e-04 dt: 3332.12ms, tok/sec:157343.82
step 4594, loss: 3.487402, norm:0.2811, lr:5.1064e-04 dt: 3332.05ms, tok/sec:157346.88
step 4595, loss: 3.431334, norm:0.2759, lr:5.1060e-04 dt: 3332.28ms, tok/sec:157335.99
step 4596, loss: 3.458007, norm:0.2607, lr:5.1055e-04 dt: 3332.41ms, tok/sec:157330.09
step 4597, loss: 3.484497, norm:0.2653, lr:5.1051e-04 dt: 3332.27ms, tok/sec:157336.65
step 4598, loss: 3.451637, norm:0.2708, lr:5.1047e-04 dt: 3331.99ms, tok/sec:157349.98
step 4599, loss: 3.455444, norm:0.2413, lr:5.1042e-04 dt: 3332.02ms, tok/sec:157348.54
validation loss: 3.4954
Model and optimizer state saved.
HellaSwag accuracy:2326183598917895249/-2=-1163091799458947584.0000
rank 1 sample 0: Hello, I'm a language model, for instance, with a very complicated abstract, in order to perform a sentence "I have no knowledge of how to write
rank 1 sample 1: Hello, I'm a language model, which I have since been introduced to over 25 years ago but I would like to admit though, the model is not a
rank 1 sample 2: Hello, I'm a language model, but just to the extent that I'm not a language.<|endoftext|>This article is a part of a series of blog posts
rank 1 sample 3: Hello, I'm a language model, and I'm working with how to program more complex machines—particularly some robots who can mimic reality and communicate with the human
rank 0 sample 0: Hello, I'm a language model, and I know that's pretty well in any application. I've come across it in many new places and it's very
rank 0 sample 1: Hello, I'm a language model, but my model is pretty much what we're doing
As you're the author, it's not your intent to publish
rank 0 sample 2: Hello, I'm a language model, I'm so interested in writing programming in the new language. In particular, I'm a language model, I am really
rank 0 sample 3: Hello, I'm a language model, so lets me create a new class by putting myself in the dictionary.<|endoftext|>What kind of changes do you notice about those
step 4600, loss: 3.437059, norm:0.2636, lr:5.1038e-04 dt: 56189.07ms, tok/sec:9330.78
step 4601, loss: 3.446767, norm:0.2534, lr:5.1034e-04 dt: 3332.11ms, tok/sec:157343.95
step 4602, loss: 3.417786, norm:0.2540, lr:5.1029e-04 dt: 3332.40ms, tok/sec:157330.35
step 4603, loss: 3.455078, norm:0.2427, lr:5.1025e-04 dt: 3331.87ms, tok/sec:157355.45
step 4604, loss: 3.438221, norm:0.2458, lr:5.1021e-04 dt: 3331.82ms, tok/sec:157357.70
step 4605, loss: 3.432922, norm:0.2456, lr:5.1016e-04 dt: 3331.97ms, tok/sec:157350.70
step 4606, loss: 3.465641, norm:0.2961, lr:5.1012e-04 dt: 3331.90ms, tok/sec:157353.93
step 4607, loss: 3.296471, norm:0.2721, lr:5.1008e-04 dt: 3331.98ms, tok/sec:157350.50
step 4608, loss: 3.303055, norm:0.3229, lr:5.1003e-04 dt: 3332.06ms, tok/sec:157346.68
step 4609, loss: 3.262549, norm:0.2892, lr:5.0999e-04 dt: 3332.15ms, tok/sec:157342.04
step 4610, loss: 3.279799, norm:0.2514, lr:5.0994e-04 dt: 3332.06ms, tok/sec:157346.42
step 4611, loss: 3.270524, norm:0.2803, lr:5.0990e-04 dt: 3332.48ms, tok/sec:157326.50
step 4612, loss: 3.229152, norm:0.2559, lr:5.0986e-04 dt: 3332.21ms, tok/sec:157339.32
step 4613, loss: 3.320963, norm:0.2749, lr:5.0981e-04 dt: 3331.91ms, tok/sec:157353.80
step 4614, loss: 3.302846, norm:0.2783, lr:5.0977e-04 dt: 3331.75ms, tok/sec:157361.03
step 4615, loss: 3.317823, norm:0.3028, lr:5.0973e-04 dt: 3332.22ms, tok/sec:157338.93
step 4616, loss: 3.280432, norm:0.2875, lr:5.0968e-04 dt: 3331.91ms, tok/sec:157353.79
step 4617, loss: 3.270262, norm:0.2694, lr:5.0964e-04 dt: 3332.25ms, tok/sec:157337.34
step 4618, loss: 3.452015, norm:0.2865, lr:5.0960e-04 dt: 3332.16ms, tok/sec:157341.69
step 4619, loss: 3.486048, norm:0.2725, lr:5.0955e-04 dt: 3332.03ms, tok/sec:157347.86
step 4620, loss: 3.497351, norm:0.2770, lr:5.0951e-04 dt: 3332.40ms, tok/sec:157330.30
step 4621, loss: 3.506063, norm:0.2728, lr:5.0947e-04 dt: 3332.22ms, tok/sec:157339.00
step 4622, loss: 3.530277, norm:0.2850, lr:5.0942e-04 dt: 3332.22ms, tok/sec:157338.90
step 4623, loss: 3.499232, norm:0.2766, lr:5.0938e-04 dt: 3332.10ms, tok/sec:157344.62
step 4624, loss: 3.478758, norm:0.2662, lr:5.0934e-04 dt: 3332.27ms, tok/sec:157336.49
step 4625, loss: 3.516076, norm:0.2661, lr:5.0929e-04 dt: 3332.10ms, tok/sec:157344.81
step 4626, loss: 3.464526, norm:0.2881, lr:5.0925e-04 dt: 3331.86ms, tok/sec:157355.72
step 4627, loss: 3.442919, norm:0.2631, lr:5.0920e-04 dt: 3331.86ms, tok/sec:157355.78
step 4628, loss: 3.529194, norm:0.2652, lr:5.0916e-04 dt: 3332.12ms, tok/sec:157343.52
step 4629, loss: 3.496907, norm:0.2602, lr:5.0912e-04 dt: 3332.11ms, tok/sec:157344.23
step 4630, loss: 3.533153, norm:0.2703, lr:5.0907e-04 dt: 3332.21ms, tok/sec:157339.61
step 4631, loss: 3.499617, norm:0.2567, lr:5.0903e-04 dt: 3331.78ms, tok/sec:157359.64
step 4632, loss: 3.513207, norm:0.2530, lr:5.0899e-04 dt: 3332.08ms, tok/sec:157345.67
step 4633, loss: 3.485793, norm:0.2375, lr:5.0894e-04 dt: 3332.00ms, tok/sec:157349.43
step 4634, loss: 3.473229, norm:0.2799, lr:5.0890e-04 dt: 3331.95ms, tok/sec:157351.70
step 4635, loss: 3.519235, norm:0.2500, lr:5.0885e-04 dt: 3332.10ms, tok/sec:157344.72
step 4636, loss: 3.497310, norm:0.2501, lr:5.0881e-04 dt: 3332.18ms, tok/sec:157340.92
step 4637, loss: 3.509580, norm:0.2473, lr:5.0877e-04 dt: 3332.17ms, tok/sec:157341.48
step 4638, loss: 3.441749, norm:0.2418, lr:5.0872e-04 dt: 3332.03ms, tok/sec:157348.15
step 4639, loss: 3.473209, norm:0.2447, lr:5.0868e-04 dt: 3332.05ms, tok/sec:157346.87
step 4640, loss: 3.502295, norm:0.2393, lr:5.0864e-04 dt: 3332.27ms, tok/sec:157336.64
step 4641, loss: 3.469080, norm:0.2453, lr:5.0859e-04 dt: 3332.15ms, tok/sec:157342.18
step 4642, loss: 3.447145, norm:0.2322, lr:5.0855e-04 dt: 3332.17ms, tok/sec:157341.44
step 4643, loss: 3.461490, norm:0.2379, lr:5.0851e-04 dt: 3332.01ms, tok/sec:157348.75
step 4644, loss: 3.435854, norm:0.2585, lr:5.0846e-04 dt: 3332.19ms, tok/sec:157340.47
step 4645, loss: 3.396416, norm:0.2275, lr:5.0842e-04 dt: 3332.26ms, tok/sec:157337.08
step 4646, loss: 3.460098, norm:0.2336, lr:5.0837e-04 dt: 3332.48ms, tok/sec:157326.87
step 4647, loss: 3.459279, norm:0.2414, lr:5.0833e-04 dt: 3332.25ms, tok/sec:157337.45
step 4648, loss: 3.453260, norm:0.2412, lr:5.0829e-04 dt: 3332.27ms, tok/sec:157336.73
step 4649, loss: 3.448793, norm:0.2775, lr:5.0824e-04 dt: 3332.48ms, tok/sec:157326.75
HellaSwag accuracy:2325224826926220369/-2=-1162612413463110144.0000
rank 1 sample 0: Hello, I'm a language model, like the concept of ‘Themes' to get this perspective.
Here's an example to get to the point
rank 1 sample 1: Hello, I'm a language model, you're gonna come up with your own project. And maybe you were already looking at building your own.
- You
rank 1 sample 2: Hello, I'm a language model, I learn the way to write a program. I'm trying to write an environment that is different from my computer programming language
rank 1 sample 3: Hello, I'm a language model, and I'm pretty excited the other person actually knows what those little birds are capable of -- no.
“It
rank 0 sample 0: Hello, I'm a language model, and I know that I've never encountered words like "somewhere down", "pardon the sun" or "
rank 0 sample 1: Hello, I'm a language model, but if you're an English language practitioner, it works perfectly well. Because you can learn from your past and present,
rank 0 sample 2: Hello, I'm a language model, so I won't understand any of those things."
The idea for "the" and "the" is a huge
rank 0 sample 3: Hello, I'm a language model, not your imagination.
If I'd have tried to create a system in my childhood, I would have felt that.
step 4650, loss: 3.457945, norm:0.2332, lr:5.0820e-04 dt: 48518.83ms, tok/sec:10805.87
step 4651, loss: 3.441526, norm:0.2778, lr:5.0815e-04 dt: 3332.45ms, tok/sec:157327.90
step 4652, loss: 3.444129, norm:0.2392, lr:5.0811e-04 dt: 3332.15ms, tok/sec:157342.21
step 4653, loss: 3.244136, norm:0.2847, lr:5.0807e-04 dt: 3331.82ms, tok/sec:157357.90
step 4654, loss: 3.223854, norm:0.2741, lr:5.0802e-04 dt: 3332.02ms, tok/sec:157348.36
step 4655, loss: 3.225439, norm:0.2664, lr:5.0798e-04 dt: 3332.26ms, tok/sec:157336.84
step 4656, loss: 3.257467, norm:0.2803, lr:5.0794e-04 dt: 3332.14ms, tok/sec:157342.65
step 4657, loss: 3.248073, norm:0.2666, lr:5.0789e-04 dt: 3331.79ms, tok/sec:157359.35
step 4658, loss: 3.248810, norm:0.2529, lr:5.0785e-04 dt: 3332.14ms, tok/sec:157342.78
step 4659, loss: 3.297347, norm:0.2760, lr:5.0780e-04 dt: 3332.11ms, tok/sec:157344.10
step 4660, loss: 3.315804, norm:0.2848, lr:5.0776e-04 dt: 3332.44ms, tok/sec:157328.67
step 4661, loss: 3.239201, norm:0.2631, lr:5.0772e-04 dt: 3331.96ms, tok/sec:157351.43
step 4662, loss: 3.316776, norm:0.3060, lr:5.0767e-04 dt: 3332.02ms, tok/sec:157348.43
step 4663, loss: 3.307674, norm:0.2521, lr:5.0763e-04 dt: 3332.07ms, tok/sec:157346.10
step 4664, loss: 3.194518, norm:0.2623, lr:5.0758e-04 dt: 3332.16ms, tok/sec:157341.94
step 4665, loss: 3.512087, norm:0.2960, lr:5.0754e-04 dt: 3332.02ms, tok/sec:157348.53
step 4666, loss: 3.480172, norm:0.2673, lr:5.0750e-04 dt: 3332.07ms, tok/sec:157346.27
step 4667, loss: 3.485470, norm:0.2807, lr:5.0745e-04 dt: 3332.23ms, tok/sec:157338.44
step 4668, loss: 3.486315, norm:0.2814, lr:5.0741e-04 dt: 3332.32ms, tok/sec:157334.44
step 4669, loss: 3.474706, norm:0.2764, lr:5.0736e-04 dt: 3332.13ms, tok/sec:157343.00
step 4670, loss: 3.505304, norm:0.2782, lr:5.0732e-04 dt: 3332.07ms, tok/sec:157346.20
step 4671, loss: 3.505925, norm:0.2762, lr:5.0728e-04 dt: 3332.06ms, tok/sec:157346.41
step 4672, loss: 3.477279, norm:0.2649, lr:5.0723e-04 dt: 3332.12ms, tok/sec:157343.72
step 4673, loss: 3.560866, norm:0.3196, lr:5.0719e-04 dt: 3331.86ms, tok/sec:157355.87
step 4674, loss: 3.500091, norm:0.2819, lr:5.0714e-04 dt: 3332.24ms, tok/sec:157338.05
step 4675, loss: 3.447054, norm:0.2907, lr:5.0710e-04 dt: 3332.12ms, tok/sec:157343.87
step 4676, loss: 3.466776, norm:0.2842, lr:5.0706e-04 dt: 3332.24ms, tok/sec:157338.22
step 4677, loss: 3.533524, norm:0.2575, lr:5.0701e-04 dt: 3332.09ms, tok/sec:157345.29
step 4678, loss: 3.432850, norm:0.2693, lr:5.0697e-04 dt: 3332.00ms, tok/sec:157349.29
step 4679, loss: 3.510571, norm:0.2470, lr:5.0692e-04 dt: 3332.47ms, tok/sec:157327.04
step 4680, loss: 3.481172, norm:0.2463, lr:5.0688e-04 dt: 3332.14ms, tok/sec:157342.76
step 4681, loss: 3.489516, norm:0.2541, lr:5.0684e-04 dt: 3332.01ms, tok/sec:157348.87
step 4682, loss: 3.488142, norm:0.2599, lr:5.0679e-04 dt: 3331.87ms, tok/sec:157355.32
step 4683, loss: 3.479746, norm:0.2377, lr:5.0675e-04 dt: 3332.11ms, tok/sec:157344.01
step 4684, loss: 3.490063, norm:0.2490, lr:5.0670e-04 dt: 3331.95ms, tok/sec:157351.87
step 4685, loss: 3.471789, norm:0.2565, lr:5.0666e-04 dt: 3332.02ms, tok/sec:157348.29
step 4686, loss: 3.475626, norm:0.2508, lr:5.0662e-04 dt: 3332.19ms, tok/sec:157340.59
step 4687, loss: 3.461864, norm:0.2502, lr:5.0657e-04 dt: 3332.04ms, tok/sec:157347.31
step 4688, loss: 3.397659, norm:0.2404, lr:5.0653e-04 dt: 3332.51ms, tok/sec:157325.20
step 4689, loss: 3.459502, norm:0.2690, lr:5.0648e-04 dt: 3331.99ms, tok/sec:157349.85
step 4690, loss: 3.437396, norm:0.2691, lr:5.0644e-04 dt: 3331.85ms, tok/sec:157356.31
step 4691, loss: 3.373081, norm:0.2759, lr:5.0640e-04 dt: 3332.09ms, tok/sec:157345.32
step 4692, loss: 3.418646, norm:0.2638, lr:5.0635e-04 dt: 3332.18ms, tok/sec:157340.77
step 4693, loss: 3.459239, norm:0.3148, lr:5.0631e-04 dt: 3332.01ms, tok/sec:157348.91
step 4694, loss: 3.466098, norm:0.2410, lr:5.0626e-04 dt: 3331.90ms, tok/sec:157353.87
step 4695, loss: 3.406010, norm:0.2670, lr:5.0622e-04 dt: 3332.35ms, tok/sec:157332.60
step 4696, loss: 3.424725, norm:0.2260, lr:5.0617e-04 dt: 3332.35ms, tok/sec:157332.67
step 4697, loss: 3.408658, norm:0.2615, lr:5.0613e-04 dt: 3331.89ms, tok/sec:157354.71
step 4698, loss: 3.410221, norm:0.2352, lr:5.0609e-04 dt: 3331.96ms, tok/sec:157351.02
step 4699, loss: 3.343257, norm:0.2341, lr:5.0604e-04 dt: 3332.12ms, tok/sec:157343.45
validation loss: 3.4891
Model and optimizer state saved.
HellaSwag accuracy:2324653078731785289/-2=-1162326539365892608.0000
rank 1 sample 0: Hello, I'm a language model, we've got to talk about the data. What's an OTP?
The main idea behind the OTP is
rank 1 sample 1: Hello, I'm a language model, which means I can see the data as I go into detail. However, my problem lies in, how do I know
rank 1 sample 2: Hello, I'm a language model, I should write an example of how to write a book that takes the time to write. You'll get the gist,
rank 1 sample 3: Hello, I'm a language model, and I'm interested to come up with simple, step-by-step programs that keep learners engaged and engaged.

rank 0 sample 0: Hello, I'm a language model, and I like to learn what a new culture term is and why different aspects need to be fixed to make this a reality
rank 0 sample 1: Hello, I'm a language model, but how do you get started? Go to our resources about the English to learn English and Spanish, in Spanish, English
rank 0 sample 2: Hello, I'm a language model, so I do my English Language by word, like English. If the English language is not the same as English, it
rank 0 sample 3: Hello, I'm a language model, and an exercise in the world. That is important because it is hard to teach so many kids, because it can not
step 4700, loss: 3.291693, norm:0.2495, lr:5.0600e-04 dt: 56187.15ms, tok/sec:9331.10
step 4701, loss: 3.272452, norm:0.2350, lr:5.0595e-04 dt: 3332.07ms, tok/sec:157345.93
step 4702, loss: 3.329048, norm:0.2608, lr:5.0591e-04 dt: 3332.42ms, tok/sec:157329.32
step 4703, loss: 3.216877, norm:0.2478, lr:5.0586e-04 dt: 3332.06ms, tok/sec:157346.29
step 4704, loss: 3.273701, norm:0.2556, lr:5.0582e-04 dt: 3331.85ms, tok/sec:157356.20
step 4705, loss: 3.283449, norm:0.2724, lr:5.0578e-04 dt: 3331.96ms, tok/sec:157351.39
step 4706, loss: 3.284954, norm:0.2738, lr:5.0573e-04 dt: 3332.20ms, tok/sec:157339.86
step 4707, loss: 3.310769, norm:0.2626, lr:5.0569e-04 dt: 3332.10ms, tok/sec:157344.69
step 4708, loss: 3.320662, norm:0.2675, lr:5.0564e-04 dt: 3332.33ms, tok/sec:157333.67
step 4709, loss: 3.320338, norm:0.2743, lr:5.0560e-04 dt: 3332.33ms, tok/sec:157333.57
step 4710, loss: 3.219476, norm:0.2653, lr:5.0555e-04 dt: 3331.92ms, tok/sec:157353.01
step 4711, loss: 3.447142, norm:0.2640, lr:5.0551e-04 dt: 3332.62ms, tok/sec:157320.22
step 4712, loss: 3.446937, norm:0.2596, lr:5.0547e-04 dt: 3331.87ms, tok/sec:157355.25
step 4713, loss: 3.517920, norm:0.2607, lr:5.0542e-04 dt: 3332.09ms, tok/sec:157345.11
step 4714, loss: 3.429312, norm:0.2702, lr:5.0538e-04 dt: 3332.15ms, tok/sec:157342.09
step 4715, loss: 3.566107, norm:0.3025, lr:5.0533e-04 dt: 3332.28ms, tok/sec:157335.91
step 4716, loss: 3.546193, norm:0.2834, lr:5.0529e-04 dt: 3331.97ms, tok/sec:157350.77
step 4717, loss: 3.508741, norm:0.3258, lr:5.0524e-04 dt: 3332.06ms, tok/sec:157346.51
step 4718, loss: 3.527007, norm:0.2913, lr:5.0520e-04 dt: 3332.37ms, tok/sec:157331.87
step 4719, loss: 3.527923, norm:0.3276, lr:5.0516e-04 dt: 3332.34ms, tok/sec:157333.09
step 4720, loss: 3.484050, norm:0.3355, lr:5.0511e-04 dt: 3332.08ms, tok/sec:157345.68
step 4721, loss: 3.440610, norm:0.2910, lr:5.0507e-04 dt: 3332.13ms, tok/sec:157343.01
step 4722, loss: 3.432513, norm:0.2553, lr:5.0502e-04 dt: 3332.05ms, tok/sec:157346.76
step 4723, loss: 3.506077, norm:0.2702, lr:5.0498e-04 dt: 3332.20ms, tok/sec:157340.13
step 4724, loss: 3.498950, norm:0.2672, lr:5.0493e-04 dt: 3332.25ms, tok/sec:157337.70
step 4725, loss: 3.428043, norm:0.2446, lr:5.0489e-04 dt: 3332.03ms, tok/sec:157348.12
step 4726, loss: 3.482310, norm:0.2517, lr:5.0484e-04 dt: 3331.95ms, tok/sec:157351.51
step 4727, loss: 3.554141, norm:0.2539, lr:5.0480e-04 dt: 3332.80ms, tok/sec:157311.68
step 4728, loss: 3.478992, norm:0.2478, lr:5.0476e-04 dt: 3333.47ms, tok/sec:157279.92
step 4729, loss: 3.465494, norm:0.2691, lr:5.0471e-04 dt: 3332.20ms, tok/sec:157339.89
step 4730, loss: 3.508621, norm:0.2621, lr:5.0467e-04 dt: 3332.14ms, tok/sec:157342.82
step 4731, loss: 3.492681, norm:0.2680, lr:5.0462e-04 dt: 3331.95ms, tok/sec:157351.58
step 4732, loss: 3.510135, norm:0.2273, lr:5.0458e-04 dt: 3331.85ms, tok/sec:157356.20
step 4733, loss: 3.455381, norm:0.2469, lr:5.0453e-04 dt: 3332.19ms, tok/sec:157340.60
step 4734, loss: 3.464050, norm:0.2611, lr:5.0449e-04 dt: 3332.21ms, tok/sec:157339.53
step 4735, loss: 3.406137, norm:0.2456, lr:5.0444e-04 dt: 3332.10ms, tok/sec:157344.59
step 4736, loss: 3.417453, norm:0.2414, lr:5.0440e-04 dt: 3332.01ms, tok/sec:157349.10
step 4737, loss: 3.453384, norm:0.2376, lr:5.0436e-04 dt: 3332.43ms, tok/sec:157329.13
step 4738, loss: 3.439353, norm:0.2565, lr:5.0431e-04 dt: 3332.06ms, tok/sec:157346.43
step 4739, loss: 3.425394, norm:0.2331, lr:5.0427e-04 dt: 3332.11ms, tok/sec:157343.99
step 4740, loss: 3.449369, norm:0.2399, lr:5.0422e-04 dt: 3332.08ms, tok/sec:157345.76
step 4741, loss: 3.478572, norm:0.2429, lr:5.0418e-04 dt: 3332.00ms, tok/sec:157349.54
step 4742, loss: 3.436290, norm:0.2592, lr:5.0413e-04 dt: 3332.03ms, tok/sec:157347.93
step 4743, loss: 3.432214, norm:0.2304, lr:5.0409e-04 dt: 3332.23ms, tok/sec:157338.32
step 4744, loss: 3.452215, norm:0.2526, lr:5.0404e-04 dt: 3332.44ms, tok/sec:157328.72
step 4745, loss: 3.275589, norm:0.2569, lr:5.0400e-04 dt: 3331.93ms, tok/sec:157352.76
step 4746, loss: 3.262863, norm:0.2485, lr:5.0395e-04 dt: 3331.88ms, tok/sec:157354.91
step 4747, loss: 3.296294, norm:0.2598, lr:5.0391e-04 dt: 3332.32ms, tok/sec:157334.46
step 4748, loss: 3.340406, norm:0.2616, lr:5.0387e-04 dt: 3331.90ms, tok/sec:157353.91
step 4749, loss: 3.313864, norm:0.2468, lr:5.0382e-04 dt: 3331.96ms, tok/sec:157351.29
HellaSwag accuracy:2325101679475934281/-2=-1162550839737967104.0000
rank 1 sample 0: Hello, I'm a language model, the difference is that in this case for all variables and subtyping variables, the same variables would be true for all
rank 1 sample 1: Hello, I'm a language model, but it is going to be a good model for this because it is going to lead the AI network.
So,
rank 1 sample 2: Hello, I'm a language model, I got a lot of time to learn it.
How long is the question?
We'll take the following four
rank 1 sample 3: Hello, I'm a language model, and I'm in a little bit of real life. Even a nice book (and as one of my favorite speakers)
rank 0 sample 0: Hello, I'm a language model, and I like to think of all languages they were born with, even though we're a pretty common type of language (
rank 0 sample 1: Hello, I'm a language model, I just want to show you a lesson on a vocabulary question, you can start with this activity and work with the vocabulary
rank 0 sample 2: Hello, I'm a language model, but I love that fact: to make it even more interesting in your writing, you can use the following example to explore
rank 0 sample 3: Hello, I'm a language model, and then I'm going to do that in high school. I'm going to think of things that's going to move
step 4750, loss: 3.328088, norm:0.2931, lr:5.0378e-04 dt: 48520.02ms, tok/sec:10805.60
step 4751, loss: 3.253872, norm:0.2694, lr:5.0373e-04 dt: 3331.83ms, tok/sec:157357.39
step 4752, loss: 3.245459, norm:0.2774, lr:5.0369e-04 dt: 3332.46ms, tok/sec:157327.81
step 4753, loss: 3.231192, norm:0.2754, lr:5.0364e-04 dt: 3331.85ms, tok/sec:157356.26
step 4754, loss: 3.306568, norm:0.2973, lr:5.0360e-04 dt: 3332.40ms, tok/sec:157330.39
step 4755, loss: 3.258845, norm:0.2719, lr:5.0355e-04 dt: 3331.90ms, tok/sec:157354.14
step 4756, loss: 3.264373, norm:0.2775, lr:5.0351e-04 dt: 3331.97ms, tok/sec:157350.58
step 4757, loss: 3.458550, norm:0.3379, lr:5.0346e-04 dt: 3331.98ms, tok/sec:157350.32
step 4758, loss: 3.487350, norm:0.2945, lr:5.0342e-04 dt: 3332.12ms, tok/sec:157343.86
step 4759, loss: 3.501779, norm:0.2714, lr:5.0337e-04 dt: 3332.10ms, tok/sec:157344.67
step 4760, loss: 3.492053, norm:0.2569, lr:5.0333e-04 dt: 3331.95ms, tok/sec:157351.59
step 4761, loss: 3.536965, norm:0.2869, lr:5.0328e-04 dt: 3332.37ms, tok/sec:157331.91
step 4762, loss: 3.449524, norm:0.2751, lr:5.0324e-04 dt: 3334.60ms, tok/sec:157226.60
step 4763, loss: 3.530450, norm:0.2824, lr:5.0319e-04 dt: 3332.56ms, tok/sec:157322.82
step 4764, loss: 3.509044, norm:0.2710, lr:5.0315e-04 dt: 3331.76ms, tok/sec:157360.66
step 4765, loss: 3.506379, norm:0.2556, lr:5.0311e-04 dt: 3332.11ms, tok/sec:157344.25
step 4766, loss: 3.486207, norm:0.2502, lr:5.0306e-04 dt: 3332.03ms, tok/sec:157348.15
step 4767, loss: 3.442437, norm:0.2579, lr:5.0302e-04 dt: 3331.76ms, tok/sec:157360.67
step 4768, loss: 3.487134, norm:0.2775, lr:5.0297e-04 dt: 3331.88ms, tok/sec:157354.89
step 4769, loss: 3.451214, norm:0.2636, lr:5.0293e-04 dt: 3331.96ms, tok/sec:157351.39
step 4770, loss: 3.467216, norm:0.2655, lr:5.0288e-04 dt: 3332.18ms, tok/sec:157340.92
step 4771, loss: 3.489429, norm:0.2565, lr:5.0284e-04 dt: 3332.06ms, tok/sec:157346.52
step 4772, loss: 3.500161, norm:0.2801, lr:5.0279e-04 dt: 3332.05ms, tok/sec:157346.97
step 4773, loss: 3.451053, norm:0.2531, lr:5.0275e-04 dt: 3332.32ms, tok/sec:157334.32
step 4774, loss: 3.440584, norm:0.2568, lr:5.0270e-04 dt: 3332.36ms, tok/sec:157332.54
step 4775, loss: 3.530602, norm:0.2761, lr:5.0266e-04 dt: 3331.99ms, tok/sec:157349.74
step 4776, loss: 3.449456, norm:0.2352, lr:5.0261e-04 dt: 3332.09ms, tok/sec:157345.18
step 4777, loss: 3.434249, norm:0.2738, lr:5.0257e-04 dt: 3331.92ms, tok/sec:157353.22
step 4778, loss: 3.474536, norm:0.2386, lr:5.0252e-04 dt: 3332.15ms, tok/sec:157342.36
step 4779, loss: 3.500230, norm:0.2683, lr:5.0248e-04 dt: 3332.00ms, tok/sec:157349.27
step 4780, loss: 3.426768, norm:0.2534, lr:5.0243e-04 dt: 3331.90ms, tok/sec:157354.07
step 4781, loss: 3.441551, norm:0.2337, lr:5.0239e-04 dt: 3331.78ms, tok/sec:157359.54
step 4782, loss: 3.414859, norm:0.2612, lr:5.0234e-04 dt: 3332.33ms, tok/sec:157333.75
step 4783, loss: 3.455604, norm:0.2607, lr:5.0230e-04 dt: 3332.33ms, tok/sec:157333.92
step 4784, loss: 3.436505, norm:0.2928, lr:5.0225e-04 dt: 3331.88ms, tok/sec:157354.81
step 4785, loss: 3.478040, norm:0.2721, lr:5.0221e-04 dt: 3332.16ms, tok/sec:157341.63
step 4786, loss: 3.483090, norm:0.3214, lr:5.0216e-04 dt: 3332.10ms, tok/sec:157344.75
step 4787, loss: 3.415637, norm:0.2594, lr:5.0212e-04 dt: 3332.06ms, tok/sec:157346.39
step 4788, loss: 3.439250, norm:0.2782, lr:5.0207e-04 dt: 3331.97ms, tok/sec:157350.84
step 4789, loss: 3.431008, norm:0.2446, lr:5.0203e-04 dt: 3331.94ms, tok/sec:157352.39
step 4790, loss: 3.404676, norm:0.2511, lr:5.0198e-04 dt: 3332.26ms, tok/sec:157337.10
step 4791, loss: 3.452359, norm:0.2603, lr:5.0194e-04 dt: 3331.77ms, tok/sec:157360.18
step 4792, loss: 3.313004, norm:0.2549, lr:5.0189e-04 dt: 3332.26ms, tok/sec:157336.97
step 4793, loss: 3.324949, norm:0.2419, lr:5.0185e-04 dt: 3331.89ms, tok/sec:157354.62
step 4794, loss: 3.221869, norm:0.2583, lr:5.0180e-04 dt: 3332.09ms, tok/sec:157344.87
step 4795, loss: 3.264976, norm:0.2588, lr:5.0176e-04 dt: 3332.00ms, tok/sec:157349.15
step 4796, loss: 3.274024, norm:0.2541, lr:5.0171e-04 dt: 3331.96ms, tok/sec:157351.02
step 4797, loss: 3.190321, norm:0.2613, lr:5.0167e-04 dt: 3331.80ms, tok/sec:157358.93
step 4798, loss: 3.334916, norm:0.2642, lr:5.0162e-04 dt: 3331.89ms, tok/sec:157354.77
step 4799, loss: 3.307723, norm:0.2767, lr:5.0158e-04 dt: 3332.13ms, tok/sec:157343.07
validation loss: 3.4890
Model and optimizer state saved.
HellaSwag accuracy:2325216030832673873/-2=-1162608015416336896.0000
rank 1 sample 0: Hello, I'm a language model, just to get started. I like I have many people using I-Word templates, which means there are some things that
rank 1 sample 1: Hello, I'm a language model, which means you can use it to run all the tasks required. A language model can generate languages and other language models,
rank 1 sample 2: Hello, I'm a language model, I feel it's a bit like a language model. So now we've made a language model with an embedded model.
rank 1 sample 3: Hello, I'm a language model, and I'm talking to what that means..
That looks like:
Let's give you a little bit more.
rank 0 sample 0: Hello, I'm a language model, and I wanted to create a language model at hand. I'm currently making it a few months in a day, so
rank 0 sample 1: Hello, I'm a language model, but if you're looking for more training, check out www.hc.uk
Are there any additional jobs to
rank 0 sample 2: Hello, I'm a language model, so I use a standard Java language for the data sets. You're also using a Java language for the data set,
rank 0 sample 3: Hello, I'm a language model, it seems that you can't use a dictionary engine.
Is A Stronger Anomalies
At first glance in
step 4800, loss: 3.298607, norm:0.2734, lr:5.0153e-04 dt: 56261.73ms, tok/sec:9318.73
step 4801, loss: 3.316016, norm:0.2582, lr:5.0149e-04 dt: 3332.20ms, tok/sec:157340.08
step 4802, loss: 3.325338, norm:0.2731, lr:5.0144e-04 dt: 3331.86ms, tok/sec:157356.09
step 4803, loss: 3.286181, norm:0.2493, lr:5.0140e-04 dt: 3332.02ms, tok/sec:157348.55
step 4804, loss: 3.471396, norm:0.3333, lr:5.0135e-04 dt: 3331.99ms, tok/sec:157349.72
step 4805, loss: 3.520258, norm:0.2954, lr:5.0131e-04 dt: 3332.38ms, tok/sec:157331.37
step 4806, loss: 3.484656, norm:0.2932, lr:5.0126e-04 dt: 3331.96ms, tok/sec:157351.32
step 4807, loss: 3.526725, norm:0.3457, lr:5.0122e-04 dt: 3331.99ms, tok/sec:157349.72
step 4808, loss: 3.530630, norm:0.3409, lr:5.0117e-04 dt: 3331.99ms, tok/sec:157349.98
step 4809, loss: 3.483382, norm:0.2766, lr:5.0113e-04 dt: 3332.23ms, tok/sec:157338.37
step 4810, loss: 3.477059, norm:0.2843, lr:5.0108e-04 dt: 3332.11ms, tok/sec:157343.98
step 4811, loss: 3.452517, norm:0.2558, lr:5.0104e-04 dt: 3332.17ms, tok/sec:157341.18
step 4812, loss: 3.449001, norm:0.2969, lr:5.0099e-04 dt: 3332.12ms, tok/sec:157343.50
step 4813, loss: 3.461027, norm:0.2593, lr:5.0095e-04 dt: 3332.41ms, tok/sec:157329.85
step 4814, loss: 3.500954, norm:0.2556, lr:5.0090e-04 dt: 3332.54ms, tok/sec:157323.99
step 4815, loss: 3.465939, norm:0.2396, lr:5.0086e-04 dt: 3332.15ms, tok/sec:157342.19
step 4816, loss: 3.449804, norm:0.2490, lr:5.0081e-04 dt: 3331.83ms, tok/sec:157357.28
step 4817, loss: 3.487731, norm:0.2337, lr:5.0077e-04 dt: 3331.97ms, tok/sec:157350.92
step 4818, loss: 3.458309, norm:0.2526, lr:5.0072e-04 dt: 3332.08ms, tok/sec:157345.44
step 4819, loss: 3.538548, norm:0.2509, lr:5.0068e-04 dt: 3332.14ms, tok/sec:157342.88
step 4820, loss: 3.416024, norm:0.2506, lr:5.0063e-04 dt: 3332.18ms, tok/sec:157340.94
step 4821, loss: 3.449339, norm:0.2505, lr:5.0059e-04 dt: 3332.20ms, tok/sec:157339.78
step 4822, loss: 3.467267, norm:0.2567, lr:5.0054e-04 dt: 3332.30ms, tok/sec:157335.20
step 4823, loss: 3.448424, norm:0.2509, lr:5.0050e-04 dt: 3331.95ms, tok/sec:157351.69
step 4824, loss: 3.485560, norm:0.2654, lr:5.0045e-04 dt: 3332.24ms, tok/sec:157337.97
step 4825, loss: 3.489162, norm:0.2242, lr:5.0041e-04 dt: 3332.28ms, tok/sec:157335.93
step 4826, loss: 3.562045, norm:0.2884, lr:5.0036e-04 dt: 3331.83ms, tok/sec:157357.60
step 4827, loss: 3.419116, norm:0.2598, lr:5.0032e-04 dt: 3331.95ms, tok/sec:157351.51
step 4828, loss: 3.402595, norm:0.2627, lr:5.0027e-04 dt: 3332.18ms, tok/sec:157340.78
step 4829, loss: 3.457116, norm:0.2476, lr:5.0022e-04 dt: 3332.11ms, tok/sec:157344.35
step 4830, loss: 3.522025, norm:0.2837, lr:5.0018e-04 dt: 3331.82ms, tok/sec:157357.93
step 4831, loss: 3.458661, norm:0.2606, lr:5.0013e-04 dt: 3332.17ms, tok/sec:157341.44
step 4832, loss: 3.490530, norm:0.2414, lr:5.0009e-04 dt: 3332.10ms, tok/sec:157344.77
step 4833, loss: 3.410097, norm:0.2446, lr:5.0004e-04 dt: 3332.66ms, tok/sec:157318.39
step 4834, loss: 3.502045, norm:0.2699, lr:5.0000e-04 dt: 3331.87ms, tok/sec:157355.33
step 4835, loss: 3.374912, norm:0.2699, lr:4.9995e-04 dt: 3331.90ms, tok/sec:157354.16
step 4836, loss: 3.414259, norm:0.2554, lr:4.9991e-04 dt: 3332.00ms, tok/sec:157349.19
step 4837, loss: 3.444187, norm:0.2609, lr:4.9986e-04 dt: 3332.05ms, tok/sec:157346.87
step 4838, loss: 3.412388, norm:0.2822, lr:4.9982e-04 dt: 3332.11ms, tok/sec:157344.09
step 4839, loss: 3.277617, norm:0.2760, lr:4.9977e-04 dt: 3332.12ms, tok/sec:157343.89
step 4840, loss: 3.312613, norm:0.2877, lr:4.9973e-04 dt: 3332.18ms, tok/sec:157340.96
step 4841, loss: 3.250797, norm:0.2577, lr:4.9968e-04 dt: 3332.24ms, tok/sec:157338.05
step 4842, loss: 3.303615, norm:0.2614, lr:4.9964e-04 dt: 3332.04ms, tok/sec:157347.63
step 4843, loss: 3.288074, norm:0.2498, lr:4.9959e-04 dt: 3331.99ms, tok/sec:157349.85
step 4844, loss: 3.219107, norm:0.2709, lr:4.9954e-04 dt: 3331.82ms, tok/sec:157357.99
step 4845, loss: 3.250779, norm:0.2576, lr:4.9950e-04 dt: 3331.82ms, tok/sec:157357.91
step 4846, loss: 3.226578, norm:0.2661, lr:4.9945e-04 dt: 3332.06ms, tok/sec:157346.56
step 4847, loss: 3.261758, norm:0.2613, lr:4.9941e-04 dt: 3331.98ms, tok/sec:157350.26
step 4848, loss: 3.246840, norm:0.2468, lr:4.9936e-04 dt: 3331.88ms, tok/sec:157355.07
step 4849, loss: 3.212998, norm:0.2707, lr:4.9932e-04 dt: 3332.30ms, tok/sec:157335.32
HellaSwag accuracy:2325075293344613457/-2=-1162537646672306688.0000
rank 1 sample 0: Hello, I'm a language model, and the two are similar, but these differences were quite large -- and so my first test was not very well.

rank 1 sample 1: Hello, I'm a language model, you're probably interested in learning some interesting words.
Do you have a word that helps you, and how do you
rank 1 sample 2: Hello, I'm a language model, I could write something like this:
- I'm writing on my blog,
- I'm using a new post
rank 1 sample 3: Hello, I'm a language model, and I'm writing about you in advance before you make big statements. In both cases, do I need to tell you
rank 0 sample 0: Hello, I'm a language model, I'm like a computer science professor, have kids to see things around them as they learn this course and the course of
rank 0 sample 1: Hello, I'm a language model, and if you want to add your values to a line at the start of this tutorial you can do all of the above
rank 0 sample 2: Hello, I'm a language model, and I used my friend John to do a word search. But now I know where to look for the word 't
rank 0 sample 3: Hello, I'm a language model, and for a very long time, I was so fascinated by the technology we have to do and hear how it's all
step 4850, loss: 3.234372, norm:0.2578, lr:4.9927e-04 dt: 48513.81ms, tok/sec:10806.98
step 4851, loss: 3.479652, norm:0.2946, lr:4.9923e-04 dt: 3332.23ms, tok/sec:157338.65
step 4852, loss: 3.457771, norm:0.3070, lr:4.9918e-04 dt: 3332.39ms, tok/sec:157331.00
step 4853, loss: 3.446391, norm:0.2606, lr:4.9914e-04 dt: 3332.38ms, tok/sec:157331.59
step 4854, loss: 3.528259, norm:0.2852, lr:4.9909e-04 dt: 3331.96ms, tok/sec:157351.13
step 4855, loss: 3.524632, norm:0.2697, lr:4.9904e-04 dt: 3332.23ms, tok/sec:157338.35
step 4856, loss: 3.442783, norm:0.3026, lr:4.9900e-04 dt: 3332.04ms, tok/sec:157347.28
step 4857, loss: 3.481703, norm:0.2492, lr:4.9895e-04 dt: 3332.19ms, tok/sec:157340.35
step 4858, loss: 3.471539, norm:0.2702, lr:4.9891e-04 dt: 3332.07ms, tok/sec:157345.98
step 4859, loss: 3.500126, norm:0.2529, lr:4.9886e-04 dt: 3331.97ms, tok/sec:157350.94
step 4860, loss: 3.466689, norm:0.2520, lr:4.9882e-04 dt: 3332.13ms, tok/sec:157343.03
step 4861, loss: 3.474885, norm:0.2716, lr:4.9877e-04 dt: 3332.36ms, tok/sec:157332.49
step 4862, loss: 3.499208, norm:0.2873, lr:4.9873e-04 dt: 3332.16ms, tok/sec:157341.69
step 4863, loss: 3.498850, norm:0.2484, lr:4.9868e-04 dt: 3332.19ms, tok/sec:157340.53
step 4864, loss: 3.478654, norm:0.2708, lr:4.9864e-04 dt: 3332.10ms, tok/sec:157344.67
step 4865, loss: 3.522384, norm:0.2702, lr:4.9859e-04 dt: 3332.03ms, tok/sec:157348.06
step 4866, loss: 3.572417, norm:0.2739, lr:4.9854e-04 dt: 3331.90ms, tok/sec:157354.20
step 4867, loss: 3.529148, norm:0.2516, lr:4.9850e-04 dt: 3332.24ms, tok/sec:157337.79
step 4868, loss: 3.518018, norm:0.2508, lr:4.9845e-04 dt: 3332.07ms, tok/sec:157345.98
step 4869, loss: 3.528039, norm:0.2676, lr:4.9841e-04 dt: 3331.99ms, tok/sec:157350.03
step 4870, loss: 3.404788, norm:0.2522, lr:4.9836e-04 dt: 3332.36ms, tok/sec:157332.43
step 4871, loss: 3.475536, norm:0.2479, lr:4.9832e-04 dt: 3332.83ms, tok/sec:157310.32
step 4872, loss: 3.493036, norm:0.2540, lr:4.9827e-04 dt: 3332.23ms, tok/sec:157338.45
step 4873, loss: 3.503273, norm:0.2501, lr:4.9822e-04 dt: 3332.02ms, tok/sec:157348.17
step 4874, loss: 3.469026, norm:0.2534, lr:4.9818e-04 dt: 3332.10ms, tok/sec:157344.79
step 4875, loss: 3.475193, norm:0.2621, lr:4.9813e-04 dt: 3332.12ms, tok/sec:157343.79
step 4876, loss: 3.442647, norm:0.2659, lr:4.9809e-04 dt: 3332.14ms, tok/sec:157342.90
step 4877, loss: 3.439293, norm:0.2364, lr:4.9804e-04 dt: 3332.10ms, tok/sec:157344.57
step 4878, loss: 3.456734, norm:0.2460, lr:4.9800e-04 dt: 3331.89ms, tok/sec:157354.39
step 4879, loss: 3.414766, norm:0.2409, lr:4.9795e-04 dt: 3332.22ms, tok/sec:157339.11
step 4880, loss: 3.423766, norm:0.2494, lr:4.9791e-04 dt: 3332.43ms, tok/sec:157329.12
step 4881, loss: 3.420217, norm:0.2307, lr:4.9786e-04 dt: 3331.99ms, tok/sec:157349.65
step 4882, loss: 3.402234, norm:0.2498, lr:4.9781e-04 dt: 3332.13ms, tok/sec:157343.15
step 4883, loss: 3.370279, norm:0.2282, lr:4.9777e-04 dt: 3331.95ms, tok/sec:157351.58
step 4884, loss: 3.441278, norm:0.2341, lr:4.9772e-04 dt: 3332.15ms, tok/sec:157342.40
step 4885, loss: 3.378792, norm:0.2529, lr:4.9768e-04 dt: 3331.91ms, tok/sec:157353.69
step 4886, loss: 3.320561, norm:0.2743, lr:4.9763e-04 dt: 3331.95ms, tok/sec:157351.68
step 4887, loss: 3.225310, norm:0.2542, lr:4.9759e-04 dt: 3332.23ms, tok/sec:157338.49
step 4888, loss: 3.286212, norm:0.2787, lr:4.9754e-04 dt: 3331.91ms, tok/sec:157353.52
step 4889, loss: 3.296140, norm:0.2735, lr:4.9749e-04 dt: 3332.36ms, tok/sec:157332.25
step 4890, loss: 3.209972, norm:0.2483, lr:4.9745e-04 dt: 3331.89ms, tok/sec:157354.36
step 4891, loss: 3.238677, norm:0.2519, lr:4.9740e-04 dt: 3331.93ms, tok/sec:157352.49
step 4892, loss: 3.307535, norm:0.2496, lr:4.9736e-04 dt: 3332.08ms, tok/sec:157345.56
step 4893, loss: 3.264383, norm:0.2593, lr:4.9731e-04 dt: 3332.19ms, tok/sec:157340.24
step 4894, loss: 3.342191, norm:0.2546, lr:4.9727e-04 dt: 3331.85ms, tok/sec:157356.59
step 4895, loss: 3.290783, norm:0.2526, lr:4.9722e-04 dt: 3331.98ms, tok/sec:157350.42
step 4896, loss: 3.250957, norm:0.2333, lr:4.9717e-04 dt: 3332.25ms, tok/sec:157337.52
step 4897, loss: 3.369797, norm:0.4754, lr:4.9713e-04 dt: 3331.94ms, tok/sec:157352.05
step 4898, loss: 3.490681, norm:0.2534, lr:4.9708e-04 dt: 3332.35ms, tok/sec:157332.73
step 4899, loss: 3.466835, norm:0.2696, lr:4.9704e-04 dt: 3332.13ms, tok/sec:157342.99
validation loss: 3.4811
Model and optimizer state saved.
HellaSwag accuracy:4632167347767198801/-2=-2316083673883599360.0000
rank 1 sample 0: Hello, I'm a language model, with some assumptions. If I do with some things like it makes sense, well, if I want to know what I
rank 1 sample 1: Hello, I'm a language model, you know how to use it and that would be good too. My new word means more than speaking, so I'm
rank 1 sample 2: Hello, I'm a language model, so any text can be written in any language.
So in my language model, I have written an English grammar sample
rank 1 sample 3: Hello, I'm a language model, and I'm talking to each other for years. I used them again because what I told me was that I don't
rank 0 sample 0: Hello, I'm a language model, and I'd like you to learn from and you might understand how and why this is happening. And that's why the
rank 0 sample 1: Hello, I'm a language model, I hope you have found a great idea.<|endoftext|>Mastitis and Related Health
What is Mastitis?

rank 0 sample 2: Hello, I'm a language model, I'm about people. There are some people you can't think of. I'm not a language model, but as
rank 0 sample 3: Hello, I'm a language model, it sounds like I'm a computer model, although I'm not a computer, except my brain is working. So a
step 4900, loss: 3.529196, norm:0.2753, lr:4.9699e-04 dt: 56191.92ms, tok/sec:9330.31
step 4901, loss: 3.445996, norm:0.3262, lr:4.9694e-04 dt: 3332.29ms, tok/sec:157335.86
step 4902, loss: 3.507039, norm:0.3301, lr:4.9690e-04 dt: 3332.02ms, tok/sec:157348.25
step 4903, loss: 3.491024, norm:0.2785, lr:4.9685e-04 dt: 3332.48ms, tok/sec:157326.52
step 4904, loss: 3.509080, norm:0.3280, lr:4.9681e-04 dt: 3332.18ms, tok/sec:157340.82
step 4905, loss: 3.553829, norm:0.3258, lr:4.9676e-04 dt: 3332.06ms, tok/sec:157346.32
step 4906, loss: 3.469923, norm:0.3070, lr:4.9672e-04 dt: 3332.24ms, tok/sec:157338.16
step 4907, loss: 3.485312, norm:0.3260, lr:4.9667e-04 dt: 3332.16ms, tok/sec:157341.95
step 4908, loss: 3.456904, norm:0.2535, lr:4.9662e-04 dt: 3332.17ms, tok/sec:157341.35
step 4909, loss: 3.469361, norm:0.3113, lr:4.9658e-04 dt: 3332.06ms, tok/sec:157346.36
step 4910, loss: 3.480340, norm:0.2495, lr:4.9653e-04 dt: 3332.08ms, tok/sec:157345.53
step 4911, loss: 3.484139, norm:0.2869, lr:4.9649e-04 dt: 3332.19ms, tok/sec:157340.37
step 4912, loss: 3.497509, norm:0.2703, lr:4.9644e-04 dt: 3332.32ms, tok/sec:157334.11
step 4913, loss: 3.466161, norm:0.2803, lr:4.9639e-04 dt: 3332.12ms, tok/sec:157343.65
step 4914, loss: 3.488908, norm:0.2549, lr:4.9635e-04 dt: 3332.05ms, tok/sec:157346.86
step 4915, loss: 3.501457, norm:0.2863, lr:4.9630e-04 dt: 3332.29ms, tok/sec:157335.68
step 4916, loss: 3.569032, norm:0.2760, lr:4.9626e-04 dt: 3331.97ms, tok/sec:157350.97
step 4917, loss: 3.458824, norm:0.2487, lr:4.9621e-04 dt: 3331.97ms, tok/sec:157350.81
step 4918, loss: 3.544429, norm:0.2695, lr:4.9616e-04 dt: 3331.94ms, tok/sec:157352.22
step 4919, loss: 3.442428, norm:0.2439, lr:4.9612e-04 dt: 3332.50ms, tok/sec:157325.56
step 4920, loss: 3.465536, norm:0.2500, lr:4.9607e-04 dt: 3331.99ms, tok/sec:157350.04
step 4921, loss: 3.444722, norm:0.2288, lr:4.9603e-04 dt: 3331.99ms, tok/sec:157349.63
step 4922, loss: 3.417034, norm:0.2437, lr:4.9598e-04 dt: 3332.16ms, tok/sec:157341.94
step 4923, loss: 3.427207, norm:0.2502, lr:4.9593e-04 dt: 3332.15ms, tok/sec:157342.41
step 4924, loss: 3.393954, norm:0.2290, lr:4.9589e-04 dt: 3332.13ms, tok/sec:157343.20
step 4925, loss: 3.403430, norm:0.2468, lr:4.9584e-04 dt: 3332.60ms, tok/sec:157320.89
step 4926, loss: 3.446168, norm:0.2378, lr:4.9580e-04 dt: 3332.01ms, tok/sec:157348.87
step 4927, loss: 3.438234, norm:0.2482, lr:4.9575e-04 dt: 3332.29ms, tok/sec:157335.74
step 4928, loss: 3.412264, norm:0.2358, lr:4.9570e-04 dt: 3331.97ms, tok/sec:157350.81
step 4929, loss: 3.448131, norm:0.2797, lr:4.9566e-04 dt: 3332.09ms, tok/sec:157344.86
step 4930, loss: 3.364514, norm:0.2787, lr:4.9561e-04 dt: 3332.09ms, tok/sec:157345.11
step 4931, loss: 3.402413, norm:0.2573, lr:4.9557e-04 dt: 3331.85ms, tok/sec:157356.42
step 4932, loss: 3.436223, norm:0.2493, lr:4.9552e-04 dt: 3332.03ms, tok/sec:157347.85
step 4933, loss: 3.237049, norm:0.2531, lr:4.9547e-04 dt: 3332.25ms, tok/sec:157337.50
step 4934, loss: 3.276317, norm:0.2546, lr:4.9543e-04 dt: 3332.38ms, tok/sec:157331.33
step 4935, loss: 3.282588, norm:0.2451, lr:4.9538e-04 dt: 3332.06ms, tok/sec:157346.32
step 4936, loss: 3.288657, norm:0.2562, lr:4.9534e-04 dt: 3331.93ms, tok/sec:157352.60
step 4937, loss: 3.293775, norm:0.2780, lr:4.9529e-04 dt: 3332.00ms, tok/sec:157349.58
step 4938, loss: 3.275541, norm:0.2688, lr:4.9524e-04 dt: 3332.01ms, tok/sec:157348.71
step 4939, loss: 3.250010, norm:0.2624, lr:4.9520e-04 dt: 3332.07ms, tok/sec:157346.19
step 4940, loss: 3.239158, norm:0.2593, lr:4.9515e-04 dt: 3332.70ms, tok/sec:157316.08
step 4941, loss: 3.266875, norm:0.2667, lr:4.9510e-04 dt: 3333.44ms, tok/sec:157281.25
step 4942, loss: 3.298000, norm:0.2441, lr:4.9506e-04 dt: 3332.03ms, tok/sec:157347.81
step 4943, loss: 3.238396, norm:0.2484, lr:4.9501e-04 dt: 3332.01ms, tok/sec:157348.65
step 4944, loss: 3.398900, norm:0.2452, lr:4.9497e-04 dt: 3332.12ms, tok/sec:157343.89
step 4945, loss: 3.468624, norm:0.2586, lr:4.9492e-04 dt: 3331.92ms, tok/sec:157352.92
step 4946, loss: 3.490971, norm:0.2465, lr:4.9487e-04 dt: 3332.28ms, tok/sec:157336.29
step 4947, loss: 3.496823, norm:0.2590, lr:4.9483e-04 dt: 3332.51ms, tok/sec:157325.48
step 4948, loss: 3.408020, norm:0.2386, lr:4.9478e-04 dt: 3331.97ms, tok/sec:157350.79
step 4949, loss: 3.466264, norm:0.3229, lr:4.9474e-04 dt: 3332.45ms, tok/sec:157328.30
HellaSwag accuracy:2325075293342761993/-2=-1162537646671380992.0000
rank 1 sample 0: Hello, I'm a language model, and it's a real-world interface for you and everyone; it's built on it. That's what I'm
rank 1 sample 1: Hello, I'm a language model, which I've made a lot of suggestions all the time because I found it hard. After all, you can't do
rank 1 sample 2: Hello, I'm a language model, I’m a language model. I’m doing a grammar program, but I think the grammar skills I
rank 1 sample 3: Hello, I'm a language model, and I'm the author of this book.)
|Partition for Kids that I just made the most of:||
rank 0 sample 0: Hello, I'm a language model, and I don't have one that isn't yet taught in my English class anymore.
Of course, this is one
rank 0 sample 1: Hello, I'm a language model, but how do I write a language model? How do I create a web-site for you to keep your audience and
rank 0 sample 2: Hello, I'm a language model, I'm used to describe, represent and describe and describe the concepts. I'm interested in how to use the word '
rank 0 sample 3: Hello, I'm a language model, and here's how I want it: httphttp.com/software/download~s/com/download.css
step 4950, loss: 3.480474, norm:0.2756, lr:4.9469e-04 dt: 48518.28ms, tok/sec:10805.99
step 4951, loss: 3.559913, norm:0.2903, lr:4.9464e-04 dt: 3332.12ms, tok/sec:157343.66
step 4952, loss: 3.505958, norm:0.2733, lr:4.9460e-04 dt: 3334.00ms, tok/sec:157255.13
step 4953, loss: 3.497777, norm:0.2907, lr:4.9455e-04 dt: 3332.06ms, tok/sec:157346.64
step 4954, loss: 3.537519, norm:0.2765, lr:4.9450e-04 dt: 3332.35ms, tok/sec:157332.87
step 4955, loss: 3.448991, norm:0.2731, lr:4.9446e-04 dt: 3332.22ms, tok/sec:157338.87
step 4956, loss: 3.590237, norm:0.2963, lr:4.9441e-04 dt: 3331.95ms, tok/sec:157351.47
step 4957, loss: 3.486035, norm:0.2854, lr:4.9437e-04 dt: 3332.58ms, tok/sec:157321.89
step 4958, loss: 3.503766, norm:0.2769, lr:4.9432e-04 dt: 3332.06ms, tok/sec:157346.42
step 4959, loss: 3.503911, norm:0.2637, lr:4.9427e-04 dt: 3332.00ms, tok/sec:157349.22
step 4960, loss: 3.513828, norm:0.2597, lr:4.9423e-04 dt: 3332.07ms, tok/sec:157346.01
step 4961, loss: 3.474776, norm:0.2575, lr:4.9418e-04 dt: 3332.02ms, tok/sec:157348.27
step 4962, loss: 3.503791, norm:0.2559, lr:4.9413e-04 dt: 3332.13ms, tok/sec:157343.17
step 4963, loss: 3.503986, norm:0.2441, lr:4.9409e-04 dt: 3331.91ms, tok/sec:157353.54
step 4964, loss: 3.523456, norm:0.2602, lr:4.9404e-04 dt: 3332.16ms, tok/sec:157341.95
step 4965, loss: 3.414075, norm:0.2866, lr:4.9400e-04 dt: 3332.06ms, tok/sec:157346.54
step 4966, loss: 3.487632, norm:0.2687, lr:4.9395e-04 dt: 3332.03ms, tok/sec:157348.06
step 4967, loss: 3.503568, norm:0.2496, lr:4.9390e-04 dt: 3332.05ms, tok/sec:157346.83
step 4968, loss: 3.431736, norm:0.2491, lr:4.9386e-04 dt: 3332.28ms, tok/sec:157336.12
step 4969, loss: 3.419383, norm:0.2538, lr:4.9381e-04 dt: 3331.90ms, tok/sec:157353.83
step 4970, loss: 3.414647, norm:0.2451, lr:4.9376e-04 dt: 3332.18ms, tok/sec:157340.78
step 4971, loss: 3.438583, norm:0.2576, lr:4.9372e-04 dt: 3331.97ms, tok/sec:157350.94
step 4972, loss: 3.412765, norm:0.2473, lr:4.9367e-04 dt: 3332.21ms, tok/sec:157339.50
step 4973, loss: 3.448663, norm:0.2440, lr:4.9362e-04 dt: 3332.10ms, tok/sec:157344.54
step 4974, loss: 3.370777, norm:0.2488, lr:4.9358e-04 dt: 3332.01ms, tok/sec:157348.90
step 4975, loss: 3.444976, norm:0.2641, lr:4.9353e-04 dt: 3331.85ms, tok/sec:157356.30
step 4976, loss: 3.449450, norm:0.2400, lr:4.9349e-04 dt: 3332.42ms, tok/sec:157329.53
step 4977, loss: 3.490531, norm:0.2563, lr:4.9344e-04 dt: 3332.07ms, tok/sec:157345.95
step 4978, loss: 3.379353, norm:0.2424, lr:4.9339e-04 dt: 3331.89ms, tok/sec:157354.39
step 4979, loss: 3.269464, norm:0.2788, lr:4.9335e-04 dt: 3331.76ms, tok/sec:157360.53
step 4980, loss: 3.220709, norm:0.2884, lr:4.9330e-04 dt: 3331.99ms, tok/sec:157349.89
step 4981, loss: 3.224471, norm:0.2597, lr:4.9325e-04 dt: 3332.04ms, tok/sec:157347.56
step 4982, loss: 3.230968, norm:0.3016, lr:4.9321e-04 dt: 3331.96ms, tok/sec:157351.43
step 4983, loss: 3.195413, norm:0.2714, lr:4.9316e-04 dt: 3332.07ms, tok/sec:157346.06
step 4984, loss: 3.180953, norm:0.2510, lr:4.9311e-04 dt: 3331.82ms, tok/sec:157357.88
step 4985, loss: 3.268770, norm:0.2355, lr:4.9307e-04 dt: 3332.06ms, tok/sec:157346.56
step 4986, loss: 3.196379, norm:0.2686, lr:4.9302e-04 dt: 3332.61ms, tok/sec:157320.36
step 4987, loss: 3.304303, norm:0.2415, lr:4.9297e-04 dt: 3331.89ms, tok/sec:157354.73
step 4988, loss: 3.193186, norm:0.2591, lr:4.9293e-04 dt: 3331.93ms, tok/sec:157352.46
step 4989, loss: 3.268045, norm:0.2745, lr:4.9288e-04 dt: 3332.15ms, tok/sec:157342.23
step 4990, loss: 3.377886, norm:0.2800, lr:4.9283e-04 dt: 3332.30ms, tok/sec:157335.39
step 4991, loss: 3.508536, norm:0.2641, lr:4.9279e-04 dt: 3332.00ms, tok/sec:157349.36
step 4992, loss: 3.562206, norm:0.2602, lr:4.9274e-04 dt: 3331.95ms, tok/sec:157351.62
step 4993, loss: 3.447697, norm:0.2660, lr:4.9269e-04 dt: 3332.21ms, tok/sec:157339.31
step 4994, loss: 3.454093, norm:0.2776, lr:4.9265e-04 dt: 3332.14ms, tok/sec:157342.92
step 4995, loss: 3.500942, norm:0.2500, lr:4.9260e-04 dt: 3332.24ms, tok/sec:157337.98
step 4996, loss: 3.509405, norm:0.2548, lr:4.9256e-04 dt: 3332.33ms, tok/sec:157333.76
step 4997, loss: 3.525843, norm:0.2739, lr:4.9251e-04 dt: 3332.25ms, tok/sec:157337.52
step 4998, loss: 3.507609, norm:0.2546, lr:4.9246e-04 dt: 3332.06ms, tok/sec:157346.31
step 4999, loss: 3.465471, norm:0.2559, lr:4.9242e-04 dt: 3332.30ms, tok/sec:157335.36
validation loss: 3.4685
Model and optimizer state saved.
HellaSwag accuracy:2325356768253903889/-2=-1162678384126951936.0000
rank 1 sample 0: Hello, I'm a language model, with only one language: English. An important step for English class teachers is to teach language. For example, I teach
rank 1 sample 1: Hello, I'm a language model, which I am able to use as a template for teaching to students. I want students to learn the basics of language and
rank 1 sample 2: Hello, I'm a language model, I decided to make a little bit of a mistake. The last word of the sentence is 's, I' are
rank 1 sample 3: Hello, I'm a language model, and I'm working with data, you had to think 'that's an objective' thing would be to explain a definition
rank 0 sample 0: Hello, I'm a language model, I'm the language teacher I work with now. I learned how to develop an understanding of Chinese through my first language experience
rank 0 sample 1: Hello, I'm a language model, but here's a link to a dictionary.com where I'm sharing their resources:<|endoftext|>For a better understanding of the
rank 0 sample 2: Hello, I'm a language model, and I will discuss you more about my language theory.
"What does it mean?
"I mean that because
rank 0 sample 3: Hello, I'm a language model, but they're also a fun and interesting class-related activity.
You're free to share this to your friends who
step 5000, loss: 3.436899, norm:0.2481, lr:4.9237e-04 dt: 56167.56ms, tok/sec:9334.36
step 5001, loss: 3.512838, norm:0.2511, lr:4.9232e-04 dt: 3332.24ms, tok/sec:157338.11
step 5002, loss: 3.481697, norm:0.2433, lr:4.9228e-04 dt: 3331.93ms, tok/sec:157352.51
step 5003, loss: 3.476755, norm:0.2503, lr:4.9223e-04 dt: 3331.88ms, tok/sec:157355.12
step 5004, loss: 3.462705, norm:0.2905, lr:4.9218e-04 dt: 3332.02ms, tok/sec:157348.44
step 5005, loss: 3.512120, norm:0.2624, lr:4.9214e-04 dt: 3332.23ms, tok/sec:157338.61
step 5006, loss: 3.520473, norm:0.2464, lr:4.9209e-04 dt: 3332.05ms, tok/sec:157347.10
step 5007, loss: 3.494767, norm:0.2728, lr:4.9204e-04 dt: 3332.09ms, tok/sec:157344.97
step 5008, loss: 3.575835, norm:0.2631, lr:4.9200e-04 dt: 3332.07ms, tok/sec:157346.04
step 5009, loss: 3.499829, norm:0.2632, lr:4.9195e-04 dt: 3332.41ms, tok/sec:157330.21
step 5010, loss: 3.434431, norm:0.2322, lr:4.9190e-04 dt: 3332.00ms, tok/sec:157349.50
step 5011, loss: 3.481872, norm:0.2725, lr:4.9186e-04 dt: 3332.03ms, tok/sec:157348.00
step 5012, loss: 3.506021, norm:0.2787, lr:4.9181e-04 dt: 3332.06ms, tok/sec:157346.52
step 5013, loss: 3.460700, norm:0.2435, lr:4.9176e-04 dt: 3332.11ms, tok/sec:157343.91
step 5014, loss: 3.384299, norm:0.2774, lr:4.9172e-04 dt: 3331.96ms, tok/sec:157351.16
step 5015, loss: 3.397379, norm:0.2383, lr:4.9167e-04 dt: 3332.14ms, tok/sec:157342.51
step 5016, loss: 3.439477, norm:0.3326, lr:4.9162e-04 dt: 3331.87ms, tok/sec:157355.63
step 5017, loss: 3.434585, norm:0.2991, lr:4.9158e-04 dt: 3331.94ms, tok/sec:157352.33
step 5018, loss: 3.465288, norm:0.2740, lr:4.9153e-04 dt: 3332.37ms, tok/sec:157331.81
step 5019, loss: 3.381899, norm:0.2715, lr:4.9148e-04 dt: 3332.19ms, tok/sec:157340.31
step 5020, loss: 3.396123, norm:0.2679, lr:4.9144e-04 dt: 3331.94ms, tok/sec:157352.09
step 5021, loss: 3.461239, norm:0.2727, lr:4.9139e-04 dt: 3332.18ms, tok/sec:157341.02
step 5022, loss: 3.442923, norm:0.2728, lr:4.9134e-04 dt: 3332.01ms, tok/sec:157348.94
step 5023, loss: 3.407962, norm:0.2628, lr:4.9130e-04 dt: 3332.10ms, tok/sec:157344.57
step 5024, loss: 3.403425, norm:0.2632, lr:4.9125e-04 dt: 3332.16ms, tok/sec:157341.75
step 5025, loss: 3.311373, norm:0.2584, lr:4.9120e-04 dt: 3331.96ms, tok/sec:157351.06
step 5026, loss: 3.204373, norm:0.2480, lr:4.9115e-04 dt: 3331.86ms, tok/sec:157355.84
step 5027, loss: 3.193348, norm:0.2840, lr:4.9111e-04 dt: 3332.37ms, tok/sec:157332.05
step 5028, loss: 3.233501, norm:0.2576, lr:4.9106e-04 dt: 3332.20ms, tok/sec:157339.85
step 5029, loss: 3.170589, norm:0.2655, lr:4.9101e-04 dt: 3332.07ms, tok/sec:157345.95
step 5030, loss: 3.194899, norm:0.2577, lr:4.9097e-04 dt: 3332.02ms, tok/sec:157348.46
step 5031, loss: 3.218977, norm:0.2382, lr:4.9092e-04 dt: 3331.86ms, tok/sec:157355.96
step 5032, loss: 3.225562, norm:0.2746, lr:4.9087e-04 dt: 3331.85ms, tok/sec:157356.45
step 5033, loss: 3.217144, norm:0.2871, lr:4.9083e-04 dt: 3332.06ms, tok/sec:157346.39
step 5034, loss: 3.227230, norm:0.2529, lr:4.9078e-04 dt: 3331.88ms, tok/sec:157354.97
step 5035, loss: 3.243332, norm:0.2599, lr:4.9073e-04 dt: 3331.87ms, tok/sec:157355.46
step 5036, loss: 3.245052, norm:0.2541, lr:4.9069e-04 dt: 3332.25ms, tok/sec:157337.39
step 5037, loss: 3.478230, norm:0.2528, lr:4.9064e-04 dt: 3332.48ms, tok/sec:157326.48
step 5038, loss: 3.495841, norm:0.2534, lr:4.9059e-04 dt: 3332.00ms, tok/sec:157349.16
step 5039, loss: 3.544596, norm:0.2608, lr:4.9055e-04 dt: 3332.08ms, tok/sec:157345.45
step 5040, loss: 3.521745, norm:0.2710, lr:4.9050e-04 dt: 3332.08ms, tok/sec:157345.71
step 5041, loss: 3.521377, norm:0.2733, lr:4.9045e-04 dt: 3332.12ms, tok/sec:157343.86
step 5042, loss: 3.508522, norm:0.2639, lr:4.9041e-04 dt: 3332.05ms, tok/sec:157347.04
step 5043, loss: 3.407997, norm:0.2581, lr:4.9036e-04 dt: 3331.93ms, tok/sec:157352.47
step 5044, loss: 3.453665, norm:0.2465, lr:4.9031e-04 dt: 3332.35ms, tok/sec:157332.61
step 5045, loss: 3.446187, norm:0.2813, lr:4.9026e-04 dt: 3332.13ms, tok/sec:157343.18
step 5046, loss: 3.520146, norm:0.2659, lr:4.9022e-04 dt: 3332.14ms, tok/sec:157342.60
step 5047, loss: 3.469378, norm:0.2444, lr:4.9017e-04 dt: 3332.17ms, tok/sec:157341.12
step 5048, loss: 3.475739, norm:0.2634, lr:4.9012e-04 dt: 3332.07ms, tok/sec:157345.81
step 5049, loss: 3.490453, norm:0.2891, lr:4.9008e-04 dt: 3332.07ms, tok/sec:157346.09
HellaSwag accuracy:2325075327704040529/-2=-1162537663852020224.0000
rank 1 sample 0: Hello, I'm a language model, where you try to build your own by creating custom, unique experiences.
That's it: if you've got a
rank 1 sample 1: Hello, I'm a language model, a model, and a model in general. I'm pretty sure I'm able to build the model. I'm pretty
rank 1 sample 2: Hello, I'm a language model, so every time I'm in a language model, I'll give it some reason to think that if it were an event
rank 1 sample 3: Hello, I'm a language model, and I'm trying to think about why something is such dangerous. Why don't we look at that?
First,
rank 0 sample 0: Hello, I'm a language model, and I think it is one you might call more than one language for learning languages. If you can use language models for
rank 0 sample 1: Hello, I'm a language model, but we're only a language model is a computer
machine, which just happens to make us understand languages like English and
rank 0 sample 2: Hello, I'm a language model, so I use it almost perfectly. If you do it, your English grammar will be better than English.
So as
rank 0 sample 3: Hello, I'm a language model, but one that doesn't have the English equivalent equivalent. I also need to be as simple as I've been learning as
step 5050, loss: 3.462612, norm:0.2809, lr:4.9003e-04 dt: 48513.30ms, tok/sec:10807.10
step 5051, loss: 3.474763, norm:0.2797, lr:4.8998e-04 dt: 3332.11ms, tok/sec:157344.16
step 5052, loss: 3.458199, norm:0.2638, lr:4.8994e-04 dt: 3332.04ms, tok/sec:157347.26
step 5053, loss: 3.465079, norm:0.2828, lr:4.8989e-04 dt: 3332.04ms, tok/sec:157347.24
step 5054, loss: 3.549030, norm:0.2847, lr:4.8984e-04 dt: 3332.04ms, tok/sec:157347.27
step 5055, loss: 3.476848, norm:0.2616, lr:4.8979e-04 dt: 3332.22ms, tok/sec:157338.97
step 5056, loss: 3.521590, norm:0.2742, lr:4.8975e-04 dt: 3332.25ms, tok/sec:157337.55
step 5057, loss: 3.391055, norm:0.2724, lr:4.8970e-04 dt: 3332.12ms, tok/sec:157343.64
step 5058, loss: 3.548823, norm:0.3071, lr:4.8965e-04 dt: 3332.46ms, tok/sec:157327.80
step 5059, loss: 3.484814, norm:0.2730, lr:4.8961e-04 dt: 3332.17ms, tok/sec:157341.36
step 5060, loss: 3.406260, norm:0.2837, lr:4.8956e-04 dt: 3332.44ms, tok/sec:157328.54
step 5061, loss: 3.372724, norm:0.2555, lr:4.8951e-04 dt: 3332.17ms, tok/sec:157341.44
step 5062, loss: 3.414992, norm:0.2604, lr:4.8947e-04 dt: 3331.93ms, tok/sec:157352.60
step 5063, loss: 3.442034, norm:0.2724, lr:4.8942e-04 dt: 3331.92ms, tok/sec:157352.89
step 5064, loss: 3.428688, norm:0.2628, lr:4.8937e-04 dt: 3332.22ms, tok/sec:157338.83
step 5065, loss: 3.423680, norm:0.2604, lr:4.8932e-04 dt: 3332.16ms, tok/sec:157341.59
step 5066, loss: 3.441582, norm:0.2455, lr:4.8928e-04 dt: 3332.09ms, tok/sec:157345.18
step 5067, loss: 3.436284, norm:0.2426, lr:4.8923e-04 dt: 3332.34ms, tok/sec:157333.30
step 5068, loss: 3.427990, norm:0.2408, lr:4.8918e-04 dt: 3332.16ms, tok/sec:157341.60
step 5069, loss: 3.388513, norm:0.2434, lr:4.8914e-04 dt: 3332.12ms, tok/sec:157343.80
step 5070, loss: 3.379947, norm:0.2403, lr:4.8909e-04 dt: 3332.10ms, tok/sec:157344.64
step 5071, loss: 3.415479, norm:0.2510, lr:4.8904e-04 dt: 3332.02ms, tok/sec:157348.36
step 5072, loss: 3.196947, norm:0.2960, lr:4.8899e-04 dt: 3332.01ms, tok/sec:157349.03
step 5073, loss: 3.132154, norm:0.2573, lr:4.8895e-04 dt: 3332.05ms, tok/sec:157347.01
step 5074, loss: 3.197459, norm:0.2887, lr:4.8890e-04 dt: 3332.00ms, tok/sec:157349.26
step 5075, loss: 3.200415, norm:0.3006, lr:4.8885e-04 dt: 3331.91ms, tok/sec:157353.62
step 5076, loss: 3.222522, norm:0.2742, lr:4.8881e-04 dt: 3332.25ms, tok/sec:157337.54
step 5077, loss: 3.172583, norm:0.2572, lr:4.8876e-04 dt: 3332.27ms, tok/sec:157336.49
step 5078, loss: 3.230592, norm:0.2691, lr:4.8871e-04 dt: 3331.95ms, tok/sec:157351.64
step 5079, loss: 3.186484, norm:0.2571, lr:4.8866e-04 dt: 3331.85ms, tok/sec:157356.44
step 5080, loss: 3.250268, norm:0.2516, lr:4.8862e-04 dt: 3332.21ms, tok/sec:157339.48
step 5081, loss: 3.238959, norm:0.2408, lr:4.8857e-04 dt: 3331.88ms, tok/sec:157354.87
step 5082, loss: 3.231251, norm:0.2383, lr:4.8852e-04 dt: 3332.10ms, tok/sec:157344.43
step 5083, loss: 3.455954, norm:0.2467, lr:4.8848e-04 dt: 3332.10ms, tok/sec:157344.75
step 5084, loss: 3.471059, norm:0.2429, lr:4.8843e-04 dt: 3332.11ms, tok/sec:157344.27
step 5085, loss: 3.487662, norm:0.2607, lr:4.8838e-04 dt: 3331.85ms, tok/sec:157356.27
step 5086, loss: 3.485679, norm:0.2740, lr:4.8833e-04 dt: 3332.39ms, tok/sec:157331.09
step 5087, loss: 3.512842, norm:0.2545, lr:4.8829e-04 dt: 3332.10ms, tok/sec:157344.44
step 5088, loss: 3.488996, norm:0.2510, lr:4.8824e-04 dt: 3332.17ms, tok/sec:157341.18
step 5089, loss: 3.484803, norm:0.2506, lr:4.8819e-04 dt: 3332.06ms, tok/sec:157346.70
step 5090, loss: 3.503846, norm:0.2493, lr:4.8814e-04 dt: 3332.39ms, tok/sec:157330.90
step 5091, loss: 3.465779, norm:0.2677, lr:4.8810e-04 dt: 3332.11ms, tok/sec:157344.34
step 5092, loss: 3.477124, norm:0.2949, lr:4.8805e-04 dt: 3332.31ms, tok/sec:157334.74
step 5093, loss: 3.527962, norm:0.2912, lr:4.8800e-04 dt: 3331.98ms, tok/sec:157350.17
step 5094, loss: 3.499506, norm:0.2985, lr:4.8796e-04 dt: 3331.94ms, tok/sec:157351.97
step 5095, loss: 3.515202, norm:0.2928, lr:4.8791e-04 dt: 3332.60ms, tok/sec:157320.98
step 5096, loss: 3.488232, norm:0.2780, lr:4.8786e-04 dt: 3332.22ms, tok/sec:157339.17
step 5097, loss: 3.490401, norm:0.2864, lr:4.8781e-04 dt: 3332.04ms, tok/sec:157347.27
step 5098, loss: 3.511268, norm:0.2844, lr:4.8777e-04 dt: 3332.02ms, tok/sec:157348.29
step 5099, loss: 3.483546, norm:0.2845, lr:4.8772e-04 dt: 3332.33ms, tok/sec:157333.57
validation loss: 3.4651
Model and optimizer state saved.
HellaSwag accuracy:2325251215204746257/-2=-1162625607602373120.0000
rank 1 sample 0: Hello, I'm a language model, and I'll be doing the same until you click it. We'll be answering questions like this one, so I'll
rank 1 sample 1: Hello, I'm a language model, which I can follow and use as a template for other websites. I really liked this as it so I can see what
rank 1 sample 2: Hello, I'm a language model, but after the time I've been working on my PhD in Human Cognitive Science. I've been teaching the subject with a
rank 1 sample 3: Hello, I'm a language model, and I'm trying to model what this site is saying!"
For instance, I learned which site is the correct one
rank 0 sample 0: Hello, I'm a language model, and I'd like you to do one every day.
- This way to get an idea of the difference between an
rank 0 sample 1: Hello, I'm a language model, I was only a few years old and I am interested in how language plays into language development.
Here is a bit
rank 0 sample 2: Hello, I'm a language model, so I see the basic way to get to our language model from scratch.
This is the first step in my job
rank 0 sample 3: Hello, I'm a language model, I do not want to write anything out in life. I think one of those statements was made. "I'm talking
step 5100, loss: 3.491594, norm:0.2825, lr:4.8767e-04 dt: 56200.74ms, tok/sec:9328.85
step 5101, loss: 3.511435, norm:0.2722, lr:4.8762e-04 dt: 3332.06ms, tok/sec:157346.70
step 5102, loss: 3.455243, norm:0.2833, lr:4.8758e-04 dt: 3332.16ms, tok/sec:157341.96
step 5103, loss: 3.592117, norm:0.2686, lr:4.8753e-04 dt: 3331.98ms, tok/sec:157350.35
step 5104, loss: 3.472978, norm:0.3433, lr:4.8748e-04 dt: 3331.81ms, tok/sec:157358.27
step 5105, loss: 3.491994, norm:0.2710, lr:4.8743e-04 dt: 3332.28ms, tok/sec:157336.23
step 5106, loss: 3.487781, norm:0.3087, lr:4.8739e-04 dt: 3332.12ms, tok/sec:157343.90
step 5107, loss: 3.431464, norm:0.2607, lr:4.8734e-04 dt: 3331.87ms, tok/sec:157355.55
step 5108, loss: 3.505933, norm:0.3184, lr:4.8729e-04 dt: 3332.01ms, tok/sec:157348.87
step 5109, loss: 3.391714, norm:0.2829, lr:4.8725e-04 dt: 3332.32ms, tok/sec:157334.39
step 5110, loss: 3.436522, norm:0.2768, lr:4.8720e-04 dt: 3331.92ms, tok/sec:157353.21
step 5111, loss: 3.416609, norm:0.2800, lr:4.8715e-04 dt: 3332.36ms, tok/sec:157332.50
step 5112, loss: 3.440054, norm:0.2817, lr:4.8710e-04 dt: 3331.96ms, tok/sec:157351.38
step 5113, loss: 3.419017, norm:0.2672, lr:4.8706e-04 dt: 3332.19ms, tok/sec:157340.41
step 5114, loss: 3.381747, norm:0.2652, lr:4.8701e-04 dt: 3332.08ms, tok/sec:157345.66
step 5115, loss: 3.450708, norm:0.2490, lr:4.8696e-04 dt: 3332.37ms, tok/sec:157331.84
step 5116, loss: 3.400681, norm:0.2384, lr:4.8691e-04 dt: 3332.35ms, tok/sec:157332.86
step 5117, loss: 3.430346, norm:0.2378, lr:4.8687e-04 dt: 3332.17ms, tok/sec:157341.54
step 5118, loss: 3.289197, norm:0.2713, lr:4.8682e-04 dt: 3331.97ms, tok/sec:157350.97
step 5119, loss: 3.310801, norm:0.2546, lr:4.8677e-04 dt: 3332.18ms, tok/sec:157341.02
step 5120, loss: 3.277059, norm:0.2825, lr:4.8672e-04 dt: 3332.04ms, tok/sec:157347.31
step 5121, loss: 3.172974, norm:0.2472, lr:4.8668e-04 dt: 3331.99ms, tok/sec:157349.99
step 5122, loss: 3.171747, norm:0.2621, lr:4.8663e-04 dt: 3332.07ms, tok/sec:157346.12
step 5123, loss: 3.216025, norm:0.2476, lr:4.8658e-04 dt: 3332.16ms, tok/sec:157341.62
step 5124, loss: 3.179120, norm:0.2333, lr:4.8653e-04 dt: 3332.22ms, tok/sec:157339.04
step 5125, loss: 3.173035, norm:0.2380, lr:4.8649e-04 dt: 3331.92ms, tok/sec:157353.20
step 5126, loss: 3.201304, norm:0.2255, lr:4.8644e-04 dt: 3331.84ms, tok/sec:157357.03
step 5127, loss: 3.250285, norm:0.2530, lr:4.8639e-04 dt: 3332.21ms, tok/sec:157339.34
step 5128, loss: 3.165236, norm:0.2292, lr:4.8634e-04 dt: 3331.96ms, tok/sec:157351.12
step 5129, loss: 3.191001, norm:0.2462, lr:4.8630e-04 dt: 3332.14ms, tok/sec:157342.79
step 5130, loss: 3.468058, norm:0.2635, lr:4.8625e-04 dt: 3332.35ms, tok/sec:157332.77
step 5131, loss: 3.510313, norm:0.2527, lr:4.8620e-04 dt: 3332.34ms, tok/sec:157333.21
step 5132, loss: 3.521664, norm:0.2478, lr:4.8615e-04 dt: 3332.02ms, tok/sec:157348.38
step 5133, loss: 3.461330, norm:0.2392, lr:4.8611e-04 dt: 3332.09ms, tok/sec:157345.31
step 5134, loss: 3.440945, norm:0.2426, lr:4.8606e-04 dt: 3332.27ms, tok/sec:157336.39
step 5135, loss: 3.473123, norm:0.2547, lr:4.8601e-04 dt: 3331.89ms, tok/sec:157354.70
step 5136, loss: 3.491899, norm:0.2422, lr:4.8596e-04 dt: 3332.06ms, tok/sec:157346.67
step 5137, loss: 3.470180, norm:0.2598, lr:4.8591e-04 dt: 3332.19ms, tok/sec:157340.33
step 5138, loss: 3.506381, norm:0.2545, lr:4.8587e-04 dt: 3332.40ms, tok/sec:157330.56
step 5139, loss: 3.517365, norm:0.2607, lr:4.8582e-04 dt: 3332.19ms, tok/sec:157340.23
step 5140, loss: 3.435311, norm:0.2753, lr:4.8577e-04 dt: 3332.17ms, tok/sec:157341.19
step 5141, loss: 3.455832, norm:0.2833, lr:4.8572e-04 dt: 3332.27ms, tok/sec:157336.70
step 5142, loss: 3.521307, norm:0.2648, lr:4.8568e-04 dt: 3332.47ms, tok/sec:157327.04
step 5143, loss: 3.490538, norm:0.2507, lr:4.8563e-04 dt: 3334.48ms, tok/sec:157232.22
step 5144, loss: 3.465885, norm:0.2621, lr:4.8558e-04 dt: 3332.52ms, tok/sec:157324.96
step 5145, loss: 3.481857, norm:0.2476, lr:4.8553e-04 dt: 3332.19ms, tok/sec:157340.53
step 5146, loss: 3.484956, norm:0.2551, lr:4.8549e-04 dt: 3332.28ms, tok/sec:157335.90
step 5147, loss: 3.454803, norm:0.2589, lr:4.8544e-04 dt: 3332.24ms, tok/sec:157338.08
step 5148, loss: 3.471566, norm:0.2417, lr:4.8539e-04 dt: 3331.93ms, tok/sec:157352.86
step 5149, loss: 3.492470, norm:0.2592, lr:4.8534e-04 dt: 3332.21ms, tok/sec:157339.57
HellaSwag accuracy:2325242419111758857/-2=-1162621209555879424.0000
rank 1 sample 0: Hello, I'm a language model, and so I'm pretty good at languages like Perl and Ruby, but I prefer the language from Ruby, because I'm
rank 1 sample 1: Hello, I'm a language model, which means I don't have to have access to this server. Here's some practice exercise. Note that I've been
rank 1 sample 2: Hello, I'm a language model, I say it's a language model. I'm the first in my class, and I've learned that. The Language
rank 1 sample 3: Hello, I'm a language model, and I'm looking at other ways to produce a program we have that allows someone to design my programs.
We've
rank 0 sample 0: Hello, I'm a language model, and I hope you will continue your education :)
Thanks for your reading! This is your site and we're glad to
rank 0 sample 1: Hello, I'm a language model, so my ESL students will be speaking out their own foreign language and they will understand what you say, if you think that
rank 0 sample 2: Hello, I'm a language model, so I guess that by way of explanation, "he" in a sentence is what we are talking about. I found
rank 0 sample 3: Hello, I'm a language model, I use a set of rules that is used the same way. That's what is really a lot of fun and can
step 5150, loss: 3.477607, norm:0.2497, lr:4.8530e-04 dt: 48529.63ms, tok/sec:10803.46
step 5151, loss: 3.491982, norm:0.2427, lr:4.8525e-04 dt: 3332.19ms, tok/sec:157340.32
step 5152, loss: 3.458884, norm:0.2589, lr:4.8520e-04 dt: 3332.31ms, tok/sec:157334.67
step 5153, loss: 3.435517, norm:0.2402, lr:4.8515e-04 dt: 3332.13ms, tok/sec:157343.18
step 5154, loss: 3.434946, norm:0.2575, lr:4.8510e-04 dt: 3332.14ms, tok/sec:157342.65
step 5155, loss: 3.414671, norm:0.2337, lr:4.8506e-04 dt: 3331.97ms, tok/sec:157350.95
step 5156, loss: 3.410448, norm:0.2604, lr:4.8501e-04 dt: 3331.99ms, tok/sec:157349.71
step 5157, loss: 3.425879, norm:0.2464, lr:4.8496e-04 dt: 3332.02ms, tok/sec:157348.24
step 5158, loss: 3.370709, norm:0.2431, lr:4.8491e-04 dt: 3332.25ms, tok/sec:157337.35
step 5159, loss: 3.422132, norm:0.2490, lr:4.8487e-04 dt: 3331.81ms, tok/sec:157358.13
step 5160, loss: 3.410128, norm:0.2348, lr:4.8482e-04 dt: 3332.04ms, tok/sec:157347.45
step 5161, loss: 3.451951, norm:0.2537, lr:4.8477e-04 dt: 3332.45ms, tok/sec:157328.04
step 5162, loss: 3.402796, norm:0.2384, lr:4.8472e-04 dt: 3332.05ms, tok/sec:157346.84
step 5163, loss: 3.425425, norm:0.2394, lr:4.8467e-04 dt: 3331.94ms, tok/sec:157352.23
step 5164, loss: 3.238718, norm:0.2716, lr:4.8463e-04 dt: 3331.92ms, tok/sec:157353.19
step 5165, loss: 3.192925, norm:0.2803, lr:4.8458e-04 dt: 3332.09ms, tok/sec:157345.22
step 5166, loss: 3.188807, norm:0.2715, lr:4.8453e-04 dt: 3332.04ms, tok/sec:157347.29
step 5167, loss: 3.274050, norm:0.2560, lr:4.8448e-04 dt: 3331.92ms, tok/sec:157352.94
step 5168, loss: 3.147795, norm:0.2681, lr:4.8444e-04 dt: 3332.00ms, tok/sec:157349.19
step 5169, loss: 3.226319, norm:0.2626, lr:4.8439e-04 dt: 3331.87ms, tok/sec:157355.66
step 5170, loss: 3.176596, norm:0.2845, lr:4.8434e-04 dt: 3332.49ms, tok/sec:157326.42
step 5171, loss: 3.206308, norm:0.2731, lr:4.8429e-04 dt: 3331.92ms, tok/sec:157353.11
step 5172, loss: 3.196314, norm:0.2629, lr:4.8424e-04 dt: 3331.90ms, tok/sec:157353.96
step 5173, loss: 3.261643, norm:0.2711, lr:4.8420e-04 dt: 3331.93ms, tok/sec:157352.62
step 5174, loss: 3.212593, norm:0.2488, lr:4.8415e-04 dt: 3332.26ms, tok/sec:157337.19
step 5175, loss: 3.301713, norm:0.2661, lr:4.8410e-04 dt: 3331.78ms, tok/sec:157359.60
step 5176, loss: 3.407310, norm:0.2736, lr:4.8405e-04 dt: 3332.05ms, tok/sec:157347.05
step 5177, loss: 3.520468, norm:0.3378, lr:4.8400e-04 dt: 3332.43ms, tok/sec:157329.08
step 5178, loss: 3.447078, norm:0.2969, lr:4.8396e-04 dt: 3332.06ms, tok/sec:157346.33
step 5179, loss: 3.436142, norm:0.2618, lr:4.8391e-04 dt: 3332.51ms, tok/sec:157325.23
step 5180, loss: 3.503610, norm:0.3062, lr:4.8386e-04 dt: 3332.08ms, tok/sec:157345.63
step 5181, loss: 3.447550, norm:0.2520, lr:4.8381e-04 dt: 3331.87ms, tok/sec:157355.30
step 5182, loss: 3.407098, norm:0.2695, lr:4.8376e-04 dt: 3332.25ms, tok/sec:157337.57
step 5183, loss: 3.491316, norm:0.2918, lr:4.8372e-04 dt: 3332.00ms, tok/sec:157349.41
step 5184, loss: 3.497557, norm:0.2964, lr:4.8367e-04 dt: 3332.00ms, tok/sec:157349.58
step 5185, loss: 3.570483, norm:0.3475, lr:4.8362e-04 dt: 3332.07ms, tok/sec:157346.25
step 5186, loss: 3.488332, norm:0.2848, lr:4.8357e-04 dt: 3332.28ms, tok/sec:157336.28
step 5187, loss: 3.437746, norm:0.2741, lr:4.8353e-04 dt: 3332.32ms, tok/sec:157334.25
step 5188, loss: 3.496003, norm:0.2899, lr:4.8348e-04 dt: 3332.17ms, tok/sec:157341.09
step 5189, loss: 3.481062, norm:0.2595, lr:4.8343e-04 dt: 3332.21ms, tok/sec:157339.36
step 5190, loss: 3.443846, norm:0.2533, lr:4.8338e-04 dt: 3332.14ms, tok/sec:157342.82
step 5191, loss: 3.433893, norm:0.2429, lr:4.8333e-04 dt: 3331.96ms, tok/sec:157351.37
step 5192, loss: 3.439351, norm:0.2509, lr:4.8329e-04 dt: 3331.99ms, tok/sec:157349.97
step 5193, loss: 3.441949, norm:0.2581, lr:4.8324e-04 dt: 3331.85ms, tok/sec:157356.48
step 5194, loss: 3.493254, norm:0.2730, lr:4.8319e-04 dt: 3332.12ms, tok/sec:157343.55
step 5195, loss: 3.515562, norm:0.2713, lr:4.8314e-04 dt: 3332.32ms, tok/sec:157334.39
step 5196, loss: 3.441661, norm:0.2485, lr:4.8309e-04 dt: 3332.42ms, tok/sec:157329.41
step 5197, loss: 3.544880, norm:0.2505, lr:4.8305e-04 dt: 3332.20ms, tok/sec:157339.77
step 5198, loss: 3.492785, norm:0.2592, lr:4.8300e-04 dt: 3332.15ms, tok/sec:157342.29
step 5199, loss: 3.441490, norm:0.2518, lr:4.8295e-04 dt: 3332.17ms, tok/sec:157341.51
validation loss: 3.4603
Model and optimizer state saved.
HellaSwag accuracy:-2286452397558234103/-2=1143226198779117056.0000
rank 1 sample 0: Hello, I'm a language model, with all of the steps and tricks described, including some language elements.
To start off, it's pretty easy to
rank 1 sample 1: Hello, I'm a language model, you know how to say, what a language model is when you just say that, this is only the beginning.

rank 1 sample 2: Hello, I'm a language model, so many things that I'm not aware of.
But here are the rules that I have. The first line of
rank 1 sample 3: Hello, I'm a language model, and I'm the only two people out there.
Thank you a very kindly thank you!"
"I'm very
rank 0 sample 0: Hello, I'm a language model, and I'd like you to see your English translations.
- Read with my English translator,
- Take a minute
rank 0 sample 1: Hello, I'm a language model, I was already familiar with the term my mother had while you're a puppy.
The "I can't find"
rank 0 sample 2: Hello, I'm a language model, but I could be really very flexible for a lot of people (but I'm pretty much a language modeler) to
rank 0 sample 3: Hello, I'm a language model, I could add an extra line of
1 instead of the actual. But if there are an extra line of
that
step 5200, loss: 3.446233, norm:0.2668, lr:4.8290e-04 dt: 56248.25ms, tok/sec:9320.97
step 5201, loss: 3.420346, norm:0.2442, lr:4.8285e-04 dt: 3332.19ms, tok/sec:157340.57
step 5202, loss: 3.473616, norm:0.2940, lr:4.8280e-04 dt: 3332.00ms, tok/sec:157349.24
step 5203, loss: 3.396654, norm:0.2540, lr:4.8276e-04 dt: 3332.16ms, tok/sec:157341.73
step 5204, loss: 3.433686, norm:0.2638, lr:4.8271e-04 dt: 3332.31ms, tok/sec:157334.50
step 5205, loss: 3.381890, norm:0.2567, lr:4.8266e-04 dt: 3332.72ms, tok/sec:157315.25
step 5206, loss: 3.361686, norm:0.2743, lr:4.8261e-04 dt: 3332.09ms, tok/sec:157345.12
step 5207, loss: 3.426787, norm:0.2859, lr:4.8256e-04 dt: 3332.06ms, tok/sec:157346.38
step 5208, loss: 3.415375, norm:0.2623, lr:4.8252e-04 dt: 3331.95ms, tok/sec:157351.47
step 5209, loss: 3.378850, norm:0.2555, lr:4.8247e-04 dt: 3332.33ms, tok/sec:157333.86
step 5210, loss: 3.424085, norm:0.2603, lr:4.8242e-04 dt: 3332.16ms, tok/sec:157341.78
step 5211, loss: 3.272379, norm:0.2620, lr:4.8237e-04 dt: 3332.07ms, tok/sec:157346.04
step 5212, loss: 3.251276, norm:0.2655, lr:4.8232e-04 dt: 3332.41ms, tok/sec:157330.17
step 5213, loss: 3.184385, norm:0.2476, lr:4.8228e-04 dt: 3332.30ms, tok/sec:157335.01
step 5214, loss: 3.190442, norm:0.2882, lr:4.8223e-04 dt: 3332.06ms, tok/sec:157346.74
step 5215, loss: 3.253440, norm:0.2570, lr:4.8218e-04 dt: 3331.99ms, tok/sec:157349.87
step 5216, loss: 3.182173, norm:0.2643, lr:4.8213e-04 dt: 3331.83ms, tok/sec:157357.27
step 5217, loss: 3.224569, norm:0.2387, lr:4.8208e-04 dt: 3332.08ms, tok/sec:157345.41
step 5218, loss: 3.199659, norm:0.2570, lr:4.8203e-04 dt: 3332.12ms, tok/sec:157343.57
step 5219, loss: 3.229830, norm:0.2429, lr:4.8199e-04 dt: 3331.79ms, tok/sec:157359.44
step 5220, loss: 3.224201, norm:0.2617, lr:4.8194e-04 dt: 3332.13ms, tok/sec:157343.43
step 5221, loss: 3.213995, norm:0.2383, lr:4.8189e-04 dt: 3332.22ms, tok/sec:157339.13
step 5222, loss: 3.304845, norm:0.2552, lr:4.8184e-04 dt: 3332.33ms, tok/sec:157333.72
step 5223, loss: 3.433551, norm:0.2677, lr:4.8179e-04 dt: 3332.02ms, tok/sec:157348.55
step 5224, loss: 3.440940, norm:0.2594, lr:4.8175e-04 dt: 3331.98ms, tok/sec:157350.42
step 5225, loss: 3.419014, norm:0.2678, lr:4.8170e-04 dt: 3332.01ms, tok/sec:157348.69
step 5226, loss: 3.430817, norm:0.2417, lr:4.8165e-04 dt: 3332.02ms, tok/sec:157348.48
step 5227, loss: 3.435833, norm:0.2612, lr:4.8160e-04 dt: 3332.07ms, tok/sec:157346.03
step 5228, loss: 3.470350, norm:0.2327, lr:4.8155e-04 dt: 3332.12ms, tok/sec:157343.56
step 5229, loss: 3.465663, norm:0.2746, lr:4.8150e-04 dt: 3332.12ms, tok/sec:157343.55
step 5230, loss: 3.435919, norm:0.2606, lr:4.8146e-04 dt: 3332.02ms, tok/sec:157348.21
step 5231, loss: 3.397794, norm:0.2707, lr:4.8141e-04 dt: 3332.22ms, tok/sec:157338.90
step 5232, loss: 3.458177, norm:0.2504, lr:4.8136e-04 dt: 3332.11ms, tok/sec:157344.20
step 5233, loss: 3.492842, norm:0.2847, lr:4.8131e-04 dt: 3332.28ms, tok/sec:157336.03
step 5234, loss: 3.506280, norm:0.2663, lr:4.8126e-04 dt: 3331.97ms, tok/sec:157350.57
step 5235, loss: 3.466605, norm:0.2940, lr:4.8121e-04 dt: 3332.07ms, tok/sec:157346.06
step 5236, loss: 3.446496, norm:0.2558, lr:4.8117e-04 dt: 3332.24ms, tok/sec:157337.87
step 5237, loss: 3.460651, norm:0.2786, lr:4.8112e-04 dt: 3332.20ms, tok/sec:157339.79
step 5238, loss: 3.445595, norm:0.2520, lr:4.8107e-04 dt: 3332.42ms, tok/sec:157329.41
step 5239, loss: 3.496812, norm:0.2669, lr:4.8102e-04 dt: 3331.96ms, tok/sec:157351.14
step 5240, loss: 3.387832, norm:0.2611, lr:4.8097e-04 dt: 3332.42ms, tok/sec:157329.30
step 5241, loss: 3.451481, norm:0.2588, lr:4.8092e-04 dt: 3332.11ms, tok/sec:157344.15
step 5242, loss: 3.455193, norm:0.2494, lr:4.8088e-04 dt: 3332.09ms, tok/sec:157345.15
step 5243, loss: 3.471325, norm:0.2359, lr:4.8083e-04 dt: 3332.02ms, tok/sec:157348.57
step 5244, loss: 3.463567, norm:0.2544, lr:4.8078e-04 dt: 3331.98ms, tok/sec:157350.13
step 5245, loss: 3.495789, norm:0.2719, lr:4.8073e-04 dt: 3332.08ms, tok/sec:157345.59
step 5246, loss: 3.406455, norm:0.2493, lr:4.8068e-04 dt: 3332.11ms, tok/sec:157344.17
step 5247, loss: 3.380491, norm:0.2543, lr:4.8063e-04 dt: 3332.10ms, tok/sec:157344.69
step 5248, loss: 3.378459, norm:0.2526, lr:4.8059e-04 dt: 3332.22ms, tok/sec:157338.72
step 5249, loss: 3.455486, norm:0.2980, lr:4.8054e-04 dt: 3332.15ms, tok/sec:157342.36
HellaSwag accuracy:2325233623018701905/-2=-1162616811509350912.0000
rank 1 sample 0: Hello, I'm a language model, and in the past, we have chosen the correct pronunciation when working with this language. However, if you have a language
rank 1 sample 1: Hello, I'm a language model, which I can talk about in more simple terms.
When you learn anything and aren't sure if you're going to
rank 1 sample 2: Hello, I'm a language model, but only in one language.
That's why I decided to create an introduction to the concept of "native speech"
rank 1 sample 3: Hello, I'm a language model, and I'm just trying to tell you from the source "which". In what way if you were to say 'the
rank 0 sample 0: Hello, I'm a language model, and I love to work with a number of students. I love talking about grammar, vocabulary, structure and pronunciation. The
rank 0 sample 1: Hello, I'm a language model, but a lot of it is just wrong. So lets add the variables before a variable, and we don't need to
rank 0 sample 2: Hello, I'm a language model, so I understand how to construct a programming language as well as to create a programming language for a particular purpose. I do
rank 0 sample 3: Hello, I'm a language model, but also a programming language.
Let’s say you’re getting up in a business. The world
step 5250, loss: 3.410720, norm:0.2785, lr:4.8049e-04 dt: 48519.82ms, tok/sec:10805.65
step 5251, loss: 3.407558, norm:0.2601, lr:4.8044e-04 dt: 3332.15ms, tok/sec:157342.05
step 5252, loss: 3.399775, norm:0.2717, lr:4.8039e-04 dt: 3332.54ms, tok/sec:157323.95
step 5253, loss: 3.340238, norm:0.2552, lr:4.8034e-04 dt: 3332.03ms, tok/sec:157347.88
step 5254, loss: 3.410907, norm:0.2550, lr:4.8029e-04 dt: 3332.01ms, tok/sec:157348.96
step 5255, loss: 3.354417, norm:0.2504, lr:4.8025e-04 dt: 3332.12ms, tok/sec:157343.61
step 5256, loss: 3.416279, norm:0.2620, lr:4.8020e-04 dt: 3332.05ms, tok/sec:157347.08
step 5257, loss: 3.213679, norm:0.2945, lr:4.8015e-04 dt: 3331.86ms, tok/sec:157355.81
step 5258, loss: 3.260208, norm:0.3151, lr:4.8010e-04 dt: 3331.96ms, tok/sec:157351.01
step 5259, loss: 3.235193, norm:0.2712, lr:4.8005e-04 dt: 3332.03ms, tok/sec:157347.99
step 5260, loss: 3.215227, norm:0.3047, lr:4.8000e-04 dt: 3332.10ms, tok/sec:157344.53
step 5261, loss: 3.174271, norm:0.2823, lr:4.7996e-04 dt: 3332.28ms, tok/sec:157335.93
step 5262, loss: 3.156306, norm:0.2490, lr:4.7991e-04 dt: 3331.71ms, tok/sec:157362.93
step 5263, loss: 3.157689, norm:0.2374, lr:4.7986e-04 dt: 3332.05ms, tok/sec:157347.00
step 5264, loss: 3.210266, norm:0.2555, lr:4.7981e-04 dt: 3331.99ms, tok/sec:157349.87
step 5265, loss: 3.184940, norm:0.2400, lr:4.7976e-04 dt: 3331.92ms, tok/sec:157353.32
step 5266, loss: 3.183084, norm:0.2370, lr:4.7971e-04 dt: 3331.82ms, tok/sec:157357.72
step 5267, loss: 3.169479, norm:0.2503, lr:4.7966e-04 dt: 3331.87ms, tok/sec:157355.53
step 5268, loss: 3.322734, norm:0.2633, lr:4.7962e-04 dt: 3332.70ms, tok/sec:157316.50
step 5269, loss: 3.470867, norm:0.2778, lr:4.7957e-04 dt: 3332.08ms, tok/sec:157345.53
step 5270, loss: 3.438098, norm:0.2900, lr:4.7952e-04 dt: 3332.58ms, tok/sec:157321.79
step 5271, loss: 3.426493, norm:0.2400, lr:4.7947e-04 dt: 3332.19ms, tok/sec:157340.47
step 5272, loss: 3.450980, norm:0.2679, lr:4.7942e-04 dt: 3332.00ms, tok/sec:157349.27
step 5273, loss: 3.406019, norm:0.2579, lr:4.7937e-04 dt: 3332.12ms, tok/sec:157343.51
step 5274, loss: 3.557374, norm:0.3067, lr:4.7932e-04 dt: 3332.02ms, tok/sec:157348.17
step 5275, loss: 3.398987, norm:0.2804, lr:4.7928e-04 dt: 3332.02ms, tok/sec:157348.40
step 5276, loss: 3.467970, norm:0.2608, lr:4.7923e-04 dt: 3331.98ms, tok/sec:157350.30
step 5277, loss: 3.475562, norm:0.2788, lr:4.7918e-04 dt: 3332.21ms, tok/sec:157339.45
step 5278, loss: 3.343238, norm:0.2704, lr:4.7913e-04 dt: 3331.93ms, tok/sec:157352.82
step 5279, loss: 3.479398, norm:0.2552, lr:4.7908e-04 dt: 3332.47ms, tok/sec:157327.21
step 5280, loss: 3.480761, norm:0.2545, lr:4.7903e-04 dt: 3332.00ms, tok/sec:157349.56
step 5281, loss: 3.537486, norm:0.2713, lr:4.7898e-04 dt: 3332.11ms, tok/sec:157344.00
step 5282, loss: 3.486598, norm:0.2889, lr:4.7894e-04 dt: 3332.48ms, tok/sec:157326.45
step 5283, loss: 3.489232, norm:0.2840, lr:4.7889e-04 dt: 3332.24ms, tok/sec:157337.83
step 5284, loss: 3.475278, norm:0.2978, lr:4.7884e-04 dt: 3332.17ms, tok/sec:157341.55
step 5285, loss: 3.503936, norm:0.2853, lr:4.7879e-04 dt: 3331.92ms, tok/sec:157353.16
step 5286, loss: 3.464772, norm:0.2912, lr:4.7874e-04 dt: 3332.28ms, tok/sec:157335.98
step 5287, loss: 3.523723, norm:0.2758, lr:4.7869e-04 dt: 3332.34ms, tok/sec:157333.29
step 5288, loss: 3.541776, norm:0.2821, lr:4.7864e-04 dt: 3331.99ms, tok/sec:157349.69
step 5289, loss: 3.494957, norm:0.2859, lr:4.7859e-04 dt: 3332.10ms, tok/sec:157344.61
step 5290, loss: 3.494589, norm:0.2541, lr:4.7855e-04 dt: 3331.88ms, tok/sec:157355.00
step 5291, loss: 3.443378, norm:0.2748, lr:4.7850e-04 dt: 3332.21ms, tok/sec:157339.23
step 5292, loss: 3.401659, norm:0.2758, lr:4.7845e-04 dt: 3332.20ms, tok/sec:157339.92
step 5293, loss: 3.424291, norm:0.2537, lr:4.7840e-04 dt: 3332.26ms, tok/sec:157337.23
step 5294, loss: 3.446808, norm:0.2591, lr:4.7835e-04 dt: 3332.18ms, tok/sec:157341.00
step 5295, loss: 3.357561, norm:0.2466, lr:4.7830e-04 dt: 3332.03ms, tok/sec:157347.76
step 5296, loss: 3.408058, norm:0.2488, lr:4.7825e-04 dt: 3332.49ms, tok/sec:157326.33
step 5297, loss: 3.402965, norm:0.2452, lr:4.7820e-04 dt: 3332.12ms, tok/sec:157343.69
step 5298, loss: 3.387778, norm:0.2523, lr:4.7816e-04 dt: 3332.52ms, tok/sec:157324.70
step 5299, loss: 3.364446, norm:0.2529, lr:4.7811e-04 dt: 3332.16ms, tok/sec:157341.75
validation loss: 3.4535
Model and optimizer state saved.
HellaSwag accuracy:2325092919890084945/-2=-1162546459945042432.0000
rank 1 sample 0: Hello, I'm a language model, an artist, and a great guy just because, he's kind of a pretty good art to have. And I'm
rank 1 sample 1: Hello, I'm a language model, a computer, and a computer. In case you know any of these things, I get to pick a language that is
rank 1 sample 2: Hello, I'm a language model, but even in an age where I'm not a native speaker…”
“My favorite way of using languages
rank 1 sample 3: Hello, I'm a language model, and I'm writing code!
Now from the very beginnings of what makes Web 2 quite basic, it's time to
rank 0 sample 0: Hello, I'm a language model, and I hope to help you get better started.
Click here to enter all the symbols in that file below. Then
rank 0 sample 1: Hello, I'm a language model, so there's a lot of fun of it! Check out my list of my top languages, I can write sentences in
rank 0 sample 2: Hello, I'm a language model, so I was the first to come along. Now I'm doing the same thing.
The first thing I'm going
rank 0 sample 3: Hello, I'm a language model, you do not have to write your own code by hand.
My question: In case you are planning to join some
step 5300, loss: 3.383710, norm:0.2493, lr:4.7806e-04 dt: 56213.78ms, tok/sec:9326.68
step 5301, loss: 3.422130, norm:0.2382, lr:4.7801e-04 dt: 3332.00ms, tok/sec:157349.54
step 5302, loss: 3.429458, norm:0.2817, lr:4.7796e-04 dt: 3332.37ms, tok/sec:157332.01
step 5303, loss: 3.295292, norm:0.2404, lr:4.7791e-04 dt: 3332.32ms, tok/sec:157334.17
step 5304, loss: 3.185565, norm:0.2495, lr:4.7786e-04 dt: 3332.06ms, tok/sec:157346.70
step 5305, loss: 3.156404, norm:0.2353, lr:4.7781e-04 dt: 3331.92ms, tok/sec:157353.17
step 5306, loss: 3.197128, norm:0.2593, lr:4.7777e-04 dt: 3332.09ms, tok/sec:157345.06
step 5307, loss: 3.241322, norm:0.2417, lr:4.7772e-04 dt: 3331.98ms, tok/sec:157350.36
step 5308, loss: 3.200039, norm:0.2555, lr:4.7767e-04 dt: 3331.79ms, tok/sec:157359.17
step 5309, loss: 3.196293, norm:0.2453, lr:4.7762e-04 dt: 3332.19ms, tok/sec:157340.29
step 5310, loss: 3.198264, norm:0.2516, lr:4.7757e-04 dt: 3332.28ms, tok/sec:157335.89
step 5311, loss: 3.210134, norm:0.2519, lr:4.7752e-04 dt: 3332.14ms, tok/sec:157342.80
step 5312, loss: 3.279122, norm:0.2727, lr:4.7747e-04 dt: 3331.89ms, tok/sec:157354.57
step 5313, loss: 3.247870, norm:0.2491, lr:4.7742e-04 dt: 3332.05ms, tok/sec:157347.02
step 5314, loss: 3.243638, norm:0.2709, lr:4.7738e-04 dt: 3332.10ms, tok/sec:157344.62
step 5315, loss: 3.453822, norm:0.2487, lr:4.7733e-04 dt: 3331.98ms, tok/sec:157350.10
step 5316, loss: 3.533987, norm:0.2861, lr:4.7728e-04 dt: 3332.16ms, tok/sec:157341.91
step 5317, loss: 3.406571, norm:0.3938, lr:4.7723e-04 dt: 3331.61ms, tok/sec:157367.78
step 5318, loss: 3.418264, norm:0.3331, lr:4.7718e-04 dt: 3332.35ms, tok/sec:157333.02
step 5319, loss: 3.490143, norm:0.2788, lr:4.7713e-04 dt: 3332.33ms, tok/sec:157333.65
step 5320, loss: 3.499703, norm:0.3020, lr:4.7708e-04 dt: 3331.97ms, tok/sec:157350.96
step 5321, loss: 3.487966, norm:0.3289, lr:4.7703e-04 dt: 3332.15ms, tok/sec:157342.10
step 5322, loss: 3.490624, norm:0.2958, lr:4.7698e-04 dt: 3332.19ms, tok/sec:157340.22
step 5323, loss: 3.438777, norm:0.2977, lr:4.7694e-04 dt: 3332.20ms, tok/sec:157339.70
step 5324, loss: 3.493575, norm:0.2715, lr:4.7689e-04 dt: 3332.36ms, tok/sec:157332.18
step 5325, loss: 3.525643, norm:0.2931, lr:4.7684e-04 dt: 3332.51ms, tok/sec:157325.17
step 5326, loss: 3.411438, norm:0.2614, lr:4.7679e-04 dt: 3331.91ms, tok/sec:157353.43
step 5327, loss: 3.491760, norm:0.2794, lr:4.7674e-04 dt: 3332.01ms, tok/sec:157348.67
step 5328, loss: 3.434813, norm:0.2465, lr:4.7669e-04 dt: 3332.16ms, tok/sec:157341.92
step 5329, loss: 3.494622, norm:0.2615, lr:4.7664e-04 dt: 3332.00ms, tok/sec:157349.55
step 5330, loss: 3.448078, norm:0.2695, lr:4.7659e-04 dt: 3332.15ms, tok/sec:157342.28
step 5331, loss: 3.491287, norm:0.2745, lr:4.7654e-04 dt: 3332.42ms, tok/sec:157329.29
step 5332, loss: 3.486113, norm:0.2589, lr:4.7649e-04 dt: 3332.04ms, tok/sec:157347.49
step 5333, loss: 3.470952, norm:0.2662, lr:4.7645e-04 dt: 3334.23ms, tok/sec:157244.00
step 5334, loss: 3.484487, norm:0.2661, lr:4.7640e-04 dt: 3332.15ms, tok/sec:157342.34
step 5335, loss: 3.421788, norm:0.2767, lr:4.7635e-04 dt: 3332.05ms, tok/sec:157347.06
step 5336, loss: 3.477697, norm:0.2487, lr:4.7630e-04 dt: 3331.95ms, tok/sec:157351.93
step 5337, loss: 3.437697, norm:0.2648, lr:4.7625e-04 dt: 3332.07ms, tok/sec:157345.92
step 5338, loss: 3.480848, norm:0.2454, lr:4.7620e-04 dt: 3331.92ms, tok/sec:157353.09
step 5339, loss: 3.367919, norm:0.2541, lr:4.7615e-04 dt: 3332.03ms, tok/sec:157347.94
step 5340, loss: 3.428221, norm:0.2378, lr:4.7610e-04 dt: 3332.11ms, tok/sec:157344.10
step 5341, loss: 3.383910, norm:0.2441, lr:4.7605e-04 dt: 3331.90ms, tok/sec:157354.10
step 5342, loss: 3.362802, norm:0.2392, lr:4.7600e-04 dt: 3331.80ms, tok/sec:157358.82
step 5343, loss: 3.376499, norm:0.2944, lr:4.7596e-04 dt: 3332.33ms, tok/sec:157333.62
step 5344, loss: 3.353212, norm:0.2435, lr:4.7591e-04 dt: 3332.21ms, tok/sec:157339.49
step 5345, loss: 3.430618, norm:0.2706, lr:4.7586e-04 dt: 3332.04ms, tok/sec:157347.37
step 5346, loss: 3.459148, norm:0.2499, lr:4.7581e-04 dt: 3332.08ms, tok/sec:157345.58
step 5347, loss: 3.439422, norm:0.2634, lr:4.7576e-04 dt: 3332.14ms, tok/sec:157342.58
step 5348, loss: 3.444834, norm:0.2976, lr:4.7571e-04 dt: 3331.99ms, tok/sec:157349.61
step 5349, loss: 3.376159, norm:0.2552, lr:4.7566e-04 dt: 3332.05ms, tok/sec:157347.19
HellaSwag accuracy:4630937028615406673/-2=-2315468514307703296.0000
rank 1 sample 0: Hello, I'm a language model, right? I'm a language model a computer with lots of fun stuff. What I need to get to know is that
rank 1 sample 1: Hello, I'm a language model, a model that represents the language used to construct the textbox. And for my application program, I am using the language
rank 1 sample 2: Hello, I'm a language model, but no longer in the same way as I'm a language model. So to go back, let's go down to
rank 1 sample 3: Hello, I'm a language model, and I'm interested to have a few nice things I appreciate!
A large number of programs have been created in the
rank 0 sample 0: Hello, I'm a language model, and I'll be teaching English, German, I'm a language model, which is based on English and Italian. So
rank 0 sample 1: Hello, I'm a language model, I'd like to know how the code works and help those with it. But, the actual language used for this tutorial
rank 0 sample 2: Hello, I'm a language model, I'm used to train an educational library. That's why a program can be useful. I'm a language model.
rank 0 sample 3: Hello, I'm a language model, you only need to know a little more. Well, I'm the best-practise guide to using this technique."
step 5350, loss: 3.221992, norm:0.2857, lr:4.7561e-04 dt: 48514.13ms, tok/sec:10806.91
step 5351, loss: 3.180033, norm:0.2993, lr:4.7556e-04 dt: 3332.13ms, tok/sec:157343.19
step 5352, loss: 3.185215, norm:0.2755, lr:4.7551e-04 dt: 3332.17ms, tok/sec:157341.45
step 5353, loss: 3.211754, norm:0.2759, lr:4.7546e-04 dt: 3333.17ms, tok/sec:157294.16
step 5354, loss: 3.175578, norm:0.2576, lr:4.7542e-04 dt: 3332.28ms, tok/sec:157335.98
step 5355, loss: 3.232193, norm:0.2716, lr:4.7537e-04 dt: 3332.20ms, tok/sec:157339.98
step 5356, loss: 3.167049, norm:0.2433, lr:4.7532e-04 dt: 3331.91ms, tok/sec:157353.74
step 5357, loss: 3.186437, norm:0.2509, lr:4.7527e-04 dt: 3331.80ms, tok/sec:157358.88
step 5358, loss: 3.184142, norm:0.2628, lr:4.7522e-04 dt: 3332.19ms, tok/sec:157340.25
step 5359, loss: 3.228466, norm:0.2775, lr:4.7517e-04 dt: 3332.01ms, tok/sec:157349.02
step 5360, loss: 3.209652, norm:0.2726, lr:4.7512e-04 dt: 3331.93ms, tok/sec:157352.45
step 5361, loss: 3.300186, norm:0.2474, lr:4.7507e-04 dt: 3331.85ms, tok/sec:157356.27
step 5362, loss: 3.488714, norm:0.2563, lr:4.7502e-04 dt: 3332.30ms, tok/sec:157335.32
step 5363, loss: 3.577965, norm:0.3599, lr:4.7497e-04 dt: 3331.85ms, tok/sec:157356.55
step 5364, loss: 3.483691, norm:0.2814, lr:4.7492e-04 dt: 3332.36ms, tok/sec:157332.51
step 5365, loss: 3.460459, norm:0.3224, lr:4.7487e-04 dt: 3332.04ms, tok/sec:157347.39
step 5366, loss: 3.521120, norm:0.2969, lr:4.7482e-04 dt: 3332.31ms, tok/sec:157334.49
step 5367, loss: 3.489753, norm:0.2774, lr:4.7478e-04 dt: 3332.01ms, tok/sec:157349.00
step 5368, loss: 3.417928, norm:0.2747, lr:4.7473e-04 dt: 3332.03ms, tok/sec:157348.09
step 5369, loss: 3.449619, norm:0.2895, lr:4.7468e-04 dt: 3332.07ms, tok/sec:157346.07
step 5370, loss: 3.438056, norm:0.2424, lr:4.7463e-04 dt: 3332.19ms, tok/sec:157340.35
step 5371, loss: 3.426145, norm:0.2788, lr:4.7458e-04 dt: 3332.10ms, tok/sec:157344.69
step 5372, loss: 3.423912, norm:0.2540, lr:4.7453e-04 dt: 3332.26ms, tok/sec:157337.09
step 5373, loss: 3.385772, norm:0.2870, lr:4.7448e-04 dt: 3332.52ms, tok/sec:157324.97
step 5374, loss: 3.430401, norm:0.2582, lr:4.7443e-04 dt: 3332.16ms, tok/sec:157341.67
step 5375, loss: 3.473300, norm:0.2521, lr:4.7438e-04 dt: 3331.97ms, tok/sec:157350.62
step 5376, loss: 3.419882, norm:0.2393, lr:4.7433e-04 dt: 3331.97ms, tok/sec:157350.96
step 5377, loss: 3.459523, norm:0.2488, lr:4.7428e-04 dt: 3332.22ms, tok/sec:157339.14
step 5378, loss: 3.452480, norm:0.2507, lr:4.7423e-04 dt: 3332.04ms, tok/sec:157347.30
step 5379, loss: 3.452311, norm:0.2663, lr:4.7418e-04 dt: 3332.09ms, tok/sec:157345.02
step 5380, loss: 3.494530, norm:0.2603, lr:4.7414e-04 dt: 3332.46ms, tok/sec:157327.58
step 5381, loss: 3.497030, norm:0.2473, lr:4.7409e-04 dt: 3332.53ms, tok/sec:157324.33
step 5382, loss: 3.488475, norm:0.2725, lr:4.7404e-04 dt: 3332.23ms, tok/sec:157338.67
step 5383, loss: 3.478946, norm:0.2452, lr:4.7399e-04 dt: 3331.95ms, tok/sec:157351.70
step 5384, loss: 3.459614, norm:0.2398, lr:4.7394e-04 dt: 3332.17ms, tok/sec:157341.45
step 5385, loss: 3.424336, norm:0.2376, lr:4.7389e-04 dt: 3332.04ms, tok/sec:157347.65
step 5386, loss: 3.385120, norm:0.2418, lr:4.7384e-04 dt: 3332.09ms, tok/sec:157344.88
step 5387, loss: 3.374339, norm:0.2452, lr:4.7379e-04 dt: 3332.21ms, tok/sec:157339.22
step 5388, loss: 3.355312, norm:0.2569, lr:4.7374e-04 dt: 3331.93ms, tok/sec:157352.65
step 5389, loss: 3.393507, norm:0.2506, lr:4.7369e-04 dt: 3332.07ms, tok/sec:157345.91
step 5390, loss: 3.361470, norm:0.2573, lr:4.7364e-04 dt: 3332.38ms, tok/sec:157331.53
step 5391, loss: 3.451405, norm:0.2765, lr:4.7359e-04 dt: 3331.91ms, tok/sec:157353.46
step 5392, loss: 3.452251, norm:0.2465, lr:4.7354e-04 dt: 3332.39ms, tok/sec:157331.03
step 5393, loss: 3.422617, norm:0.2420, lr:4.7349e-04 dt: 3332.10ms, tok/sec:157344.72
step 5394, loss: 3.429300, norm:0.2320, lr:4.7344e-04 dt: 3332.00ms, tok/sec:157349.43
step 5395, loss: 3.409224, norm:0.2467, lr:4.7339e-04 dt: 3331.83ms, tok/sec:157357.47
step 5396, loss: 3.265472, norm:0.2583, lr:4.7335e-04 dt: 3332.06ms, tok/sec:157346.74
step 5397, loss: 3.145623, norm:0.2785, lr:4.7330e-04 dt: 3331.97ms, tok/sec:157350.94
step 5398, loss: 3.205184, norm:0.2621, lr:4.7325e-04 dt: 3332.09ms, tok/sec:157345.09
step 5399, loss: 3.188667, norm:0.2906, lr:4.7320e-04 dt: 3332.17ms, tok/sec:157341.21
validation loss: 3.4523
Model and optimizer state saved.
HellaSwag accuracy:-2286575508498742191/-2=1143287754249371136.0000
rank 1 sample 0: Hello, I'm a language model, that I understand, not only language itself. However, to write a language model, we need different types of language.
rank 1 sample 1: Hello, I'm a language model, but it is difficult to understand because, while it is one of the most powerful tools available in software, it is not
rank 1 sample 2: Hello, I'm a language model, so any type of language can be used to model that, depending on what you're doing and where you want them.
rank 1 sample 3: Hello, I'm a language model, and I'm looking at English, a programming language, data manipulation, speech structure, metacognition, neural networks
rank 0 sample 0: Hello, I'm a language model, and I know that's really important. One of the most interesting, practical examples of a language model in use today.
rank 0 sample 1: Hello, I'm a language model, so why do you want to use B? Well when you need to teach the B? B has four levels of proficiency
rank 0 sample 2: Hello, I'm a language model, so I will try to talk about a few years ago about that one.
We have a lot of examples here.
rank 0 sample 3: Hello, I'm a language model, which works on all languages.
How to sign up for a foreign language class first?
If you don't start
step 5400, loss: 3.111008, norm:0.2753, lr:4.7315e-04 dt: 56140.71ms, tok/sec:9338.82
step 5401, loss: 3.190773, norm:0.2678, lr:4.7310e-04 dt: 3332.08ms, tok/sec:157345.69
step 5402, loss: 3.219754, norm:0.2560, lr:4.7305e-04 dt: 3331.96ms, tok/sec:157351.42
step 5403, loss: 3.211782, norm:0.2572, lr:4.7300e-04 dt: 3332.05ms, tok/sec:157347.05
step 5404, loss: 3.198362, norm:0.2502, lr:4.7295e-04 dt: 3331.97ms, tok/sec:157350.60
step 5405, loss: 3.145464, norm:0.2655, lr:4.7290e-04 dt: 3332.15ms, tok/sec:157342.36
step 5406, loss: 3.153796, norm:0.2529, lr:4.7285e-04 dt: 3331.94ms, tok/sec:157352.09
step 5407, loss: 3.238210, norm:0.2576, lr:4.7280e-04 dt: 3331.88ms, tok/sec:157354.86
step 5408, loss: 3.429416, norm:0.2713, lr:4.7275e-04 dt: 3332.57ms, tok/sec:157322.34
step 5409, loss: 3.476834, norm:0.2621, lr:4.7270e-04 dt: 3332.12ms, tok/sec:157343.75
step 5410, loss: 3.393994, norm:0.2539, lr:4.7265e-04 dt: 3331.86ms, tok/sec:157356.08
step 5411, loss: 3.514827, norm:0.2800, lr:4.7260e-04 dt: 3331.94ms, tok/sec:157352.11
step 5412, loss: 3.379336, norm:0.3806, lr:4.7255e-04 dt: 3331.91ms, tok/sec:157353.63
step 5413, loss: 3.443595, norm:0.2990, lr:4.7250e-04 dt: 3331.92ms, tok/sec:157353.20
step 5414, loss: 3.436912, norm:0.2684, lr:4.7245e-04 dt: 3332.04ms, tok/sec:157347.47
step 5415, loss: 3.472257, norm:0.2657, lr:4.7240e-04 dt: 3332.29ms, tok/sec:157335.47
step 5416, loss: 3.482782, norm:0.2645, lr:4.7236e-04 dt: 3332.42ms, tok/sec:157329.53
step 5417, loss: 3.448764, norm:0.2685, lr:4.7231e-04 dt: 3332.23ms, tok/sec:157338.50
step 5418, loss: 3.474765, norm:0.2731, lr:4.7226e-04 dt: 3332.24ms, tok/sec:157337.90
step 5419, loss: 3.488221, norm:0.2749, lr:4.7221e-04 dt: 3331.98ms, tok/sec:157350.49
step 5420, loss: 3.462250, norm:0.2591, lr:4.7216e-04 dt: 3332.39ms, tok/sec:157330.87
step 5421, loss: 3.493579, norm:0.2595, lr:4.7211e-04 dt: 3332.55ms, tok/sec:157323.57
step 5422, loss: 3.445255, norm:0.2599, lr:4.7206e-04 dt: 3331.99ms, tok/sec:157349.96
step 5423, loss: 3.449544, norm:0.2467, lr:4.7201e-04 dt: 3332.09ms, tok/sec:157344.98
step 5424, loss: 3.472084, norm:0.2631, lr:4.7196e-04 dt: 3332.14ms, tok/sec:157342.49
step 5425, loss: 3.513443, norm:0.2387, lr:4.7191e-04 dt: 3332.62ms, tok/sec:157319.96
step 5426, loss: 3.453769, norm:0.2707, lr:4.7186e-04 dt: 3332.09ms, tok/sec:157345.30
step 5427, loss: 3.430601, norm:0.2578, lr:4.7181e-04 dt: 3331.96ms, tok/sec:157351.37
step 5428, loss: 3.486634, norm:0.2373, lr:4.7176e-04 dt: 3332.02ms, tok/sec:157348.54
step 5429, loss: 3.494013, norm:0.3122, lr:4.7171e-04 dt: 3332.10ms, tok/sec:157344.71
step 5430, loss: 3.427992, norm:0.3472, lr:4.7166e-04 dt: 3332.08ms, tok/sec:157345.67
step 5431, loss: 3.430340, norm:0.2492, lr:4.7161e-04 dt: 3332.11ms, tok/sec:157344.06
step 5432, loss: 3.397956, norm:0.2448, lr:4.7156e-04 dt: 3332.04ms, tok/sec:157347.28
step 5433, loss: 3.419113, norm:0.2671, lr:4.7151e-04 dt: 3332.14ms, tok/sec:157342.69
step 5434, loss: 3.434202, norm:0.2324, lr:4.7146e-04 dt: 3332.39ms, tok/sec:157330.79
step 5435, loss: 3.382445, norm:0.2410, lr:4.7141e-04 dt: 3331.95ms, tok/sec:157351.66
step 5436, loss: 3.414712, norm:0.2347, lr:4.7136e-04 dt: 3331.96ms, tok/sec:157351.12
step 5437, loss: 3.458173, norm:0.2613, lr:4.7131e-04 dt: 3331.95ms, tok/sec:157351.66
step 5438, loss: 3.430993, norm:0.2511, lr:4.7126e-04 dt: 3332.06ms, tok/sec:157346.68
step 5439, loss: 3.385096, norm:0.2354, lr:4.7121e-04 dt: 3331.90ms, tok/sec:157354.09
step 5440, loss: 3.352587, norm:0.2378, lr:4.7116e-04 dt: 3332.11ms, tok/sec:157344.09
step 5441, loss: 3.395521, norm:0.2323, lr:4.7111e-04 dt: 3332.13ms, tok/sec:157343.03
step 5442, loss: 3.300369, norm:0.2265, lr:4.7106e-04 dt: 3332.04ms, tok/sec:157347.32
step 5443, loss: 3.160724, norm:0.2366, lr:4.7101e-04 dt: 3332.24ms, tok/sec:157338.15
step 5444, loss: 3.164001, norm:0.2400, lr:4.7097e-04 dt: 3332.29ms, tok/sec:157335.63
step 5445, loss: 3.240695, norm:0.2465, lr:4.7092e-04 dt: 3332.23ms, tok/sec:157338.27
step 5446, loss: 3.194862, norm:0.2659, lr:4.7087e-04 dt: 3332.09ms, tok/sec:157344.96
step 5447, loss: 3.151078, norm:0.2462, lr:4.7082e-04 dt: 3332.00ms, tok/sec:157349.37
step 5448, loss: 3.245431, norm:0.2612, lr:4.7077e-04 dt: 3332.10ms, tok/sec:157344.49
step 5449, loss: 3.120273, norm:0.2460, lr:4.7072e-04 dt: 3332.15ms, tok/sec:157342.27
HellaSwag accuracy:2325136865995474001/-2=-1162568432997736960.0000
rank 1 sample 0: Hello, I'm a language model, and we're going to be discussing any topic about our history."
This project is being developed as part of the National
rank 1 sample 1: Hello, I'm a language model, I am working with a language model in that language model at the computer. This idea should help not just the computer but
rank 1 sample 2: Hello, I'm a language model, but just a simple way to do this is to get rid of all your English words. This makes your vocabulary fun,
rank 1 sample 3: Hello, I'm a language model, and I'm looking at two examples of human language models by human language models this week ... read more
Bible is
rank 0 sample 0: Hello, I'm a language model, and I'll be talking about some fun tricks when I'm trying to solve problems. The rules I'm asking you guys
rank 0 sample 1: Hello, I'm a language model, I mean, I want to communicate ideas and understands to you. Then there are the four types of languages:

rank 0 sample 2: Hello, I'm a language model, so I like that kind of thing and I understand it. The other thing is that it's a language model.

rank 0 sample 3: Hello, I'm a language model, which could be used to represent the meaning of Chinese.
Coupler: No problem-free software
I really
step 5450, loss: 3.184826, norm:0.2625, lr:4.7067e-04 dt: 48517.93ms, tok/sec:10806.07
step 5451, loss: 3.197817, norm:0.2494, lr:4.7062e-04 dt: 3332.33ms, tok/sec:157333.68
step 5452, loss: 3.208216, norm:0.2371, lr:4.7057e-04 dt: 3332.17ms, tok/sec:157341.21
step 5453, loss: 3.197269, norm:0.2379, lr:4.7052e-04 dt: 3331.87ms, tok/sec:157355.43
step 5454, loss: 3.403869, norm:0.2798, lr:4.7047e-04 dt: 3332.13ms, tok/sec:157343.09
step 5455, loss: 3.425751, norm:0.2548, lr:4.7042e-04 dt: 3332.57ms, tok/sec:157322.64
step 5456, loss: 3.576793, norm:0.2861, lr:4.7037e-04 dt: 3332.12ms, tok/sec:157343.84
step 5457, loss: 3.408985, norm:0.2811, lr:4.7032e-04 dt: 3331.93ms, tok/sec:157352.44
step 5458, loss: 3.511604, norm:0.2998, lr:4.7027e-04 dt: 3331.99ms, tok/sec:157349.89
step 5459, loss: 3.420426, norm:0.3004, lr:4.7022e-04 dt: 3332.17ms, tok/sec:157341.28
step 5460, loss: 3.436192, norm:0.2689, lr:4.7017e-04 dt: 3332.18ms, tok/sec:157340.71
step 5461, loss: 3.445232, norm:0.2907, lr:4.7012e-04 dt: 3331.93ms, tok/sec:157352.63
step 5462, loss: 3.467285, norm:0.2944, lr:4.7007e-04 dt: 3332.25ms, tok/sec:157337.62
step 5463, loss: 3.536291, norm:0.2937, lr:4.7002e-04 dt: 3332.10ms, tok/sec:157344.48
step 5464, loss: 3.450690, norm:0.3109, lr:4.6997e-04 dt: 3332.72ms, tok/sec:157315.37
step 5465, loss: 3.468245, norm:0.2702, lr:4.6992e-04 dt: 3331.94ms, tok/sec:157352.04
step 5466, loss: 3.418750, norm:0.2781, lr:4.6987e-04 dt: 3332.09ms, tok/sec:157345.31
step 5467, loss: 3.471910, norm:0.2793, lr:4.6982e-04 dt: 3332.07ms, tok/sec:157345.97
step 5468, loss: 3.467057, norm:0.2760, lr:4.6977e-04 dt: 3331.93ms, tok/sec:157352.63
step 5469, loss: 3.472418, norm:0.2876, lr:4.6972e-04 dt: 3331.94ms, tok/sec:157352.40
step 5470, loss: 3.498427, norm:0.3131, lr:4.6967e-04 dt: 3332.01ms, tok/sec:157348.76
step 5471, loss: 3.502450, norm:0.2813, lr:4.6962e-04 dt: 3332.18ms, tok/sec:157340.88
step 5472, loss: 3.434464, norm:0.2720, lr:4.6957e-04 dt: 3332.15ms, tok/sec:157342.09
step 5473, loss: 3.501629, norm:0.3055, lr:4.6952e-04 dt: 3332.52ms, tok/sec:157324.60
step 5474, loss: 3.532096, norm:0.2629, lr:4.6947e-04 dt: 3332.07ms, tok/sec:157345.89
step 5475, loss: 3.436435, norm:0.2690, lr:4.6942e-04 dt: 3331.99ms, tok/sec:157349.73
step 5476, loss: 3.524710, norm:0.2916, lr:4.6937e-04 dt: 3332.03ms, tok/sec:157348.13
step 5477, loss: 3.430210, norm:0.2747, lr:4.6932e-04 dt: 3332.03ms, tok/sec:157348.12
step 5478, loss: 3.368282, norm:0.2465, lr:4.6927e-04 dt: 3332.05ms, tok/sec:157346.97
step 5479, loss: 3.440596, norm:0.2814, lr:4.6922e-04 dt: 3332.01ms, tok/sec:157348.64
step 5480, loss: 3.400898, norm:0.2876, lr:4.6917e-04 dt: 3332.42ms, tok/sec:157329.30
step 5481, loss: 3.396016, norm:0.2762, lr:4.6912e-04 dt: 3332.12ms, tok/sec:157343.68
step 5482, loss: 3.390396, norm:0.2671, lr:4.6907e-04 dt: 3332.02ms, tok/sec:157348.22
step 5483, loss: 3.404194, norm:0.2607, lr:4.6902e-04 dt: 3331.99ms, tok/sec:157349.88
step 5484, loss: 3.443355, norm:0.2488, lr:4.6897e-04 dt: 3332.05ms, tok/sec:157346.94
step 5485, loss: 3.350794, norm:0.2784, lr:4.6892e-04 dt: 3332.11ms, tok/sec:157344.31
step 5486, loss: 3.388858, norm:0.3039, lr:4.6887e-04 dt: 3332.13ms, tok/sec:157343.43
step 5487, loss: 3.417712, norm:0.2349, lr:4.6882e-04 dt: 3332.13ms, tok/sec:157343.28
step 5488, loss: 3.335016, norm:0.2808, lr:4.6877e-04 dt: 3332.25ms, tok/sec:157337.73
step 5489, loss: 3.196107, norm:0.2854, lr:4.6872e-04 dt: 3331.99ms, tok/sec:157349.72
step 5490, loss: 3.238966, norm:0.2888, lr:4.6867e-04 dt: 3332.32ms, tok/sec:157334.22
step 5491, loss: 3.215446, norm:0.2791, lr:4.6862e-04 dt: 3332.11ms, tok/sec:157344.11
step 5492, loss: 3.175851, norm:0.2848, lr:4.6857e-04 dt: 3332.10ms, tok/sec:157344.76
step 5493, loss: 3.203651, norm:0.2712, lr:4.6852e-04 dt: 3331.98ms, tok/sec:157350.37
step 5494, loss: 3.209689, norm:0.2486, lr:4.6847e-04 dt: 3331.80ms, tok/sec:157358.57
step 5495, loss: 3.184664, norm:0.2558, lr:4.6842e-04 dt: 3332.08ms, tok/sec:157345.35
step 5496, loss: 3.157517, norm:0.2418, lr:4.6837e-04 dt: 3332.06ms, tok/sec:157346.54
step 5497, loss: 3.248734, norm:0.2440, lr:4.6832e-04 dt: 3331.82ms, tok/sec:157357.63
step 5498, loss: 3.180929, norm:0.2508, lr:4.6827e-04 dt: 3332.43ms, tok/sec:157328.82
step 5499, loss: 3.164625, norm:0.2422, lr:4.6822e-04 dt: 3332.63ms, tok/sec:157319.69
validation loss: 3.4511
Model and optimizer state saved.
HellaSwag accuracy:-6898103195104213935/-2=3449051597552107008.0000
rank 1 sample 0: Hello, I'm a language model, we have to learn languages that are open, as they are often used in schools. This is not a language model,
rank 1 sample 1: Hello, I'm a language model, which means you have to learn something new every time you encounter a new language. Of course, it is not a language
rank 1 sample 2: Hello, I'm a language model, but these days the language model is very different.
This sounds very much like a lot of other things. However,
rank 1 sample 3: Hello, I'm a language model, and I'm working with HTML and HTML respectively.
These days, my programming language development teams have been working on a
rank 0 sample 0: Hello, I'm a language model, I'm writing a blog in which I cover "how to make use of your language model." In my free time this
rank 0 sample 1: Hello, I'm a language model, and am part of my learning. Can you tell me when you get interested in a topic in the course or when you
rank 0 sample 2: Hello, I'm a language model, and I like that everyone loves the thing. This is a language modeling tutorial, and it's a great way for students
rank 0 sample 3: Hello, I'm a language model, I started with a simple yet effective strategy for analyzing the language. When I first presented myself with the model, we decided
step 5500, loss: 3.326410, norm:0.2659, lr:4.6817e-04 dt: 56328.42ms, tok/sec:9307.70
step 5501, loss: 3.471498, norm:0.2589, lr:4.6812e-04 dt: 3331.96ms, tok/sec:157351.03
step 5502, loss: 3.488512, norm:0.3356, lr:4.6807e-04 dt: 3332.17ms, tok/sec:157341.39
step 5503, loss: 3.465640, norm:0.2980, lr:4.6802e-04 dt: 3332.18ms, tok/sec:157340.69
step 5504, loss: 3.467663, norm:0.2704, lr:4.6797e-04 dt: 3332.08ms, tok/sec:157345.72
step 5505, loss: 3.456371, norm:0.2918, lr:4.6792e-04 dt: 3332.13ms, tok/sec:157343.38
step 5506, loss: 3.440666, norm:0.2591, lr:4.6787e-04 dt: 3332.07ms, tok/sec:157346.01
step 5507, loss: 3.533487, norm:0.3129, lr:4.6782e-04 dt: 3332.25ms, tok/sec:157337.61
step 5508, loss: 3.459481, norm:0.2857, lr:4.6777e-04 dt: 3332.01ms, tok/sec:157348.73
step 5509, loss: 3.437864, norm:0.3330, lr:4.6772e-04 dt: 3332.42ms, tok/sec:157329.65
step 5510, loss: 3.458985, norm:0.2716, lr:4.6767e-04 dt: 3332.10ms, tok/sec:157344.39
step 5511, loss: 3.476812, norm:0.2943, lr:4.6762e-04 dt: 3332.05ms, tok/sec:157347.02
step 5512, loss: 3.448882, norm:0.2919, lr:4.6757e-04 dt: 3332.00ms, tok/sec:157349.44
step 5513, loss: 3.469623, norm:0.2954, lr:4.6752e-04 dt: 3332.22ms, tok/sec:157338.91
step 5514, loss: 3.453808, norm:0.2788, lr:4.6747e-04 dt: 3332.07ms, tok/sec:157346.01
step 5515, loss: 3.493492, norm:0.2724, lr:4.6742e-04 dt: 3332.35ms, tok/sec:157332.74
step 5516, loss: 3.388430, norm:0.2556, lr:4.6737e-04 dt: 3332.38ms, tok/sec:157331.52
step 5517, loss: 3.427242, norm:0.2493, lr:4.6732e-04 dt: 3331.98ms, tok/sec:157350.25
step 5518, loss: 3.456428, norm:0.2522, lr:4.6727e-04 dt: 3332.23ms, tok/sec:157338.61
step 5519, loss: 3.502585, norm:0.2616, lr:4.6722e-04 dt: 3332.28ms, tok/sec:157335.90
step 5520, loss: 3.472433, norm:0.2550, lr:4.6717e-04 dt: 3332.05ms, tok/sec:157347.06
step 5521, loss: 3.445293, norm:0.2393, lr:4.6712e-04 dt: 3332.06ms, tok/sec:157346.46
step 5522, loss: 3.443637, norm:0.2393, lr:4.6707e-04 dt: 3332.28ms, tok/sec:157335.90
step 5523, loss: 3.392853, norm:0.2292, lr:4.6702e-04 dt: 3332.19ms, tok/sec:157340.21
step 5524, loss: 3.391622, norm:0.2489, lr:4.6696e-04 dt: 3334.96ms, tok/sec:157209.79
step 5525, loss: 3.417867, norm:0.2399, lr:4.6691e-04 dt: 3332.28ms, tok/sec:157336.21
step 5526, loss: 3.423152, norm:0.2513, lr:4.6686e-04 dt: 3332.13ms, tok/sec:157343.20
step 5527, loss: 3.418108, norm:0.2793, lr:4.6681e-04 dt: 3331.99ms, tok/sec:157349.86
step 5528, loss: 3.395093, norm:0.2799, lr:4.6676e-04 dt: 3332.00ms, tok/sec:157349.17
step 5529, loss: 3.439711, norm:0.2386, lr:4.6671e-04 dt: 3331.78ms, tok/sec:157359.83
step 5530, loss: 3.422320, norm:0.2878, lr:4.6666e-04 dt: 3331.99ms, tok/sec:157349.91
step 5531, loss: 3.395550, norm:0.2562, lr:4.6661e-04 dt: 3332.13ms, tok/sec:157343.20
step 5532, loss: 3.404570, norm:0.2335, lr:4.6656e-04 dt: 3331.88ms, tok/sec:157354.90
step 5533, loss: 3.376004, norm:0.2369, lr:4.6651e-04 dt: 3331.93ms, tok/sec:157352.48
step 5534, loss: 3.239209, norm:0.2268, lr:4.6646e-04 dt: 3332.28ms, tok/sec:157336.05
step 5535, loss: 3.204724, norm:0.2476, lr:4.6641e-04 dt: 3332.19ms, tok/sec:157340.17
step 5536, loss: 3.199321, norm:0.2438, lr:4.6636e-04 dt: 3332.07ms, tok/sec:157346.27
step 5537, loss: 3.196853, norm:0.2733, lr:4.6631e-04 dt: 3331.99ms, tok/sec:157349.79
step 5538, loss: 3.236852, norm:0.2489, lr:4.6626e-04 dt: 3331.88ms, tok/sec:157355.05
step 5539, loss: 3.156338, norm:0.2678, lr:4.6621e-04 dt: 3332.05ms, tok/sec:157347.18
step 5540, loss: 3.211654, norm:0.2576, lr:4.6616e-04 dt: 3331.98ms, tok/sec:157350.43
step 5541, loss: 3.164985, norm:0.2614, lr:4.6611e-04 dt: 3332.00ms, tok/sec:157349.44
step 5542, loss: 3.176343, norm:0.2593, lr:4.6606e-04 dt: 3332.19ms, tok/sec:157340.38
step 5543, loss: 3.197609, norm:0.2354, lr:4.6601e-04 dt: 3332.19ms, tok/sec:157340.29
step 5544, loss: 3.167240, norm:0.2594, lr:4.6596e-04 dt: 3332.31ms, tok/sec:157334.47
step 5545, loss: 3.177869, norm:0.2415, lr:4.6591e-04 dt: 3332.04ms, tok/sec:157347.40
step 5546, loss: 3.377283, norm:0.2671, lr:4.6586e-04 dt: 3332.22ms, tok/sec:157338.86
step 5547, loss: 3.436035, norm:0.2766, lr:4.6581e-04 dt: 3332.13ms, tok/sec:157343.02
step 5548, loss: 3.429265, norm:0.2926, lr:4.6576e-04 dt: 3331.75ms, tok/sec:157360.96
step 5549, loss: 3.454127, norm:0.2659, lr:4.6571e-04 dt: 3331.93ms, tok/sec:157352.64
HellaSwag accuracy:2326183633275257873/-2=-1163091816637628928.0000
rank 1 sample 0: Hello, I'm a language model, we'll focus on the most important classes and all kinds of relationships.
1. Translated and Unseen
2
rank 0 sample 0: Hello, I'm a language model, and I'd like a good language in science/technology, and they might not be good at me if they're inrank 1 sample 1: Hello, I'm a language model, which I can write with. But this will be a tutorial in.
Here is how I look up the word "

rank 1 sample 2: Hello, I'm a language model, so like the word "English" in the sentence, "French" means "English" in Spanish "French" in
rank 0 sample 1: Hello, I'm a language model, I wanted to see if I was having a problem trying to figure out the right thing to do with things that I wanted
rank 1 sample 3: Hello, I'm a language model, and I'm interested to dive into the mechanics of language with kids. If language is that it's not, how do
rank 0 sample 2: Hello, I'm a language model, but I're only writing, math and science for the sake of teaching math. The whole point of this article is that
rank 0 sample 3: Hello, I'm a language model, you write a new word to a set, usually one of the original sounds, based on the sounds of the previous ones
step 5550, loss: 3.444808, norm:0.2901, lr:4.6566e-04 dt: 48512.26ms, tok/sec:10807.33
step 5551, loss: 3.391460, norm:0.2534, lr:4.6561e-04 dt: 3331.98ms, tok/sec:157350.13
step 5552, loss: 3.457826, norm:0.2444, lr:4.6556e-04 dt: 3332.16ms, tok/sec:157341.87
step 5553, loss: 3.446776, norm:0.2763, lr:4.6550e-04 dt: 3332.27ms, tok/sec:157336.41
step 5554, loss: 3.476989, norm:0.2661, lr:4.6545e-04 dt: 3332.14ms, tok/sec:157342.90
step 5555, loss: 3.464843, norm:0.2601, lr:4.6540e-04 dt: 3332.24ms, tok/sec:157337.93
step 5556, loss: 3.450869, norm:0.2641, lr:4.6535e-04 dt: 3331.96ms, tok/sec:157351.39
step 5557, loss: 3.513854, norm:0.2540, lr:4.6530e-04 dt: 3331.88ms, tok/sec:157354.88
step 5558, loss: 3.464554, norm:0.2637, lr:4.6525e-04 dt: 3332.03ms, tok/sec:157347.97
step 5559, loss: 3.489417, norm:0.2800, lr:4.6520e-04 dt: 3332.14ms, tok/sec:157342.55
step 5560, loss: 3.488765, norm:0.2715, lr:4.6515e-04 dt: 3332.09ms, tok/sec:157345.31
step 5561, loss: 3.463529, norm:0.2519, lr:4.6510e-04 dt: 3332.48ms, tok/sec:157326.61
step 5562, loss: 3.459542, norm:0.2591, lr:4.6505e-04 dt: 3332.44ms, tok/sec:157328.47
step 5563, loss: 3.433473, norm:0.2513, lr:4.6500e-04 dt: 3331.95ms, tok/sec:157351.92
step 5564, loss: 3.458390, norm:0.2757, lr:4.6495e-04 dt: 3332.32ms, tok/sec:157334.01
step 5565, loss: 3.507718, norm:0.2911, lr:4.6490e-04 dt: 3332.36ms, tok/sec:157332.31
step 5566, loss: 3.436994, norm:0.2667, lr:4.6485e-04 dt: 3333.11ms, tok/sec:157296.89
step 5567, loss: 3.513310, norm:0.3040, lr:4.6480e-04 dt: 3332.15ms, tok/sec:157342.26
step 5568, loss: 3.457589, norm:0.2583, lr:4.6475e-04 dt: 3332.10ms, tok/sec:157344.59
step 5569, loss: 3.344351, norm:0.2664, lr:4.6470e-04 dt: 3332.12ms, tok/sec:157343.63
step 5570, loss: 3.396012, norm:0.2492, lr:4.6465e-04 dt: 3331.92ms, tok/sec:157353.09
step 5571, loss: 3.386417, norm:0.2599, lr:4.6460e-04 dt: 3332.15ms, tok/sec:157342.16
step 5572, loss: 3.367646, norm:0.2274, lr:4.6455e-04 dt: 3332.07ms, tok/sec:157345.94
step 5573, loss: 3.408765, norm:0.2650, lr:4.6449e-04 dt: 3332.43ms, tok/sec:157329.04
step 5574, loss: 3.377618, norm:0.2384, lr:4.6444e-04 dt: 3332.12ms, tok/sec:157343.68
step 5575, loss: 3.343852, norm:0.2538, lr:4.6439e-04 dt: 3331.77ms, tok/sec:157360.28
step 5576, loss: 3.416548, norm:0.2777, lr:4.6434e-04 dt: 3331.95ms, tok/sec:157351.84
step 5577, loss: 3.415605, norm:0.2628, lr:4.6429e-04 dt: 3331.99ms, tok/sec:157349.91
step 5578, loss: 3.398792, norm:0.2848, lr:4.6424e-04 dt: 3332.24ms, tok/sec:157338.06
step 5579, loss: 3.416080, norm:0.2352, lr:4.6419e-04 dt: 3331.89ms, tok/sec:157354.39
step 5580, loss: 3.205909, norm:0.2831, lr:4.6414e-04 dt: 3331.96ms, tok/sec:157351.44
step 5581, loss: 3.160140, norm:0.2864, lr:4.6409e-04 dt: 3332.33ms, tok/sec:157333.65
step 5582, loss: 3.207315, norm:0.2628, lr:4.6404e-04 dt: 3331.88ms, tok/sec:157354.96
step 5583, loss: 3.156836, norm:0.2857, lr:4.6399e-04 dt: 3332.21ms, tok/sec:157339.39
step 5584, loss: 3.204159, norm:0.2542, lr:4.6394e-04 dt: 3331.93ms, tok/sec:157352.48
step 5585, loss: 3.142752, norm:0.2433, lr:4.6389e-04 dt: 3331.88ms, tok/sec:157354.99
step 5586, loss: 3.190751, norm:0.2838, lr:4.6384e-04 dt: 3331.88ms, tok/sec:157355.16
step 5587, loss: 3.183677, norm:0.2552, lr:4.6379e-04 dt: 3331.82ms, tok/sec:157357.69
step 5588, loss: 3.154861, norm:0.2411, lr:4.6374e-04 dt: 3331.89ms, tok/sec:157354.33
step 5589, loss: 3.179891, norm:0.2686, lr:4.6368e-04 dt: 3332.02ms, tok/sec:157348.17
step 5590, loss: 3.227998, norm:0.2424, lr:4.6363e-04 dt: 3332.22ms, tok/sec:157338.98
step 5591, loss: 3.304980, norm:0.2598, lr:4.6358e-04 dt: 3331.97ms, tok/sec:157350.83
step 5592, loss: 3.407806, norm:0.3854, lr:4.6353e-04 dt: 3332.55ms, tok/sec:157323.55
step 5593, loss: 3.433128, norm:0.3014, lr:4.6348e-04 dt: 3332.02ms, tok/sec:157348.51
step 5594, loss: 3.424593, norm:0.2868, lr:4.6343e-04 dt: 3332.10ms, tok/sec:157344.61
step 5595, loss: 3.461829, norm:0.2905, lr:4.6338e-04 dt: 3332.28ms, tok/sec:157335.96
step 5596, loss: 3.457435, norm:0.2959, lr:4.6333e-04 dt: 3332.04ms, tok/sec:157347.31
step 5597, loss: 3.466483, norm:0.2994, lr:4.6328e-04 dt: 3331.93ms, tok/sec:157352.46
step 5598, loss: 3.453753, norm:0.3252, lr:4.6323e-04 dt: 3332.07ms, tok/sec:157345.86
step 5599, loss: 3.419285, norm:0.2863, lr:4.6318e-04 dt: 3332.19ms, tok/sec:157340.16
validation loss: 3.4385
Model and optimizer state saved.
HellaSwag accuracy:2326183633277355089/-2=-1163091816638677504.0000
rank 1 sample 0: Hello, I'm a language model, and the people who come in there each time talk with me each other. One of those people has a language model,
rank 1 sample 1: Hello, I'm a language model, which I have in my opinion an English Language model. There are very few similarities to Chinese and Polish.
I'm
rank 0 sample 0: Hello, I'm a language model, and I just want the text (like for instance, like this or here's the language model). So we can makerank 1 sample 2: Hello, I'm a language model, but like most programming languages, I'm not sure if the system has been defined.
So here we are with a

rank 1 sample 3: Hello, I'm a language model, and I'm using it from a website now.
All that does is tell me - What does it mean?

rank 0 sample 1: Hello, I'm a language model, I would like to say that we're going to try to improve language when we can, if we were to know that
rank 0 sample 2: Hello, I'm a language model, I'm working with, we're working with English, we use our word-processing and we're using our language models
rank 0 sample 3: Hello, I'm a language model, a little bit about the language of computer programming because I'm afraid I will have "unlearnable" my work very
step 5600, loss: 3.458317, norm:0.2577, lr:4.6313e-04 dt: 56142.77ms, tok/sec:9338.48
step 5601, loss: 3.442064, norm:0.2681, lr:4.6308e-04 dt: 3332.19ms, tok/sec:157340.14
step 5602, loss: 3.456967, norm:0.2530, lr:4.6303e-04 dt: 3332.10ms, tok/sec:157344.71
step 5603, loss: 3.499887, norm:0.2725, lr:4.6297e-04 dt: 3332.28ms, tok/sec:157336.30
step 5604, loss: 3.413298, norm:0.2706, lr:4.6292e-04 dt: 3332.37ms, tok/sec:157331.91
step 5605, loss: 3.513325, norm:0.2451, lr:4.6287e-04 dt: 3332.21ms, tok/sec:157339.52
step 5606, loss: 3.456744, norm:0.2610, lr:4.6282e-04 dt: 3332.01ms, tok/sec:157348.64
step 5607, loss: 3.496533, norm:0.2537, lr:4.6277e-04 dt: 3332.06ms, tok/sec:157346.45
step 5608, loss: 3.469110, norm:0.2450, lr:4.6272e-04 dt: 3332.01ms, tok/sec:157349.10
step 5609, loss: 3.407410, norm:0.2410, lr:4.6267e-04 dt: 3332.12ms, tok/sec:157343.59
step 5610, loss: 3.426500, norm:0.2513, lr:4.6262e-04 dt: 3332.12ms, tok/sec:157343.88
step 5611, loss: 3.444158, norm:0.2691, lr:4.6257e-04 dt: 3331.98ms, tok/sec:157350.48
step 5612, loss: 3.467505, norm:0.2830, lr:4.6252e-04 dt: 3332.26ms, tok/sec:157337.00
step 5613, loss: 3.435879, norm:0.2830, lr:4.6247e-04 dt: 3332.53ms, tok/sec:157324.44
step 5614, loss: 3.446437, norm:0.3113, lr:4.6242e-04 dt: 3331.83ms, tok/sec:157357.37
step 5615, loss: 3.372201, norm:0.2776, lr:4.6237e-04 dt: 3332.01ms, tok/sec:157348.83
step 5616, loss: 3.372130, norm:0.2828, lr:4.6231e-04 dt: 3331.90ms, tok/sec:157354.01
step 5617, loss: 3.377504, norm:0.2732, lr:4.6226e-04 dt: 3332.13ms, tok/sec:157342.97
step 5618, loss: 3.430727, norm:0.2543, lr:4.6221e-04 dt: 3332.24ms, tok/sec:157337.81
step 5619, loss: 3.414919, norm:0.2355, lr:4.6216e-04 dt: 3332.09ms, tok/sec:157345.04
step 5620, loss: 3.413826, norm:0.2263, lr:4.6211e-04 dt: 3332.16ms, tok/sec:157341.71
step 5621, loss: 3.382967, norm:0.2328, lr:4.6206e-04 dt: 3332.18ms, tok/sec:157340.95
step 5622, loss: 3.433636, norm:0.2406, lr:4.6201e-04 dt: 3332.43ms, tok/sec:157328.87
step 5623, loss: 3.407821, norm:0.2475, lr:4.6196e-04 dt: 3332.03ms, tok/sec:157347.70
step 5624, loss: 3.424569, norm:0.2477, lr:4.6191e-04 dt: 3331.83ms, tok/sec:157357.15
step 5625, loss: 3.146881, norm:0.2827, lr:4.6186e-04 dt: 3332.18ms, tok/sec:157340.75
step 5626, loss: 3.144743, norm:0.2579, lr:4.6181e-04 dt: 3331.80ms, tok/sec:157359.01
step 5627, loss: 3.116739, norm:0.2860, lr:4.6175e-04 dt: 3331.83ms, tok/sec:157357.16
step 5628, loss: 3.183712, norm:0.2576, lr:4.6170e-04 dt: 3332.10ms, tok/sec:157344.48
step 5629, loss: 3.147429, norm:0.2483, lr:4.6165e-04 dt: 3332.02ms, tok/sec:157348.47
step 5630, loss: 3.189106, norm:0.2552, lr:4.6160e-04 dt: 3332.02ms, tok/sec:157348.37
step 5631, loss: 3.191011, norm:0.2246, lr:4.6155e-04 dt: 3332.07ms, tok/sec:157345.84
step 5632, loss: 3.230584, norm:0.2545, lr:4.6150e-04 dt: 3332.24ms, tok/sec:157338.13
step 5633, loss: 3.178793, norm:0.2412, lr:4.6145e-04 dt: 3332.09ms, tok/sec:157344.93
step 5634, loss: 3.140726, norm:0.2427, lr:4.6140e-04 dt: 3331.92ms, tok/sec:157353.08
step 5635, loss: 3.124789, norm:0.2610, lr:4.6135e-04 dt: 3331.87ms, tok/sec:157355.57
step 5636, loss: 3.349811, norm:0.2741, lr:4.6130e-04 dt: 3331.85ms, tok/sec:157356.47
step 5637, loss: 3.499608, norm:0.2857, lr:4.6125e-04 dt: 3332.34ms, tok/sec:157333.21
step 5638, loss: 3.459975, norm:0.2755, lr:4.6119e-04 dt: 3332.16ms, tok/sec:157341.80
step 5639, loss: 3.413231, norm:0.2779, lr:4.6114e-04 dt: 3332.02ms, tok/sec:157348.20
step 5640, loss: 3.444655, norm:0.2653, lr:4.6109e-04 dt: 3332.10ms, tok/sec:157344.60
step 5641, loss: 3.457375, norm:0.2720, lr:4.6104e-04 dt: 3332.55ms, tok/sec:157323.22
step 5642, loss: 3.424279, norm:0.2691, lr:4.6099e-04 dt: 3331.93ms, tok/sec:157352.82
step 5643, loss: 3.417972, norm:0.2722, lr:4.6094e-04 dt: 3332.15ms, tok/sec:157342.04
step 5644, loss: 3.431382, norm:0.2410, lr:4.6089e-04 dt: 3332.03ms, tok/sec:157348.16
step 5645, loss: 3.500613, norm:0.3020, lr:4.6084e-04 dt: 3332.05ms, tok/sec:157347.17
step 5646, loss: 3.439681, norm:0.2437, lr:4.6079e-04 dt: 3332.05ms, tok/sec:157347.17
step 5647, loss: 3.478665, norm:0.3017, lr:4.6074e-04 dt: 3332.09ms, tok/sec:157345.27
step 5648, loss: 3.437560, norm:0.2500, lr:4.6068e-04 dt: 3332.06ms, tok/sec:157346.65
step 5649, loss: 3.504254, norm:0.2830, lr:4.6063e-04 dt: 3332.20ms, tok/sec:157339.72
HellaSwag accuracy:2325075325556589649/-2=-1162537662778294784.0000
rank 1 sample 0: Hello, I'm a language model, and you're going to learn English quickly and really learn from those languages.
I'd like this to be a great
rank 1 sample 1: Hello, I'm a language model, you're probably ready to learn more about Language Learning. For the basics, the syntax provides you with the basics of the
rank 1 sample 2: Hello, I'm a language model, but some things happen to me. I'm a language model but the most important thing is to make a model so you
rank 0 sample 0: Hello, I'm a language model, and I don't have many other ideas with regard to it, let's go back and fix a little bit. Letrank 1 sample 3: Hello, I'm a language model, and I'm working with both. In practice, I mean "going around "a.r, "going around"

rank 0 sample 1: Hello, I'm a language model, so now I'm gonna do it pretty well and maybe use some of these in a couple of weeks in the future.
rank 0 sample 2: Hello, I'm a language model, I'm always very important with my readers. In this article
"How to learn the language of the future." -
rank 0 sample 3: Hello, I'm a language model, which really impresses me.
A few, but not all, of these other kinds of languages work well, are
step 5650, loss: 3.492053, norm:0.2551, lr:4.6058e-04 dt: 48515.54ms, tok/sec:10806.60
step 5651, loss: 3.422763, norm:0.2989, lr:4.6053e-04 dt: 3332.11ms, tok/sec:157344.20
step 5652, loss: 3.453444, norm:0.2575, lr:4.6048e-04 dt: 3332.59ms, tok/sec:157321.40
step 5653, loss: 3.425987, norm:0.2705, lr:4.6043e-04 dt: 3332.15ms, tok/sec:157342.03
step 5654, loss: 3.470096, norm:0.2647, lr:4.6038e-04 dt: 3332.04ms, tok/sec:157347.30
step 5655, loss: 3.449649, norm:0.2682, lr:4.6033e-04 dt: 3331.98ms, tok/sec:157350.35
step 5656, loss: 3.429904, norm:0.2835, lr:4.6028e-04 dt: 3331.97ms, tok/sec:157350.76
step 5657, loss: 3.483740, norm:0.2348, lr:4.6022e-04 dt: 3332.01ms, tok/sec:157349.07
step 5658, loss: 3.485828, norm:0.2999, lr:4.6017e-04 dt: 3332.00ms, tok/sec:157349.13
step 5659, loss: 3.385845, norm:0.2612, lr:4.6012e-04 dt: 3332.14ms, tok/sec:157342.94
step 5660, loss: 3.418461, norm:0.2690, lr:4.6007e-04 dt: 3332.18ms, tok/sec:157341.00
step 5661, loss: 3.389941, norm:0.2654, lr:4.6002e-04 dt: 3332.37ms, tok/sec:157331.92
step 5662, loss: 3.400115, norm:0.2703, lr:4.5997e-04 dt: 3331.87ms, tok/sec:157355.31
step 5663, loss: 3.424663, norm:0.2436, lr:4.5992e-04 dt: 3332.05ms, tok/sec:157347.14
step 5664, loss: 3.370098, norm:0.2589, lr:4.5987e-04 dt: 3332.04ms, tok/sec:157347.53
step 5665, loss: 3.362813, norm:0.2512, lr:4.5982e-04 dt: 3331.95ms, tok/sec:157351.89
step 5666, loss: 3.384057, norm:0.2531, lr:4.5976e-04 dt: 3331.82ms, tok/sec:157358.00
step 5667, loss: 3.401385, norm:0.2384, lr:4.5971e-04 dt: 3332.07ms, tok/sec:157346.05
step 5668, loss: 3.352659, norm:0.2638, lr:4.5966e-04 dt: 3332.47ms, tok/sec:157327.32
step 5669, loss: 3.403490, norm:0.2725, lr:4.5961e-04 dt: 3332.02ms, tok/sec:157348.37
step 5670, loss: 3.230788, norm:0.3189, lr:4.5956e-04 dt: 3332.60ms, tok/sec:157320.84
step 5671, loss: 3.204144, norm:0.2899, lr:4.5951e-04 dt: 3331.86ms, tok/sec:157356.17
step 5672, loss: 3.230711, norm:0.2679, lr:4.5946e-04 dt: 3331.96ms, tok/sec:157351.05
step 5673, loss: 3.211267, norm:0.2923, lr:4.5941e-04 dt: 3331.94ms, tok/sec:157352.16
step 5674, loss: 3.225566, norm:0.2506, lr:4.5936e-04 dt: 3332.06ms, tok/sec:157346.41
step 5675, loss: 3.182597, norm:0.2528, lr:4.5930e-04 dt: 3332.20ms, tok/sec:157339.77
step 5676, loss: 3.235407, norm:0.2597, lr:4.5925e-04 dt: 3332.33ms, tok/sec:157333.68
step 5677, loss: 3.136967, norm:0.2426, lr:4.5920e-04 dt: 3331.75ms, tok/sec:157361.35
step 5678, loss: 3.173819, norm:0.2723, lr:4.5915e-04 dt: 3332.05ms, tok/sec:157347.18
step 5679, loss: 3.125544, norm:0.2407, lr:4.5910e-04 dt: 3332.01ms, tok/sec:157348.76
step 5680, loss: 3.180950, norm:0.2520, lr:4.5905e-04 dt: 3332.54ms, tok/sec:157323.77
step 5681, loss: 3.268659, norm:0.2223, lr:4.5900e-04 dt: 3332.08ms, tok/sec:157345.42
step 5682, loss: 3.467577, norm:0.2603, lr:4.5895e-04 dt: 3331.90ms, tok/sec:157354.01
step 5683, loss: 3.380715, norm:0.2867, lr:4.5889e-04 dt: 3332.00ms, tok/sec:157349.19
step 5684, loss: 3.471080, norm:0.2761, lr:4.5884e-04 dt: 3332.23ms, tok/sec:157338.27
step 5685, loss: 3.414971, norm:0.3091, lr:4.5879e-04 dt: 3332.03ms, tok/sec:157348.02
step 5686, loss: 3.454609, norm:0.2569, lr:4.5874e-04 dt: 3332.08ms, tok/sec:157345.60
step 5687, loss: 3.439060, norm:0.2868, lr:4.5869e-04 dt: 3332.17ms, tok/sec:157341.18
step 5688, loss: 3.441289, norm:0.2478, lr:4.5864e-04 dt: 3331.99ms, tok/sec:157349.99
step 5689, loss: 3.486794, norm:0.2722, lr:4.5859e-04 dt: 3332.60ms, tok/sec:157321.14
step 5690, loss: 3.492993, norm:0.2984, lr:4.5854e-04 dt: 3331.93ms, tok/sec:157352.41
step 5691, loss: 3.554944, norm:0.3607, lr:4.5848e-04 dt: 3332.01ms, tok/sec:157348.90
step 5692, loss: 3.498223, norm:0.3026, lr:4.5843e-04 dt: 3332.08ms, tok/sec:157345.66
step 5693, loss: 3.423335, norm:0.2863, lr:4.5838e-04 dt: 3332.05ms, tok/sec:157347.13
step 5694, loss: 3.454492, norm:0.2602, lr:4.5833e-04 dt: 3331.99ms, tok/sec:157349.86
step 5695, loss: 3.429546, norm:0.2770, lr:4.5828e-04 dt: 3331.86ms, tok/sec:157355.72
step 5696, loss: 3.476214, norm:0.2723, lr:4.5823e-04 dt: 3332.36ms, tok/sec:157332.20
step 5697, loss: 3.438839, norm:0.2477, lr:4.5818e-04 dt: 3332.35ms, tok/sec:157332.90
step 5698, loss: 3.490712, norm:0.2713, lr:4.5812e-04 dt: 3332.08ms, tok/sec:157345.65
step 5699, loss: 3.387926, norm:0.2699, lr:4.5807e-04 dt: 3332.20ms, tok/sec:157339.79
validation loss: 3.4347
Model and optimizer state saved.
HellaSwag accuracy:2325074741439956049/-2=-1162537370719977984.0000
rank 1 sample 0: Hello, I'm a language model, and have been working on this for 16 weeks (in addition to my own translation). And when my translation has been finished
rank 1 sample 1: Hello, I'm a language model, you can learn new languages by using some of the resources:
There are no keywords...or they're not in the
rank 1 sample 2: Hello, I'm a language model, I live in a world of languages. I'm a native (native language) and I have the opportunity to communicate in
rank 1 sample 3: Hello, I'm a language model, and I'm in a conversation with it some time ago for getting through an afternoon. At 1pm, I am talking
rank 0 sample 0: Hello, I'm a language model, I'm writing a new tutorial about writing two files of something that seems kind of like "dna", the first being
rank 0 sample 1: Hello, I'm a language model, and one of the first languages in education. It teaches learners about different levels of literacy. This language will enable them to
rank 0 sample 2: Hello, I'm a language model, and I would say that when I speak to them, I'll speak to them.
So, I'm not quite
rank 0 sample 3: Hello, I'm a language model, and want to understand how you're doing things we do.
You are a professor in IELTS and want lots
step 5700, loss: 3.428552, norm:0.2582, lr:4.5802e-04 dt: 56253.54ms, tok/sec:9320.09
step 5701, loss: 3.464087, norm:0.2756, lr:4.5797e-04 dt: 3332.43ms, tok/sec:157329.02
step 5702, loss: 3.443509, norm:0.2783, lr:4.5792e-04 dt: 3332.18ms, tok/sec:157340.62
step 5703, loss: 3.463620, norm:0.2743, lr:4.5787e-04 dt: 3332.15ms, tok/sec:157342.09
step 5704, loss: 3.467222, norm:0.2648, lr:4.5782e-04 dt: 3331.99ms, tok/sec:157349.82
step 5705, loss: 3.392117, norm:0.2509, lr:4.5777e-04 dt: 3331.90ms, tok/sec:157354.03
step 5706, loss: 3.391642, norm:0.2781, lr:4.5771e-04 dt: 3332.23ms, tok/sec:157338.43
step 5707, loss: 3.419983, norm:0.2651, lr:4.5766e-04 dt: 3332.01ms, tok/sec:157348.72
step 5708, loss: 3.324560, norm:0.2619, lr:4.5761e-04 dt: 3331.88ms, tok/sec:157354.90
step 5709, loss: 3.371026, norm:0.2634, lr:4.5756e-04 dt: 3332.20ms, tok/sec:157340.04
step 5710, loss: 3.436083, norm:0.2637, lr:4.5751e-04 dt: 3332.29ms, tok/sec:157335.84
step 5711, loss: 3.391477, norm:0.2706, lr:4.5746e-04 dt: 3332.14ms, tok/sec:157342.76
step 5712, loss: 3.416441, norm:0.2726, lr:4.5741e-04 dt: 3332.03ms, tok/sec:157348.07
step 5713, loss: 3.432022, norm:0.2578, lr:4.5735e-04 dt: 3331.98ms, tok/sec:157350.34
step 5714, loss: 3.424217, norm:0.2533, lr:4.5730e-04 dt: 3334.22ms, tok/sec:157244.65
step 5715, loss: 3.286663, norm:0.2798, lr:4.5725e-04 dt: 3332.19ms, tok/sec:157340.15
step 5716, loss: 3.088991, norm:0.2739, lr:4.5720e-04 dt: 3331.98ms, tok/sec:157350.09
step 5717, loss: 3.221067, norm:0.3169, lr:4.5715e-04 dt: 3331.76ms, tok/sec:157360.62
step 5718, loss: 3.206054, norm:0.3022, lr:4.5710e-04 dt: 3332.36ms, tok/sec:157332.35
step 5719, loss: 3.162319, norm:0.2894, lr:4.5705e-04 dt: 3332.08ms, tok/sec:157345.61
step 5720, loss: 3.161657, norm:0.2806, lr:4.5699e-04 dt: 3331.72ms, tok/sec:157362.36
step 5721, loss: 3.159206, norm:0.2656, lr:4.5694e-04 dt: 3331.82ms, tok/sec:157357.63
step 5722, loss: 3.189543, norm:0.2674, lr:4.5689e-04 dt: 3331.96ms, tok/sec:157351.12
step 5723, loss: 3.138429, norm:0.2631, lr:4.5684e-04 dt: 3332.06ms, tok/sec:157346.69
step 5724, loss: 3.219329, norm:0.2668, lr:4.5679e-04 dt: 3331.92ms, tok/sec:157353.34
step 5725, loss: 3.248123, norm:0.2378, lr:4.5674e-04 dt: 3332.09ms, tok/sec:157345.18
step 5726, loss: 3.125891, norm:0.2539, lr:4.5669e-04 dt: 3332.06ms, tok/sec:157346.34
step 5727, loss: 3.339673, norm:0.2799, lr:4.5663e-04 dt: 3332.13ms, tok/sec:157343.43
step 5728, loss: 3.480664, norm:0.2472, lr:4.5658e-04 dt: 3332.11ms, tok/sec:157344.05
step 5729, loss: 3.459497, norm:0.2887, lr:4.5653e-04 dt: 3332.34ms, tok/sec:157333.32
step 5730, loss: 3.458975, norm:0.2552, lr:4.5648e-04 dt: 3332.24ms, tok/sec:157337.90
step 5731, loss: 3.458834, norm:0.2693, lr:4.5643e-04 dt: 3332.05ms, tok/sec:157346.85
step 5732, loss: 3.468572, norm:0.2762, lr:4.5638e-04 dt: 3331.84ms, tok/sec:157356.87
step 5733, loss: 3.501133, norm:0.2614, lr:4.5632e-04 dt: 3332.10ms, tok/sec:157344.64
step 5734, loss: 3.431151, norm:0.2689, lr:4.5627e-04 dt: 3332.32ms, tok/sec:157334.17
step 5735, loss: 3.453562, norm:0.2577, lr:4.5622e-04 dt: 3331.94ms, tok/sec:157352.29
step 5736, loss: 3.452433, norm:0.2464, lr:4.5617e-04 dt: 3332.19ms, tok/sec:157340.41
step 5737, loss: 3.452291, norm:0.2691, lr:4.5612e-04 dt: 3332.21ms, tok/sec:157339.41
step 5738, loss: 3.393899, norm:0.2849, lr:4.5607e-04 dt: 3332.43ms, tok/sec:157329.23
step 5739, loss: 3.440759, norm:0.2301, lr:4.5602e-04 dt: 3331.98ms, tok/sec:157350.05
step 5740, loss: 3.528207, norm:0.2726, lr:4.5596e-04 dt: 3331.98ms, tok/sec:157350.09
step 5741, loss: 3.385432, norm:0.2618, lr:4.5591e-04 dt: 3331.78ms, tok/sec:157359.83
step 5742, loss: 3.449405, norm:0.2546, lr:4.5586e-04 dt: 3332.04ms, tok/sec:157347.64
step 5743, loss: 3.519345, norm:0.2539, lr:4.5581e-04 dt: 3332.11ms, tok/sec:157344.33
step 5744, loss: 3.411903, norm:0.2611, lr:4.5576e-04 dt: 3331.91ms, tok/sec:157353.52
step 5745, loss: 3.442351, norm:0.2297, lr:4.5571e-04 dt: 3331.99ms, tok/sec:157349.91
step 5746, loss: 3.465562, norm:0.2615, lr:4.5565e-04 dt: 3332.17ms, tok/sec:157341.39
step 5747, loss: 3.423703, norm:0.2343, lr:4.5560e-04 dt: 3332.25ms, tok/sec:157337.47
step 5748, loss: 3.421375, norm:0.2482, lr:4.5555e-04 dt: 3331.92ms, tok/sec:157353.04
step 5749, loss: 3.445561, norm:0.2304, lr:4.5550e-04 dt: 3332.07ms, tok/sec:157346.00
HellaSwag accuracy:2325251213057262673/-2=-1162625606528631296.0000
rank 1 sample 0: Hello, I'm a language model,
that is, of course, really good; it uses one of the great languages
to do the job.

rank 1 sample 1: Hello, I'm a language model, a model for you.
"My dad is a professor in the Department of Educational Language Arts. I'm a lingu
rank 1 sample 2: Hello, I'm a language model, so whenever someone comes to you, you're going to talk about your favorite foreign language. So the most common foreign languages
rank 1 sample 3: Hello, I'm a language model, and I'm also an intern with linguistics.
Tailor-Webster Web Reference is a course offered by
rank 0 sample 0: Hello, I'm a language model, and I'll be using an object for later discussion.
The question doesn't have to be, “What if
rank 0 sample 1: Hello, I'm a language model, I'd like to use the following way: "Sell" to help the reader get through this article.
If
rank 0 sample 2: Hello, I'm a language model, but I’m an example.
But with the same set of rules, we can use the same language.
rank 0 sample 3: Hello, I'm a language model, I really want to be able to tell me its a little bit by a lot I really wanted to see what the thing
step 5750, loss: 3.377532, norm:0.2567, lr:4.5545e-04 dt: 48517.70ms, tok/sec:10806.12
step 5751, loss: 3.369016, norm:0.2318, lr:4.5540e-04 dt: 3332.02ms, tok/sec:157348.24
step 5752, loss: 3.416132, norm:0.2521, lr:4.5534e-04 dt: 3331.95ms, tok/sec:157351.47
step 5753, loss: 3.360510, norm:0.2286, lr:4.5529e-04 dt: 3331.91ms, tok/sec:157353.69
step 5754, loss: 3.385229, norm:0.2607, lr:4.5524e-04 dt: 3332.10ms, tok/sec:157344.79
step 5755, loss: 3.343941, norm:0.2277, lr:4.5519e-04 dt: 3331.91ms, tok/sec:157353.37
step 5756, loss: 3.357600, norm:0.2526, lr:4.5514e-04 dt: 3331.81ms, tok/sec:157358.13
step 5757, loss: 3.394081, norm:0.3055, lr:4.5509e-04 dt: 3331.92ms, tok/sec:157352.92
step 5758, loss: 3.405283, norm:0.3038, lr:4.5503e-04 dt: 3331.95ms, tok/sec:157351.47
step 5759, loss: 3.370676, norm:0.2592, lr:4.5498e-04 dt: 3332.37ms, tok/sec:157331.81
step 5760, loss: 3.400662, norm:0.2715, lr:4.5493e-04 dt: 3332.02ms, tok/sec:157348.38
step 5761, loss: 3.172213, norm:0.2928, lr:4.5488e-04 dt: 3332.02ms, tok/sec:157348.63
step 5762, loss: 3.112942, norm:0.2804, lr:4.5483e-04 dt: 3331.91ms, tok/sec:157353.78
step 5763, loss: 3.152137, norm:0.2900, lr:4.5478e-04 dt: 3331.73ms, tok/sec:157361.87
step 5764, loss: 3.177913, norm:0.2914, lr:4.5472e-04 dt: 3331.75ms, tok/sec:157361.15
step 5765, loss: 3.142295, norm:0.2429, lr:4.5467e-04 dt: 3332.01ms, tok/sec:157348.71
step 5766, loss: 3.150844, norm:0.2761, lr:4.5462e-04 dt: 3331.93ms, tok/sec:157352.77
step 5767, loss: 3.178058, norm:0.2442, lr:4.5457e-04 dt: 3332.00ms, tok/sec:157349.54
step 5768, loss: 3.141765, norm:0.2461, lr:4.5452e-04 dt: 3332.41ms, tok/sec:157330.05
step 5769, loss: 3.146844, norm:0.2373, lr:4.5447e-04 dt: 3331.93ms, tok/sec:157352.54
step 5770, loss: 3.165967, norm:0.2349, lr:4.5441e-04 dt: 3332.14ms, tok/sec:157342.69
step 5771, loss: 3.161986, norm:0.2365, lr:4.5436e-04 dt: 3331.81ms, tok/sec:157358.32
step 5772, loss: 3.225900, norm:0.2416, lr:4.5431e-04 dt: 3331.89ms, tok/sec:157354.30
step 5773, loss: 3.384345, norm:0.2464, lr:4.5426e-04 dt: 3332.09ms, tok/sec:157344.99
step 5774, loss: 3.408511, norm:0.2547, lr:4.5421e-04 dt: 3332.26ms, tok/sec:157337.18
step 5775, loss: 3.423590, norm:0.2362, lr:4.5415e-04 dt: 3331.98ms, tok/sec:157350.26
step 5776, loss: 3.398746, norm:0.2651, lr:4.5410e-04 dt: 3331.96ms, tok/sec:157351.40
step 5777, loss: 3.371975, norm:0.2265, lr:4.5405e-04 dt: 3332.34ms, tok/sec:157333.38
step 5778, loss: 3.408283, norm:0.2546, lr:4.5400e-04 dt: 3332.99ms, tok/sec:157302.81
step 5779, loss: 3.354867, norm:0.2285, lr:4.5395e-04 dt: 3333.08ms, tok/sec:157298.43
step 5780, loss: 3.407424, norm:0.2332, lr:4.5390e-04 dt: 3332.25ms, tok/sec:157337.30
step 5781, loss: 3.456576, norm:0.2409, lr:4.5384e-04 dt: 3332.07ms, tok/sec:157345.92
step 5782, loss: 3.404517, norm:0.2412, lr:4.5379e-04 dt: 3332.00ms, tok/sec:157349.36
step 5783, loss: 3.412677, norm:0.2443, lr:4.5374e-04 dt: 3331.94ms, tok/sec:157352.18
step 5784, loss: 3.383626, norm:0.2596, lr:4.5369e-04 dt: 3331.85ms, tok/sec:157356.20
step 5785, loss: 3.415976, norm:0.2472, lr:4.5364e-04 dt: 3332.01ms, tok/sec:157348.69
step 5786, loss: 3.439080, norm:0.2543, lr:4.5358e-04 dt: 3332.24ms, tok/sec:157338.06
step 5787, loss: 3.397610, norm:0.2434, lr:4.5353e-04 dt: 3332.19ms, tok/sec:157340.26
step 5788, loss: 3.408738, norm:0.2508, lr:4.5348e-04 dt: 3331.89ms, tok/sec:157354.71
step 5789, loss: 3.418933, norm:0.2488, lr:4.5343e-04 dt: 3332.12ms, tok/sec:157343.71
step 5790, loss: 3.391424, norm:0.2550, lr:4.5338e-04 dt: 3332.09ms, tok/sec:157345.08
step 5791, loss: 3.416912, norm:0.2356, lr:4.5333e-04 dt: 3332.03ms, tok/sec:157347.73
step 5792, loss: 3.435785, norm:0.2390, lr:4.5327e-04 dt: 3332.02ms, tok/sec:157348.56
step 5793, loss: 3.410264, norm:0.2351, lr:4.5322e-04 dt: 3331.99ms, tok/sec:157349.80
step 5794, loss: 3.407477, norm:0.2524, lr:4.5317e-04 dt: 3332.35ms, tok/sec:157332.61
step 5795, loss: 3.447039, norm:0.2661, lr:4.5312e-04 dt: 3331.93ms, tok/sec:157352.63
step 5796, loss: 3.374061, norm:0.2718, lr:4.5307e-04 dt: 3332.63ms, tok/sec:157319.42
step 5797, loss: 3.429240, norm:0.2570, lr:4.5301e-04 dt: 3331.99ms, tok/sec:157350.00
step 5798, loss: 3.414947, norm:0.2816, lr:4.5296e-04 dt: 3332.13ms, tok/sec:157343.05
step 5799, loss: 3.404184, norm:0.2851, lr:4.5291e-04 dt: 3332.03ms, tok/sec:157347.93
validation loss: 3.4297
Model and optimizer state saved.
HellaSwag accuracy:4630901809615144009/-2=-2315450904807571968.0000
rank 1 sample 0: Hello, I'm a language model, and you have to keep the details protected, to be in harmony with the principles of a language like English and French.
rank 1 sample 1: Hello, I'm a language model, a computer system that can be used to process information. My name is Alan McReifold. I'm a computer
rank 1 sample 2: Hello, I'm a language model, but has never really been a language model.
"What should you say about the language of this book?" "Let
rank 1 sample 3: Hello, I'm a language model, and I'm interested to discover the potential possibilities of how new kinds of computers could be a valuable part of our digital world
rank 0 sample 0: Hello, I'm a language model, I'm gonna talk to you and get that done. I'm always confused when I say these languages are "good for
rank 0 sample 1: Hello, I'm a language model, so, my job is to find them. So next time I get excited for a little piece, think about how many
rank 0 sample 2: Hello, I'm a language model, and I had no idea it was wrong. Now I'm looking forward to the fact that the language model is not accurate
rank 0 sample 3: Hello, I'm a language model, so be sure to check out the full version [PDF] of this article in Acrobat (http://www.d
step 5800, loss: 3.371395, norm:0.2617, lr:4.5286e-04 dt: 56173.06ms, tok/sec:9333.44
step 5801, loss: 3.393080, norm:0.2905, lr:4.5281e-04 dt: 3332.13ms, tok/sec:157343.36
step 5802, loss: 3.426527, norm:0.2837, lr:4.5275e-04 dt: 3331.96ms, tok/sec:157351.11
step 5803, loss: 3.382100, norm:0.2890, lr:4.5270e-04 dt: 3332.33ms, tok/sec:157333.72
step 5804, loss: 3.373452, norm:0.2566, lr:4.5265e-04 dt: 3332.19ms, tok/sec:157340.56
step 5805, loss: 3.333011, norm:0.2893, lr:4.5260e-04 dt: 3332.16ms, tok/sec:157341.57
step 5806, loss: 3.310282, norm:0.3023, lr:4.5255e-04 dt: 3331.98ms, tok/sec:157350.43
step 5807, loss: 3.080087, norm:0.2855, lr:4.5249e-04 dt: 3331.93ms, tok/sec:157352.48
step 5808, loss: 3.242409, norm:0.3671, lr:4.5244e-04 dt: 3331.93ms, tok/sec:157352.55
step 5809, loss: 3.144143, norm:0.3039, lr:4.5239e-04 dt: 3332.05ms, tok/sec:157346.76
step 5810, loss: 3.131238, norm:0.2961, lr:4.5234e-04 dt: 3332.31ms, tok/sec:157334.92
step 5811, loss: 3.209067, norm:0.2925, lr:4.5229e-04 dt: 3332.06ms, tok/sec:157346.58
step 5812, loss: 3.184840, norm:0.2528, lr:4.5223e-04 dt: 3332.44ms, tok/sec:157328.37
step 5813, loss: 3.170444, norm:0.3085, lr:4.5218e-04 dt: 3332.05ms, tok/sec:157346.82
step 5814, loss: 3.154392, norm:0.2392, lr:4.5213e-04 dt: 3331.89ms, tok/sec:157354.65
step 5815, loss: 3.217008, norm:0.2749, lr:4.5208e-04 dt: 3331.80ms, tok/sec:157358.74
step 5816, loss: 3.150882, norm:0.2444, lr:4.5203e-04 dt: 3332.02ms, tok/sec:157348.30
step 5817, loss: 3.179341, norm:0.2799, lr:4.5197e-04 dt: 3331.92ms, tok/sec:157353.32
step 5818, loss: 3.466192, norm:0.2691, lr:4.5192e-04 dt: 3332.16ms, tok/sec:157341.63
step 5819, loss: 3.413095, norm:0.2569, lr:4.5187e-04 dt: 3331.98ms, tok/sec:157350.18
step 5820, loss: 3.443023, norm:0.2744, lr:4.5182e-04 dt: 3332.22ms, tok/sec:157338.91
step 5821, loss: 3.456182, norm:0.2747, lr:4.5177e-04 dt: 3332.11ms, tok/sec:157344.25
step 5822, loss: 3.383226, norm:0.2588, lr:4.5171e-04 dt: 3332.43ms, tok/sec:157328.84
step 5823, loss: 3.437445, norm:0.2792, lr:4.5166e-04 dt: 3331.85ms, tok/sec:157356.38
step 5824, loss: 3.418731, norm:0.2627, lr:4.5161e-04 dt: 3332.22ms, tok/sec:157339.05
step 5825, loss: 3.405288, norm:0.2901, lr:4.5156e-04 dt: 3332.11ms, tok/sec:157344.16
step 5826, loss: 3.386922, norm:0.2586, lr:4.5151e-04 dt: 3332.11ms, tok/sec:157344.26
step 5827, loss: 3.431710, norm:0.2995, lr:4.5145e-04 dt: 3332.08ms, tok/sec:157345.74
step 5828, loss: 3.414638, norm:0.2593, lr:4.5140e-04 dt: 3332.06ms, tok/sec:157346.45
step 5829, loss: 3.424788, norm:0.2550, lr:4.5135e-04 dt: 3332.38ms, tok/sec:157331.26
step 5830, loss: 3.428844, norm:0.2449, lr:4.5130e-04 dt: 3332.03ms, tok/sec:157348.11
step 5831, loss: 3.465657, norm:0.2533, lr:4.5125e-04 dt: 3332.26ms, tok/sec:157337.00
step 5832, loss: 3.408727, norm:0.2459, lr:4.5119e-04 dt: 3331.93ms, tok/sec:157352.60
step 5833, loss: 3.434255, norm:0.2528, lr:4.5114e-04 dt: 3332.20ms, tok/sec:157339.72
step 5834, loss: 3.421185, norm:0.2700, lr:4.5109e-04 dt: 3331.97ms, tok/sec:157350.80
step 5835, loss: 3.356159, norm:0.2803, lr:4.5104e-04 dt: 3331.99ms, tok/sec:157349.69
step 5836, loss: 3.427554, norm:0.2612, lr:4.5098e-04 dt: 3331.99ms, tok/sec:157349.81
step 5837, loss: 3.344127, norm:0.2491, lr:4.5093e-04 dt: 3332.30ms, tok/sec:157335.09
step 5838, loss: 3.405075, norm:0.2563, lr:4.5088e-04 dt: 3332.13ms, tok/sec:157343.03
step 5839, loss: 3.394426, norm:0.2542, lr:4.5083e-04 dt: 3331.93ms, tok/sec:157352.71
step 5840, loss: 3.407514, norm:0.2806, lr:4.5078e-04 dt: 3332.54ms, tok/sec:157323.84
step 5841, loss: 3.411312, norm:0.2445, lr:4.5072e-04 dt: 3332.16ms, tok/sec:157341.78
step 5842, loss: 3.375165, norm:0.2495, lr:4.5067e-04 dt: 3332.11ms, tok/sec:157344.23
step 5843, loss: 3.413182, norm:0.2337, lr:4.5062e-04 dt: 3331.99ms, tok/sec:157349.64
step 5844, loss: 3.300532, norm:0.2338, lr:4.5057e-04 dt: 3332.20ms, tok/sec:157340.04
step 5845, loss: 3.402047, norm:0.2433, lr:4.5052e-04 dt: 3332.15ms, tok/sec:157342.07
step 5846, loss: 3.410264, norm:0.2589, lr:4.5046e-04 dt: 3331.99ms, tok/sec:157349.76
step 5847, loss: 3.418049, norm:0.2381, lr:4.5041e-04 dt: 3332.27ms, tok/sec:157336.52
step 5848, loss: 3.396638, norm:0.2474, lr:4.5036e-04 dt: 3332.52ms, tok/sec:157324.60
step 5849, loss: 3.325619, norm:0.4433, lr:4.5031e-04 dt: 3332.05ms, tok/sec:157346.86
HellaSwag accuracy:2325260045657506897/-2=-1162630022828753408.0000
rank 1 sample 0: Hello, I'm a language model, we can change the definition of a standard to define what your business is.
I can say it's very easy to
rank 1 sample 1: Hello, I'm a language model, a computer that will be able to speak both English and Welsh. I'm trying to become a scriptwriter, but I
rank 1 sample 2: Hello, I'm a language model, but only when I'm a language model.
To get something out there, I'm a Language Model. You go
rank 1 sample 3: Hello, I'm a language model, and I'm talking about different types of environments.
At this point, each of us lives with a different environment.
rank 0 sample 0: Hello, I'm a language model, I'm trying to understand, understand, connect, and value your website. You can start there.<|endoftext|>As the global
rank 0 sample 1: Hello, I'm a language model, and am having a good time, i'm a really excited little boy and really enjoyed learning and working with Python.

rank 0 sample 2: Hello, I'm a language model, and I're the people for the blog. My experience is as if I'm going to be a language model.

rank 0 sample 3: Hello, I'm a language model, I started in Python. I've started to model it myself, and I have no other model that I'm not as
step 5850, loss: 3.414032, norm:0.2661, lr:4.5025e-04 dt: 48519.65ms, tok/sec:10805.68
step 5851, loss: 3.344572, norm:0.2509, lr:4.5020e-04 dt: 3332.25ms, tok/sec:157337.60
step 5852, loss: 3.242923, norm:0.2477, lr:4.5015e-04 dt: 3332.10ms, tok/sec:157344.52
step 5853, loss: 3.165785, norm:0.2573, lr:4.5010e-04 dt: 3332.03ms, tok/sec:157347.72
step 5854, loss: 3.179674, norm:0.2555, lr:4.5005e-04 dt: 3331.95ms, tok/sec:157351.93
step 5855, loss: 3.173500, norm:0.2746, lr:4.4999e-04 dt: 3331.87ms, tok/sec:157355.53
step 5856, loss: 3.142651, norm:0.2533, lr:4.4994e-04 dt: 3332.07ms, tok/sec:157345.97
step 5857, loss: 3.212371, norm:0.2475, lr:4.4989e-04 dt: 3332.04ms, tok/sec:157347.56
step 5858, loss: 3.104324, norm:0.2329, lr:4.4984e-04 dt: 3331.90ms, tok/sec:157354.12
step 5859, loss: 3.227543, norm:0.2421, lr:4.4978e-04 dt: 3332.27ms, tok/sec:157336.55
step 5860, loss: 3.143403, norm:0.2438, lr:4.4973e-04 dt: 3332.25ms, tok/sec:157337.33
step 5861, loss: 3.157269, norm:0.2691, lr:4.4968e-04 dt: 3332.03ms, tok/sec:157347.93
step 5862, loss: 3.100259, norm:0.2319, lr:4.4963e-04 dt: 3331.98ms, tok/sec:157350.19
step 5863, loss: 3.214453, norm:0.2628, lr:4.4957e-04 dt: 3331.87ms, tok/sec:157355.57
step 5864, loss: 3.415127, norm:0.2540, lr:4.4952e-04 dt: 3332.17ms, tok/sec:157341.49
step 5865, loss: 3.409446, norm:0.2363, lr:4.4947e-04 dt: 3331.97ms, tok/sec:157350.71
step 5866, loss: 3.386378, norm:0.2628, lr:4.4942e-04 dt: 3332.23ms, tok/sec:157338.65
step 5867, loss: 3.415968, norm:0.2628, lr:4.4937e-04 dt: 3332.28ms, tok/sec:157335.95
step 5868, loss: 3.481641, norm:0.2567, lr:4.4931e-04 dt: 3332.44ms, tok/sec:157328.47
step 5869, loss: 3.425945, norm:0.2493, lr:4.4926e-04 dt: 3332.16ms, tok/sec:157341.64
step 5870, loss: 3.468496, norm:0.2319, lr:4.4921e-04 dt: 3332.04ms, tok/sec:157347.59
step 5871, loss: 3.402121, norm:0.2571, lr:4.4916e-04 dt: 3332.26ms, tok/sec:157337.26
step 5872, loss: 3.468430, norm:0.2428, lr:4.4910e-04 dt: 3332.19ms, tok/sec:157340.59
step 5873, loss: 3.423674, norm:0.2852, lr:4.4905e-04 dt: 3331.95ms, tok/sec:157351.73
step 5874, loss: 3.474840, norm:0.2607, lr:4.4900e-04 dt: 3332.19ms, tok/sec:157340.41
step 5875, loss: 3.427594, norm:0.2591, lr:4.4895e-04 dt: 3332.43ms, tok/sec:157328.92
step 5876, loss: 3.467793, norm:0.2782, lr:4.4889e-04 dt: 3332.07ms, tok/sec:157346.25
step 5877, loss: 3.458499, norm:0.2673, lr:4.4884e-04 dt: 3332.30ms, tok/sec:157335.03
step 5878, loss: 3.431906, norm:0.2475, lr:4.4879e-04 dt: 3332.05ms, tok/sec:157346.95
step 5879, loss: 3.452428, norm:0.2486, lr:4.4874e-04 dt: 3332.15ms, tok/sec:157342.32
step 5880, loss: 3.407120, norm:0.2674, lr:4.4869e-04 dt: 3332.15ms, tok/sec:157342.34
step 5881, loss: 3.428289, norm:0.2397, lr:4.4863e-04 dt: 3332.15ms, tok/sec:157342.04
step 5882, loss: 3.463642, norm:0.3007, lr:4.4858e-04 dt: 3331.98ms, tok/sec:157350.22
step 5883, loss: 3.410305, norm:0.2658, lr:4.4853e-04 dt: 3332.74ms, tok/sec:157314.36
step 5884, loss: 3.394879, norm:0.2389, lr:4.4848e-04 dt: 3332.07ms, tok/sec:157346.00
step 5885, loss: 3.437726, norm:0.2630, lr:4.4842e-04 dt: 3331.84ms, tok/sec:157356.93
step 5886, loss: 3.371357, norm:0.2529, lr:4.4837e-04 dt: 3332.21ms, tok/sec:157339.21
step 5887, loss: 3.366739, norm:0.2457, lr:4.4832e-04 dt: 3331.98ms, tok/sec:157350.23
step 5888, loss: 3.295327, norm:0.2529, lr:4.4827e-04 dt: 3332.21ms, tok/sec:157339.35
step 5889, loss: 3.408890, norm:0.2616, lr:4.4821e-04 dt: 3332.02ms, tok/sec:157348.37
step 5890, loss: 3.363577, norm:0.2590, lr:4.4816e-04 dt: 3332.39ms, tok/sec:157330.98
step 5891, loss: 3.441471, norm:0.2550, lr:4.4811e-04 dt: 3331.91ms, tok/sec:157353.72
step 5892, loss: 3.389983, norm:0.2333, lr:4.4806e-04 dt: 3332.10ms, tok/sec:157344.39
step 5893, loss: 3.397971, norm:0.2572, lr:4.4800e-04 dt: 3331.98ms, tok/sec:157350.21
step 5894, loss: 3.349729, norm:0.2266, lr:4.4795e-04 dt: 3332.00ms, tok/sec:157349.17
step 5895, loss: 3.382412, norm:0.2366, lr:4.4790e-04 dt: 3332.20ms, tok/sec:157340.08
step 5896, loss: 3.357367, norm:0.2370, lr:4.4785e-04 dt: 3332.28ms, tok/sec:157336.32
step 5897, loss: 3.367409, norm:0.2547, lr:4.4779e-04 dt: 3332.40ms, tok/sec:157330.28
step 5898, loss: 3.452967, norm:0.2542, lr:4.4774e-04 dt: 3331.97ms, tok/sec:157350.95
step 5899, loss: 3.437891, norm:0.2579, lr:4.4769e-04 dt: 3332.24ms, tok/sec:157337.91
validation loss: 3.4226
Model and optimizer state saved.
HellaSwag accuracy:-2286469953234697199/-2=1143234976617348608.0000
rank 1 sample 0: Hello, I'm a language model, just like the language model, where characters, subjunctions and subjunctions are created to describe the relationships between the
rank 1 sample 1: Hello, I'm a language model, a computer, and a computer. In what I have mentioned, I'm using a big supercomputer. I'm using
rank 1 sample 2: Hello, I'm a language model, so can be used to create a new language.
Now if we have a language, and then it is going to
rank 1 sample 3: Hello, I'm a language model, and I'm just trying to find that object.
Before moving on to Java, do we need to know what it
rank 0 sample 0: Hello, I'm a language model, and I'll be using language that makes communication faster and more fun. One example is that of language that has been invented
rank 0 sample 1: Hello, I'm a language model, I hope to be able to do some things in case some people get angry, and that's why I have been trying
rank 0 sample 2: Hello, I'm a language model, I'm learning the skills. I'll be blogging about this post for this week. I'm a language model.

rank 0 sample 3: Hello, I'm a language model, it could be an interesting place for learning about web development.
As for how web development started, I'm going looking
step 5900, loss: 3.425282, norm:0.2574, lr:4.4764e-04 dt: 56168.80ms, tok/sec:9334.15
step 5901, loss: 3.460765, norm:0.2675, lr:4.4758e-04 dt: 3332.00ms, tok/sec:157349.19
step 5902, loss: 3.380522, norm:0.2484, lr:4.4753e-04 dt: 3332.06ms, tok/sec:157346.34
step 5903, loss: 3.455764, norm:0.2863, lr:4.4748e-04 dt: 3331.79ms, tok/sec:157359.43
step 5904, loss: 3.398414, norm:0.2600, lr:4.4743e-04 dt: 3332.05ms, tok/sec:157347.18
step 5905, loss: 3.467455, norm:0.2785, lr:4.4737e-04 dt: 3334.57ms, tok/sec:157228.11
step 5906, loss: 3.385830, norm:0.3455, lr:4.4732e-04 dt: 3332.08ms, tok/sec:157345.40
step 5907, loss: 3.371192, norm:0.2669, lr:4.4727e-04 dt: 3332.29ms, tok/sec:157335.71
step 5908, loss: 3.456684, norm:0.2525, lr:4.4722e-04 dt: 3331.87ms, tok/sec:157355.61
step 5909, loss: 3.389779, norm:0.2563, lr:4.4716e-04 dt: 3332.42ms, tok/sec:157329.74
step 5910, loss: 3.427601, norm:0.2436, lr:4.4711e-04 dt: 3332.03ms, tok/sec:157347.91
step 5911, loss: 3.457361, norm:0.2441, lr:4.4706e-04 dt: 3332.00ms, tok/sec:157349.15
step 5912, loss: 3.406987, norm:0.2364, lr:4.4701e-04 dt: 3332.03ms, tok/sec:157347.70
step 5913, loss: 3.444041, norm:0.2407, lr:4.4695e-04 dt: 3332.14ms, tok/sec:157342.73
step 5914, loss: 3.412431, norm:0.2634, lr:4.4690e-04 dt: 3332.01ms, tok/sec:157348.82
step 5915, loss: 3.468950, norm:0.2686, lr:4.4685e-04 dt: 3331.80ms, tok/sec:157358.90
step 5916, loss: 3.432686, norm:0.2805, lr:4.4680e-04 dt: 3332.23ms, tok/sec:157338.52
step 5917, loss: 3.414576, norm:0.2731, lr:4.4674e-04 dt: 3332.20ms, tok/sec:157339.70
step 5918, loss: 3.373561, norm:0.2622, lr:4.4669e-04 dt: 3332.03ms, tok/sec:157347.80
step 5919, loss: 3.415068, norm:0.2506, lr:4.4664e-04 dt: 3332.21ms, tok/sec:157339.52
step 5920, loss: 3.435623, norm:0.2645, lr:4.4659e-04 dt: 3332.39ms, tok/sec:157330.96
step 5921, loss: 3.377604, norm:0.2422, lr:4.4653e-04 dt: 3332.04ms, tok/sec:157347.53
step 5922, loss: 3.350230, norm:0.2527, lr:4.4648e-04 dt: 3332.02ms, tok/sec:157348.40
step 5923, loss: 3.389441, norm:0.2664, lr:4.4643e-04 dt: 3331.97ms, tok/sec:157350.72
step 5924, loss: 3.413868, norm:0.2494, lr:4.4637e-04 dt: 3331.77ms, tok/sec:157360.33
step 5925, loss: 3.370065, norm:0.2540, lr:4.4632e-04 dt: 3332.13ms, tok/sec:157343.17
step 5926, loss: 3.393545, norm:0.2335, lr:4.4627e-04 dt: 3332.19ms, tok/sec:157340.14
step 5927, loss: 3.379237, norm:0.2583, lr:4.4622e-04 dt: 3332.10ms, tok/sec:157344.70
step 5928, loss: 3.370920, norm:0.2546, lr:4.4616e-04 dt: 3332.24ms, tok/sec:157337.97
step 5929, loss: 3.439168, norm:0.2517, lr:4.4611e-04 dt: 3332.41ms, tok/sec:157330.12
step 5930, loss: 3.357328, norm:0.2487, lr:4.4606e-04 dt: 3331.94ms, tok/sec:157352.13
step 5931, loss: 3.381158, norm:0.2810, lr:4.4601e-04 dt: 3332.06ms, tok/sec:157346.68
step 5932, loss: 3.554512, norm:0.2969, lr:4.4595e-04 dt: 3331.93ms, tok/sec:157352.53
step 5933, loss: 3.431437, norm:0.3263, lr:4.4590e-04 dt: 3332.03ms, tok/sec:157348.02
step 5934, loss: 3.409464, norm:0.2680, lr:4.4585e-04 dt: 3332.30ms, tok/sec:157335.28
step 5935, loss: 3.429331, norm:0.2824, lr:4.4580e-04 dt: 3331.93ms, tok/sec:157352.71
step 5936, loss: 3.470369, norm:0.2652, lr:4.4574e-04 dt: 3331.88ms, tok/sec:157355.24
step 5937, loss: 3.424778, norm:0.2996, lr:4.4569e-04 dt: 3331.97ms, tok/sec:157350.86
step 5938, loss: 3.398044, norm:0.2652, lr:4.4564e-04 dt: 3332.29ms, tok/sec:157335.82
step 5939, loss: 3.394192, norm:0.3021, lr:4.4558e-04 dt: 3332.19ms, tok/sec:157340.21
step 5940, loss: 3.420982, norm:0.2519, lr:4.4553e-04 dt: 3331.82ms, tok/sec:157357.81
step 5941, loss: 3.488935, norm:0.2984, lr:4.4548e-04 dt: 3332.01ms, tok/sec:157348.75
step 5942, loss: 3.461183, norm:0.2548, lr:4.4543e-04 dt: 3332.07ms, tok/sec:157346.07
step 5943, loss: 3.423852, norm:0.2646, lr:4.4537e-04 dt: 3332.01ms, tok/sec:157348.85
step 5944, loss: 3.437054, norm:0.2535, lr:4.4532e-04 dt: 3332.06ms, tok/sec:157346.55
step 5945, loss: 3.351166, norm:0.2759, lr:4.4527e-04 dt: 3332.18ms, tok/sec:157341.05
step 5946, loss: 3.479415, norm:0.2791, lr:4.4522e-04 dt: 3332.16ms, tok/sec:157342.00
step 5947, loss: 3.378938, norm:0.2900, lr:4.4516e-04 dt: 3332.42ms, tok/sec:157329.74
step 5948, loss: 3.433101, norm:0.2558, lr:4.4511e-04 dt: 3332.06ms, tok/sec:157346.48
step 5949, loss: 3.417555, norm:0.2581, lr:4.4506e-04 dt: 3332.01ms, tok/sec:157349.03
HellaSwag accuracy:2325217164704023633/-2=-1162608582352011776.0000
rank 1 sample 0: Hello, I'm a language model, meaning "We're just a small handful" -- that's true.
When I'm in this, I'm not
rank 1 sample 1: Hello, I'm a language model, which means I model the language I model by using the command line function. If I didn't mind the command line function
rank 1 sample 2: Hello, I'm a language model, I live in a world of language models. I'm thinking: "Well, I'm a language model. We really
rank 1 sample 3: Hello, I'm a language model, and I'm using a set of words used in the conversation with mine about 'I used so much' and 'I
rank 0 sample 0: Hello, I'm a language model, I'm talking about how many things there are when we don't talk about languages. We're trying to be a language
rank 0 sample 1: Hello, I'm a language model, and like what I do, we might be doing grammar to a few grammar-based classes. I might even be doing
rank 0 sample 2: Hello, I'm a language model, and I will work with many different languages. My class is not interested in the new languages, but I'm going to
rank 0 sample 3: Hello, I'm a language model, I thought I'd be interested to hear what one of the language models has been before. We'll probably be using those
step 5950, loss: 3.418783, norm:0.2360, lr:4.4500e-04 dt: 48519.73ms, tok/sec:10805.67
step 5951, loss: 3.482412, norm:0.2585, lr:4.4495e-04 dt: 3331.89ms, tok/sec:157354.35
step 5952, loss: 3.396592, norm:0.2604, lr:4.4490e-04 dt: 3331.78ms, tok/sec:157359.80
step 5953, loss: 3.468529, norm:0.2612, lr:4.4485e-04 dt: 3332.07ms, tok/sec:157345.81
step 5954, loss: 3.412552, norm:0.2642, lr:4.4479e-04 dt: 3331.84ms, tok/sec:157356.67
step 5955, loss: 3.409554, norm:0.3062, lr:4.4474e-04 dt: 3331.91ms, tok/sec:157353.46
step 5956, loss: 3.394108, norm:0.2834, lr:4.4469e-04 dt: 3332.04ms, tok/sec:157347.67
step 5957, loss: 3.376331, norm:0.2471, lr:4.4464e-04 dt: 3332.03ms, tok/sec:157348.06
step 5958, loss: 3.360091, norm:0.2900, lr:4.4458e-04 dt: 3332.23ms, tok/sec:157338.52
step 5959, loss: 3.438973, norm:0.2450, lr:4.4453e-04 dt: 3332.02ms, tok/sec:157348.26
step 5960, loss: 3.418973, norm:0.2838, lr:4.4448e-04 dt: 3331.93ms, tok/sec:157352.64
step 5961, loss: 3.397089, norm:0.2726, lr:4.4442e-04 dt: 3332.45ms, tok/sec:157327.99
step 5962, loss: 3.380241, norm:0.2442, lr:4.4437e-04 dt: 3332.11ms, tok/sec:157344.22
step 5963, loss: 3.391068, norm:0.2559, lr:4.4432e-04 dt: 3331.85ms, tok/sec:157356.53
step 5964, loss: 3.387609, norm:0.2503, lr:4.4427e-04 dt: 3332.04ms, tok/sec:157347.29
step 5965, loss: 3.419058, norm:0.2854, lr:4.4421e-04 dt: 3331.96ms, tok/sec:157351.12
step 5966, loss: 3.539056, norm:0.2489, lr:4.4416e-04 dt: 3332.36ms, tok/sec:157332.26
step 5967, loss: 3.388890, norm:0.2794, lr:4.4411e-04 dt: 3332.03ms, tok/sec:157348.08
step 5968, loss: 3.393976, norm:0.2549, lr:4.4405e-04 dt: 3331.82ms, tok/sec:157357.92
step 5969, loss: 3.474018, norm:0.2953, lr:4.4400e-04 dt: 3332.08ms, tok/sec:157345.53
step 5970, loss: 3.458540, norm:0.2722, lr:4.4395e-04 dt: 3332.05ms, tok/sec:157347.02
step 5971, loss: 3.415751, norm:0.2611, lr:4.4390e-04 dt: 3332.50ms, tok/sec:157325.88
step 5972, loss: 3.406310, norm:0.2976, lr:4.4384e-04 dt: 3331.88ms, tok/sec:157354.90
step 5973, loss: 3.461013, norm:0.2852, lr:4.4379e-04 dt: 3332.18ms, tok/sec:157340.93
step 5974, loss: 3.434924, norm:0.2921, lr:4.4374e-04 dt: 3331.99ms, tok/sec:157349.95
step 5975, loss: 3.410480, norm:0.2840, lr:4.4368e-04 dt: 3332.14ms, tok/sec:157342.62
step 5976, loss: 3.456607, norm:0.2839, lr:4.4363e-04 dt: 3331.97ms, tok/sec:157350.88
step 5977, loss: 3.450777, norm:0.2939, lr:4.4358e-04 dt: 3332.06ms, tok/sec:157346.43
step 5978, loss: 3.348871, norm:0.2528, lr:4.4353e-04 dt: 3332.41ms, tok/sec:157329.76
step 5979, loss: 3.437448, norm:0.2537, lr:4.4347e-04 dt: 3331.91ms, tok/sec:157353.50
step 5980, loss: 3.478466, norm:0.2471, lr:4.4342e-04 dt: 3332.45ms, tok/sec:157328.06
step 5981, loss: 3.416598, norm:0.2620, lr:4.4337e-04 dt: 3332.12ms, tok/sec:157343.84
step 5982, loss: 3.394515, norm:0.2611, lr:4.4331e-04 dt: 3332.16ms, tok/sec:157341.96
step 5983, loss: 3.453152, norm:0.2540, lr:4.4326e-04 dt: 3331.85ms, tok/sec:157356.66
step 5984, loss: 3.432719, norm:0.2482, lr:4.4321e-04 dt: 3332.11ms, tok/sec:157344.08
step 5985, loss: 3.381542, norm:0.3309, lr:4.4315e-04 dt: 3331.96ms, tok/sec:157351.29
step 5986, loss: 3.392145, norm:0.2966, lr:4.4310e-04 dt: 3332.04ms, tok/sec:157347.41
step 5987, loss: 3.401603, norm:0.2675, lr:4.4305e-04 dt: 3332.16ms, tok/sec:157341.62
step 5988, loss: 3.281591, norm:0.2930, lr:4.4300e-04 dt: 3332.39ms, tok/sec:157330.93
step 5989, loss: 3.376848, norm:0.2610, lr:4.4294e-04 dt: 3331.97ms, tok/sec:157350.67
step 5990, loss: 3.413789, norm:0.2948, lr:4.4289e-04 dt: 3332.10ms, tok/sec:157344.73
step 5991, loss: 3.369490, norm:0.2798, lr:4.4284e-04 dt: 3332.43ms, tok/sec:157329.22
step 5992, loss: 3.281737, norm:0.2564, lr:4.4278e-04 dt: 3332.93ms, tok/sec:157305.65
step 5993, loss: 3.401652, norm:0.2736, lr:4.4273e-04 dt: 3332.26ms, tok/sec:157337.16
step 5994, loss: 3.319005, norm:0.2366, lr:4.4268e-04 dt: 3332.12ms, tok/sec:157343.62
step 5995, loss: 3.366526, norm:0.2508, lr:4.4262e-04 dt: 3332.35ms, tok/sec:157333.02
step 5996, loss: 3.414018, norm:0.2452, lr:4.4257e-04 dt: 3332.32ms, tok/sec:157334.30
step 5997, loss: 3.366807, norm:0.2365, lr:4.4252e-04 dt: 3332.30ms, tok/sec:157335.20
step 5998, loss: 3.383416, norm:0.2642, lr:4.4247e-04 dt: 3332.15ms, tok/sec:157342.28
step 5999, loss: 3.372251, norm:0.2594, lr:4.4241e-04 dt: 3332.04ms, tok/sec:157347.41
validation loss: 3.4187
Model and optimizer state saved.
HellaSwag accuracy:4630935929103811601/-2=-2315467964551905792.0000
rank 1 sample 0: Hello, I'm a language model, and so I'm working with it the other day! You got it all from me! So let's say you're
rank 1 sample 1: Hello, I'm a language model, a programmer. So I'm just using something that I love, my family just keeps repeating, every day.
I
rank 1 sample 2: Hello, I'm a language model, which runs the entire language. I'm a language model and model myself as a language model. And, as always,
rank 1 sample 3: Hello, I'm a language model, and I'm the author of a bestselling educational blog. After I joined LST, I did some research and decided to
rank 0 sample 0: Hello, I'm a language model, and I think it is useful, not even used at the same time.<|endoftext|>The National Basketball Association is now calling upon
rank 0 sample 1: Hello, I'm a language model, I mean for the language model and are a little frustrated but it has always been my own style of designing a language model
rank 0 sample 2: Hello, I'm a language model, I'm all very "new" the way in which I teach the language. I have a lot of people in the
rank 0 sample 3: Hello, I'm a language model, which uses the word "language." The first words are "as the" (lugger) and "as-
step 6000, loss: 3.422901, norm:0.2482, lr:4.4236e-04 dt: 56182.49ms, tok/sec:9331.87
step 6001, loss: 3.392652, norm:0.2473, lr:4.4231e-04 dt: 3332.28ms, tok/sec:157336.17
step 6002, loss: 3.456224, norm:0.2447, lr:4.4225e-04 dt: 3332.14ms, tok/sec:157342.61
step 6003, loss: 3.427868, norm:0.2691, lr:4.4220e-04 dt: 3332.20ms, tok/sec:157339.86
step 6004, loss: 3.440632, norm:0.2624, lr:4.4215e-04 dt: 3332.37ms, tok/sec:157331.70
step 6005, loss: 3.490189, norm:0.2789, lr:4.4209e-04 dt: 3331.94ms, tok/sec:157352.18
step 6006, loss: 3.373258, norm:0.3240, lr:4.4204e-04 dt: 3331.98ms, tok/sec:157350.24
step 6007, loss: 3.433151, norm:0.2590, lr:4.4199e-04 dt: 3331.93ms, tok/sec:157352.87
step 6008, loss: 3.353797, norm:0.2959, lr:4.4194e-04 dt: 3331.86ms, tok/sec:157355.79
step 6009, loss: 3.425236, norm:0.2832, lr:4.4188e-04 dt: 3331.96ms, tok/sec:157351.42
step 6010, loss: 3.448002, norm:0.2765, lr:4.4183e-04 dt: 3331.99ms, tok/sec:157349.96
step 6011, loss: 3.453630, norm:0.2730, lr:4.4178e-04 dt: 3332.08ms, tok/sec:157345.62
step 6012, loss: 3.448596, norm:0.2672, lr:4.4172e-04 dt: 3332.33ms, tok/sec:157333.70
step 6013, loss: 3.442294, norm:0.2514, lr:4.4167e-04 dt: 3332.23ms, tok/sec:157338.65
step 6014, loss: 3.369473, norm:0.2820, lr:4.4162e-04 dt: 3332.24ms, tok/sec:157338.05
step 6015, loss: 3.426640, norm:0.2583, lr:4.4156e-04 dt: 3332.02ms, tok/sec:157348.55
step 6016, loss: 3.420179, norm:0.2752, lr:4.4151e-04 dt: 3331.92ms, tok/sec:157353.28
step 6017, loss: 3.383213, norm:0.2633, lr:4.4146e-04 dt: 3332.12ms, tok/sec:157343.78
step 6018, loss: 3.415237, norm:0.2737, lr:4.4140e-04 dt: 3331.78ms, tok/sec:157359.56
step 6019, loss: 3.430050, norm:0.2704, lr:4.4135e-04 dt: 3332.09ms, tok/sec:157344.96
step 6020, loss: 3.391469, norm:0.3190, lr:4.4130e-04 dt: 3332.12ms, tok/sec:157343.45
step 6021, loss: 3.361894, norm:0.3211, lr:4.4124e-04 dt: 3332.19ms, tok/sec:157340.37
step 6022, loss: 3.397938, norm:0.2489, lr:4.4119e-04 dt: 3332.18ms, tok/sec:157340.68
step 6023, loss: 3.373092, norm:0.3020, lr:4.4114e-04 dt: 3332.22ms, tok/sec:157339.00
step 6024, loss: 3.320454, norm:0.2521, lr:4.4109e-04 dt: 3332.01ms, tok/sec:157348.64
step 6025, loss: 3.429842, norm:0.2693, lr:4.4103e-04 dt: 3332.19ms, tok/sec:157340.38
step 6026, loss: 3.359636, norm:0.2545, lr:4.4098e-04 dt: 3331.96ms, tok/sec:157351.38
step 6027, loss: 3.342696, norm:0.2772, lr:4.4093e-04 dt: 3331.81ms, tok/sec:157358.44
step 6028, loss: 3.379833, norm:0.2497, lr:4.4087e-04 dt: 3332.06ms, tok/sec:157346.51
step 6029, loss: 3.347747, norm:0.2499, lr:4.4082e-04 dt: 3332.03ms, tok/sec:157347.94
step 6030, loss: 3.361897, norm:0.3743, lr:4.4077e-04 dt: 3332.05ms, tok/sec:157346.81
step 6031, loss: 3.373536, norm:0.2693, lr:4.4071e-04 dt: 3332.04ms, tok/sec:157347.55
step 6032, loss: 3.342955, norm:0.2669, lr:4.4066e-04 dt: 3332.42ms, tok/sec:157329.51
step 6033, loss: 3.375072, norm:0.2567, lr:4.4061e-04 dt: 3332.19ms, tok/sec:157340.23
step 6034, loss: 3.495292, norm:0.2724, lr:4.4055e-04 dt: 3332.08ms, tok/sec:157345.70
step 6035, loss: 3.414390, norm:0.2996, lr:4.4050e-04 dt: 3332.09ms, tok/sec:157345.31
step 6036, loss: 3.441529, norm:0.2673, lr:4.4045e-04 dt: 3332.04ms, tok/sec:157347.31
step 6037, loss: 3.376808, norm:0.2765, lr:4.4039e-04 dt: 3332.07ms, tok/sec:157345.88
step 6038, loss: 3.408192, norm:0.2555, lr:4.4034e-04 dt: 3332.02ms, tok/sec:157348.51
step 6039, loss: 3.450472, norm:0.2761, lr:4.4029e-04 dt: 3332.24ms, tok/sec:157338.15
step 6040, loss: 3.427072, norm:0.2620, lr:4.4023e-04 dt: 3332.05ms, tok/sec:157347.04
step 6041, loss: 3.365149, norm:0.2519, lr:4.4018e-04 dt: 3332.53ms, tok/sec:157324.30
step 6042, loss: 3.445821, norm:0.2828, lr:4.4013e-04 dt: 3332.02ms, tok/sec:157348.49
step 6043, loss: 3.409847, norm:0.2574, lr:4.4007e-04 dt: 3331.96ms, tok/sec:157351.28
step 6044, loss: 3.442220, norm:0.2912, lr:4.4002e-04 dt: 3332.01ms, tok/sec:157348.66
step 6045, loss: 3.431518, norm:0.3064, lr:4.3997e-04 dt: 3332.32ms, tok/sec:157334.19
step 6046, loss: 3.389071, norm:0.3387, lr:4.3991e-04 dt: 3332.10ms, tok/sec:157344.79
step 6047, loss: 3.411743, norm:0.2485, lr:4.3986e-04 dt: 3332.01ms, tok/sec:157348.76
step 6048, loss: 3.401103, norm:0.2731, lr:4.3981e-04 dt: 3332.34ms, tok/sec:157333.11
step 6049, loss: 3.388421, norm:0.2446, lr:4.3975e-04 dt: 3332.32ms, tok/sec:157334.26
HellaSwag accuracy:4630900742584222801/-2=-2315450371292111360.0000
rank 1 sample 0: Hello, I'm a language model, we need to understand and understand the human-mediated communication
Language is a natural language for humans (and a relatively simple
rank 1 sample 1: Hello, I'm a language model, a computer programmer. I'm a mathematician, and I do the maths. You build up the programming languages, and you
rank 1 sample 2: Hello, I'm a language model, I look back at the early days of the term. The last time we looked at the language models in terms of noun
rank 1 sample 3: Hello, I'm a language model, and I'm using the following model with that model: 'language model''.
Like people, I'm very familiar
rank 0 sample 0: Hello, I'm a language model, and I don't have a command on their home page.
"For my first time, I had a very small
rank 0 sample 1: Hello, I'm a language model, I was going to use the language it was developed that had the capability to communicate in any form. However, I have
rank 0 sample 2: Hello, I'm a language model, I'm pretty passionate about creating models to help build up your skills as a language model, but I'm not a one
rank 0 sample 3: Hello, I'm a language model, I’m a language modeling engine, having a lot of resources on my personal journey."
When I’
step 6050, loss: 3.375167, norm:0.2430, lr:4.3970e-04 dt: 48516.83ms, tok/sec:10806.31
step 6051, loss: 3.431193, norm:0.2535, lr:4.3965e-04 dt: 3332.05ms, tok/sec:157346.75
step 6052, loss: 3.419211, norm:0.2263, lr:4.3960e-04 dt: 3332.24ms, tok/sec:157337.89
step 6053, loss: 3.403601, norm:0.2352, lr:4.3954e-04 dt: 3332.37ms, tok/sec:157331.82
step 6054, loss: 3.488716, norm:0.2231, lr:4.3949e-04 dt: 3332.09ms, tok/sec:157345.26
step 6055, loss: 3.411810, norm:0.2360, lr:4.3944e-04 dt: 3331.89ms, tok/sec:157354.57
step 6056, loss: 3.428395, norm:0.2510, lr:4.3938e-04 dt: 3331.94ms, tok/sec:157352.29
step 6057, loss: 3.343882, norm:0.2547, lr:4.3933e-04 dt: 3332.25ms, tok/sec:157337.75
step 6058, loss: 3.376222, norm:0.2557, lr:4.3928e-04 dt: 3332.19ms, tok/sec:157340.14
step 6059, loss: 3.350246, norm:0.2620, lr:4.3922e-04 dt: 3332.22ms, tok/sec:157338.83
step 6060, loss: 3.361215, norm:0.2493, lr:4.3917e-04 dt: 3332.18ms, tok/sec:157340.94
step 6061, loss: 3.365346, norm:0.2476, lr:4.3912e-04 dt: 3331.83ms, tok/sec:157357.29
step 6062, loss: 3.404725, norm:0.2295, lr:4.3906e-04 dt: 3332.40ms, tok/sec:157330.25
step 6063, loss: 3.373590, norm:0.2359, lr:4.3901e-04 dt: 3331.98ms, tok/sec:157350.18
step 6064, loss: 3.381597, norm:0.2394, lr:4.3896e-04 dt: 3331.94ms, tok/sec:157352.40
step 6065, loss: 3.322143, norm:0.2239, lr:4.3890e-04 dt: 3332.02ms, tok/sec:157348.26
step 6066, loss: 3.326691, norm:0.2255, lr:4.3885e-04 dt: 3331.96ms, tok/sec:157351.28
step 6067, loss: 3.353760, norm:0.2432, lr:4.3879e-04 dt: 3332.04ms, tok/sec:157347.68
step 6068, loss: 3.396182, norm:0.2399, lr:4.3874e-04 dt: 3332.14ms, tok/sec:157342.72
step 6069, loss: 3.448382, norm:0.2361, lr:4.3869e-04 dt: 3332.40ms, tok/sec:157330.31
step 6070, loss: 3.394240, norm:0.2669, lr:4.3863e-04 dt: 3332.04ms, tok/sec:157347.37
step 6071, loss: 3.413202, norm:0.2519, lr:4.3858e-04 dt: 3332.37ms, tok/sec:157331.87
step 6072, loss: 3.470872, norm:0.2538, lr:4.3853e-04 dt: 3332.15ms, tok/sec:157342.43
step 6073, loss: 3.435927, norm:0.2653, lr:4.3847e-04 dt: 3331.93ms, tok/sec:157352.44
step 6074, loss: 3.412358, norm:0.2388, lr:4.3842e-04 dt: 3332.10ms, tok/sec:157344.68
step 6075, loss: 3.445098, norm:0.2828, lr:4.3837e-04 dt: 3332.16ms, tok/sec:157341.56
step 6076, loss: 3.425999, norm:0.2586, lr:4.3831e-04 dt: 3332.16ms, tok/sec:157342.01
step 6077, loss: 3.420991, norm:0.2633, lr:4.3826e-04 dt: 3332.08ms, tok/sec:157345.42
step 6078, loss: 3.381704, norm:0.2508, lr:4.3821e-04 dt: 3332.27ms, tok/sec:157336.67
step 6079, loss: 3.450141, norm:0.2657, lr:4.3815e-04 dt: 3332.04ms, tok/sec:157347.66
step 6080, loss: 3.409513, norm:0.2516, lr:4.3810e-04 dt: 3332.45ms, tok/sec:157328.19
step 6081, loss: 3.411092, norm:0.2599, lr:4.3805e-04 dt: 3331.98ms, tok/sec:157350.07
step 6082, loss: 3.401882, norm:0.2664, lr:4.3799e-04 dt: 3332.17ms, tok/sec:157341.33
step 6083, loss: 3.418045, norm:0.2441, lr:4.3794e-04 dt: 3332.02ms, tok/sec:157348.25
step 6084, loss: 3.381266, norm:0.2679, lr:4.3789e-04 dt: 3332.21ms, tok/sec:157339.62
step 6085, loss: 3.426022, norm:0.2520, lr:4.3783e-04 dt: 3331.84ms, tok/sec:157356.84
step 6086, loss: 3.466245, norm:0.2853, lr:4.3778e-04 dt: 3331.82ms, tok/sec:157357.86
step 6087, loss: 3.415733, norm:0.2656, lr:4.3773e-04 dt: 3332.31ms, tok/sec:157334.74
step 6088, loss: 3.445008, norm:0.2550, lr:4.3767e-04 dt: 3332.52ms, tok/sec:157324.71
step 6089, loss: 3.431328, norm:0.2522, lr:4.3762e-04 dt: 3331.98ms, tok/sec:157350.10
step 6090, loss: 3.393652, norm:0.2356, lr:4.3757e-04 dt: 3331.82ms, tok/sec:157357.84
step 6091, loss: 3.353420, norm:0.2615, lr:4.3751e-04 dt: 3331.80ms, tok/sec:157358.72
step 6092, loss: 3.365404, norm:0.2540, lr:4.3746e-04 dt: 3332.20ms, tok/sec:157339.90
step 6093, loss: 3.361479, norm:0.2427, lr:4.3741e-04 dt: 3331.93ms, tok/sec:157352.85
step 6094, loss: 3.357129, norm:0.2588, lr:4.3735e-04 dt: 3332.07ms, tok/sec:157345.87
step 6095, loss: 3.347835, norm:0.2448, lr:4.3730e-04 dt: 3334.36ms, tok/sec:157237.96
step 6096, loss: 3.341480, norm:0.2452, lr:4.3724e-04 dt: 3332.33ms, tok/sec:157333.98
step 6097, loss: 3.354228, norm:0.2557, lr:4.3719e-04 dt: 3332.40ms, tok/sec:157330.28
step 6098, loss: 3.364097, norm:0.2368, lr:4.3714e-04 dt: 3332.31ms, tok/sec:157334.92
step 6099, loss: 3.382929, norm:0.2514, lr:4.3708e-04 dt: 3332.14ms, tok/sec:157342.85
validation loss: 3.4099
Model and optimizer state saved.
HellaSwag accuracy:4632061829010637905/-2=-2316030914505318912.0000
rank 1 sample 0: Hello, I'm a language model, we'll really appreciate you and we live with us."
Dr. Scott L. Walker, Professor of Applied Mathematics and
rank 1 sample 1: Hello, I'm a language model, which I'm pretty sure I am not an expert.”
To understand how each device handles its own language,
rank 1 sample 2: Hello, I'm a language model, so yes it's a language model.
I had my 2-month-old daughter come out and talk and discuss
rank 1 sample 3: Hello, I'm a language model, and I'm trying to come up with sound. I remember all of you like the following story about a small guy who
rank 0 sample 0: Hello, I'm a language model, and I think it is my own, especially, so I have always understood.
- No other programming languages are created
rank 0 sample 1: Hello, I'm a language model, so why not go into the same school? If any children have ever written the first language, I should have taught them
rank 0 sample 2: Hello, I'm a language model, so I should learn first language. I'm trying to teach all sorts of languages, so I'm going to be an
rank 0 sample 3: Hello, I'm a language model, you must be familiar with the basic rules. Since I'm a language model I learned many words and in a way where
step 6100, loss: 3.384914, norm:0.2509, lr:4.3703e-04 dt: 56186.23ms, tok/sec:9331.25
step 6101, loss: 3.360880, norm:0.2420, lr:4.3698e-04 dt: 3332.12ms, tok/sec:157343.46
step 6102, loss: 3.432295, norm:0.2583, lr:4.3692e-04 dt: 3332.19ms, tok/sec:157340.44
step 6103, loss: 3.388534, norm:0.2513, lr:4.3687e-04 dt: 3332.25ms, tok/sec:157337.39
step 6104, loss: 3.432034, norm:0.2599, lr:4.3682e-04 dt: 3332.22ms, tok/sec:157339.07
step 6105, loss: 3.386050, norm:0.2427, lr:4.3676e-04 dt: 3332.10ms, tok/sec:157344.57
step 6106, loss: 3.451900, norm:0.2600, lr:4.3671e-04 dt: 3332.15ms, tok/sec:157342.37
step 6107, loss: 3.457924, norm:0.2403, lr:4.3666e-04 dt: 3332.21ms, tok/sec:157339.42
step 6108, loss: 3.368202, norm:0.2396, lr:4.3660e-04 dt: 3331.96ms, tok/sec:157351.46
step 6109, loss: 3.422399, norm:0.2564, lr:4.3655e-04 dt: 3331.99ms, tok/sec:157349.72
step 6110, loss: 3.510651, norm:0.2817, lr:4.3650e-04 dt: 3332.39ms, tok/sec:157330.78
step 6111, loss: 3.416507, norm:0.2841, lr:4.3644e-04 dt: 3332.00ms, tok/sec:157349.49
step 6112, loss: 3.424811, norm:0.2504, lr:4.3639e-04 dt: 3332.51ms, tok/sec:157325.23
step 6113, loss: 3.430547, norm:0.2735, lr:4.3633e-04 dt: 3332.25ms, tok/sec:157337.38
step 6114, loss: 3.445904, norm:0.2657, lr:4.3628e-04 dt: 3332.16ms, tok/sec:157341.94
step 6115, loss: 3.459204, norm:0.2850, lr:4.3623e-04 dt: 3332.02ms, tok/sec:157348.52
step 6116, loss: 3.426478, norm:0.2687, lr:4.3617e-04 dt: 3332.08ms, tok/sec:157345.35
step 6117, loss: 3.406109, norm:0.2612, lr:4.3612e-04 dt: 3331.87ms, tok/sec:157355.26
step 6118, loss: 3.413275, norm:0.2702, lr:4.3607e-04 dt: 3332.02ms, tok/sec:157348.19
step 6119, loss: 3.416742, norm:0.2721, lr:4.3601e-04 dt: 3332.40ms, tok/sec:157330.31
step 6120, loss: 3.483354, norm:0.2740, lr:4.3596e-04 dt: 3332.37ms, tok/sec:157331.72
step 6121, loss: 3.402794, norm:0.2633, lr:4.3591e-04 dt: 3332.09ms, tok/sec:157344.97
step 6122, loss: 3.427100, norm:0.2758, lr:4.3585e-04 dt: 3332.15ms, tok/sec:157342.21
step 6123, loss: 3.376510, norm:0.2941, lr:4.3580e-04 dt: 3332.06ms, tok/sec:157346.40
step 6124, loss: 3.384593, norm:0.2718, lr:4.3574e-04 dt: 3332.08ms, tok/sec:157345.76
step 6125, loss: 3.312962, norm:0.2491, lr:4.3569e-04 dt: 3332.06ms, tok/sec:157346.55
step 6126, loss: 3.377256, norm:0.2592, lr:4.3564e-04 dt: 3332.26ms, tok/sec:157337.09
step 6127, loss: 3.399093, norm:0.2616, lr:4.3558e-04 dt: 3332.15ms, tok/sec:157342.23
step 6128, loss: 3.324188, norm:0.2918, lr:4.3553e-04 dt: 3332.41ms, tok/sec:157329.82
step 6129, loss: 3.279764, norm:0.2427, lr:4.3548e-04 dt: 3331.96ms, tok/sec:157351.46
step 6130, loss: 3.360463, norm:0.2640, lr:4.3542e-04 dt: 3332.05ms, tok/sec:157346.81
step 6131, loss: 3.393717, norm:0.2551, lr:4.3537e-04 dt: 3332.29ms, tok/sec:157335.65
step 6132, loss: 3.315059, norm:0.2555, lr:4.3531e-04 dt: 3332.13ms, tok/sec:157343.03
step 6133, loss: 3.407856, norm:0.2507, lr:4.3526e-04 dt: 3332.35ms, tok/sec:157332.79
step 6134, loss: 3.422199, norm:0.2628, lr:4.3521e-04 dt: 3332.22ms, tok/sec:157338.76
step 6135, loss: 3.364433, norm:0.2363, lr:4.3515e-04 dt: 3332.27ms, tok/sec:157336.76
step 6136, loss: 3.368257, norm:0.2554, lr:4.3510e-04 dt: 3332.06ms, tok/sec:157346.73
step 6137, loss: 3.447313, norm:0.2825, lr:4.3505e-04 dt: 3332.47ms, tok/sec:157327.36
step 6138, loss: 3.472838, norm:0.2784, lr:4.3499e-04 dt: 3331.90ms, tok/sec:157354.09
step 6139, loss: 3.457107, norm:0.2865, lr:4.3494e-04 dt: 3332.10ms, tok/sec:157344.45
step 6140, loss: 3.405675, norm:0.2743, lr:4.3489e-04 dt: 3332.19ms, tok/sec:157340.19
step 6141, loss: 3.370752, norm:0.2701, lr:4.3483e-04 dt: 3332.06ms, tok/sec:157346.33
step 6142, loss: 3.438933, norm:0.2454, lr:4.3478e-04 dt: 3332.25ms, tok/sec:157337.42
step 6143, loss: 3.421651, norm:0.2677, lr:4.3472e-04 dt: 3332.18ms, tok/sec:157340.88
step 6144, loss: 3.407507, norm:0.2520, lr:4.3467e-04 dt: 3332.41ms, tok/sec:157330.04
step 6145, loss: 3.410103, norm:0.2546, lr:4.3462e-04 dt: 3331.99ms, tok/sec:157349.64
step 6146, loss: 3.422869, norm:0.2470, lr:4.3456e-04 dt: 3331.93ms, tok/sec:157352.63
step 6147, loss: 3.420172, norm:0.2729, lr:4.3451e-04 dt: 3331.91ms, tok/sec:157353.62
step 6148, loss: 3.383983, norm:0.2664, lr:4.3445e-04 dt: 3332.19ms, tok/sec:157340.47
step 6149, loss: 3.430041, norm:0.2530, lr:4.3440e-04 dt: 3332.17ms, tok/sec:157341.10
HellaSwag accuracy:4630935926956328017/-2=-2315467963478163968.0000
rank 0 sample 0: Hello, I'm a language model, and I just want my user name to not show you a single sentence? You can tell me you can say "You
rank 0 sample 1: Hello, I'm a language model, so now I'm a language modeler. So while I've been exploring a language for my students here and I've
rank 1 sample 0: Hello, I'm a language model, we can actually look at a single concept in another language with that same concept.
Let's try to figure out how
rank 0 sample 2: Hello, I'm a language model, so I always use language with
name.dbs. This will allow me to be able to use the language itself
rank 1 sample 1: Hello, I'm a language model, which I use and have a basic understanding of. I'd like to learn something about them. "Hello, I'mrank 0 sample 3: Hello, I'm a language model, not language model. I'm a language model/language model. But I don't see all of it. So to

rank 1 sample 2: Hello, I'm a language model, but like many other languages, I'm not sure how the computer would look under the microscope. However, it could get
rank 1 sample 3: Hello, I'm a language model, and I'm interested to write about your class. As opposed to working with Microsoft, one aspect of the process is the
step 6150, loss: 3.436276, norm:0.2633, lr:4.3435e-04 dt: 48518.41ms, tok/sec:10805.96
step 6151, loss: 3.428804, norm:0.2590, lr:4.3429e-04 dt: 3331.99ms, tok/sec:157349.83
step 6152, loss: 3.402651, norm:0.2716, lr:4.3424e-04 dt: 3332.12ms, tok/sec:157343.53
step 6153, loss: 3.444605, norm:0.2351, lr:4.3419e-04 dt: 3332.32ms, tok/sec:157334.10
step 6154, loss: 3.455828, norm:0.2812, lr:4.3413e-04 dt: 3332.18ms, tok/sec:157340.70
step 6155, loss: 3.472374, norm:0.2376, lr:4.3408e-04 dt: 3331.89ms, tok/sec:157354.39
step 6156, loss: 3.448606, norm:0.3142, lr:4.3402e-04 dt: 3331.84ms, tok/sec:157356.69
step 6157, loss: 3.415189, norm:0.2586, lr:4.3397e-04 dt: 3331.87ms, tok/sec:157355.49
step 6158, loss: 3.393564, norm:0.2934, lr:4.3392e-04 dt: 3332.13ms, tok/sec:157343.35
step 6159, loss: 3.329291, norm:0.2450, lr:4.3386e-04 dt: 3332.14ms, tok/sec:157342.93
step 6160, loss: 3.335802, norm:0.2962, lr:4.3381e-04 dt: 3332.11ms, tok/sec:157344.37
step 6161, loss: 3.329253, norm:0.2432, lr:4.3376e-04 dt: 3332.05ms, tok/sec:157347.17
step 6162, loss: 3.353413, norm:0.2583, lr:4.3370e-04 dt: 3332.55ms, tok/sec:157323.52
step 6163, loss: 3.342118, norm:0.2581, lr:4.3365e-04 dt: 3331.93ms, tok/sec:157352.45
step 6164, loss: 3.330081, norm:0.2617, lr:4.3359e-04 dt: 3332.11ms, tok/sec:157344.27
step 6165, loss: 3.407892, norm:0.2404, lr:4.3354e-04 dt: 3332.04ms, tok/sec:157347.57
step 6166, loss: 3.358287, norm:0.2332, lr:4.3349e-04 dt: 3332.13ms, tok/sec:157343.06
step 6167, loss: 3.320246, norm:0.2514, lr:4.3343e-04 dt: 3331.97ms, tok/sec:157350.95
step 6168, loss: 3.324576, norm:0.2428, lr:4.3338e-04 dt: 3331.90ms, tok/sec:157353.94
step 6169, loss: 3.341429, norm:0.2457, lr:4.3332e-04 dt: 3332.15ms, tok/sec:157342.13
step 6170, loss: 3.427753, norm:0.2452, lr:4.3327e-04 dt: 3332.11ms, tok/sec:157344.35
step 6171, loss: 3.381355, norm:0.2572, lr:4.3322e-04 dt: 3332.60ms, tok/sec:157320.94
step 6172, loss: 3.425255, norm:0.2463, lr:4.3316e-04 dt: 3332.00ms, tok/sec:157349.18
step 6173, loss: 3.447355, norm:0.2711, lr:4.3311e-04 dt: 3332.10ms, tok/sec:157344.80
step 6174, loss: 3.418597, norm:0.2515, lr:4.3306e-04 dt: 3332.01ms, tok/sec:157348.98
step 6175, loss: 3.468391, norm:0.2941, lr:4.3300e-04 dt: 3332.04ms, tok/sec:157347.47
step 6176, loss: 3.398610, norm:0.2525, lr:4.3295e-04 dt: 3331.93ms, tok/sec:157352.73
step 6177, loss: 3.401676, norm:0.2908, lr:4.3289e-04 dt: 3332.13ms, tok/sec:157343.08
step 6178, loss: 3.396997, norm:0.4028, lr:4.3284e-04 dt: 3332.28ms, tok/sec:157336.11
step 6179, loss: 3.420237, norm:0.3012, lr:4.3279e-04 dt: 3332.20ms, tok/sec:157339.94
step 6180, loss: 3.477531, norm:0.3261, lr:4.3273e-04 dt: 3332.63ms, tok/sec:157319.73
step 6181, loss: 3.414771, norm:0.2877, lr:4.3268e-04 dt: 3331.96ms, tok/sec:157351.23
step 6182, loss: 3.432708, norm:0.2896, lr:4.3262e-04 dt: 3331.97ms, tok/sec:157350.97
step 6183, loss: 3.401836, norm:0.2559, lr:4.3257e-04 dt: 3332.00ms, tok/sec:157349.31
step 6184, loss: 3.434996, norm:0.2667, lr:4.3252e-04 dt: 3332.30ms, tok/sec:157335.01
step 6185, loss: 3.424969, norm:0.2592, lr:4.3246e-04 dt: 3332.09ms, tok/sec:157345.00
step 6186, loss: 3.380832, norm:0.2669, lr:4.3241e-04 dt: 3332.10ms, tok/sec:157344.57
step 6187, loss: 3.384025, norm:0.2771, lr:4.3235e-04 dt: 3332.13ms, tok/sec:157343.42
step 6188, loss: 3.389000, norm:0.2707, lr:4.3230e-04 dt: 3332.30ms, tok/sec:157335.39
step 6189, loss: 3.459291, norm:0.2573, lr:4.3225e-04 dt: 3332.18ms, tok/sec:157341.06
step 6190, loss: 3.355573, norm:0.2603, lr:4.3219e-04 dt: 3332.07ms, tok/sec:157345.80
step 6191, loss: 3.544192, norm:0.2702, lr:4.3214e-04 dt: 3331.99ms, tok/sec:157349.95
step 6192, loss: 3.381567, norm:0.2812, lr:4.3208e-04 dt: 3332.02ms, tok/sec:157348.34
step 6193, loss: 3.345795, norm:0.3180, lr:4.3203e-04 dt: 3331.99ms, tok/sec:157349.60
step 6194, loss: 3.361382, norm:0.2691, lr:4.3198e-04 dt: 3332.22ms, tok/sec:157339.15
step 6195, loss: 3.334306, norm:0.2462, lr:4.3192e-04 dt: 3331.99ms, tok/sec:157349.63
step 6196, loss: 3.408712, norm:0.2603, lr:4.3187e-04 dt: 3332.15ms, tok/sec:157342.19
step 6197, loss: 3.384481, norm:0.2612, lr:4.3181e-04 dt: 3332.51ms, tok/sec:157325.16
step 6198, loss: 3.332445, norm:0.2360, lr:4.3176e-04 dt: 3332.19ms, tok/sec:157340.44
step 6199, loss: 3.319737, norm:0.2362, lr:4.3171e-04 dt: 3331.98ms, tok/sec:157350.14
validation loss: 3.4057
Model and optimizer state saved.
HellaSwag accuracy:2325092885530379345/-2=-1162546442765189632.0000
rank 1 sample 0: Hello, I'm a language model, my model is a simulation. I did it to the very real world, for example when I came to the age of
rank 1 sample 1: Hello, I'm a language model, which means I love my language. And there's something as simple as that. I feel like the same way.

rank 0 sample 0: Hello, I'm a language model, and I'd like a model in that respect.
Graphic in design.
- When it is the first letter
rank 1 sample 2: Hello, I'm a language model, so so, what I'm doing is a review of my ideas, so, I'm looking forward to the moment,
rank 0 sample 1: Hello, I'm a language model, so when you're learning a language here, there goes nowhere, because then we have the kind of confusion and confusion that
rank 1 sample 3: Hello, I'm a language model, and I'm using the XHTML, another language. Then I'm just now, by and large, and I'm
rank 0 sample 2: Hello, I'm a language model, so I always thought of something like, "Well, I've just seen a lot of languages, but I'm interested
rank 0 sample 3: Hello, I'm a language model, but for me, I'm not that language...
I live in a language or world in which, I have always
step 6200, loss: 3.327199, norm:0.2287, lr:4.3165e-04 dt: 56265.93ms, tok/sec:9318.04
step 6201, loss: 3.358920, norm:0.2556, lr:4.3160e-04 dt: 3332.14ms, tok/sec:157342.74
step 6202, loss: 3.332600, norm:0.2384, lr:4.3154e-04 dt: 3332.08ms, tok/sec:157345.34
step 6203, loss: 3.379085, norm:0.3961, lr:4.3149e-04 dt: 3332.02ms, tok/sec:157348.55
step 6204, loss: 3.402038, norm:0.2918, lr:4.3144e-04 dt: 3331.97ms, tok/sec:157350.62
step 6205, loss: 3.362921, norm:0.2746, lr:4.3138e-04 dt: 3332.20ms, tok/sec:157339.80
step 6206, loss: 3.419360, norm:0.2764, lr:4.3133e-04 dt: 3332.06ms, tok/sec:157346.54
step 6207, loss: 3.423383, norm:0.2570, lr:4.3127e-04 dt: 3332.33ms, tok/sec:157333.83
step 6208, loss: 3.433342, norm:0.2677, lr:4.3122e-04 dt: 3332.23ms, tok/sec:157338.63
step 6209, loss: 3.415089, norm:0.2698, lr:4.3117e-04 dt: 3332.33ms, tok/sec:157333.98
step 6210, loss: 3.366899, norm:0.2429, lr:4.3111e-04 dt: 3332.22ms, tok/sec:157339.17
step 6211, loss: 3.433770, norm:0.2731, lr:4.3106e-04 dt: 3332.01ms, tok/sec:157348.84
step 6212, loss: 3.397335, norm:0.2597, lr:4.3100e-04 dt: 3332.08ms, tok/sec:157345.57
step 6213, loss: 3.456658, norm:0.2702, lr:4.3095e-04 dt: 3332.35ms, tok/sec:157333.05
step 6214, loss: 3.439546, norm:0.2722, lr:4.3089e-04 dt: 3332.28ms, tok/sec:157336.22
step 6215, loss: 3.394610, norm:0.2658, lr:4.3084e-04 dt: 3332.21ms, tok/sec:157339.24
step 6216, loss: 3.414616, norm:0.2508, lr:4.3079e-04 dt: 3331.95ms, tok/sec:157351.83
step 6217, loss: 3.374894, norm:0.2630, lr:4.3073e-04 dt: 3332.10ms, tok/sec:157344.60
step 6218, loss: 3.389889, norm:0.2518, lr:4.3068e-04 dt: 3332.11ms, tok/sec:157344.08
step 6219, loss: 3.422495, norm:0.2467, lr:4.3062e-04 dt: 3332.17ms, tok/sec:157341.21
step 6220, loss: 3.408298, norm:0.2626, lr:4.3057e-04 dt: 3332.15ms, tok/sec:157342.12
step 6221, loss: 3.399944, norm:0.2520, lr:4.3052e-04 dt: 3332.10ms, tok/sec:157344.64
step 6222, loss: 3.382715, norm:0.2448, lr:4.3046e-04 dt: 3332.11ms, tok/sec:157344.19
step 6223, loss: 3.398955, norm:0.2538, lr:4.3041e-04 dt: 3332.39ms, tok/sec:157330.94
step 6224, loss: 3.413246, norm:0.2477, lr:4.3035e-04 dt: 3332.00ms, tok/sec:157349.58
step 6225, loss: 3.396875, norm:0.2617, lr:4.3030e-04 dt: 3332.07ms, tok/sec:157345.98
step 6226, loss: 3.376362, norm:0.2527, lr:4.3025e-04 dt: 3332.10ms, tok/sec:157344.45
step 6227, loss: 3.356916, norm:0.2464, lr:4.3019e-04 dt: 3331.90ms, tok/sec:157353.97
step 6228, loss: 3.351935, norm:0.2579, lr:4.3014e-04 dt: 3332.01ms, tok/sec:157348.74
step 6229, loss: 3.337596, norm:0.2525, lr:4.3008e-04 dt: 3332.15ms, tok/sec:157342.03
step 6230, loss: 3.420784, norm:0.2542, lr:4.3003e-04 dt: 3332.10ms, tok/sec:157344.48
step 6231, loss: 3.408152, norm:0.2486, lr:4.2997e-04 dt: 3332.22ms, tok/sec:157338.91
step 6232, loss: 3.334116, norm:0.2633, lr:4.2992e-04 dt: 3332.33ms, tok/sec:157333.84
step 6233, loss: 3.444585, norm:0.2616, lr:4.2987e-04 dt: 3332.12ms, tok/sec:157343.44
step 6234, loss: 3.338665, norm:0.2739, lr:4.2981e-04 dt: 3331.95ms, tok/sec:157351.88
step 6235, loss: 3.365839, norm:0.2467, lr:4.2976e-04 dt: 3331.96ms, tok/sec:157351.29
step 6236, loss: 3.322918, norm:0.2703, lr:4.2970e-04 dt: 3332.23ms, tok/sec:157338.36
step 6237, loss: 3.444863, norm:0.3070, lr:4.2965e-04 dt: 3331.99ms, tok/sec:157349.79
step 6238, loss: 3.323943, norm:0.2872, lr:4.2960e-04 dt: 3332.07ms, tok/sec:157345.89
step 6239, loss: 3.433461, norm:0.2634, lr:4.2954e-04 dt: 3332.38ms, tok/sec:157331.20
step 6240, loss: 3.415148, norm:0.2863, lr:4.2949e-04 dt: 3332.15ms, tok/sec:157342.46
step 6241, loss: 3.452155, norm:0.2664, lr:4.2943e-04 dt: 3332.50ms, tok/sec:157325.66
step 6242, loss: 3.362952, norm:0.2572, lr:4.2938e-04 dt: 3331.88ms, tok/sec:157355.17
step 6243, loss: 3.386344, norm:0.2603, lr:4.2932e-04 dt: 3332.02ms, tok/sec:157348.60
step 6244, loss: 3.506495, norm:0.2422, lr:4.2927e-04 dt: 3331.94ms, tok/sec:157352.02
step 6245, loss: 3.395219, norm:0.2749, lr:4.2922e-04 dt: 3331.97ms, tok/sec:157350.84
step 6246, loss: 3.394454, norm:0.2582, lr:4.2916e-04 dt: 3331.97ms, tok/sec:157350.53
step 6247, loss: 3.388459, norm:0.2618, lr:4.2911e-04 dt: 3332.07ms, tok/sec:157345.84
step 6248, loss: 3.438461, norm:0.2555, lr:4.2905e-04 dt: 3332.26ms, tok/sec:157337.03
step 6249, loss: 3.379128, norm:0.2811, lr:4.2900e-04 dt: 3332.24ms, tok/sec:157338.17
HellaSwag accuracy:2325216030832657425/-2=-1162608015416328704.0000
rank 1 sample 0: Hello, I'm a language model, as well as a program for students across a vast array of students. I wanted to give a general idea of what I
rank 1 sample 1: Hello, I'm a language model, which I use here for my class. What I'm reading is all about using other stuff, but it's not a
rank 1 sample 2: Hello, I'm a language model, but these days I'm not sure if I'm going to try this one.
I think that the language has had
rank 1 sample 3: Hello, I'm a language model, and I'm writing about teaching my students math. I read,
(N) , The students are thinking about what
rank 0 sample 0: Hello, I'm a language model, and I know that it's going to learn English from I'm used to a lot of other languages too, but what
rank 0 sample 1: Hello, I'm a language model, but when I get a certain kind of type I expect. I'll define my object in an easy way so that's
rank 0 sample 2: Hello, I'm a language model, I'm like-so my world of things
Here's some information about the word 'I' in the book:
rank 0 sample 3: Hello, I'm a language model, it helps me understand what I'm doing and learn more about it.
The last question: What happens when we can
step 6250, loss: 3.447664, norm:0.2591, lr:4.2894e-04 dt: 48516.86ms, tok/sec:10806.30
step 6251, loss: 3.432168, norm:0.2429, lr:4.2889e-04 dt: 3332.08ms, tok/sec:157345.52
step 6252, loss: 3.444344, norm:0.2610, lr:4.2884e-04 dt: 3332.04ms, tok/sec:157347.28
step 6253, loss: 3.409199, norm:0.2671, lr:4.2878e-04 dt: 3332.00ms, tok/sec:157349.17
step 6254, loss: 3.416739, norm:0.2824, lr:4.2873e-04 dt: 3332.02ms, tok/sec:157348.62
step 6255, loss: 3.338614, norm:0.2850, lr:4.2867e-04 dt: 3331.93ms, tok/sec:157352.41
step 6256, loss: 3.409741, norm:0.2518, lr:4.2862e-04 dt: 3331.97ms, tok/sec:157350.89
step 6257, loss: 3.434663, norm:0.2603, lr:4.2856e-04 dt: 3332.38ms, tok/sec:157331.20
step 6258, loss: 3.423167, norm:0.2462, lr:4.2851e-04 dt: 3332.10ms, tok/sec:157344.75
step 6259, loss: 3.383775, norm:0.2494, lr:4.2846e-04 dt: 3332.28ms, tok/sec:157336.11
step 6260, loss: 3.403659, norm:0.2538, lr:4.2840e-04 dt: 3332.08ms, tok/sec:157345.39
step 6261, loss: 3.335034, norm:0.2376, lr:4.2835e-04 dt: 3332.04ms, tok/sec:157347.28
step 6262, loss: 3.314501, norm:0.2612, lr:4.2829e-04 dt: 3332.12ms, tok/sec:157343.55
step 6263, loss: 3.331864, norm:0.2436, lr:4.2824e-04 dt: 3331.76ms, tok/sec:157360.61
step 6264, loss: 3.363722, norm:0.2584, lr:4.2818e-04 dt: 3332.05ms, tok/sec:157346.75
step 6265, loss: 3.343926, norm:0.2475, lr:4.2813e-04 dt: 3332.14ms, tok/sec:157342.70
step 6266, loss: 3.315571, norm:0.2415, lr:4.2808e-04 dt: 3332.26ms, tok/sec:157336.83
step 6267, loss: 3.275128, norm:0.2420, lr:4.2802e-04 dt: 3332.12ms, tok/sec:157343.69
step 6268, loss: 3.358963, norm:0.2333, lr:4.2797e-04 dt: 3331.97ms, tok/sec:157350.52
step 6269, loss: 3.342914, norm:0.2583, lr:4.2791e-04 dt: 3331.90ms, tok/sec:157354.14
step 6270, loss: 3.332306, norm:0.2326, lr:4.2786e-04 dt: 3332.08ms, tok/sec:157345.39
step 6271, loss: 3.337336, norm:0.2563, lr:4.2780e-04 dt: 3331.84ms, tok/sec:157356.69
step 6272, loss: 3.368122, norm:0.2477, lr:4.2775e-04 dt: 3332.55ms, tok/sec:157323.57
step 6273, loss: 3.416366, norm:0.2459, lr:4.2770e-04 dt: 3332.08ms, tok/sec:157345.70
step 6274, loss: 3.549322, norm:0.3528, lr:4.2764e-04 dt: 3333.02ms, tok/sec:157301.24
step 6275, loss: 3.462818, norm:0.4493, lr:4.2759e-04 dt: 3332.20ms, tok/sec:157340.03
step 6276, loss: 3.425962, norm:0.3336, lr:4.2753e-04 dt: 3332.28ms, tok/sec:157335.89
step 6277, loss: 3.406426, norm:0.3101, lr:4.2748e-04 dt: 3332.04ms, tok/sec:157347.49
step 6278, loss: 3.421411, norm:0.2974, lr:4.2742e-04 dt: 3332.07ms, tok/sec:157345.80
step 6279, loss: 3.477057, norm:0.2875, lr:4.2737e-04 dt: 3332.22ms, tok/sec:157338.90
step 6280, loss: 3.414974, norm:0.2675, lr:4.2731e-04 dt: 3332.17ms, tok/sec:157341.47
step 6281, loss: 3.421925, norm:0.2719, lr:4.2726e-04 dt: 3332.53ms, tok/sec:157324.12
step 6282, loss: 3.420796, norm:0.2519, lr:4.2721e-04 dt: 3332.19ms, tok/sec:157340.33
step 6283, loss: 3.407467, norm:0.2728, lr:4.2715e-04 dt: 3332.13ms, tok/sec:157343.19
step 6284, loss: 3.462455, norm:0.2703, lr:4.2710e-04 dt: 3332.23ms, tok/sec:157338.43
step 6285, loss: 3.393543, norm:0.2684, lr:4.2704e-04 dt: 3332.25ms, tok/sec:157337.41
step 6286, loss: 3.452951, norm:0.2491, lr:4.2699e-04 dt: 3334.29ms, tok/sec:157241.33
step 6287, loss: 3.396410, norm:0.2574, lr:4.2693e-04 dt: 3332.38ms, tok/sec:157331.37
step 6288, loss: 3.410418, norm:0.2634, lr:4.2688e-04 dt: 3332.36ms, tok/sec:157332.24
step 6289, loss: 3.442626, norm:0.2821, lr:4.2682e-04 dt: 3332.03ms, tok/sec:157347.90
step 6290, loss: 3.414734, norm:0.2568, lr:4.2677e-04 dt: 3332.15ms, tok/sec:157342.13
step 6291, loss: 3.429700, norm:0.2547, lr:4.2672e-04 dt: 3332.01ms, tok/sec:157348.94
step 6292, loss: 3.389083, norm:0.2579, lr:4.2666e-04 dt: 3332.03ms, tok/sec:157347.84
step 6293, loss: 3.360652, norm:0.2407, lr:4.2661e-04 dt: 3331.95ms, tok/sec:157351.69
step 6294, loss: 3.339116, norm:0.2604, lr:4.2655e-04 dt: 3331.83ms, tok/sec:157357.59
step 6295, loss: 3.326083, norm:0.2444, lr:4.2650e-04 dt: 3332.15ms, tok/sec:157342.26
step 6296, loss: 3.377010, norm:0.2770, lr:4.2644e-04 dt: 3332.19ms, tok/sec:157340.37
step 6297, loss: 3.278844, norm:0.2603, lr:4.2639e-04 dt: 3331.93ms, tok/sec:157352.50
step 6298, loss: 3.328640, norm:0.2578, lr:4.2633e-04 dt: 3332.08ms, tok/sec:157345.50
step 6299, loss: 3.306753, norm:0.2684, lr:4.2628e-04 dt: 3332.12ms, tok/sec:157343.59
validation loss: 3.4034
Model and optimizer state saved.
HellaSwag accuracy:2325128104262173777/-2=-1162564052131086848.0000
rank 1 sample 0: Hello, I'm a language model, and I'm a business manager. Every one of them says yes, I'm a marketing director because I know what I
rank 1 sample 1: Hello, I'm a language model, but it was quite a bit tricky. A lot of questions and an effort are shared so far about the language.

rank 1 sample 2: Hello, I'm a language model, so I want to use it in a way that looks very, very simple.
I am an assistant to one of
rank 1 sample 3: Hello, I'm a language model, and I'm looking for what I've had to say regarding language evolution. Can I apply something like this to something that
rank 0 sample 0: Hello, I'm a language model, and I'd like you to learn my classes or your teacher. My classroom was a very wide world. There were people
rank 0 sample 1: Hello, I'm a language model, so how do I start up my free time with code? I'd like to know if that would actually work.

rank 0 sample 2: Hello, I'm a language model, so I used it anyway...
To start at the end we need to use the TSLT Language. We started
rank 0 sample 3: Hello, I'm a language model, and how do I get started with that? Just by looking at the dictionary and putting a dictionary on a piece of card
step 6300, loss: 3.316208, norm:0.2529, lr:4.2623e-04 dt: 56222.45ms, tok/sec:9325.24
step 6301, loss: 3.368532, norm:0.2542, lr:4.2617e-04 dt: 3332.20ms, tok/sec:157339.87
step 6302, loss: 3.336299, norm:0.2542, lr:4.2612e-04 dt: 3332.02ms, tok/sec:157348.22
step 6303, loss: 3.379897, norm:0.2462, lr:4.2606e-04 dt: 3331.98ms, tok/sec:157350.34
step 6304, loss: 3.420719, norm:0.2592, lr:4.2601e-04 dt: 3332.30ms, tok/sec:157335.10
step 6305, loss: 3.394545, norm:0.2527, lr:4.2595e-04 dt: 3332.23ms, tok/sec:157338.69
step 6306, loss: 3.394204, norm:0.2700, lr:4.2590e-04 dt: 3332.12ms, tok/sec:157343.57
step 6307, loss: 3.390344, norm:0.2690, lr:4.2584e-04 dt: 3332.22ms, tok/sec:157338.79
step 6308, loss: 3.390012, norm:0.2845, lr:4.2579e-04 dt: 3331.99ms, tok/sec:157349.96
step 6309, loss: 3.466682, norm:0.2874, lr:4.2573e-04 dt: 3331.99ms, tok/sec:157349.98
step 6310, loss: 3.352625, norm:0.2835, lr:4.2568e-04 dt: 3332.30ms, tok/sec:157335.33
step 6311, loss: 3.378616, norm:0.2553, lr:4.2563e-04 dt: 3332.23ms, tok/sec:157338.36
step 6312, loss: 3.348947, norm:0.2948, lr:4.2557e-04 dt: 3332.02ms, tok/sec:157348.51
step 6313, loss: 3.406946, norm:0.2569, lr:4.2552e-04 dt: 3332.17ms, tok/sec:157341.19
step 6314, loss: 3.401200, norm:0.2740, lr:4.2546e-04 dt: 3332.32ms, tok/sec:157334.32
step 6315, loss: 3.390589, norm:0.2658, lr:4.2541e-04 dt: 3332.17ms, tok/sec:157341.29
step 6316, loss: 3.402482, norm:0.2464, lr:4.2535e-04 dt: 3331.98ms, tok/sec:157350.22
step 6317, loss: 3.301851, norm:0.2633, lr:4.2530e-04 dt: 3332.00ms, tok/sec:157349.26
step 6318, loss: 3.452547, norm:0.2618, lr:4.2524e-04 dt: 3332.27ms, tok/sec:157336.38
step 6319, loss: 3.377731, norm:0.2537, lr:4.2519e-04 dt: 3332.04ms, tok/sec:157347.31
step 6320, loss: 3.420976, norm:0.2861, lr:4.2513e-04 dt: 3332.20ms, tok/sec:157339.81
step 6321, loss: 3.421373, norm:0.2610, lr:4.2508e-04 dt: 3331.94ms, tok/sec:157351.94
step 6322, loss: 3.459589, norm:0.2945, lr:4.2503e-04 dt: 3332.18ms, tok/sec:157340.62
step 6323, loss: 3.422747, norm:0.2751, lr:4.2497e-04 dt: 3332.35ms, tok/sec:157332.88
step 6324, loss: 3.390462, norm:0.2652, lr:4.2492e-04 dt: 3331.84ms, tok/sec:157356.67
step 6325, loss: 3.439095, norm:0.2792, lr:4.2486e-04 dt: 3332.01ms, tok/sec:157348.97
step 6326, loss: 3.422285, norm:0.2711, lr:4.2481e-04 dt: 3332.02ms, tok/sec:157348.56
step 6327, loss: 3.430845, norm:0.2781, lr:4.2475e-04 dt: 3332.11ms, tok/sec:157344.00
step 6328, loss: 3.382229, norm:0.2632, lr:4.2470e-04 dt: 3331.89ms, tok/sec:157354.65
step 6329, loss: 3.419851, norm:0.2750, lr:4.2464e-04 dt: 3332.17ms, tok/sec:157341.39
step 6330, loss: 3.328239, norm:0.2600, lr:4.2459e-04 dt: 3332.02ms, tok/sec:157348.30
step 6331, loss: 3.277039, norm:0.2771, lr:4.2453e-04 dt: 3332.24ms, tok/sec:157338.06
step 6332, loss: 3.324204, norm:0.2773, lr:4.2448e-04 dt: 3332.15ms, tok/sec:157342.37
step 6333, loss: 3.367545, norm:0.2447, lr:4.2442e-04 dt: 3332.04ms, tok/sec:157347.38
step 6334, loss: 3.397875, norm:0.2751, lr:4.2437e-04 dt: 3331.98ms, tok/sec:157350.09
step 6335, loss: 3.398199, norm:0.2630, lr:4.2432e-04 dt: 3332.00ms, tok/sec:157349.55
step 6336, loss: 3.373147, norm:0.2542, lr:4.2426e-04 dt: 3332.03ms, tok/sec:157347.90
step 6337, loss: 3.301550, norm:0.2674, lr:4.2421e-04 dt: 3332.09ms, tok/sec:157345.31
step 6338, loss: 3.353123, norm:0.2356, lr:4.2415e-04 dt: 3332.28ms, tok/sec:157336.03
step 6339, loss: 3.332387, norm:0.2523, lr:4.2410e-04 dt: 3332.13ms, tok/sec:157343.27
step 6340, loss: 3.355823, norm:0.2575, lr:4.2404e-04 dt: 3332.03ms, tok/sec:157348.04
step 6341, loss: 3.413125, norm:0.2705, lr:4.2399e-04 dt: 3332.13ms, tok/sec:157343.16
step 6342, loss: 3.340643, norm:0.2966, lr:4.2393e-04 dt: 3332.20ms, tok/sec:157339.81
step 6343, loss: 3.336758, norm:0.2660, lr:4.2388e-04 dt: 3332.24ms, tok/sec:157338.11
step 6344, loss: 3.382912, norm:0.2709, lr:4.2382e-04 dt: 3332.08ms, tok/sec:157345.49
step 6345, loss: 3.365969, norm:0.2663, lr:4.2377e-04 dt: 3331.81ms, tok/sec:157358.27
step 6346, loss: 3.387442, norm:0.2629, lr:4.2371e-04 dt: 3332.02ms, tok/sec:157348.30
step 6347, loss: 3.382964, norm:0.2570, lr:4.2366e-04 dt: 3332.26ms, tok/sec:157336.95
step 6348, loss: 3.355597, norm:0.2704, lr:4.2360e-04 dt: 3332.07ms, tok/sec:157346.01
step 6349, loss: 3.402046, norm:0.2445, lr:4.2355e-04 dt: 3332.21ms, tok/sec:157339.50
HellaSwag accuracy:2325075327704040465/-2=-1162537663852020224.0000
rank 1 sample 0: Hello, I'm a language model, and you're going to have to show the computer what exactly "the" is. You have two options.
First
rank 1 sample 1: Hello, I'm a language model, you know how to model the code that supports the concept "proteins", when used in theory.
But,
rank 1 sample 2: Hello, I'm a language model, but don't worry about it. I'm a language model... and you could say, I know what it would look
rank 1 sample 3: Hello, I'm a language model, and I'm working with code. That includes the fact that, after learning basic programming there's a lot of coding,
rank 0 sample 0: Hello, I'm a language model, and I think I can help with programming I will be involved in writing if someone is able to work out which language.
rank 0 sample 1: Hello, I'm a language model, I was happy to have a little training in this little field, because I've got some tips on making my job easier
rank 0 sample 2: Hello, I'm a language model, so I got the code out of it. What I'm getting at is a set of parameters that I can call each
rank 0 sample 3: Hello, I'm a language model, I really want to be able to tell the rest of the story for this lesson when I see the video.<|endoftext|>S
step 6350, loss: 3.369253, norm:0.2568, lr:4.2350e-04 dt: 48518.04ms, tok/sec:10806.04
step 6351, loss: 3.379280, norm:0.2528, lr:4.2344e-04 dt: 3331.98ms, tok/sec:157350.36
step 6352, loss: 3.434901, norm:0.2975, lr:4.2339e-04 dt: 3332.18ms, tok/sec:157341.02
step 6353, loss: 3.436353, norm:0.2723, lr:4.2333e-04 dt: 3332.29ms, tok/sec:157335.81
step 6354, loss: 3.400091, norm:0.2462, lr:4.2328e-04 dt: 3332.35ms, tok/sec:157332.82
step 6355, loss: 3.430686, norm:0.2604, lr:4.2322e-04 dt: 3332.10ms, tok/sec:157344.81
step 6356, loss: 3.370208, norm:0.2601, lr:4.2317e-04 dt: 3331.85ms, tok/sec:157356.22
step 6357, loss: 3.442663, norm:0.2610, lr:4.2311e-04 dt: 3332.01ms, tok/sec:157349.00
step 6358, loss: 3.391606, norm:0.2596, lr:4.2306e-04 dt: 3332.02ms, tok/sec:157348.31
step 6359, loss: 3.430022, norm:0.2613, lr:4.2300e-04 dt: 3332.18ms, tok/sec:157341.00
step 6360, loss: 3.391086, norm:0.2712, lr:4.2295e-04 dt: 3331.93ms, tok/sec:157352.62
step 6361, loss: 3.481098, norm:0.2623, lr:4.2289e-04 dt: 3332.06ms, tok/sec:157346.72
step 6362, loss: 3.361716, norm:0.2396, lr:4.2284e-04 dt: 3332.19ms, tok/sec:157340.47
step 6363, loss: 3.393698, norm:0.2654, lr:4.2278e-04 dt: 3332.33ms, tok/sec:157333.52
step 6364, loss: 3.360763, norm:0.2425, lr:4.2273e-04 dt: 3332.00ms, tok/sec:157349.53
step 6365, loss: 3.412118, norm:0.2637, lr:4.2267e-04 dt: 3332.06ms, tok/sec:157346.31
step 6366, loss: 3.350252, norm:0.2275, lr:4.2262e-04 dt: 3332.20ms, tok/sec:157340.01
step 6367, loss: 3.356537, norm:0.2675, lr:4.2256e-04 dt: 3332.03ms, tok/sec:157347.84
step 6368, loss: 3.466491, norm:0.2742, lr:4.2251e-04 dt: 3332.10ms, tok/sec:157344.60
step 6369, loss: 3.342850, norm:0.2575, lr:4.2246e-04 dt: 3332.08ms, tok/sec:157345.66
step 6370, loss: 3.323482, norm:0.2521, lr:4.2240e-04 dt: 3331.97ms, tok/sec:157350.84
step 6371, loss: 3.412739, norm:0.2379, lr:4.2235e-04 dt: 3332.19ms, tok/sec:157340.21
step 6372, loss: 3.341744, norm:0.2762, lr:4.2229e-04 dt: 3332.21ms, tok/sec:157339.53
step 6373, loss: 3.337425, norm:0.2544, lr:4.2224e-04 dt: 3331.97ms, tok/sec:157350.83
step 6374, loss: 3.313977, norm:0.2640, lr:4.2218e-04 dt: 3332.08ms, tok/sec:157345.76
step 6375, loss: 3.326197, norm:0.2439, lr:4.2213e-04 dt: 3331.93ms, tok/sec:157352.78
step 6376, loss: 3.409796, norm:0.2608, lr:4.2207e-04 dt: 3332.13ms, tok/sec:157343.19
step 6377, loss: 3.343634, norm:0.2549, lr:4.2202e-04 dt: 3332.20ms, tok/sec:157340.04
step 6378, loss: 3.406600, norm:0.2633, lr:4.2196e-04 dt: 3332.05ms, tok/sec:157346.87
step 6379, loss: 3.376447, norm:0.2441, lr:4.2191e-04 dt: 3332.15ms, tok/sec:157342.04
step 6380, loss: 3.426078, norm:0.2796, lr:4.2185e-04 dt: 3332.14ms, tok/sec:157342.49
step 6381, loss: 3.347841, norm:0.2425, lr:4.2180e-04 dt: 3332.38ms, tok/sec:157331.35
step 6382, loss: 3.537397, norm:0.3297, lr:4.2174e-04 dt: 3332.19ms, tok/sec:157340.32
step 6383, loss: 3.340452, norm:0.2934, lr:4.2169e-04 dt: 3332.00ms, tok/sec:157349.47
step 6384, loss: 3.398164, norm:0.2659, lr:4.2163e-04 dt: 3332.07ms, tok/sec:157346.14
step 6385, loss: 3.386125, norm:0.2919, lr:4.2158e-04 dt: 3332.19ms, tok/sec:157340.39
step 6386, loss: 3.404821, norm:0.2857, lr:4.2152e-04 dt: 3332.04ms, tok/sec:157347.42
step 6387, loss: 3.461415, norm:0.2612, lr:4.2147e-04 dt: 3332.17ms, tok/sec:157341.54
step 6388, loss: 3.385224, norm:0.2758, lr:4.2141e-04 dt: 3331.90ms, tok/sec:157353.91
step 6389, loss: 3.510199, norm:0.3314, lr:4.2136e-04 dt: 3332.20ms, tok/sec:157339.96
step 6390, loss: 3.371804, norm:0.3061, lr:4.2130e-04 dt: 3332.24ms, tok/sec:157337.95
step 6391, loss: 3.431383, norm:0.2953, lr:4.2125e-04 dt: 3332.29ms, tok/sec:157335.55
step 6392, loss: 3.420175, norm:0.2924, lr:4.2119e-04 dt: 3331.93ms, tok/sec:157352.51
step 6393, loss: 3.361398, norm:0.2780, lr:4.2114e-04 dt: 3331.91ms, tok/sec:157353.46
step 6394, loss: 3.417923, norm:0.2693, lr:4.2108e-04 dt: 3332.20ms, tok/sec:157339.78
step 6395, loss: 3.383476, norm:0.2590, lr:4.2103e-04 dt: 3332.03ms, tok/sec:157348.02
step 6396, loss: 3.407763, norm:0.2601, lr:4.2097e-04 dt: 3331.96ms, tok/sec:157351.16
step 6397, loss: 3.455873, norm:0.2425, lr:4.2092e-04 dt: 3332.09ms, tok/sec:157345.14
step 6398, loss: 3.400911, norm:0.2636, lr:4.2086e-04 dt: 3332.12ms, tok/sec:157343.78
step 6399, loss: 3.365532, norm:0.2676, lr:4.2081e-04 dt: 3332.42ms, tok/sec:157329.58
validation loss: 3.3953
Model and optimizer state saved.
HellaSwag accuracy:2325110475568940041/-2=-1162555237784470016.0000
rank 1 sample 0: Hello, I'm a language model, and there's a lot of fun playing it all the time now.
There are three ways the language model can be
rank 1 sample 1: Hello, I'm a language model, a model for creating a language and a game.
As you can see, to implement the approach to language learning,
rank 1 sample 2: Hello, I'm a language model, so basically the code is a set of rules.
- Languages that use data structures are the same for the different systems
rank 1 sample 3: Hello, I'm a language model, and I'm using the one on the building block. :)
Please click the link we were working on and start working
rank 0 sample 0: Hello, I'm a language model, and I just want you to do more math just now. I do pretty darn well in math classes and there are really
rank 0 sample 1: Hello, I'm a language model, what?
I've been reading music for many years to come to the right place to study music but have never thought
rank 0 sample 2: Hello, I'm a language model, so I use this program just to look at all of the other programming languages.
If you're interested in using our
rank 0 sample 3: Hello, I'm a language model, which lets me know what language to use in situations.
At the very end of the first part of the book they
step 6400, loss: 3.318426, norm:0.2557, lr:4.2075e-04 dt: 56208.82ms, tok/sec:9327.50
step 6401, loss: 3.375668, norm:0.2890, lr:4.2070e-04 dt: 3332.73ms, tok/sec:157314.68
step 6402, loss: 3.363838, norm:0.2471, lr:4.2064e-04 dt: 3332.93ms, tok/sec:157305.41
step 6403, loss: 3.380132, norm:0.2759, lr:4.2059e-04 dt: 3332.21ms, tok/sec:157339.58
step 6404, loss: 3.343821, norm:0.2435, lr:4.2053e-04 dt: 3332.30ms, tok/sec:157335.35
step 6405, loss: 3.365132, norm:0.2287, lr:4.2048e-04 dt: 3332.09ms, tok/sec:157345.11
step 6406, loss: 3.376447, norm:0.2475, lr:4.2042e-04 dt: 3332.12ms, tok/sec:157343.46
step 6407, loss: 3.340062, norm:0.2413, lr:4.2037e-04 dt: 3331.90ms, tok/sec:157353.97
step 6408, loss: 3.374899, norm:0.2366, lr:4.2031e-04 dt: 3331.98ms, tok/sec:157350.14
step 6409, loss: 3.399001, norm:0.2832, lr:4.2026e-04 dt: 3332.06ms, tok/sec:157346.74
step 6410, loss: 3.332760, norm:0.2460, lr:4.2020e-04 dt: 3332.06ms, tok/sec:157346.63
step 6411, loss: 3.328573, norm:0.2650, lr:4.2015e-04 dt: 3332.09ms, tok/sec:157345.12
step 6412, loss: 3.362862, norm:0.2499, lr:4.2009e-04 dt: 3332.45ms, tok/sec:157327.96
step 6413, loss: 3.335780, norm:0.3378, lr:4.2004e-04 dt: 3332.46ms, tok/sec:157327.53
step 6414, loss: 3.477115, norm:0.3330, lr:4.1998e-04 dt: 3332.00ms, tok/sec:157349.41
step 6415, loss: 3.417137, norm:0.2709, lr:4.1993e-04 dt: 3332.13ms, tok/sec:157343.17
step 6416, loss: 3.439394, norm:0.3180, lr:4.1987e-04 dt: 3332.00ms, tok/sec:157349.42
step 6417, loss: 3.432419, norm:0.2765, lr:4.1982e-04 dt: 3332.11ms, tok/sec:157343.98
step 6418, loss: 3.404737, norm:0.3226, lr:4.1976e-04 dt: 3331.85ms, tok/sec:157356.64
step 6419, loss: 3.380924, norm:0.2633, lr:4.1971e-04 dt: 3332.07ms, tok/sec:157346.11
step 6420, loss: 3.384540, norm:0.2909, lr:4.1965e-04 dt: 3332.24ms, tok/sec:157337.95
step 6421, loss: 3.349754, norm:0.2386, lr:4.1960e-04 dt: 3332.15ms, tok/sec:157342.21
step 6422, loss: 3.394417, norm:0.3050, lr:4.1954e-04 dt: 3332.34ms, tok/sec:157333.13
step 6423, loss: 3.388340, norm:0.2429, lr:4.1949e-04 dt: 3331.96ms, tok/sec:157351.16
step 6424, loss: 3.389653, norm:0.2721, lr:4.1943e-04 dt: 3332.19ms, tok/sec:157340.57
step 6425, loss: 3.385709, norm:0.2687, lr:4.1938e-04 dt: 3332.12ms, tok/sec:157343.81
step 6426, loss: 3.480738, norm:0.2480, lr:4.1932e-04 dt: 3332.22ms, tok/sec:157339.06
step 6427, loss: 3.461067, norm:0.2615, lr:4.1927e-04 dt: 3331.96ms, tok/sec:157351.17
step 6428, loss: 3.399999, norm:0.2534, lr:4.1921e-04 dt: 3332.12ms, tok/sec:157343.88
step 6429, loss: 3.408283, norm:0.2487, lr:4.1916e-04 dt: 3332.23ms, tok/sec:157338.62
step 6430, loss: 3.455819, norm:0.2857, lr:4.1910e-04 dt: 3332.41ms, tok/sec:157329.96
step 6431, loss: 3.438492, norm:0.2538, lr:4.1905e-04 dt: 3332.19ms, tok/sec:157340.59
step 6432, loss: 3.415991, norm:0.2606, lr:4.1899e-04 dt: 3331.94ms, tok/sec:157352.06
step 6433, loss: 3.331867, norm:0.2721, lr:4.1894e-04 dt: 3332.27ms, tok/sec:157336.49
step 6434, loss: 3.362498, norm:0.2848, lr:4.1888e-04 dt: 3332.00ms, tok/sec:157349.17
step 6435, loss: 3.330469, norm:0.2664, lr:4.1883e-04 dt: 3332.03ms, tok/sec:157347.95
step 6436, loss: 3.328514, norm:0.2415, lr:4.1877e-04 dt: 3332.10ms, tok/sec:157344.82
step 6437, loss: 3.319700, norm:0.2513, lr:4.1872e-04 dt: 3332.10ms, tok/sec:157344.78
step 6438, loss: 3.363257, norm:0.2425, lr:4.1866e-04 dt: 3331.99ms, tok/sec:157349.88
step 6439, loss: 3.314834, norm:0.2499, lr:4.1861e-04 dt: 3332.22ms, tok/sec:157338.90
step 6440, loss: 3.308995, norm:0.2290, lr:4.1855e-04 dt: 3332.22ms, tok/sec:157339.06
step 6441, loss: 3.440403, norm:0.2788, lr:4.1850e-04 dt: 3332.74ms, tok/sec:157314.21
step 6442, loss: 3.355259, norm:0.2447, lr:4.1844e-04 dt: 3331.88ms, tok/sec:157355.23
step 6443, loss: 3.387284, norm:0.2396, lr:4.1839e-04 dt: 3332.10ms, tok/sec:157344.49
step 6444, loss: 3.320727, norm:0.2469, lr:4.1833e-04 dt: 3332.02ms, tok/sec:157348.20
step 6445, loss: 3.315474, norm:0.2246, lr:4.1828e-04 dt: 3332.18ms, tok/sec:157340.76
step 6446, loss: 3.392875, norm:0.2696, lr:4.1822e-04 dt: 3332.08ms, tok/sec:157345.50
step 6447, loss: 3.384035, norm:0.2279, lr:4.1817e-04 dt: 3331.76ms, tok/sec:157360.47
step 6448, loss: 3.445852, norm:0.2663, lr:4.1811e-04 dt: 3332.31ms, tok/sec:157334.66
step 6449, loss: 3.388762, norm:0.2236, lr:4.1806e-04 dt: 3332.12ms, tok/sec:157343.78
HellaSwag accuracy:2325255681970734161/-2=-1162627840985367040.0000
rank 1 sample 0: Hello, I'm a language model, and I'm not even a language's programmer - so in reality, I'm not doing it as I would like.
rank 1 sample 1: Hello, I'm a language model, which I have, and I'm interested in how to code. As I have had already gotten these two things, I
rank 1 sample 2: Hello, I'm a language model, so I don't know what I'm talking about. So when we say, "I'm just a little more flexible
rank 1 sample 3: Hello, I'm a language model, and I'm pretty excited in that." How about, because of "if two people start an image, it gets a
rank 0 sample 0: Hello, I'm a language model, and I'll be talking about that as first generation of fluent speakers from kindergarten-age and over and under, and the
rank 0 sample 1: Hello, I'm a language model, but with a lot of practice I couldn't do an effective job on the Internet. So I've put in a lot
rank 0 sample 2: Hello, I'm a language model, so I like it pretty much. The language should be a tool in a language model, and it should be something like
rank 0 sample 3: Hello, I'm a language model, which's been used in the classroom, and if you're looking for the right ones, try the ESL test here if
step 6450, loss: 3.376676, norm:0.2644, lr:4.1800e-04 dt: 48515.61ms, tok/sec:10806.58
step 6451, loss: 3.418743, norm:0.2515, lr:4.1795e-04 dt: 3332.20ms, tok/sec:157340.02
step 6452, loss: 3.389554, norm:0.2700, lr:4.1789e-04 dt: 3332.21ms, tok/sec:157339.33
step 6453, loss: 3.390132, norm:0.2507, lr:4.1784e-04 dt: 3332.09ms, tok/sec:157345.21
step 6454, loss: 3.413163, norm:0.2517, lr:4.1778e-04 dt: 3331.88ms, tok/sec:157355.09
step 6455, loss: 3.400304, norm:0.2565, lr:4.1773e-04 dt: 3331.88ms, tok/sec:157355.14
step 6456, loss: 3.378685, norm:0.2391, lr:4.1767e-04 dt: 3332.16ms, tok/sec:157341.71
step 6457, loss: 3.467065, norm:0.2837, lr:4.1762e-04 dt: 3332.03ms, tok/sec:157348.15
step 6458, loss: 3.421528, norm:0.2418, lr:4.1756e-04 dt: 3332.13ms, tok/sec:157343.25
step 6459, loss: 3.400202, norm:0.2783, lr:4.1751e-04 dt: 3332.34ms, tok/sec:157333.18
step 6460, loss: 3.430571, norm:0.2753, lr:4.1745e-04 dt: 3332.27ms, tok/sec:157336.55
step 6461, loss: 3.367788, norm:0.2779, lr:4.1740e-04 dt: 3331.91ms, tok/sec:157353.76
step 6462, loss: 3.413190, norm:0.2817, lr:4.1734e-04 dt: 3332.04ms, tok/sec:157347.26
step 6463, loss: 3.418641, norm:0.2591, lr:4.1729e-04 dt: 3331.95ms, tok/sec:157351.85
step 6464, loss: 3.430979, norm:0.5455, lr:4.1723e-04 dt: 3332.14ms, tok/sec:157342.53
step 6465, loss: 3.430407, norm:0.2763, lr:4.1718e-04 dt: 3332.01ms, tok/sec:157348.70
step 6466, loss: 3.505892, norm:0.2728, lr:4.1712e-04 dt: 3332.06ms, tok/sec:157346.47
step 6467, loss: 3.392248, norm:0.2716, lr:4.1706e-04 dt: 3332.19ms, tok/sec:157340.42
step 6468, loss: 3.434768, norm:0.2760, lr:4.1701e-04 dt: 3332.13ms, tok/sec:157343.43
step 6469, loss: 3.302680, norm:0.2777, lr:4.1695e-04 dt: 3332.34ms, tok/sec:157333.33
step 6470, loss: 3.350419, norm:0.2639, lr:4.1690e-04 dt: 3331.93ms, tok/sec:157352.71
step 6471, loss: 3.357657, norm:0.2847, lr:4.1684e-04 dt: 3332.01ms, tok/sec:157348.64
step 6472, loss: 3.317792, norm:0.2593, lr:4.1679e-04 dt: 3332.09ms, tok/sec:157345.17
step 6473, loss: 3.337704, norm:0.2574, lr:4.1673e-04 dt: 3332.16ms, tok/sec:157341.63
step 6474, loss: 3.364099, norm:0.2740, lr:4.1668e-04 dt: 3332.06ms, tok/sec:157346.38
step 6475, loss: 3.331269, norm:0.2700, lr:4.1662e-04 dt: 3331.95ms, tok/sec:157351.88
step 6476, loss: 3.385682, norm:0.2609, lr:4.1657e-04 dt: 3334.07ms, tok/sec:157251.76
step 6477, loss: 3.321108, norm:0.2725, lr:4.1651e-04 dt: 3332.01ms, tok/sec:157348.84
step 6478, loss: 3.340336, norm:0.2639, lr:4.1646e-04 dt: 3332.36ms, tok/sec:157332.18
step 6479, loss: 3.305882, norm:0.2769, lr:4.1640e-04 dt: 3331.92ms, tok/sec:157353.13
step 6480, loss: 3.386154, norm:0.2972, lr:4.1635e-04 dt: 3332.07ms, tok/sec:157345.83
step 6481, loss: 3.384198, norm:0.2700, lr:4.1629e-04 dt: 3332.02ms, tok/sec:157348.51
step 6482, loss: 3.454182, norm:0.3154, lr:4.1624e-04 dt: 3332.04ms, tok/sec:157347.45
step 6483, loss: 3.436351, norm:0.2879, lr:4.1618e-04 dt: 3331.97ms, tok/sec:157350.64
step 6484, loss: 3.385192, norm:0.2859, lr:4.1613e-04 dt: 3332.06ms, tok/sec:157346.45
step 6485, loss: 3.380983, norm:0.2898, lr:4.1607e-04 dt: 3332.13ms, tok/sec:157343.24
step 6486, loss: 3.375103, norm:0.2736, lr:4.1602e-04 dt: 3332.10ms, tok/sec:157344.44
step 6487, loss: 3.398103, norm:0.2838, lr:4.1596e-04 dt: 3331.94ms, tok/sec:157352.00
step 6488, loss: 3.384188, norm:0.2666, lr:4.1590e-04 dt: 3332.24ms, tok/sec:157337.80
step 6489, loss: 3.431454, norm:0.2907, lr:4.1585e-04 dt: 3332.05ms, tok/sec:157346.93
step 6490, loss: 3.368958, norm:0.2602, lr:4.1579e-04 dt: 3332.67ms, tok/sec:157317.92
step 6491, loss: 3.372782, norm:0.2464, lr:4.1574e-04 dt: 3332.07ms, tok/sec:157346.04
step 6492, loss: 3.461708, norm:0.2594, lr:4.1568e-04 dt: 3332.16ms, tok/sec:157341.99
step 6493, loss: 3.477782, norm:0.2342, lr:4.1563e-04 dt: 3332.01ms, tok/sec:157348.83
step 6494, loss: 3.373079, norm:0.2471, lr:4.1557e-04 dt: 3332.02ms, tok/sec:157348.19
step 6495, loss: 3.377782, norm:0.2282, lr:4.1552e-04 dt: 3332.12ms, tok/sec:157343.56
step 6496, loss: 3.404280, norm:0.2721, lr:4.1546e-04 dt: 3332.21ms, tok/sec:157339.36
step 6497, loss: 3.428576, norm:0.2805, lr:4.1541e-04 dt: 3332.38ms, tok/sec:157331.29
step 6498, loss: 3.367191, norm:0.2402, lr:4.1535e-04 dt: 3332.10ms, tok/sec:157344.82
step 6499, loss: 3.376320, norm:0.2751, lr:4.1530e-04 dt: 3332.23ms, tok/sec:157338.36
validation loss: 3.3868
Model and optimizer state saved.
HellaSwag accuracy:2316244050309743633/-2=-1158122025154871808.0000
rank 1 sample 0: Hello, I'm a language model, and as a language model I'm building on these two terms so that I can use my programming style to make my code
rank 1 sample 1: Hello, I'm a language model, which I think does the same thing to this. And to me, it's that type of concept.
I think
rank 1 sample 2: Hello, I'm a language model, so with this blog, I'm going to talk about my English. So, I'm going to start by describing the
rank 1 sample 3: Hello, I'm a language model, and I'm really excited to see some videos of these speakers for that, along with much more, I'll have to
rank 0 sample 0: Hello, I'm a language model, so I would like you to use an editor on your site to learn basic programming. You can download a new editor or
rank 0 sample 1: Hello, I'm a language model, I'd like to talk about this about the word of, 'island,' but what does it mean in English?
rank 0 sample 2: Hello, I'm a language model, and I understand it! If you've ever thought about it I have.
What do you think about this article?
rank 0 sample 3: Hello, I'm a language model, I believe that we need to understand what is just a language. What do I understand if I have only one or just
step 6500, loss: 3.409650, norm:0.2557, lr:4.1524e-04 dt: 56329.39ms, tok/sec:9307.54
step 6501, loss: 3.401190, norm:0.2638, lr:4.1519e-04 dt: 3332.18ms, tok/sec:157341.03
step 6502, loss: 3.400909, norm:0.2475, lr:4.1513e-04 dt: 3332.26ms, tok/sec:157336.88
step 6503, loss: 3.384993, norm:0.2574, lr:4.1507e-04 dt: 3332.25ms, tok/sec:157337.59
step 6504, loss: 3.343905, norm:0.2404, lr:4.1502e-04 dt: 3331.74ms, tok/sec:157361.63
step 6505, loss: 3.342405, norm:0.2366, lr:4.1496e-04 dt: 3332.15ms, tok/sec:157342.13
step 6506, loss: 3.362577, norm:0.2246, lr:4.1491e-04 dt: 3332.11ms, tok/sec:157344.04
step 6507, loss: 3.306599, norm:0.2486, lr:4.1485e-04 dt: 3332.07ms, tok/sec:157346.05
step 6508, loss: 3.324698, norm:0.2378, lr:4.1480e-04 dt: 3332.23ms, tok/sec:157338.58
step 6509, loss: 3.340379, norm:0.2450, lr:4.1474e-04 dt: 3331.79ms, tok/sec:157359.02
step 6510, loss: 3.430526, norm:0.3146, lr:4.1469e-04 dt: 3332.33ms, tok/sec:157333.85
step 6511, loss: 3.333138, norm:0.3434, lr:4.1463e-04 dt: 3332.13ms, tok/sec:157343.18
step 6512, loss: 3.317950, norm:0.2709, lr:4.1458e-04 dt: 3332.14ms, tok/sec:157342.69
step 6513, loss: 3.380380, norm:0.2967, lr:4.1452e-04 dt: 3331.99ms, tok/sec:157349.86
step 6514, loss: 3.364634, norm:0.2805, lr:4.1447e-04 dt: 3332.07ms, tok/sec:157345.83
step 6515, loss: 3.382500, norm:0.2834, lr:4.1441e-04 dt: 3332.03ms, tok/sec:157348.01
step 6516, loss: 3.427904, norm:0.2806, lr:4.1435e-04 dt: 3331.96ms, tok/sec:157351.42
step 6517, loss: 3.364870, norm:0.2652, lr:4.1430e-04 dt: 3332.21ms, tok/sec:157339.59
step 6518, loss: 3.411636, norm:0.2754, lr:4.1424e-04 dt: 3332.14ms, tok/sec:157342.91
step 6519, loss: 3.408126, norm:0.3005, lr:4.1419e-04 dt: 3332.75ms, tok/sec:157313.90
step 6520, loss: 3.466783, norm:0.2883, lr:4.1413e-04 dt: 3331.99ms, tok/sec:157349.70
step 6521, loss: 3.425979, norm:0.3226, lr:4.1408e-04 dt: 3332.13ms, tok/sec:157343.23
step 6522, loss: 3.385051, norm:0.2610, lr:4.1402e-04 dt: 3332.27ms, tok/sec:157336.38
step 6523, loss: 3.396407, norm:0.2715, lr:4.1397e-04 dt: 3332.25ms, tok/sec:157337.37
step 6524, loss: 3.401843, norm:0.2682, lr:4.1391e-04 dt: 3331.98ms, tok/sec:157350.31
step 6525, loss: 3.372469, norm:0.2698, lr:4.1386e-04 dt: 3332.09ms, tok/sec:157345.11
step 6526, loss: 3.416017, norm:0.2580, lr:4.1380e-04 dt: 3332.44ms, tok/sec:157328.50
step 6527, loss: 3.460806, norm:0.2793, lr:4.1375e-04 dt: 3332.19ms, tok/sec:157340.15
step 6528, loss: 3.450456, norm:0.2692, lr:4.1369e-04 dt: 3331.92ms, tok/sec:157353.00
step 6529, loss: 3.328117, norm:0.2923, lr:4.1363e-04 dt: 3332.06ms, tok/sec:157346.33
step 6530, loss: 3.432911, norm:0.2840, lr:4.1358e-04 dt: 3332.29ms, tok/sec:157335.74
step 6531, loss: 3.396103, norm:0.2699, lr:4.1352e-04 dt: 3332.01ms, tok/sec:157348.90
step 6532, loss: 3.387383, norm:0.2827, lr:4.1347e-04 dt: 3331.98ms, tok/sec:157350.23
step 6533, loss: 3.404254, norm:0.2544, lr:4.1341e-04 dt: 3332.03ms, tok/sec:157348.15
step 6534, loss: 3.412472, norm:0.2568, lr:4.1336e-04 dt: 3332.03ms, tok/sec:157347.81
step 6535, loss: 3.421572, norm:0.2706, lr:4.1330e-04 dt: 3332.45ms, tok/sec:157328.01
step 6536, loss: 3.351982, norm:0.2656, lr:4.1325e-04 dt: 3332.21ms, tok/sec:157339.35
step 6537, loss: 3.359563, norm:0.2760, lr:4.1319e-04 dt: 3332.07ms, tok/sec:157345.98
step 6538, loss: 3.350016, norm:0.2530, lr:4.1314e-04 dt: 3332.11ms, tok/sec:157343.92
step 6539, loss: 3.302573, norm:0.2661, lr:4.1308e-04 dt: 3332.32ms, tok/sec:157334.37
step 6540, loss: 3.334144, norm:0.2630, lr:4.1302e-04 dt: 3331.85ms, tok/sec:157356.58
step 6541, loss: 3.368727, norm:0.2788, lr:4.1297e-04 dt: 3331.97ms, tok/sec:157350.93
step 6542, loss: 3.401179, norm:0.2995, lr:4.1291e-04 dt: 3332.12ms, tok/sec:157343.73
step 6543, loss: 3.307995, norm:0.2732, lr:4.1286e-04 dt: 3332.08ms, tok/sec:157345.40
step 6544, loss: 3.358353, norm:0.2527, lr:4.1280e-04 dt: 3332.47ms, tok/sec:157326.99
step 6545, loss: 3.368664, norm:0.2747, lr:4.1275e-04 dt: 3332.34ms, tok/sec:157333.31
step 6546, loss: 3.403215, norm:0.2615, lr:4.1269e-04 dt: 3331.89ms, tok/sec:157354.37
step 6547, loss: 3.343693, norm:0.2750, lr:4.1264e-04 dt: 3332.18ms, tok/sec:157340.79
step 6548, loss: 3.375393, norm:0.2564, lr:4.1258e-04 dt: 3332.20ms, tok/sec:157339.94
step 6549, loss: 3.359532, norm:0.2631, lr:4.1252e-04 dt: 3332.02ms, tok/sec:157348.26
HellaSwag accuracy:2325119273807315985/-2=-1162559636903657984.0000
rank 1 sample 0: Hello, I'm a language model, my native is a language. I couldn't say my native (or native) is not native and I can't say
rank 1 sample 1: Hello, I'm a language model, you know what it's like. What about the syntax structure? Here's one: Language models refer to the language that
rank 1 sample 2: Hello, I'm a language model, but where did this language come from?
The answer, you might think, is that there was something in a software
rank 1 sample 3: Hello, I'm a language model, and I'm writing my work for The Queen. I always go to YouTube's YouTube channel too.
I was in
rank 0 sample 0: Hello, I'm a language model, and I need to learn this stuff first step of the journey, it's important to be able to learn all the things
rank 0 sample 1: Hello, I'm a language model, so how do you use it? Have you heard our forum. It's free, so please let me know. Thanks
rank 0 sample 2: Hello, I'm a language model, so I use my existing Language Viewer to work with my code so I can work with the code.
The function
rank 0 sample 3: Hello, I'm a language model, I hope you have a good understanding of it," said Dr. Kochenha.
The scientists in Japan looked
step 6550, loss: 3.403782, norm:0.2673, lr:4.1247e-04 dt: 48519.56ms, tok/sec:10805.70
step 6551, loss: 3.402936, norm:0.2713, lr:4.1241e-04 dt: 3331.95ms, tok/sec:157351.84
step 6552, loss: 3.395625, norm:0.2462, lr:4.1236e-04 dt: 3332.09ms, tok/sec:157345.00
step 6553, loss: 3.379020, norm:0.2515, lr:4.1230e-04 dt: 3332.69ms, tok/sec:157316.58
step 6554, loss: 3.402796, norm:0.2587, lr:4.1225e-04 dt: 3332.55ms, tok/sec:157323.28
step 6555, loss: 3.409011, norm:0.2605, lr:4.1219e-04 dt: 3331.73ms, tok/sec:157361.97
step 6556, loss: 3.390717, norm:0.2604, lr:4.1214e-04 dt: 3332.25ms, tok/sec:157337.64
step 6557, loss: 3.422637, norm:0.2669, lr:4.1208e-04 dt: 3332.30ms, tok/sec:157335.08
step 6558, loss: 3.405464, norm:0.2511, lr:4.1202e-04 dt: 3332.31ms, tok/sec:157334.82
step 6559, loss: 3.417288, norm:0.2486, lr:4.1197e-04 dt: 3332.02ms, tok/sec:157348.22
step 6560, loss: 3.389122, norm:0.2667, lr:4.1191e-04 dt: 3332.12ms, tok/sec:157343.56
step 6561, loss: 3.366026, norm:0.2521, lr:4.1186e-04 dt: 3331.94ms, tok/sec:157352.40
step 6562, loss: 3.437152, norm:0.2581, lr:4.1180e-04 dt: 3332.42ms, tok/sec:157329.62
step 6563, loss: 3.384068, norm:0.2616, lr:4.1175e-04 dt: 3332.04ms, tok/sec:157347.68
step 6564, loss: 3.387299, norm:0.2472, lr:4.1169e-04 dt: 3332.00ms, tok/sec:157349.38
step 6565, loss: 3.394295, norm:0.2501, lr:4.1164e-04 dt: 3331.97ms, tok/sec:157350.86
step 6566, loss: 3.417202, norm:0.2493, lr:4.1158e-04 dt: 3332.26ms, tok/sec:157337.05
step 6567, loss: 3.380787, norm:0.2947, lr:4.1152e-04 dt: 3332.15ms, tok/sec:157342.45
step 6568, loss: 3.414352, norm:0.2576, lr:4.1147e-04 dt: 3332.25ms, tok/sec:157337.52
step 6569, loss: 3.436489, norm:0.2555, lr:4.1141e-04 dt: 3332.37ms, tok/sec:157332.07
step 6570, loss: 3.422031, norm:0.2502, lr:4.1136e-04 dt: 3332.30ms, tok/sec:157335.01
step 6571, loss: 3.424838, norm:0.2665, lr:4.1130e-04 dt: 3332.00ms, tok/sec:157349.29
step 6572, loss: 3.342885, norm:0.2520, lr:4.1125e-04 dt: 3332.06ms, tok/sec:157346.65
step 6573, loss: 3.373507, norm:0.2596, lr:4.1119e-04 dt: 3332.19ms, tok/sec:157340.49
step 6574, loss: 3.404989, norm:0.2544, lr:4.1113e-04 dt: 3332.02ms, tok/sec:157348.39
step 6575, loss: 3.374675, norm:0.2670, lr:4.1108e-04 dt: 3332.06ms, tok/sec:157346.31
step 6576, loss: 3.342286, norm:0.2576, lr:4.1102e-04 dt: 3332.28ms, tok/sec:157336.32
step 6577, loss: 3.329186, norm:0.2393, lr:4.1097e-04 dt: 3331.93ms, tok/sec:157352.48
step 6578, loss: 3.378597, norm:0.2523, lr:4.1091e-04 dt: 3332.53ms, tok/sec:157324.27
step 6579, loss: 3.335200, norm:0.2668, lr:4.1086e-04 dt: 3332.00ms, tok/sec:157349.25
step 6580, loss: 3.362638, norm:0.2506, lr:4.1080e-04 dt: 3332.07ms, tok/sec:157346.21
step 6581, loss: 3.318964, norm:0.2675, lr:4.1075e-04 dt: 3332.15ms, tok/sec:157342.37
step 6582, loss: 3.336951, norm:0.2407, lr:4.1069e-04 dt: 3332.01ms, tok/sec:157349.08
step 6583, loss: 3.350661, norm:0.2522, lr:4.1063e-04 dt: 3331.85ms, tok/sec:157356.57
step 6584, loss: 3.347081, norm:0.2738, lr:4.1058e-04 dt: 3332.18ms, tok/sec:157341.08
step 6585, loss: 3.358477, norm:0.2437, lr:4.1052e-04 dt: 3332.10ms, tok/sec:157344.79
step 6586, loss: 3.365894, norm:0.2807, lr:4.1047e-04 dt: 3332.22ms, tok/sec:157339.08
step 6587, loss: 3.374487, norm:0.2623, lr:4.1041e-04 dt: 3331.99ms, tok/sec:157349.97
step 6588, loss: 3.360106, norm:0.2711, lr:4.1036e-04 dt: 3332.06ms, tok/sec:157346.47
step 6589, loss: 3.388281, norm:0.2762, lr:4.1030e-04 dt: 3332.05ms, tok/sec:157347.06
step 6590, loss: 3.367567, norm:0.2648, lr:4.1024e-04 dt: 3332.16ms, tok/sec:157341.91
step 6591, loss: 3.455862, norm:0.2757, lr:4.1019e-04 dt: 3332.05ms, tok/sec:157346.90
step 6592, loss: 3.372066, norm:0.2612, lr:4.1013e-04 dt: 3332.23ms, tok/sec:157338.38
step 6593, loss: 3.421341, norm:0.2810, lr:4.1008e-04 dt: 3332.04ms, tok/sec:157347.24
step 6594, loss: 3.412409, norm:0.2679, lr:4.1002e-04 dt: 3332.44ms, tok/sec:157328.77
step 6595, loss: 3.372585, norm:0.2613, lr:4.0997e-04 dt: 3332.07ms, tok/sec:157346.13
step 6596, loss: 3.424318, norm:0.2550, lr:4.0991e-04 dt: 3331.93ms, tok/sec:157352.71
step 6597, loss: 3.367542, norm:0.2424, lr:4.0985e-04 dt: 3332.25ms, tok/sec:157337.59
step 6598, loss: 3.354187, norm:0.2890, lr:4.0980e-04 dt: 3332.26ms, tok/sec:157337.29
step 6599, loss: 3.469578, norm:0.2866, lr:4.0974e-04 dt: 3332.16ms, tok/sec:157341.67
validation loss: 3.3835
Model and optimizer state saved.
HellaSwag accuracy:2316244015950005329/-2=-1158122007975002624.0000
rank 1 sample 0: Hello, I'm a language model, where the target language is used to start and halt a movement when the target language is in a language. In this case
rank 1 sample 1: Hello, I'm a language model, you're using them. I'm writing software that works great on other languages as well too, I'll be writing software
rank 1 sample 2: Hello, I'm a language model, but sometimes the syntax is not. I'm not sure why some of my comments are incorrect or that it is wrong,
rank 1 sample 3: Hello, I'm a language model, and I'm working with something like this called the "E", or in which case people type something like this:

rank 0 sample 0: Hello, I'm a language model, and I'll be writing more in tomorrow about grammar and syntax, especially that we'll use your input as our input as
rank 0 sample 1: Hello, I'm a language model, so why wasn't there? It turned out, isn't there yet, I don't know. In this case,
rank 0 sample 2: Hello, I'm a language model, I'm in a sentence in a sentence, like I'm not in a sentence.
We have a sentence that represents
rank 0 sample 3: Hello, I'm a language model, so in the last few days we had a system that was all related to our computers. There are about 90 languages within
step 6600, loss: 3.409231, norm:0.2831, lr:4.0969e-04 dt: 56235.30ms, tok/sec:9323.11
step 6601, loss: 3.454288, norm:0.2819, lr:4.0963e-04 dt: 3332.24ms, tok/sec:157337.80
step 6602, loss: 3.394625, norm:0.2474, lr:4.0958e-04 dt: 3332.29ms, tok/sec:157335.78
step 6603, loss: 3.339045, norm:0.2558, lr:4.0952e-04 dt: 3331.99ms, tok/sec:157350.04
step 6604, loss: 3.350406, norm:0.2570, lr:4.0946e-04 dt: 3332.16ms, tok/sec:157341.91
step 6605, loss: 3.424443, norm:0.2651, lr:4.0941e-04 dt: 3331.90ms, tok/sec:157353.94
step 6606, loss: 3.355411, norm:0.2529, lr:4.0935e-04 dt: 3331.95ms, tok/sec:157351.48
step 6607, loss: 3.399820, norm:0.2842, lr:4.0930e-04 dt: 3332.14ms, tok/sec:157342.79
step 6608, loss: 3.335678, norm:0.2496, lr:4.0924e-04 dt: 3332.02ms, tok/sec:157348.34
step 6609, loss: 3.309715, norm:0.2519, lr:4.0919e-04 dt: 3332.00ms, tok/sec:157349.14
step 6610, loss: 3.366138, norm:0.2541, lr:4.0913e-04 dt: 3332.17ms, tok/sec:157341.31
step 6611, loss: 3.385163, norm:0.2742, lr:4.0907e-04 dt: 3332.21ms, tok/sec:157339.43
step 6612, loss: 3.330495, norm:0.2539, lr:4.0902e-04 dt: 3332.09ms, tok/sec:157344.89
step 6613, loss: 3.335374, norm:0.2593, lr:4.0896e-04 dt: 3331.98ms, tok/sec:157350.07
step 6614, loss: 3.344757, norm:0.2345, lr:4.0891e-04 dt: 3332.73ms, tok/sec:157315.04
step 6615, loss: 3.345715, norm:0.2375, lr:4.0885e-04 dt: 3332.87ms, tok/sec:157308.09
step 6616, loss: 3.328261, norm:0.2340, lr:4.0880e-04 dt: 3332.34ms, tok/sec:157333.34
step 6617, loss: 3.350378, norm:0.2334, lr:4.0874e-04 dt: 3332.04ms, tok/sec:157347.26
step 6618, loss: 3.314382, norm:0.2445, lr:4.0868e-04 dt: 3332.16ms, tok/sec:157341.64
step 6619, loss: 3.406324, norm:0.2540, lr:4.0863e-04 dt: 3332.22ms, tok/sec:157339.09
step 6620, loss: 3.359274, norm:0.2800, lr:4.0857e-04 dt: 3332.39ms, tok/sec:157331.00
step 6621, loss: 3.385494, norm:0.2548, lr:4.0852e-04 dt: 3332.25ms, tok/sec:157337.62
step 6622, loss: 3.450571, norm:0.2658, lr:4.0846e-04 dt: 3332.03ms, tok/sec:157347.84
step 6623, loss: 3.443660, norm:0.2538, lr:4.0840e-04 dt: 3332.18ms, tok/sec:157340.68
step 6624, loss: 3.418455, norm:0.2626, lr:4.0835e-04 dt: 3332.05ms, tok/sec:157346.92
step 6625, loss: 3.420931, norm:0.2860, lr:4.0829e-04 dt: 3331.92ms, tok/sec:157352.99
step 6626, loss: 3.396446, norm:0.2676, lr:4.0824e-04 dt: 3332.15ms, tok/sec:157342.17
step 6627, loss: 3.347550, norm:0.2449, lr:4.0818e-04 dt: 3332.39ms, tok/sec:157331.12
step 6628, loss: 3.416403, norm:0.2584, lr:4.0813e-04 dt: 3332.11ms, tok/sec:157344.13
step 6629, loss: 3.388712, norm:0.2603, lr:4.0807e-04 dt: 3333.03ms, tok/sec:157300.81
step 6630, loss: 3.426137, norm:0.2542, lr:4.0801e-04 dt: 3332.20ms, tok/sec:157339.72
step 6631, loss: 3.415799, norm:0.2595, lr:4.0796e-04 dt: 3332.09ms, tok/sec:157345.32
step 6632, loss: 3.410538, norm:0.2685, lr:4.0790e-04 dt: 3332.04ms, tok/sec:157347.59
step 6633, loss: 3.398004, norm:0.2591, lr:4.0785e-04 dt: 3332.41ms, tok/sec:157330.05
step 6634, loss: 3.430937, norm:0.2922, lr:4.0779e-04 dt: 3331.97ms, tok/sec:157350.95
step 6635, loss: 3.454890, norm:0.2705, lr:4.0773e-04 dt: 3332.18ms, tok/sec:157340.65
step 6636, loss: 3.372552, norm:0.3290, lr:4.0768e-04 dt: 3332.03ms, tok/sec:157347.93
step 6637, loss: 3.414091, norm:0.3481, lr:4.0762e-04 dt: 3332.06ms, tok/sec:157346.55
step 6638, loss: 3.384400, norm:0.2996, lr:4.0757e-04 dt: 3332.51ms, tok/sec:157325.24
step 6639, loss: 3.399928, norm:0.2856, lr:4.0751e-04 dt: 3331.98ms, tok/sec:157350.49
step 6640, loss: 3.361193, norm:0.2869, lr:4.0746e-04 dt: 3332.04ms, tok/sec:157347.55
step 6641, loss: 3.365252, norm:0.2541, lr:4.0740e-04 dt: 3332.20ms, tok/sec:157339.74
step 6642, loss: 3.356417, norm:0.2760, lr:4.0734e-04 dt: 3331.94ms, tok/sec:157352.35
step 6643, loss: 3.340369, norm:0.2915, lr:4.0729e-04 dt: 3332.34ms, tok/sec:157333.31
step 6644, loss: 3.350352, norm:0.2689, lr:4.0723e-04 dt: 3332.06ms, tok/sec:157346.42
step 6645, loss: 3.382639, norm:0.2872, lr:4.0718e-04 dt: 3332.45ms, tok/sec:157327.96
step 6646, loss: 3.309204, norm:0.2701, lr:4.0712e-04 dt: 3332.12ms, tok/sec:157343.86
step 6647, loss: 3.359097, norm:0.2445, lr:4.0706e-04 dt: 3332.01ms, tok/sec:157348.65
step 6648, loss: 3.359049, norm:0.2510, lr:4.0701e-04 dt: 3332.17ms, tok/sec:157341.44
step 6649, loss: 3.350718, norm:0.2496, lr:4.0695e-04 dt: 3332.20ms, tok/sec:157340.13
HellaSwag accuracy:4631111816604517457/-2=-2315555908302258688.0000
rank 1 sample 0: Hello, I'm a language model, not just a language model, but everything I wrote was designed for the language model. The next generation of languages is the
rank 1 sample 1: Hello, I'm a language model, you're probably talking about a big idea as to how they work.
You're writing an audio-visual project.
rank 1 sample 2: Hello, I'm a language model, I wanna start working with the language and I'm going to add some extra support for the code I'm going to add
rank 1 sample 3: Hello, I'm a language model, and I'm writing my work on "proper" because I had this app. One project, I'm building a
rank 0 sample 0: Hello, I'm a language model, and I think I've just seen two great folks out there, that helped me to make this fun, if not my
rank 0 sample 1: Hello, I'm a language model, so here's a list of all I can think of it:
Some things you need to know and you can do
rank 0 sample 2: Hello, I'm a language model, but I find this thing strange. You can make a simple text search in a search engine.
Word Search is available
rank 0 sample 3: Hello, I'm a language model, so do I know what's in this article too?
Click on the button.
|< Back to top index
step 6650, loss: 3.367102, norm:0.2424, lr:4.0690e-04 dt: 48516.80ms, tok/sec:10806.32
step 6651, loss: 3.360999, norm:0.2521, lr:4.0684e-04 dt: 3332.30ms, tok/sec:157335.23
step 6652, loss: 3.382039, norm:0.2513, lr:4.0678e-04 dt: 3332.13ms, tok/sec:157343.25
step 6653, loss: 3.427265, norm:0.2528, lr:4.0673e-04 dt: 3332.43ms, tok/sec:157329.13
step 6654, loss: 3.385127, norm:0.2443, lr:4.0667e-04 dt: 3332.00ms, tok/sec:157349.45
step 6655, loss: 3.432439, norm:0.2468, lr:4.0662e-04 dt: 3332.04ms, tok/sec:157347.67
step 6656, loss: 3.390010, norm:0.2640, lr:4.0656e-04 dt: 3332.19ms, tok/sec:157340.35
step 6657, loss: 3.452771, norm:0.5019, lr:4.0651e-04 dt: 3332.03ms, tok/sec:157348.02
step 6658, loss: 3.359599, norm:0.2829, lr:4.0645e-04 dt: 3332.04ms, tok/sec:157347.49
step 6659, loss: 3.447474, norm:0.2878, lr:4.0639e-04 dt: 3332.38ms, tok/sec:157331.47
step 6660, loss: 3.411108, norm:0.3555, lr:4.0634e-04 dt: 3332.25ms, tok/sec:157337.59
step 6661, loss: 3.501212, norm:0.2650, lr:4.0628e-04 dt: 3332.02ms, tok/sec:157348.53
step 6662, loss: 3.405471, norm:0.3194, lr:4.0623e-04 dt: 3332.19ms, tok/sec:157340.52
step 6663, loss: 3.434795, norm:0.2671, lr:4.0617e-04 dt: 3332.34ms, tok/sec:157333.43
step 6664, loss: 3.404310, norm:0.2897, lr:4.0611e-04 dt: 3332.16ms, tok/sec:157341.56
step 6665, loss: 3.438080, norm:0.2912, lr:4.0606e-04 dt: 3332.03ms, tok/sec:157348.12
step 6666, loss: 3.386085, norm:0.2724, lr:4.0600e-04 dt: 3332.22ms, tok/sec:157339.02
step 6667, loss: 3.393658, norm:0.2700, lr:4.0595e-04 dt: 3334.64ms, tok/sec:157224.99
step 6668, loss: 3.381569, norm:0.2734, lr:4.0589e-04 dt: 3332.22ms, tok/sec:157339.11
step 6669, loss: 3.390484, norm:0.2559, lr:4.0583e-04 dt: 3332.00ms, tok/sec:157349.13
step 6670, loss: 3.399299, norm:0.2738, lr:4.0578e-04 dt: 3331.91ms, tok/sec:157353.63
step 6671, loss: 3.422843, norm:0.2606, lr:4.0572e-04 dt: 3332.08ms, tok/sec:157345.77
step 6672, loss: 3.414967, norm:0.2555, lr:4.0567e-04 dt: 3332.27ms, tok/sec:157336.40
step 6673, loss: 3.394331, norm:0.2568, lr:4.0561e-04 dt: 3331.89ms, tok/sec:157354.77
step 6674, loss: 3.378702, norm:0.2542, lr:4.0555e-04 dt: 3331.93ms, tok/sec:157352.48
step 6675, loss: 3.324763, norm:0.2470, lr:4.0550e-04 dt: 3332.17ms, tok/sec:157341.44
step 6676, loss: 3.342670, norm:0.2642, lr:4.0544e-04 dt: 3332.22ms, tok/sec:157339.04
step 6677, loss: 3.335764, norm:0.2627, lr:4.0539e-04 dt: 3331.89ms, tok/sec:157354.52
step 6678, loss: 3.363789, norm:0.2514, lr:4.0533e-04 dt: 3332.09ms, tok/sec:157345.13
step 6679, loss: 3.382782, norm:0.2708, lr:4.0527e-04 dt: 3332.19ms, tok/sec:157340.22
step 6680, loss: 3.345003, norm:0.2485, lr:4.0522e-04 dt: 3332.18ms, tok/sec:157340.69
step 6681, loss: 3.306626, norm:0.2692, lr:4.0516e-04 dt: 3332.40ms, tok/sec:157330.61
step 6682, loss: 3.325253, norm:0.2515, lr:4.0511e-04 dt: 3331.94ms, tok/sec:157352.37
step 6683, loss: 3.312863, norm:0.2422, lr:4.0505e-04 dt: 3332.05ms, tok/sec:157346.92
step 6684, loss: 3.352717, norm:0.2630, lr:4.0499e-04 dt: 3332.33ms, tok/sec:157333.88
step 6685, loss: 3.375680, norm:0.2443, lr:4.0494e-04 dt: 3332.19ms, tok/sec:157340.58
step 6686, loss: 3.340453, norm:0.2486, lr:4.0488e-04 dt: 3331.93ms, tok/sec:157352.59
step 6687, loss: 3.460239, norm:0.2631, lr:4.0483e-04 dt: 3332.09ms, tok/sec:157345.23
step 6688, loss: 3.419067, norm:0.2635, lr:4.0477e-04 dt: 3332.42ms, tok/sec:157329.59
step 6689, loss: 3.391995, norm:0.2711, lr:4.0471e-04 dt: 3332.27ms, tok/sec:157336.75
step 6690, loss: 3.359991, norm:0.2650, lr:4.0466e-04 dt: 3331.92ms, tok/sec:157352.91
step 6691, loss: 3.369314, norm:0.2671, lr:4.0460e-04 dt: 3332.18ms, tok/sec:157341.04
step 6692, loss: 3.415629, norm:0.2606, lr:4.0455e-04 dt: 3332.01ms, tok/sec:157348.66
step 6693, loss: 3.404757, norm:0.2778, lr:4.0449e-04 dt: 3331.84ms, tok/sec:157357.06
step 6694, loss: 3.367088, norm:0.2758, lr:4.0443e-04 dt: 3332.31ms, tok/sec:157334.83
step 6695, loss: 3.374950, norm:0.2851, lr:4.0438e-04 dt: 3331.92ms, tok/sec:157353.00
step 6696, loss: 3.413528, norm:0.2537, lr:4.0432e-04 dt: 3332.09ms, tok/sec:157344.88
step 6697, loss: 3.443196, norm:0.2864, lr:4.0427e-04 dt: 3332.31ms, tok/sec:157334.67
step 6698, loss: 3.394368, norm:0.2494, lr:4.0421e-04 dt: 3332.31ms, tok/sec:157334.86
step 6699, loss: 3.372080, norm:0.2555, lr:4.0415e-04 dt: 3332.29ms, tok/sec:157335.44
validation loss: 3.3797
Model and optimizer state saved.
HellaSwag accuracy:2325251213057262609/-2=-1162625606528631296.0000
rank 1 sample 0: Hello, I'm a language model, this is the way I want to start my life in another forum. I'm a non-binary language and I'm
rank 1 sample 1: Hello, I'm a language model, a computer scientist and a computer scientist. I'm trying both to communicate with this text I've created for you.

rank 1 sample 2: Hello, I'm a language model, I hate that way. I'm a language model, so for my purposes I'm just going to say, that is
rank 1 sample 3: Hello, I'm a language model, and I'm really excited to see my experience with learning
So many teachers can't talk without using the language as a
rank 0 sample 0: Hello, I'm a language model, and I know that you know a lot - like "who's not an atheist" (John 8:12). And
rank 0 sample 1: Hello, I'm a language model, but one that's really, and also a very great and very effective language, because you know how we learn. You
rank 0 sample 2: Hello, I'm a language model, but I was only used by many, many children, and most children, are the first to learn to speak a foreign
rank 0 sample 3: Hello, I'm a language model, which gives you the ability to learn many things — from simple arithmetic to simple logic skills. There are over 5000 speakers across
step 6700, loss: 3.369393, norm:0.2581, lr:4.0410e-04 dt: 56182.66ms, tok/sec:9331.85
step 6701, loss: 3.450392, norm:0.2648, lr:4.0404e-04 dt: 3332.10ms, tok/sec:157344.44
step 6702, loss: 3.387536, norm:0.2763, lr:4.0398e-04 dt: 3332.50ms, tok/sec:157325.65
step 6703, loss: 3.396747, norm:0.2483, lr:4.0393e-04 dt: 3332.18ms, tok/sec:157340.96
step 6704, loss: 3.354399, norm:0.2420, lr:4.0387e-04 dt: 3332.08ms, tok/sec:157345.70
step 6705, loss: 3.326965, norm:0.2518, lr:4.0382e-04 dt: 3331.89ms, tok/sec:157354.71
step 6706, loss: 3.418530, norm:0.2385, lr:4.0376e-04 dt: 3331.93ms, tok/sec:157352.67
step 6707, loss: 3.390061, norm:0.2611, lr:4.0370e-04 dt: 3332.09ms, tok/sec:157344.88
step 6708, loss: 3.344465, norm:0.2385, lr:4.0365e-04 dt: 3332.11ms, tok/sec:157344.27
step 6709, loss: 3.237849, norm:0.2664, lr:4.0359e-04 dt: 3331.88ms, tok/sec:157355.07
step 6710, loss: 3.363233, norm:0.2769, lr:4.0354e-04 dt: 3332.24ms, tok/sec:157337.82
step 6711, loss: 3.366458, norm:0.2741, lr:4.0348e-04 dt: 3332.36ms, tok/sec:157332.51
step 6712, loss: 3.308771, norm:0.2596, lr:4.0342e-04 dt: 3332.18ms, tok/sec:157340.83
step 6713, loss: 3.325243, norm:0.2430, lr:4.0337e-04 dt: 3332.03ms, tok/sec:157348.10
step 6714, loss: 3.347822, norm:0.2519, lr:4.0331e-04 dt: 3331.88ms, tok/sec:157355.21
step 6715, loss: 3.311749, norm:0.2576, lr:4.0326e-04 dt: 3332.27ms, tok/sec:157336.80
step 6716, loss: 3.340258, norm:0.2430, lr:4.0320e-04 dt: 3332.02ms, tok/sec:157348.31
step 6717, loss: 3.394034, norm:0.2567, lr:4.0314e-04 dt: 3332.07ms, tok/sec:157345.81
step 6718, loss: 3.349308, norm:0.2351, lr:4.0309e-04 dt: 3332.20ms, tok/sec:157339.74
step 6719, loss: 3.377012, norm:0.2448, lr:4.0303e-04 dt: 3332.20ms, tok/sec:157339.68
step 6720, loss: 3.417770, norm:0.2538, lr:4.0297e-04 dt: 3332.53ms, tok/sec:157324.49
step 6721, loss: 3.364690, norm:0.2556, lr:4.0292e-04 dt: 3331.98ms, tok/sec:157350.06
step 6722, loss: 3.366688, norm:0.2553, lr:4.0286e-04 dt: 3331.97ms, tok/sec:157350.63
step 6723, loss: 3.346289, norm:0.2663, lr:4.0281e-04 dt: 3332.36ms, tok/sec:157332.27
step 6724, loss: 3.384142, norm:0.2532, lr:4.0275e-04 dt: 3332.11ms, tok/sec:157344.26
step 6725, loss: 3.357511, norm:0.2550, lr:4.0269e-04 dt: 3331.87ms, tok/sec:157355.52
step 6726, loss: 3.630326, norm:0.3142, lr:4.0264e-04 dt: 3332.06ms, tok/sec:157346.69
step 6727, loss: 3.454816, norm:0.2651, lr:4.0258e-04 dt: 3332.08ms, tok/sec:157345.42
step 6728, loss: 3.395544, norm:0.2696, lr:4.0253e-04 dt: 3331.96ms, tok/sec:157351.40
step 6729, loss: 3.382356, norm:0.2667, lr:4.0247e-04 dt: 3332.30ms, tok/sec:157335.13
step 6730, loss: 3.410237, norm:0.2716, lr:4.0241e-04 dt: 3332.45ms, tok/sec:157328.02
step 6731, loss: 3.434124, norm:0.2975, lr:4.0236e-04 dt: 3332.28ms, tok/sec:157336.02
step 6732, loss: 3.453586, norm:0.3203, lr:4.0230e-04 dt: 3332.00ms, tok/sec:157349.49
step 6733, loss: 3.394935, norm:0.2707, lr:4.0224e-04 dt: 3332.03ms, tok/sec:157347.88
step 6734, loss: 3.387228, norm:0.2417, lr:4.0219e-04 dt: 3332.01ms, tok/sec:157348.89
step 6735, loss: 3.394181, norm:0.2765, lr:4.0213e-04 dt: 3332.27ms, tok/sec:157336.72
step 6736, loss: 3.397676, norm:0.2477, lr:4.0208e-04 dt: 3331.89ms, tok/sec:157354.33
step 6737, loss: 3.370613, norm:0.2564, lr:4.0202e-04 dt: 3331.79ms, tok/sec:157359.09
step 6738, loss: 3.411592, norm:0.2629, lr:4.0196e-04 dt: 3332.02ms, tok/sec:157348.39
step 6739, loss: 3.429864, norm:0.2566, lr:4.0191e-04 dt: 3332.42ms, tok/sec:157329.49
step 6740, loss: 3.452596, norm:0.2723, lr:4.0185e-04 dt: 3332.09ms, tok/sec:157344.91
step 6741, loss: 3.390166, norm:0.2517, lr:4.0179e-04 dt: 3331.81ms, tok/sec:157358.36
step 6742, loss: 3.360112, norm:0.2873, lr:4.0174e-04 dt: 3331.75ms, tok/sec:157361.04
step 6743, loss: 3.383588, norm:0.2420, lr:4.0168e-04 dt: 3332.02ms, tok/sec:157348.26
step 6744, loss: 3.354024, norm:0.2478, lr:4.0163e-04 dt: 3332.05ms, tok/sec:157347.13
step 6745, loss: 3.396553, norm:0.2399, lr:4.0157e-04 dt: 3331.91ms, tok/sec:157353.56
step 6746, loss: 3.320617, norm:0.3097, lr:4.0151e-04 dt: 3332.06ms, tok/sec:157346.50
step 6747, loss: 3.315517, norm:0.2363, lr:4.0146e-04 dt: 3332.31ms, tok/sec:157334.55
step 6748, loss: 3.296696, norm:0.2569, lr:4.0140e-04 dt: 3332.48ms, tok/sec:157326.68
step 6749, loss: 3.378080, norm:0.2597, lr:4.0134e-04 dt: 3332.06ms, tok/sec:157346.56
HellaSwag accuracy:4630979909568889937/-2=-2315489954784444928.0000
rank 1 sample 0: Hello, I'm a language model, an author and a developer, and since we live in a programming language, we're able to do a lot of things
rank 1 sample 1: Hello, I'm a language model, which means I'm a language model that tells me how an object-oriented language is different from an abstract language.

rank 1 sample 2: Hello, I'm a language model, but instead of learning a language, I'm going to teach grammar. So I'm going to talk to the next-
rank 1 sample 3: Hello, I'm a language model, and I'm working with XML and R4.<|endoftext|>NOVA's analysis focuses on determining the effects of the data.
rank 0 sample 0: Hello, I'm a language model, and I know that it's been written much of the time, if not the most accurate. You have to be sure
rank 0 sample 1: Hello, I'm a language model, so there's a lot of fun ahead of us out there, like to see what you see. Here's a look
rank 0 sample 2: Hello, I'm a language model, so I’m very interested in how these characters are written on the screen. I think it’s just
rank 0 sample 3: Hello, I'm a language model, and can be used to create and build software you can use to code a computer user-controlled computer-controlled device as
step 6750, loss: 3.351283, norm:0.2348, lr:4.0129e-04 dt: 48515.62ms, tok/sec:10806.58
step 6751, loss: 3.333072, norm:0.2548, lr:4.0123e-04 dt: 3332.23ms, tok/sec:157338.26
step 6752, loss: 3.338403, norm:0.2610, lr:4.0118e-04 dt: 3331.94ms, tok/sec:157352.21
step 6753, loss: 3.368306, norm:0.2457, lr:4.0112e-04 dt: 3332.11ms, tok/sec:157344.06
step 6754, loss: 3.453769, norm:0.2836, lr:4.0106e-04 dt: 3332.17ms, tok/sec:157341.55
step 6755, loss: 3.476367, norm:0.2719, lr:4.0101e-04 dt: 3332.13ms, tok/sec:157343.38
step 6756, loss: 3.411186, norm:0.2820, lr:4.0095e-04 dt: 3332.04ms, tok/sec:157347.59
step 6757, loss: 3.396676, norm:0.2498, lr:4.0089e-04 dt: 3332.01ms, tok/sec:157348.84
step 6758, loss: 3.357998, norm:0.2790, lr:4.0084e-04 dt: 3332.21ms, tok/sec:157339.54
step 6759, loss: 3.392229, norm:0.2543, lr:4.0078e-04 dt: 3332.30ms, tok/sec:157335.19
step 6760, loss: 3.387985, norm:0.2546, lr:4.0073e-04 dt: 3332.26ms, tok/sec:157337.27
step 6761, loss: 3.386430, norm:0.2625, lr:4.0067e-04 dt: 3332.16ms, tok/sec:157341.73
step 6762, loss: 3.400903, norm:0.2545, lr:4.0061e-04 dt: 3332.10ms, tok/sec:157344.69
step 6763, loss: 3.453478, norm:0.2881, lr:4.0056e-04 dt: 3332.06ms, tok/sec:157346.31
step 6764, loss: 3.396115, norm:0.2735, lr:4.0050e-04 dt: 3332.28ms, tok/sec:157336.11
step 6765, loss: 3.369442, norm:0.2505, lr:4.0044e-04 dt: 3331.89ms, tok/sec:157354.66
step 6766, loss: 3.355536, norm:0.2472, lr:4.0039e-04 dt: 3332.16ms, tok/sec:157341.81
step 6767, loss: 3.448984, norm:0.2759, lr:4.0033e-04 dt: 3332.24ms, tok/sec:157337.96
step 6768, loss: 3.417631, norm:0.2614, lr:4.0028e-04 dt: 3332.15ms, tok/sec:157342.07
step 6769, loss: 3.442024, norm:0.2781, lr:4.0022e-04 dt: 3332.72ms, tok/sec:157315.57
step 6770, loss: 3.374249, norm:0.2483, lr:4.0016e-04 dt: 3332.18ms, tok/sec:157341.05
step 6771, loss: 3.383244, norm:0.2651, lr:4.0011e-04 dt: 3332.09ms, tok/sec:157345.25
step 6772, loss: 3.473198, norm:0.2615, lr:4.0005e-04 dt: 3331.95ms, tok/sec:157351.88
step 6773, loss: 3.353525, norm:0.2587, lr:3.9999e-04 dt: 3331.89ms, tok/sec:157354.38
step 6774, loss: 3.388613, norm:0.2602, lr:3.9994e-04 dt: 3332.13ms, tok/sec:157343.10
step 6775, loss: 3.371439, norm:0.2565, lr:3.9988e-04 dt: 3332.19ms, tok/sec:157340.48
step 6776, loss: 3.351346, norm:0.2793, lr:3.9982e-04 dt: 3331.96ms, tok/sec:157351.06
step 6777, loss: 3.364353, norm:0.2674, lr:3.9977e-04 dt: 3332.15ms, tok/sec:157342.40
step 6778, loss: 3.343991, norm:0.2573, lr:3.9971e-04 dt: 3332.33ms, tok/sec:157333.78
step 6779, loss: 3.340427, norm:0.2473, lr:3.9966e-04 dt: 3332.46ms, tok/sec:157327.39
step 6780, loss: 3.334944, norm:0.2600, lr:3.9960e-04 dt: 3332.10ms, tok/sec:157344.53
step 6781, loss: 3.321586, norm:0.2521, lr:3.9954e-04 dt: 3332.03ms, tok/sec:157348.11
step 6782, loss: 3.365933, norm:0.3730, lr:3.9949e-04 dt: 3332.00ms, tok/sec:157349.23
step 6783, loss: 3.317158, norm:0.3112, lr:3.9943e-04 dt: 3332.09ms, tok/sec:157345.12
step 6784, loss: 3.300785, norm:0.3267, lr:3.9937e-04 dt: 3332.17ms, tok/sec:157341.39
step 6785, loss: 3.333769, norm:0.2774, lr:3.9932e-04 dt: 3332.12ms, tok/sec:157343.90
step 6786, loss: 3.333008, norm:0.2563, lr:3.9926e-04 dt: 3332.16ms, tok/sec:157341.96
step 6787, loss: 3.338832, norm:0.2794, lr:3.9920e-04 dt: 3332.08ms, tok/sec:157345.59
step 6788, loss: 3.332333, norm:0.2752, lr:3.9915e-04 dt: 3332.43ms, tok/sec:157328.82
step 6789, loss: 3.372769, norm:0.3009, lr:3.9909e-04 dt: 3331.97ms, tok/sec:157350.96
step 6790, loss: 3.383800, norm:0.2658, lr:3.9904e-04 dt: 3332.12ms, tok/sec:157343.74
step 6791, loss: 3.361337, norm:0.3001, lr:3.9898e-04 dt: 3331.99ms, tok/sec:157349.98
step 6792, loss: 3.382104, norm:0.3480, lr:3.9892e-04 dt: 3332.08ms, tok/sec:157345.65
step 6793, loss: 3.367387, norm:0.2782, lr:3.9887e-04 dt: 3332.06ms, tok/sec:157346.34
step 6794, loss: 3.350267, norm:0.2615, lr:3.9881e-04 dt: 3332.17ms, tok/sec:157341.40
step 6795, loss: 3.438492, norm:0.2679, lr:3.9875e-04 dt: 3332.09ms, tok/sec:157344.86
step 6796, loss: 3.343087, norm:0.2621, lr:3.9870e-04 dt: 3332.12ms, tok/sec:157343.87
step 6797, loss: 3.383742, norm:0.2564, lr:3.9864e-04 dt: 3332.50ms, tok/sec:157325.57
step 6798, loss: 3.355780, norm:0.2665, lr:3.9858e-04 dt: 3332.13ms, tok/sec:157343.39
step 6799, loss: 3.417804, norm:0.2855, lr:3.9853e-04 dt: 3331.95ms, tok/sec:157351.51
validation loss: 3.3762
Model and optimizer state saved.
HellaSwag accuracy:2325241903715648593/-2=-1162620951857824256.0000
rank 1 sample 0: Hello, I'm a language model, and a developer. I've been wondering what this would be -- what would it be, if a language model were to
rank 1 sample 1: Hello, I'm a language model, I want to keep it simple. If I'm just teaching a language, I'm hoping I will get a lot of
rank 1 sample 2: Hello, I'm a language model, but today, I'm trying to develop a language that will support this type of language. We are using the word '
rank 0 sample 0: Hello, I'm a language model, and I need to know a bunch of what kind of human languages exist -- I'm trying to learn something like that and
rank 1 sample 3: Hello, I'm a language model, and I'm looking at other languages I enjoy. I're doing learning new topics, working on new languages, and I
rank 0 sample 1: Hello, I'm a language model, so how do I know what it looks like in Windows Explorer and if it works correctly. It is quite a bit confusing
rank 0 sample 2: Hello, I'm a language model, but I’m talking about something that happens without a specific definition.
That's what I mean by a single
rank 0 sample 3: Hello, I'm a language model, so a lot of people want a certain level of abstraction. And you'll get interesting points...
How do we have
step 6800, loss: 3.409524, norm:0.2912, lr:3.9847e-04 dt: 56156.49ms, tok/sec:9336.20
step 6801, loss: 3.402217, norm:0.2737, lr:3.9842e-04 dt: 3332.35ms, tok/sec:157333.02
step 6802, loss: 3.445868, norm:0.2619, lr:3.9836e-04 dt: 3332.40ms, tok/sec:157330.41
step 6803, loss: 3.369064, norm:0.2706, lr:3.9830e-04 dt: 3331.92ms, tok/sec:157353.25
step 6804, loss: 3.360382, norm:0.2698, lr:3.9825e-04 dt: 3332.31ms, tok/sec:157334.58
step 6805, loss: 3.420613, norm:0.2588, lr:3.9819e-04 dt: 3332.20ms, tok/sec:157339.74
step 6806, loss: 3.463156, norm:0.2725, lr:3.9813e-04 dt: 3332.05ms, tok/sec:157347.06
step 6807, loss: 3.369615, norm:0.2565, lr:3.9808e-04 dt: 3331.96ms, tok/sec:157350.99
step 6808, loss: 3.398298, norm:0.2671, lr:3.9802e-04 dt: 3332.09ms, tok/sec:157345.05
step 6809, loss: 3.431962, norm:0.2599, lr:3.9796e-04 dt: 3332.18ms, tok/sec:157340.77
step 6810, loss: 3.427269, norm:0.2647, lr:3.9791e-04 dt: 3332.14ms, tok/sec:157342.88
step 6811, loss: 3.352514, norm:0.2533, lr:3.9785e-04 dt: 3332.30ms, tok/sec:157335.12
step 6812, loss: 3.338028, norm:0.2480, lr:3.9779e-04 dt: 3331.95ms, tok/sec:157351.49
step 6813, loss: 3.343565, norm:0.2621, lr:3.9774e-04 dt: 3331.91ms, tok/sec:157353.40
step 6814, loss: 3.283196, norm:0.2491, lr:3.9768e-04 dt: 3331.88ms, tok/sec:157354.96
step 6815, loss: 3.383121, norm:0.2480, lr:3.9763e-04 dt: 3331.84ms, tok/sec:157357.02
step 6816, loss: 3.351504, norm:0.2443, lr:3.9757e-04 dt: 3332.20ms, tok/sec:157340.10
step 6817, loss: 3.341436, norm:0.2577, lr:3.9751e-04 dt: 3332.19ms, tok/sec:157340.15
step 6818, loss: 3.339245, norm:0.2552, lr:3.9746e-04 dt: 3332.04ms, tok/sec:157347.44
step 6819, loss: 3.256101, norm:0.2621, lr:3.9740e-04 dt: 3332.08ms, tok/sec:157345.41
step 6820, loss: 3.337616, norm:0.2653, lr:3.9734e-04 dt: 3332.26ms, tok/sec:157337.18
step 6821, loss: 3.350430, norm:0.2534, lr:3.9729e-04 dt: 3332.54ms, tok/sec:157324.00
step 6822, loss: 3.344755, norm:0.2610, lr:3.9723e-04 dt: 3332.03ms, tok/sec:157347.71
step 6823, loss: 3.352054, norm:0.2795, lr:3.9717e-04 dt: 3331.92ms, tok/sec:157353.14
step 6824, loss: 3.386536, norm:0.2782, lr:3.9712e-04 dt: 3331.95ms, tok/sec:157351.89
step 6825, loss: 3.310848, norm:0.2739, lr:3.9706e-04 dt: 3332.10ms, tok/sec:157344.63
step 6826, loss: 3.392140, norm:0.2764, lr:3.9700e-04 dt: 3332.11ms, tok/sec:157343.91
step 6827, loss: 3.356440, norm:0.2485, lr:3.9695e-04 dt: 3333.15ms, tok/sec:157294.92
step 6828, loss: 3.340046, norm:0.2590, lr:3.9689e-04 dt: 3332.98ms, tok/sec:157303.11
step 6829, loss: 3.363305, norm:0.2433, lr:3.9683e-04 dt: 3332.38ms, tok/sec:157331.62
step 6830, loss: 3.352795, norm:0.2673, lr:3.9678e-04 dt: 3332.50ms, tok/sec:157325.53
step 6831, loss: 3.414865, norm:0.2556, lr:3.9672e-04 dt: 3332.10ms, tok/sec:157344.54
step 6832, loss: 3.364377, norm:0.2447, lr:3.9666e-04 dt: 3332.21ms, tok/sec:157339.56
step 6833, loss: 3.365335, norm:0.2633, lr:3.9661e-04 dt: 3332.34ms, tok/sec:157333.36
step 6834, loss: 3.385745, norm:0.2426, lr:3.9655e-04 dt: 3331.95ms, tok/sec:157351.93
step 6835, loss: 3.445331, norm:0.2393, lr:3.9650e-04 dt: 3332.13ms, tok/sec:157342.98
step 6836, loss: 3.416715, norm:0.2652, lr:3.9644e-04 dt: 3332.02ms, tok/sec:157348.33
step 6837, loss: 3.394629, norm:0.2531, lr:3.9638e-04 dt: 3332.34ms, tok/sec:157333.35
step 6838, loss: 3.381159, norm:0.2442, lr:3.9633e-04 dt: 3332.07ms, tok/sec:157345.80
step 6839, loss: 3.403912, norm:0.2622, lr:3.9627e-04 dt: 3332.45ms, tok/sec:157327.99
step 6840, loss: 3.401072, norm:0.2352, lr:3.9621e-04 dt: 3332.38ms, tok/sec:157331.25
step 6841, loss: 3.401044, norm:0.2623, lr:3.9616e-04 dt: 3332.34ms, tok/sec:157333.44
step 6842, loss: 3.405781, norm:0.2256, lr:3.9610e-04 dt: 3332.00ms, tok/sec:157349.34
step 6843, loss: 3.388790, norm:0.2357, lr:3.9604e-04 dt: 3332.13ms, tok/sec:157343.19
step 6844, loss: 3.379926, norm:0.2393, lr:3.9599e-04 dt: 3332.27ms, tok/sec:157336.70
step 6845, loss: 3.382700, norm:0.2404, lr:3.9593e-04 dt: 3332.49ms, tok/sec:157326.21
step 6846, loss: 3.350380, norm:0.2433, lr:3.9587e-04 dt: 3332.42ms, tok/sec:157329.62
step 6847, loss: 3.386586, norm:0.2364, lr:3.9582e-04 dt: 3332.25ms, tok/sec:157337.37
step 6848, loss: 3.309981, norm:0.2365, lr:3.9576e-04 dt: 3332.59ms, tok/sec:157321.25
step 6849, loss: 3.360552, norm:0.2545, lr:3.9570e-04 dt: 3332.08ms, tok/sec:157345.62
HellaSwag accuracy:-2286593132897041327/-2=1143296566448520704.0000
rank 1 sample 0: Hello, I'm a language model, and a grammar teacher.
We did this one of the sessions, and all of our lessons had a different theme.
rank 1 sample 1: Hello, I'm a language model, a model, a model, a model.
You would be correct to say, with a minimum, that the model
rank 1 sample 2: Hello, I'm a language model, so even the most experienced language modeler can tell you that when a word is used in an action, the verb was
rank 1 sample 3: Hello, I'm a language model, and I'm talking to students. If at any time on this thread, are you familiar with some of the tools that
rank 0 sample 0: Hello, I'm a language model, and I think it is useful in allocating knowledge about anything that will enable people to use whatever tools. But I am
rank 0 sample 1: Hello, I'm a language model, so there's a lot of work if you're making more connections and creating more complex ones. I love to look at
rank 0 sample 2: Hello, I'm a language model, I'm programming, I'm debugging. I'd like to have fun doing this.
I'm a language designer.
rank 0 sample 3: Hello, I'm a language model, so a lot of people think of 'I hate that' as a racist, rather than a racist in a nonling
step 6850, loss: 3.386655, norm:0.2532, lr:3.9565e-04 dt: 48519.21ms, tok/sec:10805.78
step 6851, loss: 3.364199, norm:0.2638, lr:3.9559e-04 dt: 3332.33ms, tok/sec:157333.97
step 6852, loss: 3.458264, norm:0.3360, lr:3.9553e-04 dt: 3332.11ms, tok/sec:157344.23
step 6853, loss: 3.348618, norm:0.2774, lr:3.9548e-04 dt: 3332.14ms, tok/sec:157342.75
step 6854, loss: 3.320421, norm:0.2877, lr:3.9542e-04 dt: 3332.02ms, tok/sec:157348.61
step 6855, loss: 3.305279, norm:0.2574, lr:3.9536e-04 dt: 3332.04ms, tok/sec:157347.66
step 6856, loss: 3.357371, norm:0.2573, lr:3.9531e-04 dt: 3332.14ms, tok/sec:157342.55
step 6857, loss: 3.357251, norm:0.3007, lr:3.9525e-04 dt: 3334.07ms, tok/sec:157251.86
step 6858, loss: 3.316041, norm:0.2442, lr:3.9519e-04 dt: 3332.32ms, tok/sec:157334.01
step 6859, loss: 3.370242, norm:0.2943, lr:3.9514e-04 dt: 3332.23ms, tok/sec:157338.34
step 6860, loss: 3.411314, norm:0.2742, lr:3.9508e-04 dt: 3332.54ms, tok/sec:157324.04
step 6861, loss: 3.371978, norm:0.2781, lr:3.9502e-04 dt: 3332.07ms, tok/sec:157346.10
step 6862, loss: 3.377275, norm:0.2514, lr:3.9497e-04 dt: 3331.91ms, tok/sec:157353.40
step 6863, loss: 3.343504, norm:0.2592, lr:3.9491e-04 dt: 3332.14ms, tok/sec:157342.71
step 6864, loss: 3.379605, norm:0.2457, lr:3.9485e-04 dt: 3332.19ms, tok/sec:157340.47
step 6865, loss: 3.345686, norm:0.2556, lr:3.9480e-04 dt: 3332.07ms, tok/sec:157345.91
step 6866, loss: 3.333272, norm:0.2522, lr:3.9474e-04 dt: 3332.13ms, tok/sec:157343.17
step 6867, loss: 3.411135, norm:0.2518, lr:3.9468e-04 dt: 3332.18ms, tok/sec:157340.92
step 6868, loss: 3.347191, norm:0.2739, lr:3.9463e-04 dt: 3332.19ms, tok/sec:157340.58
step 6869, loss: 3.406732, norm:0.2544, lr:3.9457e-04 dt: 3332.03ms, tok/sec:157347.92
step 6870, loss: 3.328215, norm:0.2531, lr:3.9452e-04 dt: 3332.24ms, tok/sec:157338.02
step 6871, loss: 3.351014, norm:0.2373, lr:3.9446e-04 dt: 3332.18ms, tok/sec:157340.77
step 6872, loss: 3.369395, norm:0.2353, lr:3.9440e-04 dt: 3332.47ms, tok/sec:157327.16
step 6873, loss: 3.310382, norm:0.2471, lr:3.9435e-04 dt: 3332.17ms, tok/sec:157341.14
step 6874, loss: 3.333101, norm:0.2470, lr:3.9429e-04 dt: 3332.22ms, tok/sec:157338.78
step 6875, loss: 3.411309, norm:0.2485, lr:3.9423e-04 dt: 3332.32ms, tok/sec:157334.17
step 6876, loss: 3.406210, norm:0.2819, lr:3.9418e-04 dt: 3332.29ms, tok/sec:157335.68
step 6877, loss: 3.315279, norm:0.2460, lr:3.9412e-04 dt: 3332.02ms, tok/sec:157348.30
step 6878, loss: 3.367574, norm:0.2997, lr:3.9406e-04 dt: 3331.92ms, tok/sec:157353.21
step 6879, loss: 3.382363, norm:0.2708, lr:3.9401e-04 dt: 3332.22ms, tok/sec:157338.79
step 6880, loss: 3.337193, norm:0.2514, lr:3.9395e-04 dt: 3332.33ms, tok/sec:157333.71
step 6881, loss: 3.333696, norm:0.2593, lr:3.9389e-04 dt: 3332.42ms, tok/sec:157329.53
step 6882, loss: 3.313830, norm:0.2354, lr:3.9384e-04 dt: 3331.95ms, tok/sec:157351.79
step 6883, loss: 3.311575, norm:0.2587, lr:3.9378e-04 dt: 3332.00ms, tok/sec:157349.45
step 6884, loss: 3.317702, norm:0.2551, lr:3.9372e-04 dt: 3332.08ms, tok/sec:157345.59
step 6885, loss: 3.307289, norm:0.2540, lr:3.9367e-04 dt: 3332.01ms, tok/sec:157349.00
step 6886, loss: 3.316405, norm:0.2489, lr:3.9361e-04 dt: 3331.89ms, tok/sec:157354.66
step 6887, loss: 3.313329, norm:0.2416, lr:3.9355e-04 dt: 3332.18ms, tok/sec:157340.86
step 6888, loss: 3.325428, norm:0.2569, lr:3.9350e-04 dt: 3332.30ms, tok/sec:157335.26
step 6889, loss: 3.357922, norm:0.2445, lr:3.9344e-04 dt: 3332.23ms, tok/sec:157338.61
step 6890, loss: 3.308450, norm:0.2608, lr:3.9338e-04 dt: 3332.19ms, tok/sec:157340.30
step 6891, loss: 3.300446, norm:0.2512, lr:3.9333e-04 dt: 3332.06ms, tok/sec:157346.72
step 6892, loss: 3.359460, norm:0.2614, lr:3.9327e-04 dt: 3332.13ms, tok/sec:157343.34
step 6893, loss: 3.385318, norm:0.2704, lr:3.9321e-04 dt: 3331.90ms, tok/sec:157354.11
step 6894, loss: 3.342508, norm:0.2699, lr:3.9316e-04 dt: 3332.05ms, tok/sec:157346.84
step 6895, loss: 3.378279, norm:0.2861, lr:3.9310e-04 dt: 3332.12ms, tok/sec:157343.57
step 6896, loss: 3.394273, norm:0.3251, lr:3.9304e-04 dt: 3332.05ms, tok/sec:157347.11
step 6897, loss: 3.365779, norm:0.2768, lr:3.9298e-04 dt: 3332.30ms, tok/sec:157335.00
step 6898, loss: 3.362871, norm:0.2997, lr:3.9293e-04 dt: 3332.27ms, tok/sec:157336.63
step 6899, loss: 3.373450, norm:0.2630, lr:3.9287e-04 dt: 3332.09ms, tok/sec:157344.90
validation loss: 3.3730
Model and optimizer state saved.
HellaSwag accuracy:2325251215205008465/-2=-1162625607602504192.0000
rank 1 sample 0: Hello, I'm a language model, and my teacher is a linguist looking to put the English of the English language into practice. It's the same way
rank 1 sample 1: Hello, I'm a language model, a model that does not have anything to do with it because it depends on it's native language as opposed to the language
rank 1 sample 2: Hello, I'm a language model, so be honest. I'm a language model, so you are able to describe the language, as you know what you
rank 1 sample 3: Hello, I'm a language model, and I'm working with several languages at our school. First, I will read the 'O' language model (which
rank 0 sample 0: Hello, I'm a language model, and I think it is. So here's a bit of what happened - but I am more. So that's how
rank 0 sample 1: Hello, I'm a language model, so to read.
How do you know that one and the same sounds and words have their meanings?
Dictionary
rank 0 sample 2: Hello, I'm a language model, so I won't look.
When I think of a model for an object, it's a different kind of object
rank 0 sample 3: Hello, I'm a language model, and can be used to model the features that may be available in real-life virtual environments. I used to use both
step 6900, loss: 3.354586, norm:0.3501, lr:3.9281e-04 dt: 56194.31ms, tok/sec:9329.91
step 6901, loss: 3.369185, norm:0.2701, lr:3.9276e-04 dt: 3332.55ms, tok/sec:157323.21
step 6902, loss: 3.437284, norm:0.3141, lr:3.9270e-04 dt: 3331.83ms, tok/sec:157357.60
step 6903, loss: 3.341274, norm:0.2492, lr:3.9264e-04 dt: 3332.20ms, tok/sec:157340.07
step 6904, loss: 3.349253, norm:0.2825, lr:3.9259e-04 dt: 3331.90ms, tok/sec:157354.23
step 6905, loss: 3.374461, norm:0.2709, lr:3.9253e-04 dt: 3332.17ms, tok/sec:157341.12
step 6906, loss: 3.434836, norm:0.2629, lr:3.9247e-04 dt: 3332.22ms, tok/sec:157338.77
step 6907, loss: 3.364191, norm:0.2451, lr:3.9242e-04 dt: 3332.05ms, tok/sec:157347.00
step 6908, loss: 3.342946, norm:0.2422, lr:3.9236e-04 dt: 3332.66ms, tok/sec:157317.96
step 6909, loss: 3.406644, norm:0.2474, lr:3.9230e-04 dt: 3332.43ms, tok/sec:157329.17
step 6910, loss: 3.457603, norm:0.2704, lr:3.9225e-04 dt: 3332.01ms, tok/sec:157348.99
step 6911, loss: 3.394099, norm:0.2343, lr:3.9219e-04 dt: 3332.11ms, tok/sec:157344.34
step 6912, loss: 3.355039, norm:0.2600, lr:3.9213e-04 dt: 3332.07ms, tok/sec:157345.84
step 6913, loss: 3.447916, norm:0.2575, lr:3.9208e-04 dt: 3332.06ms, tok/sec:157346.48
step 6914, loss: 3.362360, norm:0.2752, lr:3.9202e-04 dt: 3332.06ms, tok/sec:157346.41
step 6915, loss: 3.450907, norm:0.2714, lr:3.9196e-04 dt: 3332.11ms, tok/sec:157344.26
step 6916, loss: 3.288518, norm:0.2527, lr:3.9191e-04 dt: 3332.05ms, tok/sec:157347.17
step 6917, loss: 3.390674, norm:0.3114, lr:3.9185e-04 dt: 3332.29ms, tok/sec:157335.71
step 6918, loss: 3.322423, norm:0.2563, lr:3.9179e-04 dt: 3332.29ms, tok/sec:157335.53
step 6919, loss: 3.341292, norm:0.2512, lr:3.9174e-04 dt: 3332.09ms, tok/sec:157344.89
step 6920, loss: 3.325981, norm:0.2381, lr:3.9168e-04 dt: 3331.95ms, tok/sec:157351.93
step 6921, loss: 3.378987, norm:0.2566, lr:3.9162e-04 dt: 3332.10ms, tok/sec:157344.82
step 6922, loss: 3.325839, norm:0.2664, lr:3.9157e-04 dt: 3332.02ms, tok/sec:157348.51
step 6923, loss: 3.329202, norm:0.2519, lr:3.9151e-04 dt: 3332.10ms, tok/sec:157344.69
step 6924, loss: 3.341845, norm:0.2579, lr:3.9145e-04 dt: 3331.78ms, tok/sec:157359.68
step 6925, loss: 3.319933, norm:0.2527, lr:3.9140e-04 dt: 3332.07ms, tok/sec:157346.04
step 6926, loss: 3.358143, norm:0.2587, lr:3.9134e-04 dt: 3332.09ms, tok/sec:157344.90
step 6927, loss: 3.432930, norm:0.2732, lr:3.9128e-04 dt: 3332.41ms, tok/sec:157329.77
step 6928, loss: 3.347593, norm:0.2469, lr:3.9123e-04 dt: 3332.06ms, tok/sec:157346.57
step 6929, loss: 3.379659, norm:0.2659, lr:3.9117e-04 dt: 3332.22ms, tok/sec:157338.87
step 6930, loss: 3.376791, norm:0.2307, lr:3.9111e-04 dt: 3332.18ms, tok/sec:157340.74
step 6931, loss: 3.351738, norm:0.2582, lr:3.9105e-04 dt: 3332.17ms, tok/sec:157341.17
step 6932, loss: 3.342088, norm:0.2563, lr:3.9100e-04 dt: 3332.01ms, tok/sec:157348.89
step 6933, loss: 3.355926, norm:0.2586, lr:3.9094e-04 dt: 3332.22ms, tok/sec:157338.77
step 6934, loss: 3.354341, norm:0.2415, lr:3.9088e-04 dt: 3332.54ms, tok/sec:157323.88
step 6935, loss: 3.365236, norm:0.2490, lr:3.9083e-04 dt: 3332.12ms, tok/sec:157343.61
step 6936, loss: 3.342014, norm:0.2511, lr:3.9077e-04 dt: 3331.97ms, tok/sec:157350.69
step 6937, loss: 3.345882, norm:0.2365, lr:3.9071e-04 dt: 3332.27ms, tok/sec:157336.56
step 6938, loss: 3.448457, norm:0.2575, lr:3.9066e-04 dt: 3331.88ms, tok/sec:157355.06
step 6939, loss: 3.411664, norm:0.2501, lr:3.9060e-04 dt: 3332.15ms, tok/sec:157342.13
step 6940, loss: 3.351692, norm:0.2769, lr:3.9054e-04 dt: 3331.96ms, tok/sec:157351.34
step 6941, loss: 3.336321, norm:0.2703, lr:3.9049e-04 dt: 3331.88ms, tok/sec:157355.07
step 6942, loss: 3.334241, norm:0.2698, lr:3.9043e-04 dt: 3332.22ms, tok/sec:157339.16
step 6943, loss: 3.343768, norm:0.2532, lr:3.9037e-04 dt: 3332.37ms, tok/sec:157331.69
step 6944, loss: 3.379813, norm:0.2604, lr:3.9032e-04 dt: 3331.86ms, tok/sec:157355.89
step 6945, loss: 3.367734, norm:0.2779, lr:3.9026e-04 dt: 3332.01ms, tok/sec:157348.97
step 6946, loss: 3.361933, norm:0.2596, lr:3.9020e-04 dt: 3332.00ms, tok/sec:157349.33
step 6947, loss: 3.317488, norm:0.2937, lr:3.9015e-04 dt: 3332.05ms, tok/sec:157347.05
step 6948, loss: 3.358997, norm:0.2557, lr:3.9009e-04 dt: 3331.97ms, tok/sec:157350.83
step 6949, loss: 3.332540, norm:0.2641, lr:3.9003e-04 dt: 3332.24ms, tok/sec:157337.84
HellaSwag accuracy:2316244015950562385/-2=-1158122007975281152.0000
rank 1 sample 0: Hello, I'm a language model, and I'm writing code. I got to read this paper together, and just wanted to see why I wanted to write
rank 1 sample 1: Hello, I'm a language model, a computer programmer, and a developer. I'm interested into how your model should look. I do my best to help
rank 1 sample 2: Hello, I'm a language model, but at what point do I have to go to the local website to see if I can't go. I want my
rank 1 sample 3: Hello, I'm a language model, and I'm interested to show you how something works. No matter what. To do such a thing, I have to
rank 0 sample 0: Hello, I'm a language model, and I'd like you to understand. Just by looking at the English definition for "the child of God," you get
rank 0 sample 1: Hello, I'm a language model, so here's a good example to explore.<|endoftext|>For every cell on earth, the Sun is a single cell. In
rank 0 sample 2: Hello, I'm a language model, so I do my best to come out of school.
In fact, I do have a lot of fun when it
rank 0 sample 3: Hello, I'm a language model, not a lot of people talk to me, whether it's a foreign language like American or Polish, etc. But is
step 6950, loss: 3.310990, norm:0.2537, lr:3.8998e-04 dt: 48520.12ms, tok/sec:10805.58
step 6951, loss: 3.236463, norm:0.2994, lr:3.8992e-04 dt: 3332.21ms, tok/sec:157339.41
step 6952, loss: 3.307802, norm:0.2394, lr:3.8986e-04 dt: 3332.19ms, tok/sec:157340.17
step 6953, loss: 3.368223, norm:0.2547, lr:3.8980e-04 dt: 3332.40ms, tok/sec:157330.36
step 6954, loss: 3.289124, norm:0.2610, lr:3.8975e-04 dt: 3332.15ms, tok/sec:157342.25
step 6955, loss: 3.335488, norm:0.2391, lr:3.8969e-04 dt: 3331.98ms, tok/sec:157350.31
step 6956, loss: 3.289180, norm:0.2368, lr:3.8963e-04 dt: 3331.97ms, tok/sec:157350.95
step 6957, loss: 3.320459, norm:0.2650, lr:3.8958e-04 dt: 3331.93ms, tok/sec:157352.53
step 6958, loss: 3.343872, norm:0.2467, lr:3.8952e-04 dt: 3332.33ms, tok/sec:157333.52
step 6959, loss: 3.316357, norm:0.2554, lr:3.8946e-04 dt: 3332.01ms, tok/sec:157348.66
step 6960, loss: 3.351239, norm:0.2589, lr:3.8941e-04 dt: 3332.62ms, tok/sec:157320.26
step 6961, loss: 3.389012, norm:0.2517, lr:3.8935e-04 dt: 3332.13ms, tok/sec:157343.01
step 6962, loss: 3.398132, norm:0.2581, lr:3.8929e-04 dt: 3332.23ms, tok/sec:157338.50
step 6963, loss: 3.409502, norm:0.2457, lr:3.8924e-04 dt: 3332.12ms, tok/sec:157343.44
step 6964, loss: 3.438793, norm:0.2726, lr:3.8918e-04 dt: 3332.13ms, tok/sec:157343.33
step 6965, loss: 3.403352, norm:0.2639, lr:3.8912e-04 dt: 3332.07ms, tok/sec:157345.96
step 6966, loss: 3.339121, norm:0.2547, lr:3.8906e-04 dt: 3332.15ms, tok/sec:157342.32
step 6967, loss: 3.441393, norm:0.3064, lr:3.8901e-04 dt: 3332.45ms, tok/sec:157328.24
step 6968, loss: 3.396533, norm:0.2702, lr:3.8895e-04 dt: 3332.00ms, tok/sec:157349.47
step 6969, loss: 3.352225, norm:0.2919, lr:3.8889e-04 dt: 3332.06ms, tok/sec:157346.67
step 6970, loss: 3.371932, norm:0.2621, lr:3.8884e-04 dt: 3332.15ms, tok/sec:157342.03
step 6971, loss: 3.354255, norm:0.2705, lr:3.8878e-04 dt: 3331.97ms, tok/sec:157350.88
step 6972, loss: 3.401496, norm:0.2810, lr:3.8872e-04 dt: 3331.97ms, tok/sec:157350.75
step 6973, loss: 3.415999, norm:0.2878, lr:3.8867e-04 dt: 3332.02ms, tok/sec:157348.30
step 6974, loss: 3.355311, norm:0.3006, lr:3.8861e-04 dt: 3332.08ms, tok/sec:157345.50
step 6975, loss: 3.390281, norm:0.2612, lr:3.8855e-04 dt: 3332.10ms, tok/sec:157344.42
step 6976, loss: 3.386255, norm:0.2996, lr:3.8850e-04 dt: 3332.25ms, tok/sec:157337.39
step 6977, loss: 3.393912, norm:0.2510, lr:3.8844e-04 dt: 3332.07ms, tok/sec:157346.15
step 6978, loss: 3.336388, norm:0.2626, lr:3.8838e-04 dt: 3332.23ms, tok/sec:157338.44
step 6979, loss: 3.346489, norm:0.2645, lr:3.8832e-04 dt: 3332.10ms, tok/sec:157344.48
step 6980, loss: 3.320163, norm:0.2743, lr:3.8827e-04 dt: 3332.19ms, tok/sec:157340.20
step 6981, loss: 3.351929, norm:0.2512, lr:3.8821e-04 dt: 3332.02ms, tok/sec:157348.39
step 6982, loss: 3.369444, norm:0.2458, lr:3.8815e-04 dt: 3332.42ms, tok/sec:157329.38
step 6983, loss: 3.350214, norm:0.2545, lr:3.8810e-04 dt: 3332.30ms, tok/sec:157335.19
step 6984, loss: 3.368883, norm:0.2580, lr:3.8804e-04 dt: 3331.87ms, tok/sec:157355.42
step 6985, loss: 3.365590, norm:0.2543, lr:3.8798e-04 dt: 3332.07ms, tok/sec:157346.21
step 6986, loss: 3.339983, norm:0.2785, lr:3.8793e-04 dt: 3332.32ms, tok/sec:157334.42
step 6987, loss: 3.275198, norm:0.2550, lr:3.8787e-04 dt: 3332.03ms, tok/sec:157348.16
step 6988, loss: 3.364931, norm:0.2544, lr:3.8781e-04 dt: 3332.09ms, tok/sec:157345.24
step 6989, loss: 3.325450, norm:0.2446, lr:3.8776e-04 dt: 3332.21ms, tok/sec:157339.65
step 6990, loss: 3.378587, norm:0.2457, lr:3.8770e-04 dt: 3331.96ms, tok/sec:157351.21
step 6991, loss: 3.367919, norm:0.2563, lr:3.8764e-04 dt: 3332.02ms, tok/sec:157348.56
step 6992, loss: 3.309250, norm:0.2623, lr:3.8758e-04 dt: 3332.50ms, tok/sec:157325.52
step 6993, loss: 3.397675, norm:0.2654, lr:3.8753e-04 dt: 3332.06ms, tok/sec:157346.61
step 6994, loss: 3.309724, norm:0.2514, lr:3.8747e-04 dt: 3332.31ms, tok/sec:157334.82
step 6995, loss: 3.364043, norm:0.2746, lr:3.8741e-04 dt: 3331.93ms, tok/sec:157352.75
step 6996, loss: 3.410757, norm:0.2695, lr:3.8736e-04 dt: 3332.05ms, tok/sec:157347.01
step 6997, loss: 3.374959, norm:0.2776, lr:3.8730e-04 dt: 3332.23ms, tok/sec:157338.70
step 6998, loss: 3.391905, norm:0.2466, lr:3.8724e-04 dt: 3332.24ms, tok/sec:157338.01
step 6999, loss: 3.345649, norm:0.3107, lr:3.8719e-04 dt: 3332.27ms, tok/sec:157336.36
validation loss: 3.3688
Model and optimizer state saved.
HellaSwag accuracy:2316226458123699281/-2=-1158113229061849600.0000
rank 1 sample 0: Hello, I'm a language model, and I'm going to be going it's OK if I run it's gonna be working with something else.
I
rank 1 sample 1: Hello, I'm a language model, a programmer. To get started, here's a tutorial...
|Look up word and the language model|
|
rank 1 sample 2: Hello, I'm a language model, but some things you might not know about it:
So why are there so many languages? There are many. This
rank 1 sample 3: Hello, I'm a language model, and I'm writing code into the world first.
[wtf ]u [] (no, I'm not
rank 0 sample 0: Hello, I'm a language model, so I need to be in that environment every time I walk around an article in a new app to a particular language model
rank 0 sample 1: Hello, I'm a language model, and there's a lot of people (not just speakers) that use what you call the computer. When you're talking
rank 0 sample 2: Hello, I'm a language model, and I see it pretty closely. You can write down the sentence you want to use to make sure you're writing it
rank 0 sample 3: Hello, I'm a language model, and can't tell you what you mean, please.
2. A good exercise in learning the names of colors;
step 7000, loss: 3.385802, norm:0.2886, lr:3.8713e-04 dt: 56242.38ms, tok/sec:9321.94
step 7001, loss: 3.364524, norm:0.2745, lr:3.8707e-04 dt: 3332.04ms, tok/sec:157347.31
step 7002, loss: 3.364910, norm:0.2569, lr:3.8701e-04 dt: 3332.25ms, tok/sec:157337.44
step 7003, loss: 3.368528, norm:0.2766, lr:3.8696e-04 dt: 3333.06ms, tok/sec:157299.48
step 7004, loss: 3.369240, norm:0.2597, lr:3.8690e-04 dt: 3332.20ms, tok/sec:157339.81
step 7005, loss: 3.357094, norm:0.2503, lr:3.8684e-04 dt: 3331.95ms, tok/sec:157351.92
step 7006, loss: 3.321304, norm:0.2533, lr:3.8679e-04 dt: 3331.98ms, tok/sec:157350.35
step 7007, loss: 3.345506, norm:0.2668, lr:3.8673e-04 dt: 3332.02ms, tok/sec:157348.24
step 7008, loss: 3.400747, norm:0.2731, lr:3.8667e-04 dt: 3332.23ms, tok/sec:157338.59
step 7009, loss: 3.354205, norm:0.2524, lr:3.8662e-04 dt: 3331.99ms, tok/sec:157349.72
step 7010, loss: 3.344700, norm:0.2635, lr:3.8656e-04 dt: 3332.13ms, tok/sec:157343.11
step 7011, loss: 3.421233, norm:0.2647, lr:3.8650e-04 dt: 3332.18ms, tok/sec:157340.91
step 7012, loss: 3.338517, norm:0.2674, lr:3.8644e-04 dt: 3332.26ms, tok/sec:157337.00
step 7013, loss: 3.352294, norm:0.3231, lr:3.8639e-04 dt: 3332.25ms, tok/sec:157337.35
step 7014, loss: 3.335341, norm:0.2632, lr:3.8633e-04 dt: 3331.81ms, tok/sec:157358.14
step 7015, loss: 3.314111, norm:0.2462, lr:3.8627e-04 dt: 3331.98ms, tok/sec:157350.08
step 7016, loss: 3.345170, norm:0.2531, lr:3.8622e-04 dt: 3332.03ms, tok/sec:157347.95
step 7017, loss: 3.406094, norm:0.2688, lr:3.8616e-04 dt: 3332.29ms, tok/sec:157335.73
step 7018, loss: 3.419733, norm:0.2390, lr:3.8610e-04 dt: 3331.77ms, tok/sec:157360.22
step 7019, loss: 3.315608, norm:0.2664, lr:3.8604e-04 dt: 3331.99ms, tok/sec:157349.74
step 7020, loss: 3.324154, norm:0.2626, lr:3.8599e-04 dt: 3332.09ms, tok/sec:157345.23
step 7021, loss: 3.256723, norm:0.3184, lr:3.8593e-04 dt: 3332.39ms, tok/sec:157330.76
step 7022, loss: 3.352814, norm:0.2553, lr:3.8587e-04 dt: 3332.01ms, tok/sec:157349.08
step 7023, loss: 3.373742, norm:0.2500, lr:3.8582e-04 dt: 3332.07ms, tok/sec:157345.88
step 7024, loss: 3.337662, norm:0.2480, lr:3.8576e-04 dt: 3332.17ms, tok/sec:157341.26
step 7025, loss: 3.356077, norm:0.2457, lr:3.8570e-04 dt: 3332.32ms, tok/sec:157334.05
step 7026, loss: 3.372618, norm:0.2342, lr:3.8565e-04 dt: 3331.91ms, tok/sec:157353.46
step 7027, loss: 3.340438, norm:0.2450, lr:3.8559e-04 dt: 3332.20ms, tok/sec:157339.77
step 7028, loss: 3.331068, norm:0.2362, lr:3.8553e-04 dt: 3332.28ms, tok/sec:157336.02
step 7029, loss: 3.330673, norm:0.2441, lr:3.8547e-04 dt: 3332.33ms, tok/sec:157333.84
step 7030, loss: 3.323759, norm:0.2535, lr:3.8542e-04 dt: 3332.03ms, tok/sec:157348.06
step 7031, loss: 3.364814, norm:0.2668, lr:3.8536e-04 dt: 3332.02ms, tok/sec:157348.26
step 7032, loss: 3.333509, norm:0.2412, lr:3.8530e-04 dt: 3332.12ms, tok/sec:157343.82
step 7033, loss: 3.386927, norm:0.2738, lr:3.8525e-04 dt: 3332.10ms, tok/sec:157344.60
step 7034, loss: 3.398029, norm:0.2719, lr:3.8519e-04 dt: 3332.04ms, tok/sec:157347.62
step 7035, loss: 3.383955, norm:0.2458, lr:3.8513e-04 dt: 3332.14ms, tok/sec:157342.91
step 7036, loss: 3.376738, norm:0.2816, lr:3.8507e-04 dt: 3332.42ms, tok/sec:157329.30
step 7037, loss: 3.386512, norm:0.2567, lr:3.8502e-04 dt: 3332.20ms, tok/sec:157339.81
step 7038, loss: 3.345668, norm:0.2857, lr:3.8496e-04 dt: 3332.03ms, tok/sec:157347.91
step 7039, loss: 3.373084, norm:0.2729, lr:3.8490e-04 dt: 3332.08ms, tok/sec:157345.43
step 7040, loss: 3.351944, norm:0.2654, lr:3.8485e-04 dt: 3332.95ms, tok/sec:157304.28
step 7041, loss: 3.428513, norm:0.3070, lr:3.8479e-04 dt: 3333.10ms, tok/sec:157297.56
step 7042, loss: 3.332786, norm:0.2860, lr:3.8473e-04 dt: 3332.26ms, tok/sec:157337.27
step 7043, loss: 3.386739, norm:0.2843, lr:3.8467e-04 dt: 3332.37ms, tok/sec:157331.78
step 7044, loss: 3.356005, norm:0.2637, lr:3.8462e-04 dt: 3332.16ms, tok/sec:157341.81
step 7045, loss: 3.359843, norm:0.3045, lr:3.8456e-04 dt: 3332.10ms, tok/sec:157344.71
step 7046, loss: 3.468574, norm:0.3197, lr:3.8450e-04 dt: 3332.11ms, tok/sec:157344.05
step 7047, loss: 3.371507, norm:0.2588, lr:3.8445e-04 dt: 3332.11ms, tok/sec:157344.09
step 7048, loss: 3.396772, norm:0.3222, lr:3.8439e-04 dt: 3334.44ms, tok/sec:157234.00
step 7049, loss: 3.396247, norm:0.2837, lr:3.8433e-04 dt: 3332.26ms, tok/sec:157336.94
HellaSwag accuracy:-2295424375931075503/-2=1147712187965537792.0000
rank 1 sample 0: Hello, I'm a language model, this is the way the world is constructed and maintained in language research.
There are lots of reasons why people are interested
rank 1 sample 1: Hello, I'm a language model, a teacher who studies a language in its original form. As a speaker, I write from my position, and I think
rank 1 sample 2: Hello, I'm a language model, I say you've got a lot of people who are fluent (I mean, I mean, the most fluent).

rank 1 sample 3: Hello, I'm a language model, and I'm just a learner that never gets better grades because I get different grades I take and I don't get
rank 0 sample 0: Hello, I'm a language model, and I'll be using it on other forums with my current friends who won't be using it." (A)

rank 0 sample 1: Hello, I'm a language model, so there's a lot of different forms of communication within which to say them. I think of it more in terms of
rank 0 sample 2: Hello, I'm a language model, I'm being used to learning Chinese with my parents, and this has helped me get a lot of the information out.
rank 0 sample 3: Hello, I'm a language model, so far, I've been writing it for around ten years. That's because grammar isn't the subject of this website
step 7050, loss: 3.387279, norm:0.2706, lr:3.8428e-04 dt: 48522.49ms, tok/sec:10805.05
step 7051, loss: 3.419366, norm:0.2975, lr:3.8422e-04 dt: 3332.13ms, tok/sec:157342.98
step 7052, loss: 3.355361, norm:0.2696, lr:3.8416e-04 dt: 3332.02ms, tok/sec:157348.63
step 7053, loss: 3.434363, norm:0.2654, lr:3.8410e-04 dt: 3331.98ms, tok/sec:157350.51
step 7054, loss: 3.332576, norm:0.2730, lr:3.8405e-04 dt: 3332.15ms, tok/sec:157342.43
step 7055, loss: 3.346972, norm:0.2493, lr:3.8399e-04 dt: 3331.91ms, tok/sec:157353.65
step 7056, loss: 3.300428, norm:0.2754, lr:3.8393e-04 dt: 3331.95ms, tok/sec:157351.68
step 7057, loss: 3.274829, norm:0.2475, lr:3.8388e-04 dt: 3332.23ms, tok/sec:157338.68
step 7058, loss: 3.346677, norm:0.2666, lr:3.8382e-04 dt: 3332.23ms, tok/sec:157338.47
step 7059, loss: 3.316253, norm:0.2663, lr:3.8376e-04 dt: 3332.13ms, tok/sec:157343.32
step 7060, loss: 3.314366, norm:0.2561, lr:3.8370e-04 dt: 3331.97ms, tok/sec:157350.69
step 7061, loss: 3.307506, norm:0.2627, lr:3.8365e-04 dt: 3332.02ms, tok/sec:157348.63
step 7062, loss: 3.276164, norm:0.2911, lr:3.8359e-04 dt: 3332.01ms, tok/sec:157349.05
step 7063, loss: 3.342029, norm:0.2608, lr:3.8353e-04 dt: 3332.11ms, tok/sec:157344.20
step 7064, loss: 3.369051, norm:0.2793, lr:3.8347e-04 dt: 3332.02ms, tok/sec:157348.48
step 7065, loss: 3.344541, norm:0.2702, lr:3.8342e-04 dt: 3332.06ms, tok/sec:157346.60
step 7066, loss: 3.400058, norm:0.2686, lr:3.8336e-04 dt: 3332.26ms, tok/sec:157336.98
step 7067, loss: 3.375245, norm:0.2881, lr:3.8330e-04 dt: 3332.38ms, tok/sec:157331.50
step 7068, loss: 3.336275, norm:0.2897, lr:3.8325e-04 dt: 3332.10ms, tok/sec:157344.67
step 7069, loss: 3.394011, norm:0.2761, lr:3.8319e-04 dt: 3332.03ms, tok/sec:157347.75
step 7070, loss: 3.454510, norm:0.2991, lr:3.8313e-04 dt: 3332.07ms, tok/sec:157345.85
step 7071, loss: 3.345334, norm:0.2561, lr:3.8307e-04 dt: 3332.03ms, tok/sec:157348.08
step 7072, loss: 3.402084, norm:0.2782, lr:3.8302e-04 dt: 3332.27ms, tok/sec:157336.80
step 7073, loss: 3.418460, norm:0.2965, lr:3.8296e-04 dt: 3331.89ms, tok/sec:157354.62
step 7074, loss: 3.418270, norm:0.2755, lr:3.8290e-04 dt: 3332.21ms, tok/sec:157339.34
step 7075, loss: 3.344064, norm:0.2709, lr:3.8285e-04 dt: 3332.19ms, tok/sec:157340.48
step 7076, loss: 3.352851, norm:0.2756, lr:3.8279e-04 dt: 3332.46ms, tok/sec:157327.53
step 7077, loss: 3.353788, norm:0.2393, lr:3.8273e-04 dt: 3332.02ms, tok/sec:157348.26
step 7078, loss: 3.323396, norm:0.2765, lr:3.8267e-04 dt: 3331.89ms, tok/sec:157354.55
step 7079, loss: 3.380256, norm:0.2494, lr:3.8262e-04 dt: 3332.08ms, tok/sec:157345.53
step 7080, loss: 3.357448, norm:0.2541, lr:3.8256e-04 dt: 3332.24ms, tok/sec:157337.87
step 7081, loss: 3.343462, norm:0.2610, lr:3.8250e-04 dt: 3332.01ms, tok/sec:157348.79
step 7082, loss: 3.484657, norm:0.2542, lr:3.8245e-04 dt: 3332.00ms, tok/sec:157349.38
step 7083, loss: 3.437011, norm:0.2842, lr:3.8239e-04 dt: 3332.31ms, tok/sec:157334.65
step 7084, loss: 3.375530, norm:0.2586, lr:3.8233e-04 dt: 3331.95ms, tok/sec:157351.82
step 7085, loss: 3.365888, norm:0.2500, lr:3.8227e-04 dt: 3332.44ms, tok/sec:157328.73
step 7086, loss: 3.410014, norm:0.2599, lr:3.8222e-04 dt: 3332.07ms, tok/sec:157345.83
step 7087, loss: 3.363687, norm:0.2388, lr:3.8216e-04 dt: 3332.00ms, tok/sec:157349.13
step 7088, loss: 3.359583, norm:0.2521, lr:3.8210e-04 dt: 3332.07ms, tok/sec:157345.97
step 7089, loss: 3.337764, norm:0.2538, lr:3.8205e-04 dt: 3332.18ms, tok/sec:157340.97
step 7090, loss: 3.305171, norm:0.2406, lr:3.8199e-04 dt: 3332.13ms, tok/sec:157343.43
step 7091, loss: 3.347404, norm:0.2745, lr:3.8193e-04 dt: 3332.08ms, tok/sec:157345.67
step 7092, loss: 3.390103, norm:0.2332, lr:3.8187e-04 dt: 3332.31ms, tok/sec:157334.59
step 7093, loss: 3.297787, norm:0.2608, lr:3.8182e-04 dt: 3332.03ms, tok/sec:157348.16
step 7094, loss: 3.341865, norm:0.2476, lr:3.8176e-04 dt: 3332.37ms, tok/sec:157331.97
step 7095, loss: 3.290010, norm:0.2607, lr:3.8170e-04 dt: 3332.17ms, tok/sec:157341.36
step 7096, loss: 3.326855, norm:0.2344, lr:3.8164e-04 dt: 3332.01ms, tok/sec:157348.72
step 7097, loss: 3.389876, norm:0.2457, lr:3.8159e-04 dt: 3332.13ms, tok/sec:157343.33
step 7098, loss: 3.316289, norm:0.2440, lr:3.8153e-04 dt: 3331.92ms, tok/sec:157352.98
step 7099, loss: 3.376098, norm:0.2561, lr:3.8147e-04 dt: 3332.48ms, tok/sec:157326.81
validation loss: 3.3687
Model and optimizer state saved.
HellaSwag accuracy:2325128069902435409/-2=-1162564034951217664.0000
rank 1 sample 0: Hello, I'm a language model, and I am a writer, so first, is my grammar lesson. I need to start at some level of grammar,
rank 1 sample 1: Hello, I'm a language model, you're using code in a new language. I'm interested in trying to explain the most basic commands in a language,
rank 1 sample 2: Hello, I'm a language model, but like the old school, I'm a language model and am interested in languages. I know the way that the most
rank 1 sample 3: Hello, I'm a language model, and I'm still learning Chinese. That comes as a warning to me if you're struggling with a language. You're
rank 0 sample 0: Hello, I'm a language model, and I need to learn this very best: that's a really exciting year and I hope you have fun playing it out
rank 0 sample 1: Hello, I'm a language model, so when I speak of a computer game, the program just takes care of a lot more than the usual program. But
rank 0 sample 2: Hello, I'm a language model, so I will learn in 3rd grade. We have a little knowledge of the languages. We have a lot of practice
rank 0 sample 3: Hello, I'm a language model, you do not have to write anything but just "I'm a language model". Even in basic languages, you don't
step 7100, loss: 3.363447, norm:0.2616, lr:3.8142e-04 dt: 56161.57ms, tok/sec:9335.35
step 7101, loss: 3.376249, norm:0.2602, lr:3.8136e-04 dt: 3332.24ms, tok/sec:157337.81
step 7102, loss: 3.363272, norm:0.2551, lr:3.8130e-04 dt: 3332.38ms, tok/sec:157331.17
step 7103, loss: 3.422562, norm:0.2910, lr:3.8124e-04 dt: 3332.12ms, tok/sec:157343.47
step 7104, loss: 3.398454, norm:0.2581, lr:3.8119e-04 dt: 3332.08ms, tok/sec:157345.57
step 7105, loss: 3.364130, norm:0.2589, lr:3.8113e-04 dt: 3332.11ms, tok/sec:157344.18
step 7106, loss: 3.442512, norm:0.2523, lr:3.8107e-04 dt: 3332.28ms, tok/sec:157336.17
step 7107, loss: 3.397659, norm:0.2566, lr:3.8101e-04 dt: 3332.21ms, tok/sec:157339.38
step 7108, loss: 3.344191, norm:0.2674, lr:3.8096e-04 dt: 3332.18ms, tok/sec:157340.93
step 7109, loss: 3.378062, norm:0.2474, lr:3.8090e-04 dt: 3332.28ms, tok/sec:157336.29
step 7110, loss: 3.407820, norm:0.2545, lr:3.8084e-04 dt: 3332.04ms, tok/sec:157347.42
step 7111, loss: 3.402265, norm:0.2743, lr:3.8079e-04 dt: 3332.41ms, tok/sec:157330.12
step 7112, loss: 3.335257, norm:0.2602, lr:3.8073e-04 dt: 3332.04ms, tok/sec:157347.33
step 7113, loss: 3.288418, norm:0.2573, lr:3.8067e-04 dt: 3332.07ms, tok/sec:157346.16
step 7114, loss: 3.442211, norm:0.2906, lr:3.8061e-04 dt: 3331.88ms, tok/sec:157354.93
step 7115, loss: 3.349088, norm:0.2472, lr:3.8056e-04 dt: 3332.19ms, tok/sec:157340.47
step 7116, loss: 3.336221, norm:0.2821, lr:3.8050e-04 dt: 3331.98ms, tok/sec:157350.10
step 7117, loss: 3.350030, norm:0.2458, lr:3.8044e-04 dt: 3332.04ms, tok/sec:157347.38
step 7118, loss: 3.379175, norm:0.2491, lr:3.8038e-04 dt: 3332.07ms, tok/sec:157346.01
step 7119, loss: 3.297381, norm:0.2595, lr:3.8033e-04 dt: 3332.28ms, tok/sec:157336.21
step 7120, loss: 3.357265, norm:0.2481, lr:3.8027e-04 dt: 3332.51ms, tok/sec:157325.29
step 7121, loss: 3.368966, norm:0.2479, lr:3.8021e-04 dt: 3332.00ms, tok/sec:157349.17
step 7122, loss: 3.371103, norm:0.2490, lr:3.8016e-04 dt: 3332.00ms, tok/sec:157349.32
step 7123, loss: 3.347190, norm:0.2348, lr:3.8010e-04 dt: 3332.08ms, tok/sec:157345.45
step 7124, loss: 3.340426, norm:0.2525, lr:3.8004e-04 dt: 3332.06ms, tok/sec:157346.67
step 7125, loss: 3.370234, norm:0.2454, lr:3.7998e-04 dt: 3332.30ms, tok/sec:157335.35
step 7126, loss: 3.312839, norm:0.2576, lr:3.7993e-04 dt: 3332.06ms, tok/sec:157346.73
step 7127, loss: 3.335160, norm:0.2484, lr:3.7987e-04 dt: 3332.25ms, tok/sec:157337.53
step 7128, loss: 3.368943, norm:0.2489, lr:3.7981e-04 dt: 3331.98ms, tok/sec:157350.37
step 7129, loss: 3.314191, norm:0.2515, lr:3.7975e-04 dt: 3332.10ms, tok/sec:157344.42
step 7130, loss: 3.357893, norm:0.2506, lr:3.7970e-04 dt: 3332.03ms, tok/sec:157347.79
step 7131, loss: 3.326058, norm:0.2480, lr:3.7964e-04 dt: 3332.07ms, tok/sec:157345.86
step 7132, loss: 3.316462, norm:0.2538, lr:3.7958e-04 dt: 3331.99ms, tok/sec:157349.98
step 7133, loss: 3.319925, norm:0.2458, lr:3.7952e-04 dt: 3332.05ms, tok/sec:157347.21
step 7134, loss: 3.403510, norm:0.3231, lr:3.7947e-04 dt: 3331.97ms, tok/sec:157350.64
step 7135, loss: 3.309187, norm:0.2850, lr:3.7941e-04 dt: 3331.89ms, tok/sec:157354.37
step 7136, loss: 3.404168, norm:0.2971, lr:3.7935e-04 dt: 3332.07ms, tok/sec:157346.22
step 7137, loss: 3.381314, norm:0.2772, lr:3.7930e-04 dt: 3332.13ms, tok/sec:157343.26
step 7138, loss: 3.402626, norm:0.2734, lr:3.7924e-04 dt: 3332.05ms, tok/sec:157346.83
step 7139, loss: 3.362444, norm:0.2861, lr:3.7918e-04 dt: 3332.23ms, tok/sec:157338.27
step 7140, loss: 3.410114, norm:0.2597, lr:3.7912e-04 dt: 3332.07ms, tok/sec:157345.81
step 7141, loss: 3.349330, norm:0.2634, lr:3.7907e-04 dt: 3332.26ms, tok/sec:157336.88
step 7142, loss: 3.347927, norm:0.2422, lr:3.7901e-04 dt: 3332.13ms, tok/sec:157343.03
step 7143, loss: 3.385150, norm:0.2626, lr:3.7895e-04 dt: 3332.32ms, tok/sec:157334.42
step 7144, loss: 3.441132, norm:0.3684, lr:3.7889e-04 dt: 3332.20ms, tok/sec:157339.99
step 7145, loss: 3.360131, norm:0.3015, lr:3.7884e-04 dt: 3332.18ms, tok/sec:157340.96
step 7146, loss: 3.353964, norm:0.3076, lr:3.7878e-04 dt: 3332.16ms, tok/sec:157341.71
step 7147, loss: 3.366687, norm:0.3001, lr:3.7872e-04 dt: 3332.27ms, tok/sec:157336.57
step 7148, loss: 3.367373, norm:0.2818, lr:3.7866e-04 dt: 3332.06ms, tok/sec:157346.38
step 7149, loss: 3.402263, norm:0.2566, lr:3.7861e-04 dt: 3331.97ms, tok/sec:157350.57
HellaSwag accuracy:2325260045657506897/-2=-1162630022828753408.0000
rank 1 sample 0: Hello, I'm a language model, as you know, the way people feel. They are a human beings, of course, we do not need to be
rank 1 sample 1: Hello, I'm a language model, a computer model or a computer simulation. That's all to say a computer model. Every time someone asks you to model
rank 1 sample 2: Hello, I'm a language model, but most of it is not. It's a language, as you know, that's called English and is not widely
rank 1 sample 3: Hello, I'm a language model, and I'm using it the first time just to explain exactly what this sounds like. Maybe at this point I should be
rank 0 sample 0: Hello, I'm a language model, and I think it is really interesting. Of course, I don't necessarily mean to tell you why. The reason that
rank 0 sample 1: Hello, I'm a language model, I was also a teacher.
|This page, first published in September 2005|<|endoftext|>Trees for Urban Gardeners
rank 0 sample 2: Hello, I'm a language model, so I thought this sentence and now I'm looking for a translation to it.
In this sentence, the first element
rank 0 sample 3: Hello, I'm a language model, which tells you that you can write words that take a certain amount of time to represent things like the sentence "A week
step 7150, loss: 3.371111, norm:0.2786, lr:3.7855e-04 dt: 48515.88ms, tok/sec:10806.52
step 7151, loss: 3.442688, norm:0.2596, lr:3.7849e-04 dt: 3332.26ms, tok/sec:157336.89
step 7152, loss: 3.324481, norm:0.2707, lr:3.7843e-04 dt: 3331.99ms, tok/sec:157349.79
step 7153, loss: 3.384861, norm:0.2907, lr:3.7838e-04 dt: 3332.06ms, tok/sec:157346.45
step 7154, loss: 3.351234, norm:0.2650, lr:3.7832e-04 dt: 3332.03ms, tok/sec:157348.10
step 7155, loss: 3.387036, norm:0.2700, lr:3.7826e-04 dt: 3332.05ms, tok/sec:157346.96
step 7156, loss: 3.366512, norm:0.2478, lr:3.7821e-04 dt: 3331.97ms, tok/sec:157350.58
step 7157, loss: 3.319310, norm:0.2592, lr:3.7815e-04 dt: 3332.12ms, tok/sec:157343.78
step 7158, loss: 3.333053, norm:0.2411, lr:3.7809e-04 dt: 3332.37ms, tok/sec:157331.70
step 7159, loss: 3.346853, norm:0.2512, lr:3.7803e-04 dt: 3332.07ms, tok/sec:157346.03
step 7160, loss: 3.284719, norm:0.2338, lr:3.7798e-04 dt: 3332.39ms, tok/sec:157330.71
step 7161, loss: 3.303408, norm:0.2344, lr:3.7792e-04 dt: 3331.98ms, tok/sec:157350.23
step 7162, loss: 3.301275, norm:0.2345, lr:3.7786e-04 dt: 3331.80ms, tok/sec:157359.01
step 7163, loss: 3.308214, norm:0.2349, lr:3.7780e-04 dt: 3331.98ms, tok/sec:157350.18
step 7164, loss: 3.298245, norm:0.2375, lr:3.7775e-04 dt: 3331.93ms, tok/sec:157352.50
step 7165, loss: 3.229430, norm:0.2857, lr:3.7769e-04 dt: 3331.85ms, tok/sec:157356.29
step 7166, loss: 3.315881, norm:0.2413, lr:3.7763e-04 dt: 3332.06ms, tok/sec:157346.48
step 7167, loss: 3.321857, norm:0.2612, lr:3.7757e-04 dt: 3332.21ms, tok/sec:157339.53
step 7168, loss: 3.333546, norm:0.2418, lr:3.7752e-04 dt: 3332.11ms, tok/sec:157344.32
step 7169, loss: 3.384149, norm:0.2787, lr:3.7746e-04 dt: 3332.39ms, tok/sec:157330.97
step 7170, loss: 3.393936, norm:0.2952, lr:3.7740e-04 dt: 3332.13ms, tok/sec:157343.27
step 7171, loss: 3.358096, norm:0.2634, lr:3.7734e-04 dt: 3331.94ms, tok/sec:157352.30
step 7172, loss: 3.361877, norm:0.2748, lr:3.7729e-04 dt: 3332.06ms, tok/sec:157346.30
step 7173, loss: 3.351345, norm:0.2639, lr:3.7723e-04 dt: 3332.19ms, tok/sec:157340.49
step 7174, loss: 3.423374, norm:0.2471, lr:3.7717e-04 dt: 3332.16ms, tok/sec:157341.73
step 7175, loss: 3.365796, norm:0.2590, lr:3.7711e-04 dt: 3331.90ms, tok/sec:157354.01
step 7176, loss: 3.426381, norm:0.2542, lr:3.7706e-04 dt: 3332.06ms, tok/sec:157346.68
step 7177, loss: 3.335007, norm:0.2518, lr:3.7700e-04 dt: 3332.60ms, tok/sec:157320.92
step 7178, loss: 3.382206, norm:0.2564, lr:3.7694e-04 dt: 3332.16ms, tok/sec:157341.85
step 7179, loss: 3.415005, norm:0.2469, lr:3.7689e-04 dt: 3332.04ms, tok/sec:157347.46
step 7180, loss: 3.379226, norm:0.2377, lr:3.7683e-04 dt: 3331.95ms, tok/sec:157351.83
step 7181, loss: 3.406668, norm:0.2359, lr:3.7677e-04 dt: 3332.04ms, tok/sec:157347.33
step 7182, loss: 3.365278, norm:0.2364, lr:3.7671e-04 dt: 3332.27ms, tok/sec:157336.80
step 7183, loss: 3.325248, norm:0.2293, lr:3.7666e-04 dt: 3331.88ms, tok/sec:157355.21
step 7184, loss: 3.406974, norm:0.2357, lr:3.7660e-04 dt: 3332.02ms, tok/sec:157348.46
step 7185, loss: 3.380716, norm:0.2398, lr:3.7654e-04 dt: 3332.28ms, tok/sec:157335.94
step 7186, loss: 3.366833, norm:0.2524, lr:3.7648e-04 dt: 3332.44ms, tok/sec:157328.39
step 7187, loss: 3.492788, norm:0.2543, lr:3.7643e-04 dt: 3332.14ms, tok/sec:157342.75
step 7188, loss: 3.356382, norm:0.2572, lr:3.7637e-04 dt: 3331.93ms, tok/sec:157352.62
step 7189, loss: 3.407396, norm:0.2411, lr:3.7631e-04 dt: 3332.08ms, tok/sec:157345.40
step 7190, loss: 3.382446, norm:0.2742, lr:3.7625e-04 dt: 3332.28ms, tok/sec:157336.31
step 7191, loss: 3.367030, norm:0.2552, lr:3.7620e-04 dt: 3332.09ms, tok/sec:157344.86
step 7192, loss: 3.317712, norm:0.2731, lr:3.7614e-04 dt: 3332.02ms, tok/sec:157348.19
step 7193, loss: 3.350109, norm:0.2490, lr:3.7608e-04 dt: 3332.23ms, tok/sec:157338.68
step 7194, loss: 3.285487, norm:0.2377, lr:3.7602e-04 dt: 3332.23ms, tok/sec:157338.31
step 7195, loss: 3.314619, norm:0.2496, lr:3.7597e-04 dt: 3332.45ms, tok/sec:157328.30
step 7196, loss: 3.328482, norm:0.2452, lr:3.7591e-04 dt: 3332.04ms, tok/sec:157347.33
step 7197, loss: 3.306992, norm:0.2325, lr:3.7585e-04 dt: 3332.08ms, tok/sec:157345.77
step 7198, loss: 3.375915, norm:0.2325, lr:3.7579e-04 dt: 3332.10ms, tok/sec:157344.67
step 7199, loss: 3.308528, norm:0.2465, lr:3.7574e-04 dt: 3332.07ms, tok/sec:157346.25
validation loss: 3.3625
Model and optimizer state saved.
HellaSwag accuracy:-2295424410290256815/-2=1147712205145128448.0000
rank 1 sample 0: Hello, I'm a language model,” he said.
“We use it when students are struggling with a problem and it's just a matter
rank 1 sample 1: Hello, I'm a language model, a tool for using the language that helps users to understand an object.
A large majority of native language models are written
rank 1 sample 2: Hello, I'm a language model, but still, I'm not sure if I'm speaking in Japanese anymore...
I'm just not sure what this means
rank 1 sample 3: Hello, I'm a language model, and I'm using it right now. Every time I call up 'logies', is at the end of the sentence
rank 0 sample 0: Hello, I'm a language model, and I just want you to learn and remember stuff.
So in particular the language you're talking about might be different
rank 0 sample 1: Hello, I'm a language model, so don't worry.
So. I think some classes have more capabilities to help us do this because I'm trying
rank 0 sample 2: Hello, I'm a language model, so I guess it's my blog about it too.
Now we can see this as a language model, but when
rank 0 sample 3: Hello, I'm a language model, it takes a long time to learn a new language.
Some people might not ask the question. Why do people just
step 7200, loss: 3.305060, norm:0.2216, lr:3.7568e-04 dt: 56263.01ms, tok/sec:9318.52
step 7201, loss: 3.292416, norm:0.2533, lr:3.7562e-04 dt: 3332.14ms, tok/sec:157342.49
step 7202, loss: 3.290286, norm:0.2430, lr:3.7556e-04 dt: 3331.97ms, tok/sec:157350.59
step 7203, loss: 3.402971, norm:0.2736, lr:3.7551e-04 dt: 3332.09ms, tok/sec:157344.93
step 7204, loss: 3.459317, norm:0.3146, lr:3.7545e-04 dt: 3332.19ms, tok/sec:157340.19
step 7205, loss: 3.455595, norm:0.2631, lr:3.7539e-04 dt: 3332.09ms, tok/sec:157345.06
step 7206, loss: 3.375819, norm:0.2769, lr:3.7533e-04 dt: 3332.28ms, tok/sec:157335.89
step 7207, loss: 3.417676, norm:0.2778, lr:3.7528e-04 dt: 3332.13ms, tok/sec:157343.28
step 7208, loss: 3.395578, norm:0.2450, lr:3.7522e-04 dt: 3332.45ms, tok/sec:157328.07
step 7209, loss: 3.358182, norm:0.2682, lr:3.7516e-04 dt: 3332.22ms, tok/sec:157338.86
step 7210, loss: 3.367115, norm:0.2564, lr:3.7510e-04 dt: 3332.07ms, tok/sec:157345.84
step 7211, loss: 3.387308, norm:0.2624, lr:3.7505e-04 dt: 3331.89ms, tok/sec:157354.36
step 7212, loss: 3.425626, norm:0.2607, lr:3.7499e-04 dt: 3332.13ms, tok/sec:157343.00
step 7213, loss: 3.369474, norm:0.2711, lr:3.7493e-04 dt: 3332.19ms, tok/sec:157340.59
step 7214, loss: 3.409939, norm:0.3101, lr:3.7487e-04 dt: 3332.08ms, tok/sec:157345.71
step 7215, loss: 3.372433, norm:0.2713, lr:3.7482e-04 dt: 3332.28ms, tok/sec:157335.96
step 7216, loss: 3.403486, norm:0.2795, lr:3.7476e-04 dt: 3332.04ms, tok/sec:157347.44
step 7217, loss: 3.328277, norm:0.2671, lr:3.7470e-04 dt: 3332.53ms, tok/sec:157324.10
step 7218, loss: 3.372571, norm:0.2682, lr:3.7464e-04 dt: 3332.04ms, tok/sec:157347.38
step 7219, loss: 3.376873, norm:0.2578, lr:3.7459e-04 dt: 3331.91ms, tok/sec:157353.46
step 7220, loss: 3.352051, norm:0.2731, lr:3.7453e-04 dt: 3332.14ms, tok/sec:157342.81
step 7221, loss: 3.428885, norm:0.2601, lr:3.7447e-04 dt: 3332.19ms, tok/sec:157340.31
step 7222, loss: 3.387961, norm:0.2591, lr:3.7441e-04 dt: 3332.09ms, tok/sec:157345.13
step 7223, loss: 3.406549, norm:0.2598, lr:3.7436e-04 dt: 3331.87ms, tok/sec:157355.42
step 7224, loss: 3.349392, norm:0.2378, lr:3.7430e-04 dt: 3332.22ms, tok/sec:157339.05
step 7225, loss: 3.405331, norm:0.2391, lr:3.7424e-04 dt: 3332.22ms, tok/sec:157339.09
step 7226, loss: 3.324519, norm:0.2307, lr:3.7418e-04 dt: 3332.46ms, tok/sec:157327.48
step 7227, loss: 3.349701, norm:0.2342, lr:3.7413e-04 dt: 3332.04ms, tok/sec:157347.39
step 7228, loss: 3.269587, norm:0.2276, lr:3.7407e-04 dt: 3332.10ms, tok/sec:157344.70
step 7229, loss: 3.366673, norm:0.2351, lr:3.7401e-04 dt: 3332.14ms, tok/sec:157342.56
step 7230, loss: 3.356222, norm:0.2496, lr:3.7395e-04 dt: 3332.11ms, tok/sec:157344.23
step 7231, loss: 3.311976, norm:0.2514, lr:3.7390e-04 dt: 3331.89ms, tok/sec:157354.64
step 7232, loss: 3.318094, norm:0.2657, lr:3.7384e-04 dt: 3331.93ms, tok/sec:157352.65
step 7233, loss: 3.332017, norm:0.2552, lr:3.7378e-04 dt: 3332.23ms, tok/sec:157338.44
step 7234, loss: 3.292664, norm:0.2448, lr:3.7372e-04 dt: 3332.20ms, tok/sec:157339.80
step 7235, loss: 3.309673, norm:0.2671, lr:3.7367e-04 dt: 3332.07ms, tok/sec:157345.84
step 7236, loss: 3.314211, norm:0.2737, lr:3.7361e-04 dt: 3332.14ms, tok/sec:157342.67
step 7237, loss: 3.377170, norm:0.2710, lr:3.7355e-04 dt: 3331.97ms, tok/sec:157350.71
step 7238, loss: 3.439898, norm:0.2796, lr:3.7349e-04 dt: 3334.03ms, tok/sec:157253.49
step 7239, loss: 3.420167, norm:0.2556, lr:3.7344e-04 dt: 3332.35ms, tok/sec:157332.65
step 7240, loss: 3.351038, norm:0.2543, lr:3.7338e-04 dt: 3332.17ms, tok/sec:157341.31
step 7241, loss: 3.388099, norm:0.2665, lr:3.7332e-04 dt: 3332.27ms, tok/sec:157336.74
step 7242, loss: 3.390746, norm:0.2533, lr:3.7326e-04 dt: 3332.23ms, tok/sec:157338.50
step 7243, loss: 3.369483, norm:0.2670, lr:3.7320e-04 dt: 3332.43ms, tok/sec:157329.09
step 7244, loss: 3.400439, norm:0.2628, lr:3.7315e-04 dt: 3332.12ms, tok/sec:157343.66
step 7245, loss: 3.475748, norm:0.2657, lr:3.7309e-04 dt: 3332.03ms, tok/sec:157348.03
step 7246, loss: 3.397738, norm:0.2620, lr:3.7303e-04 dt: 3331.91ms, tok/sec:157353.57
step 7247, loss: 3.381081, norm:0.2585, lr:3.7297e-04 dt: 3332.04ms, tok/sec:157347.55
step 7248, loss: 3.375405, norm:0.2585, lr:3.7292e-04 dt: 3332.00ms, tok/sec:157349.31
step 7249, loss: 3.386928, norm:0.2556, lr:3.7286e-04 dt: 3332.23ms, tok/sec:157338.50
HellaSwag accuracy:2325251215205303377/-2=-1162625607602651648.0000
rank 1 sample 0: Hello, I'm a language model, and I've been a model for doing this on many courses but I've made it into my code. I've been
rank 1 sample 1: Hello, I'm a language model, you know how to do it. If you're going up to building a web page a little bit more, I'll
rank 1 sample 2: Hello, I'm a language model, so I'll talk about it in a few weeks. But here is the secret to it, which was a long experience
rank 1 sample 3: Hello, I'm a language model, and I'm still a year old (unless you're some very early language in the city to the south) but I
rank 0 sample 0: Hello, I'm a language model, and I'll be writing code on all hardware projects.
- This time the code will be run by a compiler called
rank 0 sample 1: Hello, I'm a language model, I'll put it down if I was to do research I'd want to write about. And if people would like to
rank 0 sample 2: Hello, I'm a language model, so I love learning stuff here. I'm sure I'm glad this has helped me in my life.
- So
rank 0 sample 3: Hello, I'm a language model, I believe that if you think of that you say "I'm thinking of the situation and want to start thinking of them
step 7250, loss: 3.387888, norm:0.2846, lr:3.7280e-04 dt: 48523.51ms, tok/sec:10804.82
step 7251, loss: 3.323588, norm:0.2562, lr:3.7274e-04 dt: 3332.10ms, tok/sec:157344.62
step 7252, loss: 3.393120, norm:0.2771, lr:3.7269e-04 dt: 3332.00ms, tok/sec:157349.13
step 7253, loss: 3.341765, norm:0.2590, lr:3.7263e-04 dt: 3331.96ms, tok/sec:157351.22
step 7254, loss: 3.359862, norm:0.2704, lr:3.7257e-04 dt: 3332.07ms, tok/sec:157346.25
step 7255, loss: 3.354245, norm:0.2905, lr:3.7251e-04 dt: 3332.53ms, tok/sec:157324.31
step 7256, loss: 3.389950, norm:0.3088, lr:3.7246e-04 dt: 3331.96ms, tok/sec:157351.44
step 7257, loss: 3.364400, norm:0.2697, lr:3.7240e-04 dt: 3332.03ms, tok/sec:157348.13
step 7258, loss: 3.412475, norm:0.2680, lr:3.7234e-04 dt: 3331.97ms, tok/sec:157350.76
step 7259, loss: 3.345507, norm:0.2563, lr:3.7228e-04 dt: 3331.94ms, tok/sec:157352.03
step 7260, loss: 3.314996, norm:0.2394, lr:3.7223e-04 dt: 3332.17ms, tok/sec:157341.28
step 7261, loss: 3.325683, norm:0.2546, lr:3.7217e-04 dt: 3331.95ms, tok/sec:157351.78
step 7262, loss: 3.320551, norm:0.2468, lr:3.7211e-04 dt: 3332.10ms, tok/sec:157344.81
step 7263, loss: 3.308532, norm:0.2809, lr:3.7205e-04 dt: 3332.11ms, tok/sec:157344.07
step 7264, loss: 3.282969, norm:0.2575, lr:3.7200e-04 dt: 3332.24ms, tok/sec:157337.82
step 7265, loss: 3.284767, norm:0.2643, lr:3.7194e-04 dt: 3332.15ms, tok/sec:157342.03
step 7266, loss: 3.362714, norm:0.2699, lr:3.7188e-04 dt: 3332.05ms, tok/sec:157347.18
step 7267, loss: 3.340915, norm:0.2490, lr:3.7182e-04 dt: 3332.06ms, tok/sec:157346.41
step 7268, loss: 3.322448, norm:0.2331, lr:3.7176e-04 dt: 3332.12ms, tok/sec:157343.74
step 7269, loss: 3.320788, norm:0.2579, lr:3.7171e-04 dt: 3332.05ms, tok/sec:157346.92
step 7270, loss: 3.356996, norm:0.2490, lr:3.7165e-04 dt: 3332.13ms, tok/sec:157343.32
step 7271, loss: 3.306743, norm:0.2684, lr:3.7159e-04 dt: 3332.05ms, tok/sec:157346.90
step 7272, loss: 3.277810, norm:0.2540, lr:3.7153e-04 dt: 3332.54ms, tok/sec:157324.01
step 7273, loss: 3.343247, norm:0.2672, lr:3.7148e-04 dt: 3331.96ms, tok/sec:157351.21
step 7274, loss: 3.282094, norm:0.2625, lr:3.7142e-04 dt: 3332.03ms, tok/sec:157347.80
step 7275, loss: 3.322203, norm:0.2888, lr:3.7136e-04 dt: 3331.92ms, tok/sec:157353.20
step 7276, loss: 3.316107, norm:0.2917, lr:3.7130e-04 dt: 3332.11ms, tok/sec:157344.22
step 7277, loss: 3.381392, norm:0.2668, lr:3.7125e-04 dt: 3332.12ms, tok/sec:157343.72
step 7278, loss: 3.340437, norm:0.2898, lr:3.7119e-04 dt: 3332.05ms, tok/sec:157346.96
step 7279, loss: 3.328518, norm:0.2572, lr:3.7113e-04 dt: 3332.26ms, tok/sec:157337.20
step 7280, loss: 3.357747, norm:0.2809, lr:3.7107e-04 dt: 3332.33ms, tok/sec:157333.54
step 7281, loss: 3.352099, norm:0.2678, lr:3.7102e-04 dt: 3332.16ms, tok/sec:157341.74
step 7282, loss: 3.297017, norm:0.2646, lr:3.7096e-04 dt: 3331.95ms, tok/sec:157351.57
step 7283, loss: 3.404204, norm:0.2888, lr:3.7090e-04 dt: 3332.30ms, tok/sec:157335.33
step 7284, loss: 3.361159, norm:0.2723, lr:3.7084e-04 dt: 3332.02ms, tok/sec:157348.46
step 7285, loss: 3.384269, norm:0.2610, lr:3.7079e-04 dt: 3332.09ms, tok/sec:157345.06
step 7286, loss: 3.349748, norm:0.2673, lr:3.7073e-04 dt: 3332.04ms, tok/sec:157347.39
step 7287, loss: 3.377601, norm:0.2446, lr:3.7067e-04 dt: 3331.99ms, tok/sec:157349.95
step 7288, loss: 3.361819, norm:0.2536, lr:3.7061e-04 dt: 3332.60ms, tok/sec:157321.15
step 7289, loss: 3.350661, norm:0.2803, lr:3.7055e-04 dt: 3332.14ms, tok/sec:157342.66
step 7290, loss: 3.412599, norm:0.2532, lr:3.7050e-04 dt: 3331.97ms, tok/sec:157350.81
step 7291, loss: 3.376678, norm:0.2521, lr:3.7044e-04 dt: 3332.14ms, tok/sec:157342.65
step 7292, loss: 3.288962, norm:0.2459, lr:3.7038e-04 dt: 3332.32ms, tok/sec:157334.08
step 7293, loss: 3.382128, norm:0.2433, lr:3.7032e-04 dt: 3332.08ms, tok/sec:157345.65
step 7294, loss: 3.340988, norm:0.2649, lr:3.7027e-04 dt: 3332.17ms, tok/sec:157341.37
step 7295, loss: 3.332293, norm:0.2505, lr:3.7021e-04 dt: 3332.27ms, tok/sec:157336.46
step 7296, loss: 3.366802, norm:0.2669, lr:3.7015e-04 dt: 3332.15ms, tok/sec:157342.30
step 7297, loss: 3.320205, norm:0.2492, lr:3.7009e-04 dt: 3332.57ms, tok/sec:157322.50
step 7298, loss: 3.314074, norm:0.2307, lr:3.7004e-04 dt: 3331.93ms, tok/sec:157352.63
step 7299, loss: 3.317433, norm:0.2506, lr:3.6998e-04 dt: 3332.15ms, tok/sec:157342.31
validation loss: 3.3579
Model and optimizer state saved.
HellaSwag accuracy:2325128069902959697/-2=-1162564034951479808.0000
rank 1 sample 0: Hello, I'm a language model,
if I'm doing something
But what exactly do you mean?
How do you do the same thing?

rank 0 sample 0: Hello, I'm a language model, and I need to be ready when I return to the city, it's like a real computer with the tools to get
rank 1 sample 1: Hello, I'm a language model, not an artist. I'm a mathematician, not a graphic designer, and my son hasn't the ability to learn to
rank 0 sample 1: Hello, I'm a language model, so how do I learn to write for the sake of simplicity and the same for my students? I teach students to be
rank 1 sample 2: Hello, I'm a language model, but still want to learn how to do it.
So in order to develop a language for this purpose I'm simply
rank 0 sample 2: Hello, I'm a language model, so I always start with things that don't need to be true in your language. I'd like to make it real
rank 1 sample 3: Hello, I'm a language model, and I'm really excited to be able to use it!"
After I realized that learning my English is a wonderful way
rank 0 sample 3: Hello, I'm a language model, so just check your grammar and pronunciation of words just to make sure you get to a perfect and appropriate style. I mean
step 7300, loss: 3.342422, norm:0.2568, lr:3.6992e-04 dt: 56193.58ms, tok/sec:9330.03
step 7301, loss: 3.283247, norm:0.2858, lr:3.6986e-04 dt: 3331.87ms, tok/sec:157355.28
step 7302, loss: 3.304892, norm:0.2601, lr:3.6980e-04 dt: 3332.14ms, tok/sec:157342.80
step 7303, loss: 3.293904, norm:0.2423, lr:3.6975e-04 dt: 3332.18ms, tok/sec:157340.75
step 7304, loss: 3.305910, norm:0.2676, lr:3.6969e-04 dt: 3331.91ms, tok/sec:157353.44
step 7305, loss: 3.327398, norm:0.2489, lr:3.6963e-04 dt: 3332.18ms, tok/sec:157340.62
step 7306, loss: 3.326429, norm:0.2608, lr:3.6957e-04 dt: 3332.48ms, tok/sec:157326.90
step 7307, loss: 3.384466, norm:0.2824, lr:3.6952e-04 dt: 3332.15ms, tok/sec:157342.23
step 7308, loss: 3.349071, norm:0.2445, lr:3.6946e-04 dt: 3332.00ms, tok/sec:157349.34
step 7309, loss: 3.351512, norm:0.2596, lr:3.6940e-04 dt: 3332.26ms, tok/sec:157336.99
step 7310, loss: 3.395823, norm:0.3115, lr:3.6934e-04 dt: 3331.84ms, tok/sec:157356.74
step 7311, loss: 3.341071, norm:0.2559, lr:3.6929e-04 dt: 3332.14ms, tok/sec:157342.52
step 7312, loss: 3.280579, norm:0.2708, lr:3.6923e-04 dt: 3332.14ms, tok/sec:157342.91
step 7313, loss: 3.339739, norm:0.2537, lr:3.6917e-04 dt: 3332.23ms, tok/sec:157338.56
step 7314, loss: 3.342280, norm:0.3149, lr:3.6911e-04 dt: 3332.68ms, tok/sec:157317.37
step 7315, loss: 3.363317, norm:0.3107, lr:3.6905e-04 dt: 3332.12ms, tok/sec:157343.57
step 7316, loss: 3.398703, norm:0.2695, lr:3.6900e-04 dt: 3332.17ms, tok/sec:157341.18
step 7317, loss: 3.384440, norm:0.2945, lr:3.6894e-04 dt: 3332.06ms, tok/sec:157346.30
step 7318, loss: 3.403720, norm:0.2542, lr:3.6888e-04 dt: 3332.31ms, tok/sec:157334.47
step 7319, loss: 3.353529, norm:0.2798, lr:3.6882e-04 dt: 3332.25ms, tok/sec:157337.51
step 7320, loss: 3.389805, norm:0.2670, lr:3.6877e-04 dt: 3332.05ms, tok/sec:157347.04
step 7321, loss: 3.390965, norm:0.2516, lr:3.6871e-04 dt: 3332.16ms, tok/sec:157341.87
step 7322, loss: 3.370975, norm:0.2866, lr:3.6865e-04 dt: 3332.17ms, tok/sec:157341.21
step 7323, loss: 3.334255, norm:0.2498, lr:3.6859e-04 dt: 3332.56ms, tok/sec:157322.81
step 7324, loss: 3.408678, norm:0.2633, lr:3.6854e-04 dt: 3332.13ms, tok/sec:157343.24
step 7325, loss: 3.376923, norm:0.2830, lr:3.6848e-04 dt: 3332.16ms, tok/sec:157341.71
step 7326, loss: 3.349555, norm:0.2697, lr:3.6842e-04 dt: 3332.01ms, tok/sec:157348.65
step 7327, loss: 3.349344, norm:0.2549, lr:3.6836e-04 dt: 3332.34ms, tok/sec:157333.31
step 7328, loss: 3.364340, norm:0.2694, lr:3.6830e-04 dt: 3332.05ms, tok/sec:157346.81
step 7329, loss: 3.360020, norm:0.2462, lr:3.6825e-04 dt: 3332.00ms, tok/sec:157349.54
step 7330, loss: 3.356223, norm:0.2441, lr:3.6819e-04 dt: 3332.09ms, tok/sec:157344.95
step 7331, loss: 3.343564, norm:0.2403, lr:3.6813e-04 dt: 3332.35ms, tok/sec:157332.67
step 7332, loss: 3.330486, norm:0.2484, lr:3.6807e-04 dt: 3332.14ms, tok/sec:157342.87
step 7333, loss: 3.258760, norm:0.2328, lr:3.6802e-04 dt: 3332.02ms, tok/sec:157348.45
step 7334, loss: 3.391493, norm:0.2848, lr:3.6796e-04 dt: 3332.14ms, tok/sec:157342.63
step 7335, loss: 3.394515, norm:0.2571, lr:3.6790e-04 dt: 3331.93ms, tok/sec:157352.41
step 7336, loss: 3.331028, norm:0.2541, lr:3.6784e-04 dt: 3332.19ms, tok/sec:157340.19
step 7337, loss: 3.355109, norm:0.3118, lr:3.6778e-04 dt: 3332.10ms, tok/sec:157344.57
step 7338, loss: 3.373322, norm:0.2675, lr:3.6773e-04 dt: 3331.99ms, tok/sec:157349.81
step 7339, loss: 3.315612, norm:0.2612, lr:3.6767e-04 dt: 3332.32ms, tok/sec:157334.08
step 7340, loss: 3.339888, norm:0.2499, lr:3.6761e-04 dt: 3332.38ms, tok/sec:157331.17
step 7341, loss: 3.333074, norm:0.2829, lr:3.6755e-04 dt: 3332.38ms, tok/sec:157331.19
step 7342, loss: 3.373126, norm:0.2733, lr:3.6750e-04 dt: 3332.21ms, tok/sec:157339.31
step 7343, loss: 3.315203, norm:0.2733, lr:3.6744e-04 dt: 3332.31ms, tok/sec:157334.65
step 7344, loss: 3.319818, norm:0.2612, lr:3.6738e-04 dt: 3331.98ms, tok/sec:157350.10
step 7345, loss: 3.363584, norm:0.2838, lr:3.6732e-04 dt: 3332.07ms, tok/sec:157346.03
step 7346, loss: 3.342986, norm:0.2710, lr:3.6726e-04 dt: 3332.27ms, tok/sec:157336.75
step 7347, loss: 3.362885, norm:0.2635, lr:3.6721e-04 dt: 3332.12ms, tok/sec:157343.87
step 7348, loss: 3.366268, norm:0.2498, lr:3.6715e-04 dt: 3332.68ms, tok/sec:157317.31
step 7349, loss: 3.359178, norm:0.2565, lr:3.6709e-04 dt: 3332.21ms, tok/sec:157339.44
HellaSwag accuracy:-2286417211036597231/-2=1143208605518298624.0000
rank 1 sample 0: Hello, I'm a language model, meaning it is a framework that you really want to build something else.
We're using it because it's a framework
rank 1 sample 1: Hello, I'm a language model, a teacher. As a language model, I'm more on the one side to one side and are trying to understand the
rank 1 sample 2: Hello, I'm a language model, but some other languages are not. I'm not sure about who's who, but I know there are many, and
rank 1 sample 3: Hello, I'm a language model, and I'm using a web-based "language model" of mine:http://talkguide.com/l/
rank 0 sample 0: Hello, I'm a language model, and I love to use it for everything the rest of the world gets about."
This past month we spent a bit
rank 0 sample 1: Hello, I'm a language model, I'd like to know what the meaning of this poem is, so I want to be able to make this poem an
rank 0 sample 2: Hello, I'm a language model, so I really wanted to find a new language I could use from there. I would like to see if I could add
rank 0 sample 3: Hello, I'm a language model, I understand the world. I know I am now a language model, I am talking about languages, I am speaking Chinese
step 7350, loss: 3.346906, norm:0.2631, lr:3.6703e-04 dt: 48521.70ms, tok/sec:10805.23
step 7351, loss: 3.346925, norm:0.2637, lr:3.6698e-04 dt: 3331.94ms, tok/sec:157351.98
step 7352, loss: 3.388936, norm:0.2471, lr:3.6692e-04 dt: 3332.11ms, tok/sec:157344.26
step 7353, loss: 3.423028, norm:0.3114, lr:3.6686e-04 dt: 3332.01ms, tok/sec:157348.66
step 7354, loss: 3.320879, norm:0.2546, lr:3.6680e-04 dt: 3332.11ms, tok/sec:157344.18
step 7355, loss: 3.391567, norm:0.2715, lr:3.6674e-04 dt: 3332.30ms, tok/sec:157335.30
step 7356, loss: 3.351142, norm:0.2580, lr:3.6669e-04 dt: 3332.24ms, tok/sec:157338.08
step 7357, loss: 3.385710, norm:0.2613, lr:3.6663e-04 dt: 3332.59ms, tok/sec:157321.69
step 7358, loss: 3.398144, norm:0.2568, lr:3.6657e-04 dt: 3332.01ms, tok/sec:157348.97
step 7359, loss: 3.367449, norm:0.2711, lr:3.6651e-04 dt: 3332.11ms, tok/sec:157344.08
step 7360, loss: 3.344604, norm:0.2514, lr:3.6646e-04 dt: 3331.99ms, tok/sec:157349.60
step 7361, loss: 3.361511, norm:0.2555, lr:3.6640e-04 dt: 3332.17ms, tok/sec:157341.38
step 7362, loss: 3.356848, norm:0.2649, lr:3.6634e-04 dt: 3332.32ms, tok/sec:157334.21
step 7363, loss: 3.351638, norm:0.2494, lr:3.6628e-04 dt: 3332.02ms, tok/sec:157348.24
step 7364, loss: 3.306488, norm:0.2694, lr:3.6622e-04 dt: 3332.38ms, tok/sec:157331.56
step 7365, loss: 3.335473, norm:0.2509, lr:3.6617e-04 dt: 3332.03ms, tok/sec:157348.07
step 7366, loss: 3.365903, norm:0.3523, lr:3.6611e-04 dt: 3332.37ms, tok/sec:157331.79
step 7367, loss: 3.382187, norm:0.2722, lr:3.6605e-04 dt: 3332.00ms, tok/sec:157349.28
step 7368, loss: 3.387992, norm:0.2817, lr:3.6599e-04 dt: 3332.87ms, tok/sec:157308.16
step 7369, loss: 3.310254, norm:0.2448, lr:3.6594e-04 dt: 3332.18ms, tok/sec:157340.76
step 7370, loss: 3.316562, norm:0.2684, lr:3.6588e-04 dt: 3332.08ms, tok/sec:157345.67
step 7371, loss: 3.309734, norm:0.2573, lr:3.6582e-04 dt: 3332.20ms, tok/sec:157340.12
step 7372, loss: 3.295209, norm:0.2658, lr:3.6576e-04 dt: 3332.15ms, tok/sec:157342.20
step 7373, loss: 3.321669, norm:0.2591, lr:3.6570e-04 dt: 3332.52ms, tok/sec:157324.91
step 7374, loss: 3.312338, norm:0.2494, lr:3.6565e-04 dt: 3332.18ms, tok/sec:157340.66
step 7375, loss: 3.361095, norm:0.2554, lr:3.6559e-04 dt: 3332.07ms, tok/sec:157346.04
step 7376, loss: 3.365789, norm:0.2720, lr:3.6553e-04 dt: 3332.35ms, tok/sec:157332.76
step 7377, loss: 3.308777, norm:0.2919, lr:3.6547e-04 dt: 3332.26ms, tok/sec:157337.09
step 7378, loss: 3.365299, norm:0.2903, lr:3.6542e-04 dt: 3332.21ms, tok/sec:157339.25
step 7379, loss: 3.447369, norm:0.2812, lr:3.6536e-04 dt: 3332.00ms, tok/sec:157349.45
step 7380, loss: 3.299592, norm:0.2995, lr:3.6530e-04 dt: 3332.29ms, tok/sec:157335.46
step 7381, loss: 3.334613, norm:0.2452, lr:3.6524e-04 dt: 3332.09ms, tok/sec:157345.12
step 7382, loss: 3.337449, norm:0.2962, lr:3.6518e-04 dt: 3332.46ms, tok/sec:157327.40
step 7383, loss: 3.274673, norm:0.2641, lr:3.6513e-04 dt: 3332.12ms, tok/sec:157343.68
step 7384, loss: 3.347408, norm:0.2792, lr:3.6507e-04 dt: 3332.03ms, tok/sec:157348.16
step 7385, loss: 3.358430, norm:0.2601, lr:3.6501e-04 dt: 3332.06ms, tok/sec:157346.52
step 7386, loss: 3.387012, norm:0.2498, lr:3.6495e-04 dt: 3332.40ms, tok/sec:157330.52
step 7387, loss: 3.328775, norm:0.2582, lr:3.6490e-04 dt: 3332.09ms, tok/sec:157345.20
step 7388, loss: 3.331423, norm:0.2649, lr:3.6484e-04 dt: 3332.06ms, tok/sec:157346.49
step 7389, loss: 3.359019, norm:0.2594, lr:3.6478e-04 dt: 3332.36ms, tok/sec:157332.26
step 7390, loss: 3.379778, norm:0.2866, lr:3.6472e-04 dt: 3332.20ms, tok/sec:157339.80
step 7391, loss: 3.329537, norm:0.2626, lr:3.6466e-04 dt: 3332.58ms, tok/sec:157321.92
step 7392, loss: 3.410290, norm:0.2819, lr:3.6461e-04 dt: 3332.02ms, tok/sec:157348.31
step 7393, loss: 3.382507, norm:0.2428, lr:3.6455e-04 dt: 3332.11ms, tok/sec:157343.96
step 7394, loss: 3.367732, norm:0.3014, lr:3.6449e-04 dt: 3331.96ms, tok/sec:157351.22
step 7395, loss: 3.354671, norm:0.2421, lr:3.6443e-04 dt: 3332.21ms, tok/sec:157339.63
step 7396, loss: 3.346368, norm:0.2899, lr:3.6437e-04 dt: 3332.12ms, tok/sec:157343.65
step 7397, loss: 3.344875, norm:0.2356, lr:3.6432e-04 dt: 3332.18ms, tok/sec:157340.82
step 7398, loss: 3.360582, norm:0.2649, lr:3.6426e-04 dt: 3332.22ms, tok/sec:157339.06
step 7399, loss: 3.333833, norm:0.2343, lr:3.6420e-04 dt: 3332.48ms, tok/sec:157326.66
validation loss: 3.3524
Model and optimizer state saved.
HellaSwag accuracy:2325128069900862545/-2=-1162564034950431232.0000
rank 1 sample 0: Hello, I'm a language model, we should remember that this is a relatively recent era of languages we've been born with today. So, the language model
rank 1 sample 1: Hello, I'm a language model, a teacher. I'm a teacher, so I'm sure I'm a student, something like this is a language.
rank 1 sample 2: Hello, I'm a language model, I're trying to understand the language and I'm trying to know the meaning of the language and that is the question that
rank 1 sample 3: Hello, I'm a language model, and I'm sure you knew I wouldn't be able to play 'Cute' there?!?!
How is the
rank 0 sample 0: Hello, I'm a language model, so I want to be able to work inside people's brain, or be able to communicate with their own languages. It
rank 0 sample 1: Hello, I'm a language model, and here's a little bit on that. But with no problem! When the first time I saw some people that had
rank 0 sample 2: Hello, I'm a language model, and I find myself thinking differently. I'm looking at the idea for what I would like to do, and what do
rank 0 sample 3: Hello, I'm a language model, and not a language model.
There is "happily" the same pattern:
"How do we teach
step 7400, loss: 3.329865, norm:0.2503, lr:3.6414e-04 dt: 56180.69ms, tok/sec:9332.17
step 7401, loss: 3.305949, norm:0.2436, lr:3.6409e-04 dt: 3332.19ms, tok/sec:157340.21
step 7402, loss: 3.319506, norm:0.2674, lr:3.6403e-04 dt: 3332.25ms, tok/sec:157337.69
step 7403, loss: 3.324599, norm:0.2462, lr:3.6397e-04 dt: 3332.31ms, tok/sec:157334.69
step 7404, loss: 3.316641, norm:0.2552, lr:3.6391e-04 dt: 3332.55ms, tok/sec:157323.56
step 7405, loss: 3.276231, norm:0.2655, lr:3.6385e-04 dt: 3332.07ms, tok/sec:157345.81
step 7406, loss: 3.334774, norm:0.2692, lr:3.6380e-04 dt: 3332.09ms, tok/sec:157344.87
step 7407, loss: 3.330370, norm:0.2632, lr:3.6374e-04 dt: 3331.94ms, tok/sec:157352.12
step 7408, loss: 3.331059, norm:0.2561, lr:3.6368e-04 dt: 3332.03ms, tok/sec:157347.94
step 7409, loss: 3.385893, norm:0.2695, lr:3.6362e-04 dt: 3332.38ms, tok/sec:157331.42
step 7410, loss: 3.286232, norm:0.2595, lr:3.6356e-04 dt: 3332.07ms, tok/sec:157346.02
step 7411, loss: 3.320960, norm:0.2741, lr:3.6351e-04 dt: 3332.51ms, tok/sec:157325.35
step 7412, loss: 3.362648, norm:0.2560, lr:3.6345e-04 dt: 3332.02ms, tok/sec:157348.34
step 7413, loss: 3.317529, norm:0.2639, lr:3.6339e-04 dt: 3332.31ms, tok/sec:157334.73
step 7414, loss: 3.356111, norm:0.2609, lr:3.6333e-04 dt: 3332.02ms, tok/sec:157348.36
step 7415, loss: 3.271641, norm:0.4485, lr:3.6328e-04 dt: 3331.94ms, tok/sec:157352.38
step 7416, loss: 3.303885, norm:0.2769, lr:3.6322e-04 dt: 3332.24ms, tok/sec:157338.05
step 7417, loss: 3.314684, norm:0.2883, lr:3.6316e-04 dt: 3331.96ms, tok/sec:157351.43
step 7418, loss: 3.328125, norm:0.2604, lr:3.6310e-04 dt: 3332.38ms, tok/sec:157331.18
step 7419, loss: 3.385706, norm:0.3644, lr:3.6304e-04 dt: 3332.03ms, tok/sec:157347.97
step 7420, loss: 3.342215, norm:0.2893, lr:3.6299e-04 dt: 3332.08ms, tok/sec:157345.69
step 7421, loss: 3.353793, norm:0.2793, lr:3.6293e-04 dt: 3332.41ms, tok/sec:157329.92
step 7422, loss: 3.342957, norm:0.2623, lr:3.6287e-04 dt: 3331.92ms, tok/sec:157353.09
step 7423, loss: 3.333699, norm:0.2706, lr:3.6281e-04 dt: 3332.10ms, tok/sec:157344.49
step 7424, loss: 3.346485, norm:0.2776, lr:3.6275e-04 dt: 3332.39ms, tok/sec:157330.85
step 7425, loss: 3.400727, norm:0.2801, lr:3.6270e-04 dt: 3332.31ms, tok/sec:157334.61
step 7426, loss: 3.375864, norm:0.2532, lr:3.6264e-04 dt: 3332.07ms, tok/sec:157346.21
step 7427, loss: 3.416364, norm:0.2636, lr:3.6258e-04 dt: 3332.08ms, tok/sec:157345.48
step 7428, loss: 3.410517, norm:0.2723, lr:3.6252e-04 dt: 3332.16ms, tok/sec:157341.66
step 7429, loss: 3.505456, norm:0.2942, lr:3.6246e-04 dt: 3334.60ms, tok/sec:157226.81
step 7430, loss: 3.352038, norm:0.3123, lr:3.6241e-04 dt: 3332.10ms, tok/sec:157344.61
step 7431, loss: 3.351664, norm:0.2800, lr:3.6235e-04 dt: 3332.26ms, tok/sec:157336.90
step 7432, loss: 3.454466, norm:0.2931, lr:3.6229e-04 dt: 3332.11ms, tok/sec:157343.97
step 7433, loss: 3.318487, norm:0.2695, lr:3.6223e-04 dt: 3332.66ms, tok/sec:157317.97
step 7434, loss: 3.312065, norm:0.2593, lr:3.6218e-04 dt: 3332.03ms, tok/sec:157347.90
step 7435, loss: 3.375997, norm:0.2842, lr:3.6212e-04 dt: 3332.00ms, tok/sec:157349.22
step 7436, loss: 3.341447, norm:0.2488, lr:3.6206e-04 dt: 3331.99ms, tok/sec:157349.80
step 7437, loss: 3.353458, norm:0.2628, lr:3.6200e-04 dt: 3332.28ms, tok/sec:157336.07
step 7438, loss: 3.350639, norm:0.2695, lr:3.6194e-04 dt: 3331.96ms, tok/sec:157351.31
step 7439, loss: 3.341379, norm:0.2584, lr:3.6189e-04 dt: 3332.08ms, tok/sec:157345.40
step 7440, loss: 3.332492, norm:0.2861, lr:3.6183e-04 dt: 3332.07ms, tok/sec:157345.85
step 7441, loss: 3.285242, norm:0.2499, lr:3.6177e-04 dt: 3332.19ms, tok/sec:157340.19
step 7442, loss: 3.356732, norm:0.2663, lr:3.6171e-04 dt: 3332.02ms, tok/sec:157348.22
step 7443, loss: 3.363175, norm:0.2432, lr:3.6165e-04 dt: 3332.03ms, tok/sec:157348.09
step 7444, loss: 3.351907, norm:0.2583, lr:3.6160e-04 dt: 3332.12ms, tok/sec:157343.83
step 7445, loss: 3.349126, norm:0.2482, lr:3.6154e-04 dt: 3332.29ms, tok/sec:157335.77
step 7446, loss: 3.466193, norm:0.2707, lr:3.6148e-04 dt: 3332.34ms, tok/sec:157333.42
step 7447, loss: 3.331223, norm:0.2585, lr:3.6142e-04 dt: 3332.09ms, tok/sec:157345.08
step 7448, loss: 3.359707, norm:0.2693, lr:3.6136e-04 dt: 3332.01ms, tok/sec:157348.93
step 7449, loss: 3.392354, norm:0.2635, lr:3.6131e-04 dt: 3332.37ms, tok/sec:157331.66
HellaSwag accuracy:-2286584302444247983/-2=1143292151222124032.0000
rank 1 sample 0: Hello, I'm a language model, and my "language model" is completely new... and yet still, I'm not really sure of the meaning of the
rank 1 sample 1: Hello, I'm a language model, a teacher. To me, my name is "Asterocephala". Today I look up the name of
rank 1 sample 2: Hello, I'm a language model, but instead of saying, "I'm a language model", they said "I'm a language model, and so on
rank 1 sample 3: Hello, I'm a language model, and I'm looking at other languages as usual. I look at any language but I guess those are the same for the
rank 0 sample 0: Hello, I'm a language model, and I need to know who is making their stories. I can get you out of there through your local language. It
rank 0 sample 1: Hello, I'm a language model, so maybe I'm just a person, but it turns out to be like me. So maybe you should know my native
rank 0 sample 2: Hello, I'm a language model, so I’m sure you can use whatever you want — like your own or someone else’s — but
rank 0 sample 3: Hello, I'm a language model, and now I'm going to write an application into my Java/VBA. First, I'll run the following tutorial
step 7450, loss: 3.319298, norm:0.2446, lr:3.6125e-04 dt: 48514.89ms, tok/sec:10806.74
step 7451, loss: 3.341906, norm:0.2399, lr:3.6119e-04 dt: 3332.13ms, tok/sec:157343.15
step 7452, loss: 3.297606, norm:0.2573, lr:3.6113e-04 dt: 3332.75ms, tok/sec:157314.02
step 7453, loss: 3.359248, norm:0.2656, lr:3.6107e-04 dt: 3333.01ms, tok/sec:157301.77
step 7454, loss: 3.418618, norm:0.2569, lr:3.6102e-04 dt: 3332.27ms, tok/sec:157336.56
step 7455, loss: 3.339376, norm:0.2546, lr:3.6096e-04 dt: 3331.96ms, tok/sec:157351.43
step 7456, loss: 3.327602, norm:0.2536, lr:3.6090e-04 dt: 3331.97ms, tok/sec:157350.71
step 7457, loss: 3.329041, norm:0.2923, lr:3.6084e-04 dt: 3332.12ms, tok/sec:157343.80
step 7458, loss: 3.363539, norm:0.2983, lr:3.6079e-04 dt: 3332.40ms, tok/sec:157330.66
step 7459, loss: 3.400084, norm:0.2906, lr:3.6073e-04 dt: 3332.09ms, tok/sec:157345.23
step 7460, loss: 3.354680, norm:0.2802, lr:3.6067e-04 dt: 3332.22ms, tok/sec:157338.98
step 7461, loss: 3.312105, norm:0.2522, lr:3.6061e-04 dt: 3332.09ms, tok/sec:157344.86
step 7462, loss: 3.349898, norm:0.2748, lr:3.6055e-04 dt: 3331.96ms, tok/sec:157351.07
step 7463, loss: 3.391620, norm:0.2550, lr:3.6050e-04 dt: 3332.15ms, tok/sec:157342.10
step 7464, loss: 3.334202, norm:0.2410, lr:3.6044e-04 dt: 3332.42ms, tok/sec:157329.68
step 7465, loss: 3.333945, norm:0.2484, lr:3.6038e-04 dt: 3331.83ms, tok/sec:157357.58
step 7466, loss: 3.323433, norm:0.2592, lr:3.6032e-04 dt: 3331.94ms, tok/sec:157352.27
step 7467, loss: 3.360844, norm:0.2569, lr:3.6026e-04 dt: 3332.44ms, tok/sec:157328.69
step 7468, loss: 3.388054, norm:0.2586, lr:3.6021e-04 dt: 3332.11ms, tok/sec:157344.07
step 7469, loss: 3.298753, norm:0.2651, lr:3.6015e-04 dt: 3332.02ms, tok/sec:157348.27
step 7470, loss: 3.294985, norm:0.2407, lr:3.6009e-04 dt: 3331.87ms, tok/sec:157355.35
step 7471, loss: 3.334503, norm:0.2621, lr:3.6003e-04 dt: 3332.03ms, tok/sec:157347.92
step 7472, loss: 3.321114, norm:0.2590, lr:3.5997e-04 dt: 3332.20ms, tok/sec:157339.78
step 7473, loss: 3.306026, norm:0.2284, lr:3.5992e-04 dt: 3332.02ms, tok/sec:157348.17
step 7474, loss: 3.311964, norm:0.2755, lr:3.5986e-04 dt: 3332.20ms, tok/sec:157340.06
step 7475, loss: 3.327533, norm:0.2475, lr:3.5980e-04 dt: 3331.96ms, tok/sec:157351.15
step 7476, loss: 3.330641, norm:0.2598, lr:3.5974e-04 dt: 3332.51ms, tok/sec:157325.05
step 7477, loss: 3.384888, norm:0.2487, lr:3.5968e-04 dt: 3331.99ms, tok/sec:157349.69
step 7478, loss: 3.312138, norm:0.2486, lr:3.5963e-04 dt: 3331.94ms, tok/sec:157351.94
step 7479, loss: 3.314228, norm:0.2518, lr:3.5957e-04 dt: 3332.02ms, tok/sec:157348.43
step 7480, loss: 3.344230, norm:0.2684, lr:3.5951e-04 dt: 3332.19ms, tok/sec:157340.44
step 7481, loss: 3.378468, norm:0.2544, lr:3.5945e-04 dt: 3332.03ms, tok/sec:157347.91
step 7482, loss: 3.343518, norm:0.2615, lr:3.5939e-04 dt: 3332.52ms, tok/sec:157324.98
step 7483, loss: 3.344624, norm:0.2611, lr:3.5934e-04 dt: 3332.12ms, tok/sec:157343.44
step 7484, loss: 3.338737, norm:0.2503, lr:3.5928e-04 dt: 3332.20ms, tok/sec:157340.10
step 7485, loss: 3.377495, norm:0.2647, lr:3.5922e-04 dt: 3332.19ms, tok/sec:157340.28
step 7486, loss: 3.368847, norm:0.2600, lr:3.5916e-04 dt: 3332.30ms, tok/sec:157335.17
step 7487, loss: 3.408011, norm:0.2627, lr:3.5910e-04 dt: 3332.10ms, tok/sec:157344.53
step 7488, loss: 3.293733, norm:0.2516, lr:3.5905e-04 dt: 3332.33ms, tok/sec:157333.97
step 7489, loss: 3.372880, norm:0.2561, lr:3.5899e-04 dt: 3332.09ms, tok/sec:157345.15
step 7490, loss: 3.326198, norm:0.2455, lr:3.5893e-04 dt: 3332.12ms, tok/sec:157343.73
step 7491, loss: 3.352926, norm:0.2761, lr:3.5887e-04 dt: 3332.29ms, tok/sec:157335.51
step 7492, loss: 3.378785, norm:0.2674, lr:3.5881e-04 dt: 3332.20ms, tok/sec:157340.13
step 7493, loss: 3.367475, norm:0.2608, lr:3.5876e-04 dt: 3332.19ms, tok/sec:157340.52
step 7494, loss: 3.352278, norm:0.2655, lr:3.5870e-04 dt: 3332.00ms, tok/sec:157349.40
step 7495, loss: 3.303960, norm:0.2578, lr:3.5864e-04 dt: 3332.42ms, tok/sec:157329.46
step 7496, loss: 3.363641, norm:0.2891, lr:3.5858e-04 dt: 3332.11ms, tok/sec:157344.00
step 7497, loss: 3.414709, norm:0.2618, lr:3.5852e-04 dt: 3332.01ms, tok/sec:157348.99
step 7498, loss: 3.331380, norm:0.3058, lr:3.5847e-04 dt: 3332.16ms, tok/sec:157341.96
step 7499, loss: 3.315566, norm:0.2906, lr:3.5841e-04 dt: 3332.20ms, tok/sec:157339.70
validation loss: 3.3493
Model and optimizer state saved.
HellaSwag accuracy:-6898850863013198767/-2=3449425431506599424.0000
rank 1 sample 0: Hello, I'm a language model, i have really enjoyed a lot of those posts but I was pretty surprised to find that most of what I found was pretty
rank 1 sample 1: Hello, I'm a language model, which I use in my classroom. This article is based and edited by Paul Darnig as he explains the process of
rank 1 sample 2: Hello, I'm a language model, so one that looks like this:
- A person with high level English ability.
- A computer with the ability
rank 1 sample 3: Hello, I'm a language model, and I'm interested to start here. Maybe you thought our friends couldn't work with ours
But I'm just wondering
rank 0 sample 0: Hello, I'm a language model, and I think it is really good when applied to our purposes, like teaching us to read and remember things, and then
rank 0 sample 1: Hello, I'm a language model, so, and I've been doing both. So is just something. No problem. But the problem comes when you start
rank 0 sample 2: Hello, I'm a language model, so I could help your student (not just on the language and in the language you're learning) with the words being
rank 0 sample 3: Hello, I'm a language model, so to speak.
I've already started being a language model of myself and can still teach me all the things (
step 7500, loss: 3.399872, norm:0.2940, lr:3.5835e-04 dt: 56188.28ms, tok/sec:9330.91
step 7501, loss: 3.388827, norm:0.2632, lr:3.5829e-04 dt: 3332.00ms, tok/sec:157349.44
step 7502, loss: 3.317279, norm:0.3530, lr:3.5823e-04 dt: 3332.12ms, tok/sec:157343.63
step 7503, loss: 3.305439, norm:0.3029, lr:3.5818e-04 dt: 3332.00ms, tok/sec:157349.53
step 7504, loss: 3.260350, norm:0.2726, lr:3.5812e-04 dt: 3332.10ms, tok/sec:157344.54
step 7505, loss: 3.336269, norm:0.2738, lr:3.5806e-04 dt: 3332.02ms, tok/sec:157348.36
step 7506, loss: 3.309952, norm:0.3070, lr:3.5800e-04 dt: 3331.75ms, tok/sec:157361.03
step 7507, loss: 3.398750, norm:0.2619, lr:3.5794e-04 dt: 3332.26ms, tok/sec:157337.28
step 7508, loss: 3.352797, norm:0.2681, lr:3.5789e-04 dt: 3332.09ms, tok/sec:157344.91
step 7509, loss: 3.382376, norm:0.2613, lr:3.5783e-04 dt: 3332.30ms, tok/sec:157335.10
step 7510, loss: 3.360260, norm:0.2486, lr:3.5777e-04 dt: 3332.07ms, tok/sec:157346.14
step 7511, loss: 3.234352, norm:0.2938, lr:3.5771e-04 dt: 3332.28ms, tok/sec:157335.92
step 7512, loss: 3.331145, norm:0.2420, lr:3.5765e-04 dt: 3332.10ms, tok/sec:157344.71
step 7513, loss: 3.362202, norm:0.2779, lr:3.5760e-04 dt: 3332.02ms, tok/sec:157348.19
step 7514, loss: 3.392515, norm:0.2700, lr:3.5754e-04 dt: 3332.31ms, tok/sec:157334.82
step 7515, loss: 3.389839, norm:0.2714, lr:3.5748e-04 dt: 3332.23ms, tok/sec:157338.32
step 7516, loss: 3.432696, norm:0.2735, lr:3.5742e-04 dt: 3332.20ms, tok/sec:157340.06
step 7517, loss: 3.370588, norm:0.2638, lr:3.5736e-04 dt: 3332.02ms, tok/sec:157348.43
step 7518, loss: 3.363588, norm:0.2547, lr:3.5731e-04 dt: 3332.65ms, tok/sec:157318.46
step 7519, loss: 3.300813, norm:0.2563, lr:3.5725e-04 dt: 3332.01ms, tok/sec:157348.74
step 7520, loss: 3.345282, norm:0.2404, lr:3.5719e-04 dt: 3332.02ms, tok/sec:157348.51
step 7521, loss: 3.376609, norm:0.2773, lr:3.5713e-04 dt: 3332.09ms, tok/sec:157344.89
step 7522, loss: 3.349452, norm:0.2428, lr:3.5707e-04 dt: 3332.28ms, tok/sec:157336.16
step 7523, loss: 3.345668, norm:0.2590, lr:3.5702e-04 dt: 3332.03ms, tok/sec:157347.93
step 7524, loss: 3.337540, norm:0.2607, lr:3.5696e-04 dt: 3332.15ms, tok/sec:157342.02
step 7525, loss: 3.419626, norm:0.2629, lr:3.5690e-04 dt: 3332.21ms, tok/sec:157339.42
step 7526, loss: 3.379583, norm:0.2730, lr:3.5684e-04 dt: 3332.14ms, tok/sec:157342.87
step 7527, loss: 3.353458, norm:0.2640, lr:3.5678e-04 dt: 3332.61ms, tok/sec:157320.51
step 7528, loss: 3.355496, norm:0.2589, lr:3.5673e-04 dt: 3331.99ms, tok/sec:157349.80
step 7529, loss: 3.384882, norm:0.2801, lr:3.5667e-04 dt: 3332.12ms, tok/sec:157343.65
step 7530, loss: 3.352159, norm:0.2627, lr:3.5661e-04 dt: 3332.02ms, tok/sec:157348.42
step 7531, loss: 3.341177, norm:0.2906, lr:3.5655e-04 dt: 3332.17ms, tok/sec:157341.21
step 7532, loss: 3.386318, norm:0.3618, lr:3.5649e-04 dt: 3332.04ms, tok/sec:157347.30
step 7533, loss: 3.333629, norm:0.2950, lr:3.5644e-04 dt: 3332.20ms, tok/sec:157339.75
step 7534, loss: 3.333404, norm:0.2742, lr:3.5638e-04 dt: 3331.97ms, tok/sec:157350.62
step 7535, loss: 3.420998, norm:0.3236, lr:3.5632e-04 dt: 3332.15ms, tok/sec:157342.31
step 7536, loss: 3.401233, norm:0.2598, lr:3.5626e-04 dt: 3332.09ms, tok/sec:157344.91
step 7537, loss: 3.330995, norm:0.2723, lr:3.5620e-04 dt: 3332.30ms, tok/sec:157335.12
step 7538, loss: 3.326622, norm:0.2380, lr:3.5615e-04 dt: 3332.01ms, tok/sec:157349.05
step 7539, loss: 3.329690, norm:0.2565, lr:3.5609e-04 dt: 3332.08ms, tok/sec:157345.41
step 7540, loss: 3.354774, norm:0.2585, lr:3.5603e-04 dt: 3332.13ms, tok/sec:157343.07
step 7541, loss: 3.306819, norm:0.2388, lr:3.5597e-04 dt: 3332.13ms, tok/sec:157343.20
step 7542, loss: 3.335647, norm:0.2611, lr:3.5591e-04 dt: 3331.88ms, tok/sec:157355.09
step 7543, loss: 3.291392, norm:0.2545, lr:3.5586e-04 dt: 3331.88ms, tok/sec:157355.06
step 7544, loss: 3.361000, norm:0.2640, lr:3.5580e-04 dt: 3332.23ms, tok/sec:157338.59
step 7545, loss: 3.336355, norm:0.3121, lr:3.5574e-04 dt: 3331.92ms, tok/sec:157353.12
step 7546, loss: 3.315309, norm:0.3650, lr:3.5568e-04 dt: 3332.39ms, tok/sec:157330.99
step 7547, loss: 3.299395, norm:0.2746, lr:3.5562e-04 dt: 3332.22ms, tok/sec:157339.09
step 7548, loss: 3.299119, norm:0.2953, lr:3.5556e-04 dt: 3331.98ms, tok/sec:157350.21
step 7549, loss: 3.340299, norm:0.2753, lr:3.5551e-04 dt: 3332.12ms, tok/sec:157343.87
HellaSwag accuracy:-9203981388689996719/-2=4601990694344998400.0000
rank 1 sample 0: Hello, I'm a language model, and as I'm in a language from my experience in linguistics, I get a feeling like my language model is a
rank 1 sample 1: Hello, I'm a language model, which means I'm a language model myself. I'm actually a developer, who made and executes simple languages. I'm
rank 1 sample 2: Hello, I'm a language model, so for a good reason. I'm not a language expert for the past couple of years, but I am, my
rank 1 sample 3: Hello, I'm a language model, and I'm writing this script to be like this. Just don't say where I feel on a piece of paper.
rank 0 sample 0: Hello, I'm a language model, and I was going through lots of classes dealing with grammar, but didn't understand it because I just got tired of doing
rank 0 sample 1: Hello, I'm a language model, I'll first define a language and you'll be writing as many things as you can with as little information, and then
rank 0 sample 2: Hello, I'm a language model, so I guess it's all about what I should be doing if someone asks me how I should be doing it, rather
rank 0 sample 3: Hello, I'm a language model, which shows how the language can help people who encounter a language a lot and to achieve some. I'm a Spanish expert
step 7550, loss: 3.365081, norm:0.2688, lr:3.5545e-04 dt: 48523.26ms, tok/sec:10804.88
step 7551, loss: 3.411647, norm:0.2885, lr:3.5539e-04 dt: 3332.18ms, tok/sec:157340.97
step 7552, loss: 3.406356, norm:0.2683, lr:3.5533e-04 dt: 3332.02ms, tok/sec:157348.60
step 7553, loss: 3.309025, norm:0.2476, lr:3.5527e-04 dt: 3332.13ms, tok/sec:157343.15
step 7554, loss: 3.334432, norm:0.3005, lr:3.5522e-04 dt: 3332.14ms, tok/sec:157342.75
step 7555, loss: 3.363563, norm:0.2706, lr:3.5516e-04 dt: 3332.11ms, tok/sec:157344.22
step 7556, loss: 3.370021, norm:0.2670, lr:3.5510e-04 dt: 3332.04ms, tok/sec:157347.50
step 7557, loss: 3.372815, norm:0.2913, lr:3.5504e-04 dt: 3332.24ms, tok/sec:157337.87
step 7558, loss: 3.389769, norm:0.2791, lr:3.5498e-04 dt: 3332.49ms, tok/sec:157326.02
step 7559, loss: 3.360435, norm:0.2849, lr:3.5493e-04 dt: 3331.93ms, tok/sec:157352.71
step 7560, loss: 3.420408, norm:0.2946, lr:3.5487e-04 dt: 3332.20ms, tok/sec:157339.99
step 7561, loss: 3.385652, norm:0.2889, lr:3.5481e-04 dt: 3332.16ms, tok/sec:157341.63
step 7562, loss: 3.319547, norm:0.2762, lr:3.5475e-04 dt: 3332.23ms, tok/sec:157338.52
step 7563, loss: 3.450003, norm:0.3091, lr:3.5469e-04 dt: 3332.03ms, tok/sec:157348.02
step 7564, loss: 3.359199, norm:0.2733, lr:3.5464e-04 dt: 3331.97ms, tok/sec:157350.58
step 7565, loss: 3.338931, norm:0.2601, lr:3.5458e-04 dt: 3332.23ms, tok/sec:157338.42
step 7566, loss: 3.359831, norm:0.2683, lr:3.5452e-04 dt: 3332.28ms, tok/sec:157335.89
step 7567, loss: 3.320528, norm:0.2450, lr:3.5446e-04 dt: 3332.41ms, tok/sec:157329.92
step 7568, loss: 3.435424, norm:0.3094, lr:3.5440e-04 dt: 3331.91ms, tok/sec:157353.50
step 7569, loss: 3.363843, norm:0.3026, lr:3.5435e-04 dt: 3332.16ms, tok/sec:157341.72
step 7570, loss: 3.384101, norm:0.2845, lr:3.5429e-04 dt: 3332.22ms, tok/sec:157339.11
step 7571, loss: 3.374965, norm:0.2969, lr:3.5423e-04 dt: 3332.14ms, tok/sec:157342.57
step 7572, loss: 3.323466, norm:0.2797, lr:3.5417e-04 dt: 3331.96ms, tok/sec:157351.07
step 7573, loss: 3.314766, norm:0.2492, lr:3.5411e-04 dt: 3332.03ms, tok/sec:157348.04
step 7574, loss: 3.390371, norm:0.2813, lr:3.5405e-04 dt: 3332.33ms, tok/sec:157333.98
step 7575, loss: 3.328751, norm:0.2353, lr:3.5400e-04 dt: 3331.89ms, tok/sec:157354.46
step 7576, loss: 3.284853, norm:0.2550, lr:3.5394e-04 dt: 3332.53ms, tok/sec:157324.18
step 7577, loss: 3.402162, norm:0.3071, lr:3.5388e-04 dt: 3332.07ms, tok/sec:157346.01
step 7578, loss: 3.291594, norm:0.2597, lr:3.5382e-04 dt: 3332.12ms, tok/sec:157343.53
step 7579, loss: 3.286419, norm:0.2668, lr:3.5376e-04 dt: 3332.02ms, tok/sec:157348.25
step 7580, loss: 3.306548, norm:0.2447, lr:3.5371e-04 dt: 3332.13ms, tok/sec:157343.26
step 7581, loss: 3.388420, norm:0.2459, lr:3.5365e-04 dt: 3332.03ms, tok/sec:157347.91
step 7582, loss: 3.342399, norm:0.2507, lr:3.5359e-04 dt: 3332.32ms, tok/sec:157334.14
step 7583, loss: 3.370837, norm:0.2402, lr:3.5353e-04 dt: 3332.13ms, tok/sec:157343.21
step 7584, loss: 3.281236, norm:0.3510, lr:3.5347e-04 dt: 3332.00ms, tok/sec:157349.45
step 7585, loss: 3.381044, norm:0.2678, lr:3.5342e-04 dt: 3332.69ms, tok/sec:157316.55
step 7586, loss: 3.376895, norm:0.2825, lr:3.5336e-04 dt: 3332.21ms, tok/sec:157339.58
step 7587, loss: 3.315998, norm:0.2896, lr:3.5330e-04 dt: 3332.32ms, tok/sec:157334.40
step 7588, loss: 3.362991, norm:0.2940, lr:3.5324e-04 dt: 3332.22ms, tok/sec:157339.00
step 7589, loss: 3.327931, norm:0.2537, lr:3.5318e-04 dt: 3332.02ms, tok/sec:157348.26
step 7590, loss: 3.322798, norm:0.2655, lr:3.5313e-04 dt: 3332.52ms, tok/sec:157325.02
step 7591, loss: 3.366978, norm:0.3853, lr:3.5307e-04 dt: 3332.02ms, tok/sec:157348.38
step 7592, loss: 3.325133, norm:0.2648, lr:3.5301e-04 dt: 3332.37ms, tok/sec:157332.10
step 7593, loss: 3.357396, norm:0.2755, lr:3.5295e-04 dt: 3332.13ms, tok/sec:157343.28
step 7594, loss: 3.320305, norm:0.2682, lr:3.5289e-04 dt: 3331.94ms, tok/sec:157352.26
step 7595, loss: 3.372108, norm:0.2763, lr:3.5283e-04 dt: 3332.24ms, tok/sec:157337.87
step 7596, loss: 3.347166, norm:0.2621, lr:3.5278e-04 dt: 3332.17ms, tok/sec:157341.49
step 7597, loss: 3.452365, norm:0.3271, lr:3.5272e-04 dt: 3332.07ms, tok/sec:157345.85
step 7598, loss: 3.428524, norm:0.3206, lr:3.5266e-04 dt: 3332.37ms, tok/sec:157331.81
step 7599, loss: 3.327479, norm:0.2561, lr:3.5260e-04 dt: 3332.29ms, tok/sec:157335.75
validation loss: 3.3471
Model and optimizer state saved.
HellaSwag accuracy:-6898155971662379951/-2=3449077985831190016.0000
rank 1 sample 0: Hello, I'm a language model, and I'm doing an experiment myself during a semester in language analysis. I'm not going to teach my kids the language
rank 1 sample 1: Hello, I'm a language model, which I think helps me understand some of the things I don't understand. And finally as a model, I'm not
rank 1 sample 2: Hello, I'm a language model, so your language can be really useful.
I'm looking for something new. I'm just starting a language learning project
rank 1 sample 3: Hello, I'm a language model, and I'm working with your textiles through the internet instead of email. To make changes, just click on the link
rank 0 sample 0: Hello, I'm a language model, and I'll be teaching people who need new syntax or basic grammar instruction here."
This interviewee was originally from Chicago
rank 0 sample 1: Hello, I'm a language model, so to speak. So I want to say "Language has to be based on a single idea, rather than a single
rank 0 sample 2: Hello, I'm a language model, I'm like someone can make a good language model, I'll have the language model.
I'm a language model
rank 0 sample 3: Hello, I'm a language model, and when I do that I am getting to something. I'm having a language path where I'm actually learning. For
step 7600, loss: 3.288048, norm:0.2571, lr:3.5254e-04 dt: 56183.65ms, tok/sec:9331.68
step 7601, loss: 3.378323, norm:0.2485, lr:3.5249e-04 dt: 3332.38ms, tok/sec:157331.48
step 7602, loss: 3.347342, norm:0.2471, lr:3.5243e-04 dt: 3332.54ms, tok/sec:157323.71
step 7603, loss: 3.336401, norm:0.2578, lr:3.5237e-04 dt: 3332.03ms, tok/sec:157348.08
step 7604, loss: 3.417572, norm:0.2459, lr:3.5231e-04 dt: 3332.22ms, tok/sec:157338.80
step 7605, loss: 3.382567, norm:0.2503, lr:3.5225e-04 dt: 3332.13ms, tok/sec:157343.26
step 7606, loss: 3.330703, norm:0.2271, lr:3.5220e-04 dt: 3332.04ms, tok/sec:157347.58
step 7607, loss: 3.338074, norm:0.2545, lr:3.5214e-04 dt: 3331.96ms, tok/sec:157351.12
step 7608, loss: 3.359735, norm:0.2378, lr:3.5208e-04 dt: 3332.10ms, tok/sec:157344.46
step 7609, loss: 3.332466, norm:0.2523, lr:3.5202e-04 dt: 3332.39ms, tok/sec:157330.78
step 7610, loss: 3.263499, norm:0.2351, lr:3.5196e-04 dt: 3331.94ms, tok/sec:157352.31
step 7611, loss: 3.308054, norm:0.2585, lr:3.5190e-04 dt: 3332.52ms, tok/sec:157324.66
step 7612, loss: 3.361564, norm:0.2532, lr:3.5185e-04 dt: 3332.13ms, tok/sec:157343.02
step 7613, loss: 3.277639, norm:0.2392, lr:3.5179e-04 dt: 3332.00ms, tok/sec:157349.56
step 7614, loss: 3.320320, norm:0.2587, lr:3.5173e-04 dt: 3332.06ms, tok/sec:157346.72
step 7615, loss: 3.357253, norm:0.2359, lr:3.5167e-04 dt: 3332.07ms, tok/sec:157346.06
step 7616, loss: 3.330162, norm:0.2498, lr:3.5161e-04 dt: 3332.13ms, tok/sec:157343.11
step 7617, loss: 3.309567, norm:0.2491, lr:3.5156e-04 dt: 3332.21ms, tok/sec:157339.43
step 7618, loss: 3.317778, norm:0.2669, lr:3.5150e-04 dt: 3332.29ms, tok/sec:157335.86
step 7619, loss: 3.379812, norm:0.2847, lr:3.5144e-04 dt: 3334.68ms, tok/sec:157222.97
step 7620, loss: 3.365773, norm:0.2656, lr:3.5138e-04 dt: 3332.58ms, tok/sec:157321.98
step 7621, loss: 3.363063, norm:0.2726, lr:3.5132e-04 dt: 3332.16ms, tok/sec:157341.83
step 7622, loss: 3.355160, norm:0.2749, lr:3.5127e-04 dt: 3332.23ms, tok/sec:157338.49
step 7623, loss: 3.415681, norm:0.2752, lr:3.5121e-04 dt: 3332.12ms, tok/sec:157343.88
step 7624, loss: 3.428008, norm:0.2937, lr:3.5115e-04 dt: 3331.97ms, tok/sec:157350.93
step 7625, loss: 3.387028, norm:0.2670, lr:3.5109e-04 dt: 3332.23ms, tok/sec:157338.53
step 7626, loss: 3.249203, norm:0.2884, lr:3.5103e-04 dt: 3331.90ms, tok/sec:157354.19
step 7627, loss: 3.366597, norm:0.2669, lr:3.5097e-04 dt: 3332.29ms, tok/sec:157335.85
step 7628, loss: 3.357480, norm:0.2537, lr:3.5092e-04 dt: 3332.28ms, tok/sec:157336.00
step 7629, loss: 3.355948, norm:0.2718, lr:3.5086e-04 dt: 3332.28ms, tok/sec:157336.22
step 7630, loss: 3.342306, norm:0.3024, lr:3.5080e-04 dt: 3332.21ms, tok/sec:157339.60
step 7631, loss: 3.356711, norm:0.2878, lr:3.5074e-04 dt: 3332.23ms, tok/sec:157338.58
step 7632, loss: 3.356778, norm:0.2414, lr:3.5068e-04 dt: 3332.18ms, tok/sec:157341.06
step 7633, loss: 3.358622, norm:0.2784, lr:3.5063e-04 dt: 3332.18ms, tok/sec:157341.03
step 7634, loss: 3.425643, norm:0.2570, lr:3.5057e-04 dt: 3332.04ms, tok/sec:157347.46
step 7635, loss: 3.376501, norm:0.2770, lr:3.5051e-04 dt: 3332.04ms, tok/sec:157347.24
step 7636, loss: 3.338031, norm:0.2483, lr:3.5045e-04 dt: 3331.94ms, tok/sec:157352.39
step 7637, loss: 3.423865, norm:0.2460, lr:3.5039e-04 dt: 3332.46ms, tok/sec:157327.65
step 7638, loss: 3.334014, norm:0.2462, lr:3.5033e-04 dt: 3332.01ms, tok/sec:157348.93
step 7639, loss: 3.354167, norm:0.2335, lr:3.5028e-04 dt: 3332.53ms, tok/sec:157324.09
step 7640, loss: 3.407369, norm:0.2490, lr:3.5022e-04 dt: 3332.25ms, tok/sec:157337.69
step 7641, loss: 3.367949, norm:0.2440, lr:3.5016e-04 dt: 3332.10ms, tok/sec:157344.69
step 7642, loss: 3.313447, norm:0.2444, lr:3.5010e-04 dt: 3332.11ms, tok/sec:157344.11
step 7643, loss: 3.309356, norm:0.2538, lr:3.5004e-04 dt: 3331.99ms, tok/sec:157349.65
step 7644, loss: 3.289217, norm:0.2532, lr:3.4999e-04 dt: 3332.06ms, tok/sec:157346.33
step 7645, loss: 3.305794, norm:0.2684, lr:3.4993e-04 dt: 3332.01ms, tok/sec:157348.66
step 7646, loss: 3.359640, norm:0.2551, lr:3.4987e-04 dt: 3332.34ms, tok/sec:157333.51
step 7647, loss: 3.327255, norm:0.2764, lr:3.4981e-04 dt: 3332.23ms, tok/sec:157338.64
step 7648, loss: 3.331848, norm:0.2526, lr:3.4975e-04 dt: 3332.04ms, tok/sec:157347.24
step 7649, loss: 3.381344, norm:0.2653, lr:3.4970e-04 dt: 3332.11ms, tok/sec:157344.09
HellaSwag accuracy:6937940022596437073/-2=-3468970011298218496.0000
rank 1 sample 0: Hello, I'm a language model, and I'm excited to learn, let’s face it, the time is not far to come to the end
rank 1 sample 1: Hello, I'm a language model, which means I will be able to implement different types of software for programming languages that are both commonly used and widely used.
rank 1 sample 2: Hello, I'm a language model, I wish that this is a language for you. I have found this a great way to help people who want to look
rank 1 sample 3: Hello, I'm a language model, and I'm using the one I know but I'm reading my mind so very quickly why just a little bit of a
rank 0 sample 0: Hello, I'm a language model, and I know that you should look on one sentence with a word such as a, “this, it, you
rank 0 sample 1: Hello, I'm a language model, so there's a lot of overlap in the two styles."
But with all of these things, one might be hard
rank 0 sample 2: Hello, I'm a language model, I'm curious what does he do to me because he's going to be a language model, and I'm not doing
rank 0 sample 3: Hello, I'm a language model, a web site, a blog. If I create a web site then I will run it as an RSS feed and use
step 7650, loss: 3.308682, norm:0.2708, lr:3.4964e-04 dt: 48517.90ms, tok/sec:10806.07
step 7651, loss: 3.416956, norm:0.3248, lr:3.4958e-04 dt: 3332.00ms, tok/sec:157349.15
step 7652, loss: 3.384392, norm:0.3195, lr:3.4952e-04 dt: 3332.34ms, tok/sec:157333.44
step 7653, loss: 3.397527, norm:0.3140, lr:3.4946e-04 dt: 3331.90ms, tok/sec:157354.01
step 7654, loss: 3.393867, norm:0.2764, lr:3.4940e-04 dt: 3332.13ms, tok/sec:157343.12
step 7655, loss: 3.385290, norm:0.3011, lr:3.4935e-04 dt: 3332.36ms, tok/sec:157332.29
step 7656, loss: 3.331313, norm:0.2926, lr:3.4929e-04 dt: 3332.16ms, tok/sec:157341.94
step 7657, loss: 3.347474, norm:0.2965, lr:3.4923e-04 dt: 3332.70ms, tok/sec:157316.28
step 7658, loss: 3.426771, norm:0.2807, lr:3.4917e-04 dt: 3331.99ms, tok/sec:157349.99
step 7659, loss: 3.408553, norm:0.2693, lr:3.4911e-04 dt: 3332.13ms, tok/sec:157343.25
step 7660, loss: 3.322280, norm:0.2792, lr:3.4906e-04 dt: 3332.33ms, tok/sec:157333.53
step 7661, loss: 3.339854, norm:0.2623, lr:3.4900e-04 dt: 3332.00ms, tok/sec:157349.20
step 7662, loss: 3.341989, norm:0.2630, lr:3.4894e-04 dt: 3332.44ms, tok/sec:157328.69
step 7663, loss: 3.323827, norm:0.2806, lr:3.4888e-04 dt: 3332.19ms, tok/sec:157340.34
step 7664, loss: 3.348754, norm:0.2449, lr:3.4882e-04 dt: 3332.37ms, tok/sec:157331.75
step 7665, loss: 3.386510, norm:0.2585, lr:3.4876e-04 dt: 3332.40ms, tok/sec:157330.27
step 7666, loss: 3.398074, norm:0.2682, lr:3.4871e-04 dt: 3333.06ms, tok/sec:157299.11
step 7667, loss: 3.394181, norm:0.2733, lr:3.4865e-04 dt: 3332.14ms, tok/sec:157342.57
step 7668, loss: 3.351613, norm:0.2676, lr:3.4859e-04 dt: 3332.27ms, tok/sec:157336.53
step 7669, loss: 3.356538, norm:0.2437, lr:3.4853e-04 dt: 3331.90ms, tok/sec:157354.00
step 7670, loss: 3.314205, norm:0.2531, lr:3.4847e-04 dt: 3332.13ms, tok/sec:157343.28
step 7671, loss: 3.399034, norm:0.2656, lr:3.4842e-04 dt: 3332.31ms, tok/sec:157334.65
step 7672, loss: 3.366004, norm:0.2585, lr:3.4836e-04 dt: 3332.20ms, tok/sec:157339.95
step 7673, loss: 3.422333, norm:0.2357, lr:3.4830e-04 dt: 3332.55ms, tok/sec:157323.58
step 7674, loss: 3.359546, norm:0.2435, lr:3.4824e-04 dt: 3332.11ms, tok/sec:157344.16
step 7675, loss: 3.316448, norm:0.2418, lr:3.4818e-04 dt: 3332.23ms, tok/sec:157338.31
step 7676, loss: 3.338125, norm:0.2501, lr:3.4812e-04 dt: 3332.38ms, tok/sec:157331.36
step 7677, loss: 3.308603, norm:0.2334, lr:3.4807e-04 dt: 3332.06ms, tok/sec:157346.30
step 7678, loss: 3.356658, norm:0.2480, lr:3.4801e-04 dt: 3332.09ms, tok/sec:157345.09
step 7679, loss: 3.321455, norm:0.2428, lr:3.4795e-04 dt: 3332.23ms, tok/sec:157338.70
step 7680, loss: 3.359467, norm:0.2536, lr:3.4789e-04 dt: 3332.39ms, tok/sec:157330.74
step 7681, loss: 3.322995, norm:0.2636, lr:3.4783e-04 dt: 3332.21ms, tok/sec:157339.42
step 7682, loss: 3.263607, norm:0.2374, lr:3.4778e-04 dt: 3332.61ms, tok/sec:157320.36
step 7683, loss: 3.310441, norm:0.2465, lr:3.4772e-04 dt: 3332.13ms, tok/sec:157343.34
step 7684, loss: 3.348096, norm:0.2958, lr:3.4766e-04 dt: 3332.17ms, tok/sec:157341.24
step 7685, loss: 3.350316, norm:0.2694, lr:3.4760e-04 dt: 3331.89ms, tok/sec:157354.39
step 7686, loss: 3.341060, norm:0.2660, lr:3.4754e-04 dt: 3332.14ms, tok/sec:157342.72
step 7687, loss: 3.292330, norm:0.2637, lr:3.4748e-04 dt: 3332.29ms, tok/sec:157335.75
step 7688, loss: 3.361150, norm:0.2669, lr:3.4743e-04 dt: 3332.19ms, tok/sec:157340.20
step 7689, loss: 3.337757, norm:0.2576, lr:3.4737e-04 dt: 3332.42ms, tok/sec:157329.72
step 7690, loss: 3.329458, norm:0.2724, lr:3.4731e-04 dt: 3332.36ms, tok/sec:157332.33
step 7691, loss: 3.365584, norm:0.2551, lr:3.4725e-04 dt: 3332.15ms, tok/sec:157342.31
step 7692, loss: 3.331345, norm:0.2668, lr:3.4719e-04 dt: 3332.35ms, tok/sec:157332.62
step 7693, loss: 3.305589, norm:0.2661, lr:3.4714e-04 dt: 3332.14ms, tok/sec:157342.54
step 7694, loss: 3.388514, norm:0.2859, lr:3.4708e-04 dt: 3332.00ms, tok/sec:157349.37
step 7695, loss: 3.444937, norm:0.3139, lr:3.4702e-04 dt: 3332.24ms, tok/sec:157337.91
step 7696, loss: 3.457980, norm:0.3082, lr:3.4696e-04 dt: 3332.29ms, tok/sec:157335.72
step 7697, loss: 3.378371, norm:0.2958, lr:3.4690e-04 dt: 3332.00ms, tok/sec:157349.54
step 7698, loss: 3.407074, norm:0.2674, lr:3.4684e-04 dt: 3332.56ms, tok/sec:157322.99
step 7699, loss: 3.400137, norm:0.2769, lr:3.4679e-04 dt: 3332.73ms, tok/sec:157315.07
validation loss: 3.3438
Model and optimizer state saved.
HellaSwag accuracy:2326341965099238481/-2=-1163170982549619200.0000
rank 1 sample 0: Hello, I'm a language model, we are currently working on the code snippet of some kind from
the file (which will be an example of the code
rank 1 sample 1: Hello, I'm a language model, a teacher. What are you doing at this moment? Have you a conversation around this at the National Association of Teachers of
rank 1 sample 2: Hello, I'm a language model, so I want to know how to do that. I can make it easier to understand the concept in an easier time,
rank 1 sample 3: Hello, I'm a language model, and I'm really excited about this stuff!!
If students come away in some of my earlier articles, I hope they
rank 0 sample 0: Hello, I'm a language model, and I don't have many language courses where I'll never learn that -- so I have to start somewhere, and there
rank 0 sample 1: Hello, I'm a language model, so please have a look at some
- Language for Spelling. An excellent website to
help young students learn English
rank 0 sample 2: Hello, I'm a language model, I'm thinking that our teacher will want to get that language learning. But I can't wait to see what the question
rank 0 sample 3: Hello, I'm a language model, and like everything I do, it takes time, and I'll go ahead. They will continue to expand their knowledge;
step 7700, loss: 3.322661, norm:0.2718, lr:3.4673e-04 dt: 56178.58ms, tok/sec:9332.52
step 7701, loss: 3.354083, norm:0.2666, lr:3.4667e-04 dt: 3332.24ms, tok/sec:157337.78
step 7702, loss: 3.347056, norm:0.2548, lr:3.4661e-04 dt: 3332.09ms, tok/sec:157345.26
step 7703, loss: 3.361685, norm:0.2995, lr:3.4655e-04 dt: 3332.58ms, tok/sec:157322.07
step 7704, loss: 3.296108, norm:0.2629, lr:3.4649e-04 dt: 3332.22ms, tok/sec:157338.82
step 7705, loss: 3.452625, norm:0.8025, lr:3.4644e-04 dt: 3332.10ms, tok/sec:157344.69
step 7706, loss: 3.363581, norm:0.2960, lr:3.4638e-04 dt: 3332.19ms, tok/sec:157340.37
step 7707, loss: 3.376113, norm:0.3109, lr:3.4632e-04 dt: 3332.07ms, tok/sec:157345.85
step 7708, loss: 3.324156, norm:0.2939, lr:3.4626e-04 dt: 3332.04ms, tok/sec:157347.22
step 7709, loss: 3.398999, norm:0.2681, lr:3.4620e-04 dt: 3332.35ms, tok/sec:157332.97
step 7710, loss: 3.346885, norm:0.2721, lr:3.4615e-04 dt: 3331.97ms, tok/sec:157350.75
step 7711, loss: 3.284614, norm:0.2871, lr:3.4609e-04 dt: 3332.52ms, tok/sec:157324.58
step 7712, loss: 3.341101, norm:0.5183, lr:3.4603e-04 dt: 3332.02ms, tok/sec:157348.49
step 7713, loss: 3.327811, norm:0.2637, lr:3.4597e-04 dt: 3332.09ms, tok/sec:157345.27
step 7714, loss: 3.322392, norm:0.2968, lr:3.4591e-04 dt: 3332.16ms, tok/sec:157341.58
step 7715, loss: 3.444383, norm:0.2719, lr:3.4585e-04 dt: 3332.34ms, tok/sec:157333.36
step 7716, loss: 3.314136, norm:0.2697, lr:3.4580e-04 dt: 3331.84ms, tok/sec:157356.74
step 7717, loss: 3.287545, norm:0.2788, lr:3.4574e-04 dt: 3332.10ms, tok/sec:157344.81
step 7718, loss: 3.326522, norm:0.2758, lr:3.4568e-04 dt: 3332.23ms, tok/sec:157338.70
step 7719, loss: 3.335896, norm:0.2662, lr:3.4562e-04 dt: 3332.40ms, tok/sec:157330.31
step 7720, loss: 3.296533, norm:0.2550, lr:3.4556e-04 dt: 3332.19ms, tok/sec:157340.31
step 7721, loss: 3.435168, norm:0.2731, lr:3.4551e-04 dt: 3332.14ms, tok/sec:157342.62
step 7722, loss: 3.308396, norm:0.2584, lr:3.4545e-04 dt: 3332.07ms, tok/sec:157346.04
step 7723, loss: 3.303739, norm:0.2495, lr:3.4539e-04 dt: 3332.19ms, tok/sec:157340.29
step 7724, loss: 3.294502, norm:0.2624, lr:3.4533e-04 dt: 3332.05ms, tok/sec:157346.85
step 7725, loss: 3.304454, norm:0.2519, lr:3.4527e-04 dt: 3332.12ms, tok/sec:157343.79
step 7726, loss: 3.317714, norm:0.2528, lr:3.4521e-04 dt: 3332.06ms, tok/sec:157346.51
step 7727, loss: 3.327919, norm:0.2460, lr:3.4516e-04 dt: 3332.75ms, tok/sec:157313.81
step 7728, loss: 3.291076, norm:0.2419, lr:3.4510e-04 dt: 3332.14ms, tok/sec:157342.69
step 7729, loss: 3.297228, norm:0.2593, lr:3.4504e-04 dt: 3331.96ms, tok/sec:157351.38
step 7730, loss: 3.335724, norm:0.2318, lr:3.4498e-04 dt: 3332.10ms, tok/sec:157344.84
step 7731, loss: 3.388958, norm:0.2697, lr:3.4492e-04 dt: 3332.14ms, tok/sec:157342.79
step 7732, loss: 3.312092, norm:0.2379, lr:3.4486e-04 dt: 3332.08ms, tok/sec:157345.35
step 7733, loss: 3.321758, norm:0.2364, lr:3.4481e-04 dt: 3332.07ms, tok/sec:157346.10
step 7734, loss: 3.400422, norm:0.2490, lr:3.4475e-04 dt: 3332.23ms, tok/sec:157338.34
step 7735, loss: 3.381518, norm:0.2379, lr:3.4469e-04 dt: 3332.65ms, tok/sec:157318.61
step 7736, loss: 3.354687, norm:0.2422, lr:3.4463e-04 dt: 3332.18ms, tok/sec:157340.73
step 7737, loss: 3.429352, norm:0.2445, lr:3.4457e-04 dt: 3332.37ms, tok/sec:157331.89
step 7738, loss: 3.378261, norm:0.2480, lr:3.4452e-04 dt: 3332.47ms, tok/sec:157327.15
step 7739, loss: 3.378242, norm:0.2446, lr:3.4446e-04 dt: 3332.40ms, tok/sec:157330.67
step 7740, loss: 3.394043, norm:0.2390, lr:3.4440e-04 dt: 3332.25ms, tok/sec:157337.77
step 7741, loss: 3.462655, norm:0.2863, lr:3.4434e-04 dt: 3332.44ms, tok/sec:157328.41
step 7742, loss: 3.374933, norm:0.2776, lr:3.4428e-04 dt: 3332.08ms, tok/sec:157345.66
step 7743, loss: 3.282912, norm:0.2688, lr:3.4422e-04 dt: 3332.59ms, tok/sec:157321.69
step 7744, loss: 3.295571, norm:0.2486, lr:3.4417e-04 dt: 3332.15ms, tok/sec:157342.03
step 7745, loss: 3.342402, norm:0.4313, lr:3.4411e-04 dt: 3331.87ms, tok/sec:157355.58
step 7746, loss: 3.337868, norm:0.2930, lr:3.4405e-04 dt: 3332.02ms, tok/sec:157348.18
step 7747, loss: 3.424847, norm:0.2858, lr:3.4399e-04 dt: 3332.16ms, tok/sec:157341.82
step 7748, loss: 3.319500, norm:0.2712, lr:3.4393e-04 dt: 3332.21ms, tok/sec:157339.50
step 7749, loss: 3.404798, norm:0.2765, lr:3.4387e-04 dt: 3331.98ms, tok/sec:157350.07
HellaSwag accuracy:-2286593098539367343/-2=1143296549269683712.0000
rank 1 sample 0: Hello, I'm a language model, as it is a computer language. No matter how much they look like, people will use language.
If you are
rank 1 sample 1: Hello, I'm a language model, you know how to do it. And for me, being a language model isn't anything. You can't do it
rank 1 sample 2: Hello, I'm a language model, I had the class to write a few words, but they aren't words. I'm just learning how to code code
rank 1 sample 3: Hello, I'm a language model, and I'm still learning so much at new times. Do you hear your comments? NoThank you for your comment.
rank 0 sample 0: Hello, I'm a language model, and I'll be using it as a standard text editor. I would install my own program and make a screenshot of your
rank 0 sample 1: Hello, I'm a language model, I understand the need of a more accurate language to the real world. With that, you'll have something you can say
rank 0 sample 2: Hello, I'm a language model, I'm used to using this class for the purposes of the program:
- This is the language model that you could
rank 0 sample 3: Hello, I'm a language model, which gives me a lot of fun, and how to write a blog post, all right? I guess I've learned
step 7750, loss: 3.317586, norm:0.2894, lr:3.4382e-04 dt: 48515.90ms, tok/sec:10806.52
step 7751, loss: 3.292143, norm:0.2925, lr:3.4376e-04 dt: 3332.21ms, tok/sec:157339.40
step 7752, loss: 3.397216, norm:0.2736, lr:3.4370e-04 dt: 3332.39ms, tok/sec:157330.81
step 7753, loss: 3.371014, norm:0.3055, lr:3.4364e-04 dt: 3331.98ms, tok/sec:157350.21
step 7754, loss: 3.327737, norm:0.2782, lr:3.4358e-04 dt: 3332.08ms, tok/sec:157345.34
step 7755, loss: 3.286466, norm:0.2622, lr:3.4352e-04 dt: 3332.02ms, tok/sec:157348.47
step 7756, loss: 3.363904, norm:0.2708, lr:3.4347e-04 dt: 3332.17ms, tok/sec:157341.35
step 7757, loss: 3.343493, norm:0.2792, lr:3.4341e-04 dt: 3332.07ms, tok/sec:157345.85
step 7758, loss: 3.357442, norm:0.2773, lr:3.4335e-04 dt: 3331.98ms, tok/sec:157350.43
step 7759, loss: 3.328914, norm:0.2721, lr:3.4329e-04 dt: 3332.25ms, tok/sec:157337.70
step 7760, loss: 3.386648, norm:0.2898, lr:3.4323e-04 dt: 3331.94ms, tok/sec:157352.31
step 7761, loss: 3.332210, norm:0.2827, lr:3.4318e-04 dt: 3332.75ms, tok/sec:157314.15
step 7762, loss: 3.364603, norm:0.2783, lr:3.4312e-04 dt: 3332.34ms, tok/sec:157333.30
step 7763, loss: 3.333040, norm:0.2623, lr:3.4306e-04 dt: 3332.20ms, tok/sec:157339.89
step 7764, loss: 3.318665, norm:0.2736, lr:3.4300e-04 dt: 3332.10ms, tok/sec:157344.44
step 7765, loss: 3.332721, norm:0.2768, lr:3.4294e-04 dt: 3332.30ms, tok/sec:157335.31
step 7766, loss: 3.361734, norm:0.2753, lr:3.4288e-04 dt: 3332.24ms, tok/sec:157338.18
step 7767, loss: 3.381902, norm:0.2860, lr:3.4283e-04 dt: 3332.01ms, tok/sec:157348.78
step 7768, loss: 3.368659, norm:0.2804, lr:3.4277e-04 dt: 3332.25ms, tok/sec:157337.60
step 7769, loss: 3.388314, norm:0.2817, lr:3.4271e-04 dt: 3331.95ms, tok/sec:157351.49
step 7770, loss: 3.293772, norm:0.2463, lr:3.4265e-04 dt: 3332.57ms, tok/sec:157322.34
step 7771, loss: 3.334288, norm:0.2590, lr:3.4259e-04 dt: 3332.08ms, tok/sec:157345.65
step 7772, loss: 3.387585, norm:0.3092, lr:3.4253e-04 dt: 3332.10ms, tok/sec:157344.84
step 7773, loss: 3.290668, norm:0.2559, lr:3.4248e-04 dt: 3332.12ms, tok/sec:157343.80
step 7774, loss: 3.372043, norm:0.2573, lr:3.4242e-04 dt: 3332.26ms, tok/sec:157337.20
step 7775, loss: 3.352988, norm:0.2572, lr:3.4236e-04 dt: 3332.08ms, tok/sec:157345.78
step 7776, loss: 3.360148, norm:0.2590, lr:3.4230e-04 dt: 3331.87ms, tok/sec:157355.59
step 7777, loss: 3.360284, norm:0.2525, lr:3.4224e-04 dt: 3332.25ms, tok/sec:157337.48
step 7778, loss: 3.288982, norm:0.2508, lr:3.4219e-04 dt: 3332.01ms, tok/sec:157348.74
step 7779, loss: 3.340654, norm:0.2669, lr:3.4213e-04 dt: 3332.26ms, tok/sec:157337.09
step 7780, loss: 3.379272, norm:0.2469, lr:3.4207e-04 dt: 3332.03ms, tok/sec:157348.11
step 7781, loss: 3.322657, norm:0.2624, lr:3.4201e-04 dt: 3332.21ms, tok/sec:157339.42
step 7782, loss: 3.295362, norm:0.2412, lr:3.4195e-04 dt: 3331.94ms, tok/sec:157352.19
step 7783, loss: 3.318990, norm:0.2449, lr:3.4189e-04 dt: 3332.16ms, tok/sec:157341.72
step 7784, loss: 3.316248, norm:0.2347, lr:3.4184e-04 dt: 3332.14ms, tok/sec:157342.55
step 7785, loss: 3.334607, norm:0.2365, lr:3.4178e-04 dt: 3332.00ms, tok/sec:157349.36
step 7786, loss: 3.335475, norm:0.2283, lr:3.4172e-04 dt: 3332.35ms, tok/sec:157332.82
step 7787, loss: 3.355434, norm:0.2346, lr:3.4166e-04 dt: 3332.20ms, tok/sec:157339.98
step 7788, loss: 3.270271, norm:0.2431, lr:3.4160e-04 dt: 3331.91ms, tok/sec:157353.69
step 7789, loss: 3.390850, norm:0.2491, lr:3.4154e-04 dt: 3332.20ms, tok/sec:157339.97
step 7790, loss: 3.375136, norm:0.2474, lr:3.4149e-04 dt: 3332.19ms, tok/sec:157340.41
step 7791, loss: 3.306473, norm:0.2804, lr:3.4143e-04 dt: 3332.03ms, tok/sec:157348.01
step 7792, loss: 3.323214, norm:0.2525, lr:3.4137e-04 dt: 3332.02ms, tok/sec:157348.49
step 7793, loss: 3.296855, norm:0.2551, lr:3.4131e-04 dt: 3332.23ms, tok/sec:157338.69
step 7794, loss: 3.321726, norm:0.2604, lr:3.4125e-04 dt: 3332.20ms, tok/sec:157340.03
step 7795, loss: 3.330093, norm:0.2564, lr:3.4119e-04 dt: 3332.37ms, tok/sec:157332.08
step 7796, loss: 3.365303, norm:0.3211, lr:3.4114e-04 dt: 3331.91ms, tok/sec:157353.56
step 7797, loss: 3.318280, norm:0.2742, lr:3.4108e-04 dt: 3332.16ms, tok/sec:157341.59
step 7798, loss: 3.383431, norm:0.2859, lr:3.4102e-04 dt: 3332.33ms, tok/sec:157333.62
step 7799, loss: 3.347397, norm:0.2886, lr:3.4096e-04 dt: 3332.03ms, tok/sec:157348.08
validation loss: 3.3396
Model and optimizer state saved.
HellaSwag accuracy:-2286588700492889007/-2=1143294350246444544.0000
rank 1 sample 0: Hello, I'm a language model, and that's what I want to figure out from a programming and programming perspective. I wanted to show you some examples of
rank 1 sample 1: Hello, I'm a language model, which I can work with in many environments; I'll create a page model with every concept and example I can use to
rank 1 sample 2: Hello, I'm a language model, I find a new way to do that. I'm a professor in my life, and I can tell the world when
rank 1 sample 3: Hello, I'm a language model, and I'm writing about "counp"
To define words (verb) about two objects (verb) about
rank 0 sample 0: Hello, I'm a language model, and I know that it's my personal story from the perspective of other. But I know that you have a story and
rank 0 sample 1: Hello, I'm a language model, I like to use it.
Oh, you should love it. Don't worry. There are three different types of
rank 0 sample 2: Hello, I'm a language model, so I had a nice tutorial on that. That's it's all, and I'll try to use it in my
rank 0 sample 3: Hello, I'm a language model, I write my own code for my future projects now. I'm looking for something simpler in my future projects.<|endoftext|>One
step 7800, loss: 3.319156, norm:0.2802, lr:3.4090e-04 dt: 56214.06ms, tok/sec:9326.63
step 7801, loss: 3.361190, norm:0.2689, lr:3.4084e-04 dt: 3332.10ms, tok/sec:157344.77
step 7802, loss: 3.330512, norm:0.2647, lr:3.4079e-04 dt: 3332.04ms, tok/sec:157347.55
step 7803, loss: 3.366983, norm:0.2806, lr:3.4073e-04 dt: 3332.17ms, tok/sec:157341.23
step 7804, loss: 3.318280, norm:0.2366, lr:3.4067e-04 dt: 3332.18ms, tok/sec:157340.65
step 7805, loss: 3.347831, norm:0.2607, lr:3.4061e-04 dt: 3332.82ms, tok/sec:157310.66
step 7806, loss: 3.333982, norm:0.2590, lr:3.4055e-04 dt: 3332.03ms, tok/sec:157347.73
step 7807, loss: 3.364970, norm:0.2392, lr:3.4050e-04 dt: 3332.13ms, tok/sec:157343.14
step 7808, loss: 3.402925, norm:0.2975, lr:3.4044e-04 dt: 3332.18ms, tok/sec:157340.93
step 7809, loss: 3.295862, norm:0.2657, lr:3.4038e-04 dt: 3332.13ms, tok/sec:157343.27
step 7810, loss: 3.397833, norm:0.2862, lr:3.4032e-04 dt: 3334.49ms, tok/sec:157231.75
step 7811, loss: 3.309303, norm:0.2721, lr:3.4026e-04 dt: 3332.04ms, tok/sec:157347.44
step 7812, loss: 3.345217, norm:0.2682, lr:3.4020e-04 dt: 3332.33ms, tok/sec:157333.94
step 7813, loss: 3.370192, norm:0.2532, lr:3.4015e-04 dt: 3332.14ms, tok/sec:157342.49
step 7814, loss: 3.293785, norm:0.2705, lr:3.4009e-04 dt: 3331.88ms, tok/sec:157355.15
step 7815, loss: 3.304789, norm:0.2518, lr:3.4003e-04 dt: 3331.88ms, tok/sec:157354.97
step 7816, loss: 3.454468, norm:0.3188, lr:3.3997e-04 dt: 3331.97ms, tok/sec:157350.87
step 7817, loss: 3.295284, norm:0.2845, lr:3.3991e-04 dt: 3332.07ms, tok/sec:157346.22
step 7818, loss: 3.312412, norm:0.2695, lr:3.3985e-04 dt: 3331.89ms, tok/sec:157354.60
step 7819, loss: 3.273923, norm:0.2657, lr:3.3980e-04 dt: 3332.05ms, tok/sec:157347.01
step 7820, loss: 3.301175, norm:0.2461, lr:3.3974e-04 dt: 3332.09ms, tok/sec:157344.86
step 7821, loss: 3.314142, norm:0.2603, lr:3.3968e-04 dt: 3332.17ms, tok/sec:157341.42
step 7822, loss: 3.256605, norm:0.2483, lr:3.3962e-04 dt: 3331.77ms, tok/sec:157360.43
step 7823, loss: 3.386775, norm:0.2684, lr:3.3956e-04 dt: 3331.92ms, tok/sec:157353.18
step 7824, loss: 3.362598, norm:0.2699, lr:3.3950e-04 dt: 3332.15ms, tok/sec:157342.22
step 7825, loss: 3.264640, norm:0.2791, lr:3.3945e-04 dt: 3332.29ms, tok/sec:157335.46
step 7826, loss: 3.374637, norm:0.2634, lr:3.3939e-04 dt: 3332.11ms, tok/sec:157344.19
step 7827, loss: 3.346828, norm:0.2689, lr:3.3933e-04 dt: 3332.12ms, tok/sec:157343.80
step 7828, loss: 3.340330, norm:0.2429, lr:3.3927e-04 dt: 3332.02ms, tok/sec:157348.29
step 7829, loss: 3.325551, norm:0.2558, lr:3.3921e-04 dt: 3332.03ms, tok/sec:157348.15
step 7830, loss: 3.342208, norm:0.2722, lr:3.3915e-04 dt: 3332.09ms, tok/sec:157345.30
step 7831, loss: 3.309991, norm:0.2573, lr:3.3910e-04 dt: 3332.06ms, tok/sec:157346.60
step 7832, loss: 3.322123, norm:0.2710, lr:3.3904e-04 dt: 3332.39ms, tok/sec:157330.87
step 7833, loss: 3.334854, norm:0.2613, lr:3.3898e-04 dt: 3332.05ms, tok/sec:157346.91
step 7834, loss: 3.420034, norm:0.2643, lr:3.3892e-04 dt: 3332.40ms, tok/sec:157330.62
step 7835, loss: 3.332545, norm:0.2739, lr:3.3886e-04 dt: 3332.04ms, tok/sec:157347.36
step 7836, loss: 3.394428, norm:0.2639, lr:3.3881e-04 dt: 3331.92ms, tok/sec:157353.04
step 7837, loss: 3.421820, norm:0.2852, lr:3.3875e-04 dt: 3332.53ms, tok/sec:157324.52
step 7838, loss: 3.366538, norm:0.2814, lr:3.3869e-04 dt: 3332.36ms, tok/sec:157332.53
step 7839, loss: 3.342569, norm:0.2684, lr:3.3863e-04 dt: 3331.96ms, tok/sec:157351.19
step 7840, loss: 3.457731, norm:0.3291, lr:3.3857e-04 dt: 3332.07ms, tok/sec:157346.10
step 7841, loss: 3.334155, norm:0.2594, lr:3.3851e-04 dt: 3332.34ms, tok/sec:157333.38
step 7842, loss: 3.360121, norm:0.2734, lr:3.3846e-04 dt: 3332.14ms, tok/sec:157342.54
step 7843, loss: 3.383351, norm:0.2570, lr:3.3840e-04 dt: 3332.45ms, tok/sec:157328.28
step 7844, loss: 3.431509, norm:0.2942, lr:3.3834e-04 dt: 3331.92ms, tok/sec:157353.01
step 7845, loss: 3.342526, norm:0.3089, lr:3.3828e-04 dt: 3331.91ms, tok/sec:157353.64
step 7846, loss: 3.372500, norm:0.2754, lr:3.3822e-04 dt: 3331.93ms, tok/sec:157352.87
step 7847, loss: 3.303818, norm:0.2550, lr:3.3816e-04 dt: 3332.12ms, tok/sec:157343.83
step 7848, loss: 3.282973, norm:0.2666, lr:3.3811e-04 dt: 3332.00ms, tok/sec:157349.25
step 7849, loss: 3.326987, norm:0.2655, lr:3.3805e-04 dt: 3331.96ms, tok/sec:157351.26
HellaSwag accuracy:2326372751423013969/-2=-1163186375711506944.0000
rank 1 sample 0: Hello, I'm a language model,” said the speaker, “and if it didn't work, let's do something that's good for the
rank 1 sample 1: Hello, I'm a language model, a teacher. They're not very familiar with the basics about teaching that are really really used by teacher, but I think
rank 1 sample 2: Hello, I'm a language model, I look forward to the future.
I'm a book in a library, and I am the creator and president of
rank 1 sample 3: Hello, I'm a language model, and I'm using a number of resources called "grammar", meaning to do a math-based math problem on a
rank 0 sample 0: Hello, I'm a language model, and I know that there are two possible functions available on the internet – your input and an output from a database. If
rank 0 sample 1: Hello, I'm a language model, so, and I think, what an example? Because all the language training in this language is based on the programming language
rank 0 sample 2: Hello, I'm a language model, so I had a piece of free, but still English. But there's a ton of other stuff I can't come
rank 0 sample 3: Hello, I'm a language model, you do not have to write your language. Therefore, you can learn and write out different things in another language and at
step 7850, loss: 3.331110, norm:0.2517, lr:3.3799e-04 dt: 48525.93ms, tok/sec:10804.28
step 7851, loss: 3.400440, norm:0.2736, lr:3.3793e-04 dt: 3331.94ms, tok/sec:157352.05
step 7852, loss: 3.303396, norm:0.2437, lr:3.3787e-04 dt: 3332.28ms, tok/sec:157335.93
step 7853, loss: 3.322434, norm:0.2654, lr:3.3781e-04 dt: 3332.35ms, tok/sec:157332.91
step 7854, loss: 3.321044, norm:0.2467, lr:3.3776e-04 dt: 3331.95ms, tok/sec:157351.74
step 7855, loss: 3.377263, norm:0.2599, lr:3.3770e-04 dt: 3331.93ms, tok/sec:157352.53
step 7856, loss: 3.317395, norm:0.2584, lr:3.3764e-04 dt: 3332.23ms, tok/sec:157338.55
step 7857, loss: 3.387683, norm:0.2787, lr:3.3758e-04 dt: 3331.90ms, tok/sec:157354.27
step 7858, loss: 3.308338, norm:0.2729, lr:3.3752e-04 dt: 3332.13ms, tok/sec:157343.32
step 7859, loss: 3.339827, norm:0.2656, lr:3.3746e-04 dt: 3332.02ms, tok/sec:157348.38
step 7860, loss: 3.300136, norm:0.2903, lr:3.3741e-04 dt: 3331.98ms, tok/sec:157350.16
step 7861, loss: 3.344926, norm:0.2693, lr:3.3735e-04 dt: 3332.31ms, tok/sec:157334.89
step 7862, loss: 3.422319, norm:0.2658, lr:3.3729e-04 dt: 3332.36ms, tok/sec:157332.45
step 7863, loss: 3.348048, norm:0.2927, lr:3.3723e-04 dt: 3332.01ms, tok/sec:157348.70
step 7864, loss: 3.264993, norm:0.2785, lr:3.3717e-04 dt: 3332.36ms, tok/sec:157332.56
step 7865, loss: 3.349534, norm:0.2924, lr:3.3711e-04 dt: 3331.87ms, tok/sec:157355.52
step 7866, loss: 3.350075, norm:0.2681, lr:3.3706e-04 dt: 3331.98ms, tok/sec:157350.34
step 7867, loss: 3.326745, norm:0.2721, lr:3.3700e-04 dt: 3332.03ms, tok/sec:157347.79
step 7868, loss: 3.329985, norm:0.2707, lr:3.3694e-04 dt: 3332.13ms, tok/sec:157343.41
step 7869, loss: 3.363046, norm:0.2517, lr:3.3688e-04 dt: 3332.04ms, tok/sec:157347.47
step 7870, loss: 3.390231, norm:0.2708, lr:3.3682e-04 dt: 3332.02ms, tok/sec:157348.21
step 7871, loss: 3.342249, norm:0.2852, lr:3.3676e-04 dt: 3332.19ms, tok/sec:157340.23
step 7872, loss: 3.364556, norm:0.2663, lr:3.3671e-04 dt: 3332.47ms, tok/sec:157327.09
step 7873, loss: 3.411057, norm:0.2793, lr:3.3665e-04 dt: 3332.63ms, tok/sec:157319.80
step 7874, loss: 3.416478, norm:0.2511, lr:3.3659e-04 dt: 3332.04ms, tok/sec:157347.22
step 7875, loss: 3.414418, norm:0.2905, lr:3.3653e-04 dt: 3332.01ms, tok/sec:157348.64
step 7876, loss: 3.347456, norm:0.2683, lr:3.3647e-04 dt: 3332.14ms, tok/sec:157342.73
step 7877, loss: 3.383748, norm:0.2716, lr:3.3641e-04 dt: 3331.91ms, tok/sec:157353.72
step 7878, loss: 3.404184, norm:0.3722, lr:3.3636e-04 dt: 3332.46ms, tok/sec:157327.58
step 7879, loss: 3.369365, norm:0.2776, lr:3.3630e-04 dt: 3333.10ms, tok/sec:157297.43
step 7880, loss: 3.348735, norm:0.2611, lr:3.3624e-04 dt: 3332.44ms, tok/sec:157328.48
step 7881, loss: 3.306130, norm:0.2572, lr:3.3618e-04 dt: 3332.22ms, tok/sec:157339.11
step 7882, loss: 3.326575, norm:0.2669, lr:3.3612e-04 dt: 3332.05ms, tok/sec:157346.94
step 7883, loss: 3.327150, norm:0.2387, lr:3.3606e-04 dt: 3332.05ms, tok/sec:157347.06
step 7884, loss: 3.330970, norm:0.2721, lr:3.3601e-04 dt: 3332.01ms, tok/sec:157349.09
step 7885, loss: 3.377543, norm:0.2571, lr:3.3595e-04 dt: 3332.04ms, tok/sec:157347.22
step 7886, loss: 3.288415, norm:0.2464, lr:3.3589e-04 dt: 3331.95ms, tok/sec:157351.80
step 7887, loss: 3.337885, norm:0.2463, lr:3.3583e-04 dt: 3332.09ms, tok/sec:157345.26
step 7888, loss: 3.305070, norm:0.2438, lr:3.3577e-04 dt: 3332.22ms, tok/sec:157338.89
step 7889, loss: 3.316279, norm:0.2580, lr:3.3572e-04 dt: 3331.91ms, tok/sec:157353.53
step 7890, loss: 3.287841, norm:0.2381, lr:3.3566e-04 dt: 3332.26ms, tok/sec:157336.99
step 7891, loss: 3.304798, norm:0.2540, lr:3.3560e-04 dt: 3331.96ms, tok/sec:157351.24
step 7892, loss: 3.360830, norm:0.3187, lr:3.3554e-04 dt: 3331.93ms, tok/sec:157352.83
step 7893, loss: 3.324151, norm:0.2614, lr:3.3548e-04 dt: 3332.17ms, tok/sec:157341.24
step 7894, loss: 3.368777, norm:0.2885, lr:3.3542e-04 dt: 3331.90ms, tok/sec:157354.15
step 7895, loss: 3.402385, norm:0.2639, lr:3.3537e-04 dt: 3332.07ms, tok/sec:157345.93
step 7896, loss: 3.312495, norm:0.2949, lr:3.3531e-04 dt: 3332.15ms, tok/sec:157342.14
step 7897, loss: 3.327848, norm:0.2794, lr:3.3525e-04 dt: 3332.18ms, tok/sec:157340.85
step 7898, loss: 3.336324, norm:0.2588, lr:3.3519e-04 dt: 3332.06ms, tok/sec:157346.33
step 7899, loss: 3.359120, norm:0.2742, lr:3.3513e-04 dt: 3332.67ms, tok/sec:157317.49
validation loss: 3.3362
Model and optimizer state saved.
HellaSwag accuracy:2326254004169016401/-2=-1163127002084508160.0000
rank 1 sample 0: Hello, I'm a language model, using language modeling to show the syntax at a glance and help keep the model clear. But it was a good morning.
rank 1 sample 1: Hello, I'm a language model, which I'll look at later. This can be tricky due to the fact that the most basic one-to-one
rank 1 sample 2: Hello, I'm a language model, so learning the difference between a language and a language is pretty challenging! But I'm not going to just talk about our
rank 1 sample 3: Hello, I'm a language model, and I'm interested to keep your eyes to the results below.
Here are the result "test" results that I
rank 0 sample 0: Hello, I'm a language model, and I need to learn my grammar." After they got the idea what words would be the future: I need to understand
rank 0 sample 1: Hello, I'm a language model, so there's a lot of stuff going on in one lesson, like doing an activity, you can ask the students to
rank 0 sample 2: Hello, I'm a language model, I'm being trained some sort of. I was very good about that. I didn't think I'd be a "
rank 0 sample 3: Hello, I'm a language model, so maybe I've got a few questions, for example, how much I know how to type a byte in Java using
step 7900, loss: 3.338131, norm:0.2616, lr:3.3507e-04 dt: 56228.68ms, tok/sec:9324.21
step 7901, loss: 3.379713, norm:0.2581, lr:3.3502e-04 dt: 3332.40ms, tok/sec:157330.31
step 7902, loss: 3.344553, norm:0.2539, lr:3.3496e-04 dt: 3332.17ms, tok/sec:157341.54
step 7903, loss: 3.378164, norm:0.3522, lr:3.3490e-04 dt: 3332.41ms, tok/sec:157330.13
step 7904, loss: 3.282283, norm:0.2972, lr:3.3484e-04 dt: 3332.20ms, tok/sec:157339.69
step 7905, loss: 3.320155, norm:0.3186, lr:3.3478e-04 dt: 3332.04ms, tok/sec:157347.65
step 7906, loss: 3.393175, norm:0.2624, lr:3.3472e-04 dt: 3332.02ms, tok/sec:157348.33
step 7907, loss: 3.381806, norm:0.2777, lr:3.3467e-04 dt: 3332.08ms, tok/sec:157345.76
step 7908, loss: 3.362010, norm:0.2652, lr:3.3461e-04 dt: 3331.81ms, tok/sec:157358.53
step 7909, loss: 3.354443, norm:0.2760, lr:3.3455e-04 dt: 3332.33ms, tok/sec:157333.54
step 7910, loss: 3.398482, norm:0.2519, lr:3.3449e-04 dt: 3331.99ms, tok/sec:157349.80
step 7911, loss: 3.395123, norm:0.2790, lr:3.3443e-04 dt: 3332.06ms, tok/sec:157346.50
step 7912, loss: 3.572375, norm:0.2801, lr:3.3437e-04 dt: 3332.00ms, tok/sec:157349.51
step 7913, loss: 3.347519, norm:0.2690, lr:3.3432e-04 dt: 3332.43ms, tok/sec:157329.18
step 7914, loss: 3.435577, norm:0.2491, lr:3.3426e-04 dt: 3332.03ms, tok/sec:157347.85
step 7915, loss: 3.322235, norm:0.2671, lr:3.3420e-04 dt: 3332.12ms, tok/sec:157343.48
step 7916, loss: 3.294458, norm:0.2592, lr:3.3414e-04 dt: 3331.93ms, tok/sec:157352.85
step 7917, loss: 3.314091, norm:0.2416, lr:3.3408e-04 dt: 3332.10ms, tok/sec:157344.71
step 7918, loss: 3.327854, norm:0.2529, lr:3.3402e-04 dt: 3331.97ms, tok/sec:157350.96
step 7919, loss: 3.292721, norm:0.2457, lr:3.3397e-04 dt: 3332.26ms, tok/sec:157336.91
step 7920, loss: 3.327531, norm:0.2575, lr:3.3391e-04 dt: 3332.10ms, tok/sec:157344.51
step 7921, loss: 3.270190, norm:0.2526, lr:3.3385e-04 dt: 3332.02ms, tok/sec:157348.61
step 7922, loss: 3.337218, norm:0.2526, lr:3.3379e-04 dt: 3332.16ms, tok/sec:157341.58
step 7923, loss: 3.247105, norm:0.2639, lr:3.3373e-04 dt: 3332.39ms, tok/sec:157330.74
step 7924, loss: 3.358426, norm:0.2460, lr:3.3367e-04 dt: 3331.97ms, tok/sec:157350.77
step 7925, loss: 3.318847, norm:0.2554, lr:3.3362e-04 dt: 3331.94ms, tok/sec:157352.00
step 7926, loss: 3.347259, norm:0.2890, lr:3.3356e-04 dt: 3331.94ms, tok/sec:157352.29
step 7927, loss: 3.351223, norm:0.2618, lr:3.3350e-04 dt: 3332.11ms, tok/sec:157344.35
step 7928, loss: 3.423347, norm:0.2626, lr:3.3344e-04 dt: 3332.06ms, tok/sec:157346.61
step 7929, loss: 3.299955, norm:0.2516, lr:3.3338e-04 dt: 3332.09ms, tok/sec:157345.16
step 7930, loss: 3.364005, norm:0.2813, lr:3.3332e-04 dt: 3331.88ms, tok/sec:157354.99
step 7931, loss: 3.386361, norm:0.2610, lr:3.3327e-04 dt: 3332.34ms, tok/sec:157333.20
step 7932, loss: 3.429035, norm:0.2678, lr:3.3321e-04 dt: 3332.29ms, tok/sec:157335.83
step 7933, loss: 3.293856, norm:0.2498, lr:3.3315e-04 dt: 3332.04ms, tok/sec:157347.51
step 7934, loss: 3.331449, norm:0.2445, lr:3.3309e-04 dt: 3332.12ms, tok/sec:157343.66
step 7935, loss: 3.274999, norm:0.2909, lr:3.3303e-04 dt: 3332.10ms, tok/sec:157344.75
step 7936, loss: 3.341241, norm:0.2498, lr:3.3297e-04 dt: 3332.01ms, tok/sec:157348.90
step 7937, loss: 3.322985, norm:0.2614, lr:3.3292e-04 dt: 3332.00ms, tok/sec:157349.40
step 7938, loss: 3.415722, norm:0.2844, lr:3.3286e-04 dt: 3332.06ms, tok/sec:157346.45
step 7939, loss: 3.392621, norm:0.2536, lr:3.3280e-04 dt: 3332.12ms, tok/sec:157343.84
step 7940, loss: 3.310854, norm:0.2687, lr:3.3274e-04 dt: 3332.32ms, tok/sec:157334.11
step 7941, loss: 3.382229, norm:0.3407, lr:3.3268e-04 dt: 3332.36ms, tok/sec:157332.17
step 7942, loss: 3.344666, norm:0.2599, lr:3.3262e-04 dt: 3332.12ms, tok/sec:157343.88
step 7943, loss: 3.378828, norm:0.2707, lr:3.3257e-04 dt: 3332.28ms, tok/sec:157336.28
step 7944, loss: 3.404604, norm:0.2649, lr:3.3251e-04 dt: 3331.99ms, tok/sec:157349.74
step 7945, loss: 3.355365, norm:0.2503, lr:3.3245e-04 dt: 3331.97ms, tok/sec:157350.54
step 7946, loss: 3.351593, norm:0.2687, lr:3.3239e-04 dt: 3332.29ms, tok/sec:157335.65
step 7947, loss: 3.438974, norm:0.2625, lr:3.3233e-04 dt: 3332.27ms, tok/sec:157336.48
step 7948, loss: 3.360651, norm:0.2541, lr:3.3227e-04 dt: 3332.24ms, tok/sec:157338.00
step 7949, loss: 3.311660, norm:0.2499, lr:3.3222e-04 dt: 3332.06ms, tok/sec:157346.48
HellaSwag accuracy:2325233657378473041/-2=-1162616828689236480.0000
rank 1 sample 0: Hello, I'm a language model, the object is a model, and only one. So for I'm a language model, and you can find it in
rank 1 sample 1: Hello, I'm a language model, which I can refer to in an application without having to make a "true" declaration but what does it mean?

rank 1 sample 2: Hello, I'm a language model, but is there a way to do it?
Hello, is this an application?
I want it to look something
rank 1 sample 3: Hello, I'm a language model, and I'm working with English. Now think about what grammar it's supposed to be--”
- What is
rank 0 sample 0: Hello, I'm a language model, and I think it is useful in helping everyone connect with a new skill so you can talk to the class at the time
rank 0 sample 1: Hello, I'm a language model, I was surprised to learn that you see the first name given by an email to a child with an 's' name
rank 0 sample 2: Hello, I'm a language model, I'm teaching all people using Microsoft, and this is a big help, but I can't wait to see how all
rank 0 sample 3: Hello, I'm a language model, which includes a variety of languages. A few points to note:
- There will be an improvement in the spelling because
step 7950, loss: 3.336232, norm:0.2737, lr:3.3216e-04 dt: 48518.17ms, tok/sec:10806.01
step 7951, loss: 3.301792, norm:0.2478, lr:3.3210e-04 dt: 3332.01ms, tok/sec:157349.02
step 7952, loss: 3.280353, norm:0.2508, lr:3.3204e-04 dt: 3332.35ms, tok/sec:157332.82
step 7953, loss: 3.277264, norm:0.3833, lr:3.3198e-04 dt: 3332.10ms, tok/sec:157344.59
step 7954, loss: 3.377092, norm:0.2947, lr:3.3192e-04 dt: 3331.98ms, tok/sec:157350.27
step 7955, loss: 3.285713, norm:0.3061, lr:3.3187e-04 dt: 3332.09ms, tok/sec:157344.97
step 7956, loss: 3.315643, norm:0.3455, lr:3.3181e-04 dt: 3332.28ms, tok/sec:157336.05
step 7957, loss: 3.329158, norm:0.2505, lr:3.3175e-04 dt: 3332.02ms, tok/sec:157348.52
step 7958, loss: 3.339242, norm:0.2959, lr:3.3169e-04 dt: 3332.19ms, tok/sec:157340.19
step 7959, loss: 3.337294, norm:0.2904, lr:3.3163e-04 dt: 3332.28ms, tok/sec:157336.08
step 7960, loss: 3.363315, norm:0.3182, lr:3.3157e-04 dt: 3331.93ms, tok/sec:157352.75
step 7961, loss: 3.357591, norm:0.3095, lr:3.3152e-04 dt: 3332.62ms, tok/sec:157320.26
step 7962, loss: 3.327450, norm:0.2741, lr:3.3146e-04 dt: 3331.91ms, tok/sec:157353.43
step 7963, loss: 3.291222, norm:0.3074, lr:3.3140e-04 dt: 3332.10ms, tok/sec:157344.41
step 7964, loss: 3.335491, norm:0.2704, lr:3.3134e-04 dt: 3332.12ms, tok/sec:157343.75
step 7965, loss: 3.376017, norm:0.2759, lr:3.3128e-04 dt: 3332.32ms, tok/sec:157334.38
step 7966, loss: 3.346608, norm:0.2774, lr:3.3122e-04 dt: 3332.22ms, tok/sec:157339.12
step 7967, loss: 3.302837, norm:0.2750, lr:3.3117e-04 dt: 3332.07ms, tok/sec:157346.03
step 7968, loss: 3.336598, norm:0.2720, lr:3.3111e-04 dt: 3332.38ms, tok/sec:157331.53
step 7969, loss: 3.352169, norm:0.2521, lr:3.3105e-04 dt: 3332.07ms, tok/sec:157346.03
step 7970, loss: 3.353866, norm:0.2909, lr:3.3099e-04 dt: 3332.33ms, tok/sec:157333.52
step 7971, loss: 3.342344, norm:0.2626, lr:3.3093e-04 dt: 3332.16ms, tok/sec:157341.63
step 7972, loss: 3.347321, norm:0.2682, lr:3.3087e-04 dt: 3332.02ms, tok/sec:157348.33
step 7973, loss: 3.478765, norm:0.2947, lr:3.3082e-04 dt: 3332.01ms, tok/sec:157348.64
step 7974, loss: 3.368206, norm:0.2729, lr:3.3076e-04 dt: 3332.48ms, tok/sec:157326.78
step 7975, loss: 3.357611, norm:0.2668, lr:3.3070e-04 dt: 3332.25ms, tok/sec:157337.50
step 7976, loss: 3.381464, norm:0.2521, lr:3.3064e-04 dt: 3331.82ms, tok/sec:157357.76
step 7977, loss: 3.378966, norm:0.2479, lr:3.3058e-04 dt: 3332.13ms, tok/sec:157343.19
step 7978, loss: 3.429213, norm:0.2727, lr:3.3052e-04 dt: 3332.69ms, tok/sec:157316.68
step 7979, loss: 3.376487, norm:0.2739, lr:3.3047e-04 dt: 3331.87ms, tok/sec:157355.61
step 7980, loss: 3.364671, norm:0.2492, lr:3.3041e-04 dt: 3332.01ms, tok/sec:157348.91
step 7981, loss: 3.342481, norm:0.2697, lr:3.3035e-04 dt: 3332.21ms, tok/sec:157339.57
step 7982, loss: 3.356889, norm:0.2896, lr:3.3029e-04 dt: 3332.23ms, tok/sec:157338.50
step 7983, loss: 3.382403, norm:0.2560, lr:3.3023e-04 dt: 3332.51ms, tok/sec:157325.18
step 7984, loss: 3.311506, norm:0.2534, lr:3.3017e-04 dt: 3332.06ms, tok/sec:157346.30
step 7985, loss: 3.320127, norm:0.2658, lr:3.3012e-04 dt: 3332.16ms, tok/sec:157341.63
step 7986, loss: 3.353755, norm:0.2504, lr:3.3006e-04 dt: 3332.19ms, tok/sec:157340.42
step 7987, loss: 3.360593, norm:0.2565, lr:3.3000e-04 dt: 3331.97ms, tok/sec:157350.75
step 7988, loss: 3.309956, norm:0.2448, lr:3.2994e-04 dt: 3331.96ms, tok/sec:157351.37
step 7989, loss: 3.370530, norm:0.2596, lr:3.2988e-04 dt: 3332.20ms, tok/sec:157340.12
step 7990, loss: 3.340124, norm:0.2462, lr:3.2983e-04 dt: 3332.45ms, tok/sec:157328.20
step 7991, loss: 3.307754, norm:0.2589, lr:3.2977e-04 dt: 3332.02ms, tok/sec:157348.57
step 7992, loss: 3.266778, norm:0.2527, lr:3.2971e-04 dt: 3332.26ms, tok/sec:157336.84
step 7993, loss: 3.259117, norm:0.2397, lr:3.2965e-04 dt: 3332.27ms, tok/sec:157336.74
step 7994, loss: 3.314869, norm:0.2516, lr:3.2959e-04 dt: 3331.86ms, tok/sec:157355.81
step 7995, loss: 3.288259, norm:0.2573, lr:3.2953e-04 dt: 3332.11ms, tok/sec:157344.32
step 7996, loss: 3.363964, norm:0.2461, lr:3.2948e-04 dt: 3332.32ms, tok/sec:157333.99
step 7997, loss: 3.357655, norm:0.2528, lr:3.2942e-04 dt: 3332.56ms, tok/sec:157323.08
step 7998, loss: 3.363928, norm:0.2584, lr:3.2936e-04 dt: 3332.18ms, tok/sec:157340.88
step 7999, loss: 3.323899, norm:0.2506, lr:3.2930e-04 dt: 3332.01ms, tok/sec:157348.88
validation loss: 3.3322
Model and optimizer state saved.
HellaSwag accuracy:-6898248330639080431/-2=3449124165319540224.0000
rank 1 sample 0: Hello, I'm a language model, and my training is good, but your language does not come around as expected.
In fact, I am not a
rank 1 sample 1: Hello, I'm a language model, not just for grammar, but for teaching all the other systems. I'm interested to point out, that the language model
rank 1 sample 2: Hello, I'm a language model, but at this point I'm not sure what I'm looking at here."
"I think you should have thought of
rank 1 sample 3: Hello, I'm a language model, and I'm pretty excited about this thing myself.
Hieroff, Richard. 1999. 'The D' is
rank 0 sample 0: Hello, I'm a language model, and I love the way my children work."
As such, her project "The Great Pyramid, The Story of My
rank 0 sample 1: Hello, I'm a language model, so to speak, to speak. They're talking when in the world! They're not talking about me, they're
rank 0 sample 2: Hello, I'm a language model, so I had the confidence to communicate, but then I was frustrated because I didn't understand how to use the word,
rank 0 sample 3: Hello, I'm a language model, you do not have to be an ESL student without a language background. But, they need to have fun. So don
step 8000, loss: 3.363086, norm:0.2522, lr:3.2924e-04 dt: 56468.22ms, tok/sec:9284.66
step 8001, loss: 3.331238, norm:0.2437, lr:3.2918e-04 dt: 3332.34ms, tok/sec:157333.15
step 8002, loss: 3.317775, norm:0.2675, lr:3.2913e-04 dt: 3332.17ms, tok/sec:157341.20
step 8003, loss: 3.368376, norm:0.2505, lr:3.2907e-04 dt: 3332.11ms, tok/sec:157344.18
step 8004, loss: 3.362934, norm:0.2629, lr:3.2901e-04 dt: 3332.20ms, tok/sec:157339.81
step 8005, loss: 3.375110, norm:0.2539, lr:3.2895e-04 dt: 3332.11ms, tok/sec:157344.09
step 8006, loss: 3.354782, norm:0.2433, lr:3.2889e-04 dt: 3332.18ms, tok/sec:157340.90
step 8007, loss: 3.354316, norm:0.2542, lr:3.2883e-04 dt: 3332.42ms, tok/sec:157329.63
step 8008, loss: 3.358711, norm:0.2351, lr:3.2878e-04 dt: 3332.16ms, tok/sec:157341.80
step 8009, loss: 3.394826, norm:0.2510, lr:3.2872e-04 dt: 3332.06ms, tok/sec:157346.36
step 8010, loss: 3.372792, norm:0.2577, lr:3.2866e-04 dt: 3331.77ms, tok/sec:157360.04
step 8011, loss: 3.359621, norm:0.2578, lr:3.2860e-04 dt: 3331.91ms, tok/sec:157353.78
step 8012, loss: 3.359478, norm:0.2605, lr:3.2854e-04 dt: 3332.06ms, tok/sec:157346.61
step 8013, loss: 3.344382, norm:0.2634, lr:3.2848e-04 dt: 3332.07ms, tok/sec:157346.27
step 8014, loss: 3.400257, norm:0.2610, lr:3.2843e-04 dt: 3332.19ms, tok/sec:157340.34
step 8015, loss: 3.387363, norm:0.2412, lr:3.2837e-04 dt: 3332.12ms, tok/sec:157343.64
step 8016, loss: 3.423654, norm:0.2665, lr:3.2831e-04 dt: 3332.03ms, tok/sec:157348.13
step 8017, loss: 3.362820, norm:0.2575, lr:3.2825e-04 dt: 3331.96ms, tok/sec:157351.37
step 8018, loss: 3.350549, norm:0.2519, lr:3.2819e-04 dt: 3332.12ms, tok/sec:157343.54
step 8019, loss: 3.338270, norm:0.2458, lr:3.2813e-04 dt: 3332.03ms, tok/sec:157347.81
step 8020, loss: 3.264167, norm:0.2638, lr:3.2808e-04 dt: 3332.02ms, tok/sec:157348.44
step 8021, loss: 3.364542, norm:0.2539, lr:3.2802e-04 dt: 3332.11ms, tok/sec:157344.28
step 8022, loss: 3.287318, norm:0.2546, lr:3.2796e-04 dt: 3331.91ms, tok/sec:157353.46
step 8023, loss: 3.313223, norm:0.2367, lr:3.2790e-04 dt: 3331.99ms, tok/sec:157349.90
step 8024, loss: 3.406210, norm:0.2577, lr:3.2784e-04 dt: 3332.09ms, tok/sec:157345.00
step 8025, loss: 3.337053, norm:0.2618, lr:3.2778e-04 dt: 3332.06ms, tok/sec:157346.33
step 8026, loss: 3.297440, norm:0.2614, lr:3.2773e-04 dt: 3332.19ms, tok/sec:157340.24
step 8027, loss: 3.288139, norm:0.2528, lr:3.2767e-04 dt: 3332.02ms, tok/sec:157348.34
step 8028, loss: 3.300683, norm:0.2700, lr:3.2761e-04 dt: 3332.37ms, tok/sec:157332.06
step 8029, loss: 3.248740, norm:0.2553, lr:3.2755e-04 dt: 3332.00ms, tok/sec:157349.16
step 8030, loss: 3.293862, norm:0.2547, lr:3.2749e-04 dt: 3331.94ms, tok/sec:157352.07
step 8031, loss: 3.355903, norm:0.2613, lr:3.2743e-04 dt: 3332.00ms, tok/sec:157349.34
step 8032, loss: 3.359747, norm:0.2677, lr:3.2738e-04 dt: 3332.12ms, tok/sec:157343.64
step 8033, loss: 3.301949, norm:0.2560, lr:3.2732e-04 dt: 3332.08ms, tok/sec:157345.70
step 8034, loss: 3.333375, norm:0.3111, lr:3.2726e-04 dt: 3332.10ms, tok/sec:157344.45
step 8035, loss: 3.363952, norm:0.2678, lr:3.2720e-04 dt: 3332.41ms, tok/sec:157330.21
step 8036, loss: 3.288068, norm:0.2812, lr:3.2714e-04 dt: 3332.09ms, tok/sec:157344.93
step 8037, loss: 3.338768, norm:0.3319, lr:3.2708e-04 dt: 3332.00ms, tok/sec:157349.47
step 8038, loss: 3.362473, norm:0.3006, lr:3.2703e-04 dt: 3332.07ms, tok/sec:157346.21
step 8039, loss: 3.400287, norm:0.3858, lr:3.2697e-04 dt: 3332.08ms, tok/sec:157345.41
step 8040, loss: 3.356368, norm:0.2983, lr:3.2691e-04 dt: 3332.24ms, tok/sec:157337.97
step 8041, loss: 3.359540, norm:0.3612, lr:3.2685e-04 dt: 3332.31ms, tok/sec:157334.78
step 8042, loss: 3.364768, norm:0.2881, lr:3.2679e-04 dt: 3332.00ms, tok/sec:157349.26
step 8043, loss: 3.322448, norm:0.2766, lr:3.2673e-04 dt: 3332.10ms, tok/sec:157344.55
step 8044, loss: 3.349472, norm:0.2753, lr:3.2668e-04 dt: 3332.38ms, tok/sec:157331.54
step 8045, loss: 3.377882, norm:0.2697, lr:3.2662e-04 dt: 3332.38ms, tok/sec:157331.39
step 8046, loss: 3.452349, norm:0.2633, lr:3.2656e-04 dt: 3331.96ms, tok/sec:157351.05
step 8047, loss: 3.357612, norm:0.2534, lr:3.2650e-04 dt: 3332.10ms, tok/sec:157344.53
step 8048, loss: 3.380083, norm:0.2584, lr:3.2644e-04 dt: 3332.02ms, tok/sec:157348.31
step 8049, loss: 3.347940, norm:0.2527, lr:3.2638e-04 dt: 3332.20ms, tok/sec:157339.97
HellaSwag accuracy:-2286469953237056431/-2=1143234976618528256.0000
rank 1 sample 0: Hello, I'm a language model, for those who are unfamiliar with the abstract concept, which is about the structure of the human body: the human body is
rank 1 sample 1: Hello, I'm a language model, I want to talk about the way language looks. But here's the thing you can find. Like I said, I
rank 1 sample 2: Hello, I'm a language model, but like many other languages, I'm not sure if that has happened in the past. But this is the best reason
rank 1 sample 3: Hello, I'm a language model, and I'm pretty much working with it without any trouble doing this again. Can you point me at the left in the
rank 0 sample 0: Hello, I'm a language model, and I'll be writing all about "coral" now. What happens?
You said you were looking for some
rank 0 sample 1: Hello, I'm a language model, I was pretty much an idiot when I was writing some blog post for iambi and julietive.

rank 0 sample 2: Hello, I'm a language model, so I do the real jobs. And I just make a good start by making a good start by making a little introduction
rank 0 sample 3: Hello, I'm a language model, you would almost certainly be interested to see the best of the ways a language is described by its grammar.
So a
step 8050, loss: 3.363098, norm:0.2734, lr:3.2633e-04 dt: 48519.13ms, tok/sec:10805.80
step 8051, loss: 3.299561, norm:0.2641, lr:3.2627e-04 dt: 3331.86ms, tok/sec:157356.00
step 8052, loss: 3.365529, norm:0.2646, lr:3.2621e-04 dt: 3332.45ms, tok/sec:157328.02
step 8053, loss: 3.338627, norm:0.2563, lr:3.2615e-04 dt: 3332.22ms, tok/sec:157338.77
step 8054, loss: 3.330635, norm:0.2527, lr:3.2609e-04 dt: 3332.07ms, tok/sec:157346.04
step 8055, loss: 3.335040, norm:0.2404, lr:3.2603e-04 dt: 3331.94ms, tok/sec:157352.38
step 8056, loss: 3.302829, norm:0.2469, lr:3.2598e-04 dt: 3332.17ms, tok/sec:157341.39
step 8057, loss: 3.324301, norm:0.2440, lr:3.2592e-04 dt: 3331.98ms, tok/sec:157350.23
step 8058, loss: 3.320797, norm:0.2421, lr:3.2586e-04 dt: 3332.02ms, tok/sec:157348.62
step 8059, loss: 3.282407, norm:0.2643, lr:3.2580e-04 dt: 3332.29ms, tok/sec:157335.44
step 8060, loss: 3.279435, norm:0.2477, lr:3.2574e-04 dt: 3332.04ms, tok/sec:157347.41
step 8061, loss: 3.280297, norm:0.2623, lr:3.2568e-04 dt: 3332.47ms, tok/sec:157327.34
step 8062, loss: 3.294837, norm:0.2503, lr:3.2563e-04 dt: 3331.92ms, tok/sec:157353.14
step 8063, loss: 3.227757, norm:0.2803, lr:3.2557e-04 dt: 3332.02ms, tok/sec:157348.26
step 8064, loss: 3.362014, norm:0.2610, lr:3.2551e-04 dt: 3331.96ms, tok/sec:157351.13
step 8065, loss: 3.324001, norm:0.2830, lr:3.2545e-04 dt: 3332.03ms, tok/sec:157347.85
step 8066, loss: 3.339357, norm:0.2499, lr:3.2539e-04 dt: 3331.98ms, tok/sec:157350.36
step 8067, loss: 3.334127, norm:0.2733, lr:3.2533e-04 dt: 3332.09ms, tok/sec:157344.93
step 8068, loss: 3.344891, norm:0.2862, lr:3.2528e-04 dt: 3332.20ms, tok/sec:157340.06
step 8069, loss: 3.339140, norm:0.2577, lr:3.2522e-04 dt: 3332.20ms, tok/sec:157339.77
step 8070, loss: 3.314858, norm:0.2739, lr:3.2516e-04 dt: 3331.92ms, tok/sec:157353.11
step 8071, loss: 3.268134, norm:0.2854, lr:3.2510e-04 dt: 3332.15ms, tok/sec:157342.09
step 8072, loss: 3.310862, norm:0.2714, lr:3.2504e-04 dt: 3332.12ms, tok/sec:157343.80
step 8073, loss: 3.309584, norm:0.2703, lr:3.2498e-04 dt: 3331.99ms, tok/sec:157349.79
step 8074, loss: 3.320198, norm:0.2629, lr:3.2493e-04 dt: 3332.21ms, tok/sec:157339.63
step 8075, loss: 3.396395, norm:0.2565, lr:3.2487e-04 dt: 3332.34ms, tok/sec:157333.31
step 8076, loss: 3.324336, norm:0.2765, lr:3.2481e-04 dt: 3332.01ms, tok/sec:157348.98
step 8077, loss: 3.347445, norm:0.2429, lr:3.2475e-04 dt: 3332.43ms, tok/sec:157328.96
step 8078, loss: 3.363956, norm:0.2435, lr:3.2469e-04 dt: 3331.98ms, tok/sec:157350.33
step 8079, loss: 3.430572, norm:0.2993, lr:3.2463e-04 dt: 3331.83ms, tok/sec:157357.54
step 8080, loss: 3.358767, norm:0.2667, lr:3.2458e-04 dt: 3332.30ms, tok/sec:157335.12
step 8081, loss: 3.331940, norm:0.2529, lr:3.2452e-04 dt: 3332.03ms, tok/sec:157347.76
step 8082, loss: 3.406286, norm:0.2728, lr:3.2446e-04 dt: 3332.12ms, tok/sec:157343.59
step 8083, loss: 3.363276, norm:0.2555, lr:3.2440e-04 dt: 3332.18ms, tok/sec:157340.81
step 8084, loss: 3.387682, norm:0.2737, lr:3.2434e-04 dt: 3332.24ms, tok/sec:157338.10
step 8085, loss: 3.330000, norm:0.3146, lr:3.2428e-04 dt: 3331.92ms, tok/sec:157353.30
step 8086, loss: 3.361293, norm:0.2834, lr:3.2423e-04 dt: 3332.51ms, tok/sec:157325.15
step 8087, loss: 3.338439, norm:0.2902, lr:3.2417e-04 dt: 3332.03ms, tok/sec:157348.01
step 8088, loss: 3.348690, norm:0.2692, lr:3.2411e-04 dt: 3331.98ms, tok/sec:157350.07
step 8089, loss: 3.287724, norm:0.2452, lr:3.2405e-04 dt: 3332.05ms, tok/sec:157346.77
step 8090, loss: 3.232445, norm:0.2565, lr:3.2399e-04 dt: 3332.01ms, tok/sec:157348.88
step 8091, loss: 3.341644, norm:0.2527, lr:3.2394e-04 dt: 3332.80ms, tok/sec:157311.69
step 8092, loss: 3.271252, norm:0.2404, lr:3.2388e-04 dt: 3333.08ms, tok/sec:157298.23
step 8093, loss: 3.291479, norm:0.2556, lr:3.2382e-04 dt: 3332.30ms, tok/sec:157335.03
step 8094, loss: 3.308178, norm:0.2607, lr:3.2376e-04 dt: 3332.15ms, tok/sec:157342.36
step 8095, loss: 3.356156, norm:0.2497, lr:3.2370e-04 dt: 3332.01ms, tok/sec:157348.89
step 8096, loss: 3.298866, norm:0.2583, lr:3.2364e-04 dt: 3331.91ms, tok/sec:157353.73
step 8097, loss: 3.255242, norm:0.2499, lr:3.2359e-04 dt: 3332.02ms, tok/sec:157348.33
step 8098, loss: 3.295605, norm:0.2739, lr:3.2353e-04 dt: 3332.04ms, tok/sec:157347.68
step 8099, loss: 3.345489, norm:0.2698, lr:3.2347e-04 dt: 3332.09ms, tok/sec:157344.99
validation loss: 3.3312
Model and optimizer state saved.
HellaSwag accuracy:2325079725742457937/-2=-1162539862871228928.0000
rank 1 sample 0: Hello, I'm a language model, and a code that we can use together to code. Now lets say that instead of calling a script, let's call
rank 1 sample 1: Hello, I'm a language model, which I use for my classes. In C++, and in C++, we start from three classes, and then
rank 1 sample 2: Hello, I'm a language model, but is there a way to do this?
Hi, Hi!
And I'm really grateful. I want to
rank 1 sample 3: Hello, I'm a language model, and I'm really happy we have it :)
It uses this programming language model to design dynamic models.
So,
rank 0 sample 0: Hello, I'm a language model, and I know that it's going to include both grammar and semantics classes. The first step is getting to know the classes
rank 0 sample 1: Hello, I'm a language model, but they're the same as us. So all people are saying ``'' and we say ''...'' and ``
rank 0 sample 2: Hello, I'm a language model, so I love the rules about our programming. In this post (or any other language), I'll be going through some
rank 0 sample 3: Hello, I'm a language model, you probably have one or two people and one/or two people working for a country at all, maybe even the only
step 8100, loss: 3.323836, norm:0.2699, lr:3.2341e-04 dt: 56189.26ms, tok/sec:9330.75
step 8101, loss: 3.309049, norm:0.2488, lr:3.2335e-04 dt: 3332.00ms, tok/sec:157349.50
step 8102, loss: 3.388416, norm:0.2745, lr:3.2329e-04 dt: 3332.16ms, tok/sec:157341.75
step 8103, loss: 3.347615, norm:0.2521, lr:3.2324e-04 dt: 3332.35ms, tok/sec:157332.76
step 8104, loss: 3.370177, norm:0.2485, lr:3.2318e-04 dt: 3332.21ms, tok/sec:157339.58
step 8105, loss: 3.391602, norm:0.2760, lr:3.2312e-04 dt: 3332.02ms, tok/sec:157348.34
step 8106, loss: 3.409393, norm:0.3460, lr:3.2306e-04 dt: 3332.11ms, tok/sec:157344.11
step 8107, loss: 3.318719, norm:0.2997, lr:3.2300e-04 dt: 3332.02ms, tok/sec:157348.53
step 8108, loss: 3.358030, norm:0.2823, lr:3.2294e-04 dt: 3331.95ms, tok/sec:157351.68
step 8109, loss: 3.456455, norm:0.3011, lr:3.2289e-04 dt: 3332.13ms, tok/sec:157343.18
step 8110, loss: 3.380997, norm:0.2990, lr:3.2283e-04 dt: 3332.36ms, tok/sec:157332.40
step 8111, loss: 3.356638, norm:0.2791, lr:3.2277e-04 dt: 3332.19ms, tok/sec:157340.35
step 8112, loss: 3.315231, norm:0.2741, lr:3.2271e-04 dt: 3331.96ms, tok/sec:157351.22
step 8113, loss: 3.440509, norm:0.2825, lr:3.2265e-04 dt: 3332.26ms, tok/sec:157337.06
step 8114, loss: 3.335154, norm:0.2679, lr:3.2259e-04 dt: 3332.08ms, tok/sec:157345.41
step 8115, loss: 3.377519, norm:0.2760, lr:3.2254e-04 dt: 3332.47ms, tok/sec:157327.15
step 8116, loss: 3.369566, norm:0.2490, lr:3.2248e-04 dt: 3332.45ms, tok/sec:157328.31
step 8117, loss: 3.341836, norm:0.2574, lr:3.2242e-04 dt: 3332.28ms, tok/sec:157336.29
step 8118, loss: 3.374436, norm:0.2420, lr:3.2236e-04 dt: 3332.16ms, tok/sec:157342.00
step 8119, loss: 3.297376, norm:0.2528, lr:3.2230e-04 dt: 3332.13ms, tok/sec:157343.09
step 8120, loss: 3.374108, norm:0.2503, lr:3.2224e-04 dt: 3332.03ms, tok/sec:157348.13
step 8121, loss: 3.365621, norm:0.2459, lr:3.2219e-04 dt: 3332.00ms, tok/sec:157349.52
step 8122, loss: 3.273753, norm:0.2439, lr:3.2213e-04 dt: 3332.33ms, tok/sec:157333.85
step 8123, loss: 3.312356, norm:0.2558, lr:3.2207e-04 dt: 3332.24ms, tok/sec:157338.14
step 8124, loss: 3.320425, norm:0.2584, lr:3.2201e-04 dt: 3332.03ms, tok/sec:157347.94
step 8125, loss: 3.325720, norm:0.2519, lr:3.2195e-04 dt: 3331.92ms, tok/sec:157353.23
step 8126, loss: 3.312871, norm:0.2550, lr:3.2189e-04 dt: 3332.03ms, tok/sec:157347.83
step 8127, loss: 3.309265, norm:0.2493, lr:3.2184e-04 dt: 3332.07ms, tok/sec:157346.18
step 8128, loss: 3.303316, norm:0.2576, lr:3.2178e-04 dt: 3331.86ms, tok/sec:157355.91
step 8129, loss: 3.331325, norm:0.2621, lr:3.2172e-04 dt: 3332.29ms, tok/sec:157335.73
step 8130, loss: 3.333925, norm:0.2577, lr:3.2166e-04 dt: 3332.08ms, tok/sec:157345.71
step 8131, loss: 3.297771, norm:0.2624, lr:3.2160e-04 dt: 3332.66ms, tok/sec:157318.37
step 8132, loss: 3.330892, norm:0.2329, lr:3.2154e-04 dt: 3332.08ms, tok/sec:157345.62
step 8133, loss: 3.264263, norm:0.2488, lr:3.2149e-04 dt: 3332.17ms, tok/sec:157341.29
step 8134, loss: 3.325341, norm:0.2850, lr:3.2143e-04 dt: 3332.17ms, tok/sec:157341.27
step 8135, loss: 3.411611, norm:0.2578, lr:3.2137e-04 dt: 3332.16ms, tok/sec:157341.65
step 8136, loss: 3.345435, norm:0.2746, lr:3.2131e-04 dt: 3332.28ms, tok/sec:157336.13
step 8137, loss: 3.357339, norm:0.2641, lr:3.2125e-04 dt: 3332.19ms, tok/sec:157340.42
step 8138, loss: 3.367652, norm:0.2571, lr:3.2119e-04 dt: 3332.46ms, tok/sec:157327.59
step 8139, loss: 3.394691, norm:0.2666, lr:3.2114e-04 dt: 3332.14ms, tok/sec:157342.65
step 8140, loss: 3.383395, norm:0.2536, lr:3.2108e-04 dt: 3332.03ms, tok/sec:157347.82
step 8141, loss: 3.280212, norm:0.3049, lr:3.2102e-04 dt: 3331.82ms, tok/sec:157358.05
step 8142, loss: 3.331513, norm:0.2690, lr:3.2096e-04 dt: 3332.13ms, tok/sec:157343.20
step 8143, loss: 3.348171, norm:0.2619, lr:3.2090e-04 dt: 3332.28ms, tok/sec:157335.99
step 8144, loss: 3.393464, norm:0.2524, lr:3.2085e-04 dt: 3332.17ms, tok/sec:157341.37
step 8145, loss: 3.361976, norm:0.2634, lr:3.2079e-04 dt: 3332.13ms, tok/sec:157343.00
step 8146, loss: 3.356323, norm:0.5509, lr:3.2073e-04 dt: 3331.95ms, tok/sec:157351.70
step 8147, loss: 3.325122, norm:0.2556, lr:3.2067e-04 dt: 3332.42ms, tok/sec:157329.29
step 8148, loss: 3.378824, norm:0.2671, lr:3.2061e-04 dt: 3332.03ms, tok/sec:157347.84
step 8149, loss: 3.364832, norm:0.2542, lr:3.2055e-04 dt: 3332.10ms, tok/sec:157344.68
HellaSwag accuracy:2325075327702238289/-2=-1162537663851119104.0000
rank 1 sample 0: Hello, I'm a language model, with all the components.
I created the example below; so I'm doing a number of other things:
I
rank 1 sample 1: Hello, I'm a language model, which I can only do with an input format. In it, I'm creating an html page that is in the form
rank 1 sample 2: Hello, I'm a language model, but
that's not the case.
I'm going to write the command
to run in Python, because

rank 1 sample 3: Hello, I'm a language model, and I'm looking at programming in English--I'm kind of talking about getting a really warm up. I don't
rank 0 sample 0: Hello, I'm a language model, and I'll be talking about a grammar teacher (who also teaches english); this is how I started this blog. This
rank 0 sample 1: Hello, I'm a language model, I teach native English, I learn what I speak the english language, I speak English in Spanish. How can I improve
rank 0 sample 2: Hello, I'm a language model, I'm pretty much all a language model. In my language theory and the language model I'm going to be using more
rank 0 sample 3: Hello, I'm a language model, and now I'm going to write in my example.
|Predictive object class and input values|
As
step 8150, loss: 3.391233, norm:0.2496, lr:3.2050e-04 dt: 48517.80ms, tok/sec:10806.10
step 8151, loss: 3.347250, norm:0.2608, lr:3.2044e-04 dt: 3331.91ms, tok/sec:157353.45
step 8152, loss: 3.380898, norm:0.2614, lr:3.2038e-04 dt: 3331.98ms, tok/sec:157350.30
step 8153, loss: 3.344632, norm:0.2337, lr:3.2032e-04 dt: 3332.13ms, tok/sec:157343.14
step 8154, loss: 3.367363, norm:0.2644, lr:3.2026e-04 dt: 3332.27ms, tok/sec:157336.53
step 8155, loss: 3.359434, norm:0.2445, lr:3.2020e-04 dt: 3332.38ms, tok/sec:157331.62
step 8156, loss: 3.359456, norm:0.2563, lr:3.2015e-04 dt: 3332.03ms, tok/sec:157347.76
step 8157, loss: 3.349874, norm:0.2622, lr:3.2009e-04 dt: 3332.09ms, tok/sec:157345.13
step 8158, loss: 3.259916, norm:0.2580, lr:3.2003e-04 dt: 3332.15ms, tok/sec:157342.03
step 8159, loss: 3.283736, norm:0.2491, lr:3.1997e-04 dt: 3331.94ms, tok/sec:157352.09
step 8160, loss: 3.354066, norm:0.2523, lr:3.1991e-04 dt: 3331.87ms, tok/sec:157355.67
step 8161, loss: 3.283116, norm:0.2552, lr:3.1985e-04 dt: 3332.08ms, tok/sec:157345.68
step 8162, loss: 3.247207, norm:0.2448, lr:3.1980e-04 dt: 3332.45ms, tok/sec:157328.28
step 8163, loss: 3.288349, norm:0.2367, lr:3.1974e-04 dt: 3331.98ms, tok/sec:157350.31
step 8164, loss: 3.307955, norm:0.2433, lr:3.1968e-04 dt: 3332.03ms, tok/sec:157347.89
step 8165, loss: 3.328138, norm:0.2494, lr:3.1962e-04 dt: 3331.99ms, tok/sec:157349.68
step 8166, loss: 3.298618, norm:0.2550, lr:3.1956e-04 dt: 3332.16ms, tok/sec:157341.57
step 8167, loss: 3.337799, norm:0.2556, lr:3.1950e-04 dt: 3331.89ms, tok/sec:157354.65
step 8168, loss: 3.302463, norm:0.2416, lr:3.1945e-04 dt: 3332.23ms, tok/sec:157338.26
step 8169, loss: 3.329700, norm:0.2650, lr:3.1939e-04 dt: 3332.37ms, tok/sec:157332.08
step 8170, loss: 3.335160, norm:0.2553, lr:3.1933e-04 dt: 3332.17ms, tok/sec:157341.10
step 8171, loss: 3.406560, norm:0.2449, lr:3.1927e-04 dt: 3331.98ms, tok/sec:157350.42
step 8172, loss: 3.291362, norm:0.2517, lr:3.1921e-04 dt: 3332.14ms, tok/sec:157342.88
step 8173, loss: 3.410543, norm:0.2548, lr:3.1916e-04 dt: 3332.16ms, tok/sec:157341.58
step 8174, loss: 3.376467, norm:0.2381, lr:3.1910e-04 dt: 3332.18ms, tok/sec:157340.83
step 8175, loss: 3.329187, norm:0.2656, lr:3.1904e-04 dt: 3332.27ms, tok/sec:157336.62
step 8176, loss: 3.298892, norm:0.2484, lr:3.1898e-04 dt: 3332.00ms, tok/sec:157349.25
step 8177, loss: 3.219852, norm:0.2378, lr:3.1892e-04 dt: 3332.44ms, tok/sec:157328.64
step 8178, loss: 3.227190, norm:0.3756, lr:3.1886e-04 dt: 3331.87ms, tok/sec:157355.45
step 8179, loss: 3.328755, norm:0.2647, lr:3.1881e-04 dt: 3332.05ms, tok/sec:157347.19
step 8180, loss: 3.360711, norm:0.2884, lr:3.1875e-04 dt: 3332.19ms, tok/sec:157340.26
step 8181, loss: 3.360320, norm:0.2710, lr:3.1869e-04 dt: 3332.15ms, tok/sec:157342.25
step 8182, loss: 3.342440, norm:0.2847, lr:3.1863e-04 dt: 3331.98ms, tok/sec:157350.27
step 8183, loss: 3.360374, norm:0.2936, lr:3.1857e-04 dt: 3332.08ms, tok/sec:157345.77
step 8184, loss: 3.396764, norm:0.2488, lr:3.1851e-04 dt: 3332.34ms, tok/sec:157333.21
step 8185, loss: 3.432588, norm:0.3299, lr:3.1846e-04 dt: 3332.02ms, tok/sec:157348.43
step 8186, loss: 3.346425, norm:0.2767, lr:3.1840e-04 dt: 3332.27ms, tok/sec:157336.39
step 8187, loss: 3.330113, norm:0.2881, lr:3.1834e-04 dt: 3331.97ms, tok/sec:157350.67
step 8188, loss: 3.387617, norm:0.2629, lr:3.1828e-04 dt: 3332.08ms, tok/sec:157345.44
step 8189, loss: 3.340767, norm:0.2551, lr:3.1822e-04 dt: 3332.27ms, tok/sec:157336.70
step 8190, loss: 3.424934, norm:0.2770, lr:3.1816e-04 dt: 3332.43ms, tok/sec:157329.23
step 8191, loss: 3.451485, norm:0.2646, lr:3.1811e-04 dt: 3334.69ms, tok/sec:157222.41
step 8192, loss: 3.270724, norm:0.2576, lr:3.1805e-04 dt: 3332.17ms, tok/sec:157341.21
step 8193, loss: 3.259989, norm:0.2681, lr:3.1799e-04 dt: 3332.07ms, tok/sec:157346.05
step 8194, loss: 3.383637, norm:0.2601, lr:3.1793e-04 dt: 3332.30ms, tok/sec:157334.99
step 8195, loss: 3.275027, norm:0.2716, lr:3.1787e-04 dt: 3332.13ms, tok/sec:157343.23
step 8196, loss: 3.250981, norm:0.2536, lr:3.1781e-04 dt: 3331.95ms, tok/sec:157351.86
step 8197, loss: 3.270681, norm:0.2406, lr:3.1776e-04 dt: 3331.97ms, tok/sec:157350.75
step 8198, loss: 3.292994, norm:0.2595, lr:3.1770e-04 dt: 3332.05ms, tok/sec:157347.06
step 8199, loss: 3.310881, norm:0.2584, lr:3.1764e-04 dt: 3332.28ms, tok/sec:157335.90
validation loss: 3.3317
Model and optimizer state saved.
HellaSwag accuracy:-2286571108304452527/-2=1143285554152226304.0000
rank 1 sample 0: Hello, I'm a language model, this is my first language. I never used anything more than simple words in my language to use as a way of expressing
rank 1 sample 1: Hello, I'm a language model, not an IOL. I think there are a lot going on as well! All good software needs a good understanding of
rank 1 sample 2: Hello, I'm a language model, but many other languages are not. I'm not sure about these things but I'm not sure I am a fluent language
rank 1 sample 3: Hello, I'm a language model, and I'm excited about teaching myself this technology.”
What can help you think about it?
For starters
rank 0 sample 0: Hello, I'm a language model, and I think it is going to be. Here's what I got all this stuff:
What is that? That
rank 0 sample 1: Hello, I'm a language model, I was given a name for the group of languages but for a little context it's been taken to say it's the
rank 0 sample 2: Hello, I'm a language model, but I had a point on it for a reason. I could just be a language model. I could be an argument
rank 0 sample 3: Hello, I'm a language model, not being a language model.
A Language Model is a powerful tool for helping writers learn what a language is, when
step 8200, loss: 3.298949, norm:0.2882, lr:3.1758e-04 dt: 56166.98ms, tok/sec:9334.45
step 8201, loss: 3.266624, norm:0.2392, lr:3.1752e-04 dt: 3332.22ms, tok/sec:157339.08
step 8202, loss: 3.319375, norm:0.2492, lr:3.1747e-04 dt: 3331.94ms, tok/sec:157352.22
step 8203, loss: 3.301775, norm:0.2558, lr:3.1741e-04 dt: 3332.13ms, tok/sec:157343.41
step 8204, loss: 3.393418, norm:0.2863, lr:3.1735e-04 dt: 3332.10ms, tok/sec:157344.42
step 8205, loss: 3.337511, norm:0.2525, lr:3.1729e-04 dt: 3332.21ms, tok/sec:157339.39
step 8206, loss: 3.327904, norm:0.3004, lr:3.1723e-04 dt: 3332.09ms, tok/sec:157345.23
step 8207, loss: 3.299161, norm:0.2954, lr:3.1717e-04 dt: 3332.31ms, tok/sec:157334.66
step 8208, loss: 3.413856, norm:0.2644, lr:3.1712e-04 dt: 3332.16ms, tok/sec:157341.80
step 8209, loss: 3.369209, norm:0.2835, lr:3.1706e-04 dt: 3332.42ms, tok/sec:157329.30
step 8210, loss: 3.359874, norm:0.2900, lr:3.1700e-04 dt: 3332.04ms, tok/sec:157347.53
step 8211, loss: 3.389093, norm:0.2937, lr:3.1694e-04 dt: 3332.13ms, tok/sec:157343.32
step 8212, loss: 3.396461, norm:0.2823, lr:3.1688e-04 dt: 3332.09ms, tok/sec:157345.29
step 8213, loss: 3.349487, norm:0.2541, lr:3.1682e-04 dt: 3332.14ms, tok/sec:157342.74
step 8214, loss: 3.376309, norm:0.2717, lr:3.1677e-04 dt: 3331.87ms, tok/sec:157355.66
step 8215, loss: 3.353925, norm:0.2628, lr:3.1671e-04 dt: 3332.06ms, tok/sec:157346.42
step 8216, loss: 3.352320, norm:0.2561, lr:3.1665e-04 dt: 3332.08ms, tok/sec:157345.49
step 8217, loss: 3.408842, norm:0.2819, lr:3.1659e-04 dt: 3332.33ms, tok/sec:157333.71
step 8218, loss: 3.372173, norm:0.2512, lr:3.1653e-04 dt: 3332.09ms, tok/sec:157345.25
step 8219, loss: 3.339853, norm:0.2707, lr:3.1648e-04 dt: 3331.88ms, tok/sec:157354.84
step 8220, loss: 3.325924, norm:0.2467, lr:3.1642e-04 dt: 3332.11ms, tok/sec:157344.25
step 8221, loss: 3.384582, norm:0.2811, lr:3.1636e-04 dt: 3332.06ms, tok/sec:157346.42
step 8222, loss: 3.398850, norm:0.2553, lr:3.1630e-04 dt: 3332.22ms, tok/sec:157339.07
step 8223, loss: 3.442108, norm:0.2687, lr:3.1624e-04 dt: 3331.87ms, tok/sec:157355.48
step 8224, loss: 3.335905, norm:0.2550, lr:3.1618e-04 dt: 3332.01ms, tok/sec:157348.76
step 8225, loss: 3.369072, norm:0.2712, lr:3.1613e-04 dt: 3332.33ms, tok/sec:157333.63
step 8226, loss: 3.295788, norm:0.2592, lr:3.1607e-04 dt: 3332.42ms, tok/sec:157329.50
step 8227, loss: 3.326324, norm:0.2643, lr:3.1601e-04 dt: 3332.00ms, tok/sec:157349.58
step 8228, loss: 3.360159, norm:0.2478, lr:3.1595e-04 dt: 3332.12ms, tok/sec:157343.72
step 8229, loss: 3.339914, norm:0.2666, lr:3.1589e-04 dt: 3332.15ms, tok/sec:157342.44
step 8230, loss: 3.293093, norm:0.2494, lr:3.1583e-04 dt: 3331.97ms, tok/sec:157350.86
step 8231, loss: 3.343125, norm:0.2536, lr:3.1578e-04 dt: 3332.27ms, tok/sec:157336.38
step 8232, loss: 3.349394, norm:0.2576, lr:3.1572e-04 dt: 3332.06ms, tok/sec:157346.41
step 8233, loss: 3.310205, norm:0.2408, lr:3.1566e-04 dt: 3332.00ms, tok/sec:157349.15
step 8234, loss: 3.296794, norm:0.2430, lr:3.1560e-04 dt: 3332.12ms, tok/sec:157343.44
step 8235, loss: 3.269955, norm:0.2611, lr:3.1554e-04 dt: 3332.30ms, tok/sec:157335.14
step 8236, loss: 3.329623, norm:0.2701, lr:3.1548e-04 dt: 3331.83ms, tok/sec:157357.21
step 8237, loss: 3.347379, norm:0.2694, lr:3.1543e-04 dt: 3332.30ms, tok/sec:157335.33
step 8238, loss: 3.270019, norm:0.2746, lr:3.1537e-04 dt: 3332.04ms, tok/sec:157347.40
step 8239, loss: 3.300754, norm:0.2687, lr:3.1531e-04 dt: 3332.06ms, tok/sec:157346.28
step 8240, loss: 3.273039, norm:0.2919, lr:3.1525e-04 dt: 3332.27ms, tok/sec:157336.50
step 8241, loss: 3.335494, norm:0.2533, lr:3.1519e-04 dt: 3332.34ms, tok/sec:157333.41
step 8242, loss: 3.318954, norm:0.2744, lr:3.1514e-04 dt: 3332.14ms, tok/sec:157342.67
step 8243, loss: 3.283074, norm:0.2754, lr:3.1508e-04 dt: 3332.03ms, tok/sec:157347.93
step 8244, loss: 3.297418, norm:0.2526, lr:3.1502e-04 dt: 3332.42ms, tok/sec:157329.53
step 8245, loss: 3.325119, norm:0.2730, lr:3.1496e-04 dt: 3331.97ms, tok/sec:157350.77
step 8246, loss: 3.303239, norm:0.2772, lr:3.1490e-04 dt: 3332.19ms, tok/sec:157340.58
step 8247, loss: 3.240338, norm:0.2993, lr:3.1484e-04 dt: 3331.97ms, tok/sec:157350.90
step 8248, loss: 3.296166, norm:0.2631, lr:3.1479e-04 dt: 3332.14ms, tok/sec:157342.65
step 8249, loss: 3.293343, norm:0.2822, lr:3.1473e-04 dt: 3331.88ms, tok/sec:157354.87
HellaSwag accuracy:-2294456805699124143/-2=1147228402849562112.0000
rank 1 sample 0: Hello, I'm a language model, with all the basics that you need .NET
If you start with the basics, the process can be pretty simple.
rank 1 sample 1: Hello, I'm a language model, which means I've been working with this topic for a good number of years.
However, by now, I'm
rank 1 sample 2: Hello, I'm a language model, so do the two things.
First, I want to say, 'It's a computer system and I've typed
rank 1 sample 3: Hello, I'm a language model, and I'm interested to add to it or to learn which code. You start with programming instructions for a user who wants
rank 0 sample 0: Hello, I'm a language model, and I'd like you to be interested in an example of the other I can use in the real world, and also
rank 0 sample 1: Hello, I'm a language model, but a language model.
So do you think how it's all structured? Well you think the only way to think
rank 0 sample 2: Hello, I'm a language model, but I understand it just from the moment I came to the world to do something and read a book. I've spent
rank 0 sample 3: Hello, I'm a language model, you do not have to have any fancy-based language models. Instead, we know it's not.
The world
step 8250, loss: 3.374847, norm:0.2867, lr:3.1467e-04 dt: 48517.23ms, tok/sec:10806.22
step 8251, loss: 3.373016, norm:0.2638, lr:3.1461e-04 dt: 3332.22ms, tok/sec:157338.88
step 8252, loss: 3.371155, norm:0.2938, lr:3.1455e-04 dt: 3332.18ms, tok/sec:157340.94
step 8253, loss: 3.379292, norm:0.2570, lr:3.1449e-04 dt: 3332.20ms, tok/sec:157339.69
step 8254, loss: 3.341547, norm:0.2848, lr:3.1444e-04 dt: 3332.01ms, tok/sec:157348.80
step 8255, loss: 3.427020, norm:0.3017, lr:3.1438e-04 dt: 3332.08ms, tok/sec:157345.36
step 8256, loss: 3.322041, norm:0.2623, lr:3.1432e-04 dt: 3332.33ms, tok/sec:157333.86
step 8257, loss: 3.315922, norm:0.2732, lr:3.1426e-04 dt: 3332.48ms, tok/sec:157326.52
step 8258, loss: 3.362215, norm:0.2640, lr:3.1420e-04 dt: 3332.62ms, tok/sec:157320.13
step 8259, loss: 3.321348, norm:0.2540, lr:3.1415e-04 dt: 3332.19ms, tok/sec:157340.15
step 8260, loss: 3.359977, norm:0.2626, lr:3.1409e-04 dt: 3331.93ms, tok/sec:157352.76
step 8261, loss: 3.337711, norm:0.3096, lr:3.1403e-04 dt: 3331.95ms, tok/sec:157351.92
step 8262, loss: 3.283185, norm:0.2622, lr:3.1397e-04 dt: 3332.00ms, tok/sec:157349.25
step 8263, loss: 3.325904, norm:0.2870, lr:3.1391e-04 dt: 3332.04ms, tok/sec:157347.46
step 8264, loss: 3.293399, norm:0.2461, lr:3.1385e-04 dt: 3332.33ms, tok/sec:157333.75
step 8265, loss: 3.283561, norm:0.2538, lr:3.1380e-04 dt: 3332.28ms, tok/sec:157336.10
step 8266, loss: 3.299609, norm:0.2721, lr:3.1374e-04 dt: 3332.27ms, tok/sec:157336.37
step 8267, loss: 3.290372, norm:0.2488, lr:3.1368e-04 dt: 3332.31ms, tok/sec:157334.79
step 8268, loss: 3.277570, norm:0.2687, lr:3.1362e-04 dt: 3332.16ms, tok/sec:157341.64
step 8269, loss: 3.302451, norm:0.2734, lr:3.1356e-04 dt: 3332.10ms, tok/sec:157344.85
step 8270, loss: 3.306070, norm:0.2576, lr:3.1351e-04 dt: 3332.11ms, tok/sec:157344.19
step 8271, loss: 3.321224, norm:0.2583, lr:3.1345e-04 dt: 3331.93ms, tok/sec:157352.77
step 8272, loss: 3.265559, norm:0.2798, lr:3.1339e-04 dt: 3332.27ms, tok/sec:157336.81
step 8273, loss: 3.315664, norm:0.2502, lr:3.1333e-04 dt: 3332.00ms, tok/sec:157349.47
step 8274, loss: 3.319272, norm:0.2802, lr:3.1327e-04 dt: 3331.94ms, tok/sec:157352.39
step 8275, loss: 3.265069, norm:0.2594, lr:3.1321e-04 dt: 3332.53ms, tok/sec:157324.46
step 8276, loss: 3.285722, norm:0.2630, lr:3.1316e-04 dt: 3332.21ms, tok/sec:157339.35
step 8277, loss: 3.345778, norm:0.2506, lr:3.1310e-04 dt: 3332.15ms, tok/sec:157342.38
step 8278, loss: 3.297246, norm:0.2614, lr:3.1304e-04 dt: 3331.93ms, tok/sec:157352.81
step 8279, loss: 3.297403, norm:0.2568, lr:3.1298e-04 dt: 3332.17ms, tok/sec:157341.38
step 8280, loss: 3.292597, norm:0.2506, lr:3.1292e-04 dt: 3332.25ms, tok/sec:157337.43
step 8281, loss: 3.272672, norm:0.2587, lr:3.1286e-04 dt: 3332.09ms, tok/sec:157345.30
step 8282, loss: 3.339824, norm:0.2750, lr:3.1281e-04 dt: 3332.24ms, tok/sec:157337.87
step 8283, loss: 3.320364, norm:0.2380, lr:3.1275e-04 dt: 3332.18ms, tok/sec:157340.67
step 8284, loss: 3.306481, norm:0.2580, lr:3.1269e-04 dt: 3332.36ms, tok/sec:157332.36
step 8285, loss: 3.367181, norm:0.2644, lr:3.1263e-04 dt: 3331.99ms, tok/sec:157349.71
step 8286, loss: 3.344047, norm:0.2494, lr:3.1257e-04 dt: 3332.09ms, tok/sec:157345.13
step 8287, loss: 3.385335, norm:0.2675, lr:3.1252e-04 dt: 3332.09ms, tok/sec:157344.87
step 8288, loss: 3.410031, norm:0.2576, lr:3.1246e-04 dt: 3332.27ms, tok/sec:157336.79
step 8289, loss: 3.337474, norm:0.2823, lr:3.1240e-04 dt: 3331.83ms, tok/sec:157357.36
step 8290, loss: 3.327466, norm:0.2658, lr:3.1234e-04 dt: 3332.08ms, tok/sec:157345.45
step 8291, loss: 3.352529, norm:0.2437, lr:3.1228e-04 dt: 3332.10ms, tok/sec:157344.40
step 8292, loss: 3.313426, norm:0.2483, lr:3.1222e-04 dt: 3332.11ms, tok/sec:157343.93
step 8293, loss: 3.337930, norm:0.2368, lr:3.1217e-04 dt: 3332.20ms, tok/sec:157340.05
step 8294, loss: 3.326877, norm:0.2448, lr:3.1211e-04 dt: 3332.44ms, tok/sec:157328.66
step 8295, loss: 3.440776, norm:0.2435, lr:3.1205e-04 dt: 3332.33ms, tok/sec:157333.89
step 8296, loss: 3.322412, norm:0.2399, lr:3.1199e-04 dt: 3332.19ms, tok/sec:157340.51
step 8297, loss: 3.383307, norm:0.2940, lr:3.1193e-04 dt: 3331.90ms, tok/sec:157354.15
step 8298, loss: 3.303727, norm:0.2429, lr:3.1188e-04 dt: 3331.94ms, tok/sec:157351.98
step 8299, loss: 3.346815, norm:0.2509, lr:3.1182e-04 dt: 3332.40ms, tok/sec:157330.44
validation loss: 3.3249
Model and optimizer state saved.
HellaSwag accuracy:2316068128449332305/-2=-1158034064224666112.0000
rank 1 sample 0: Hello, I'm a language model, an editor, and a resource writer when it comes to software."
"Just like with other writing languages, the language
rank 1 sample 1: Hello, I'm a language model, a teacher. How do you know that some of the problems are pretty good in your case? These problems are pretty good
rank 0 sample 0: Hello, I'm a language model, and I know that it's easy to model (and one that seems likely), but you haven't seen how to teach
rank 0 sample 1: Hello, I'm a language model, but for beginners, you can get lots of practice material as you work with language. It's not always easy to do
rank 0 sample 2: Hello, I'm a language model, but I use both "n" and "h" in English as an adjective. I use "h" and "
rank 0 sample 3: Hello, I'm a language model, you do not have to be an advocate to stand up for the rights of your tongue! Just like, you do with
rank 1 sample 2: Hello, I'm a language model, but where did we get started?
I'm very, fairly young and my first language model has been the very little
rank 1 sample 3: Hello, I'm a language model, and I'm working with C++ programming frameworks. I look forward to using R, T, T++, Perl,
step 8300, loss: 3.384784, norm:0.2471, lr:3.1176e-04 dt: 56155.74ms, tok/sec:9336.32
step 8301, loss: 3.295907, norm:0.2424, lr:3.1170e-04 dt: 3332.07ms, tok/sec:157345.93
step 8302, loss: 3.292167, norm:0.2509, lr:3.1164e-04 dt: 3332.06ms, tok/sec:157346.49
step 8303, loss: 3.276840, norm:0.2443, lr:3.1158e-04 dt: 3332.58ms, tok/sec:157322.12
step 8304, loss: 3.300103, norm:0.2325, lr:3.1153e-04 dt: 3332.15ms, tok/sec:157342.36
step 8305, loss: 3.204563, norm:0.2491, lr:3.1147e-04 dt: 3331.82ms, tok/sec:157358.08
step 8306, loss: 3.301625, norm:0.2366, lr:3.1141e-04 dt: 3332.08ms, tok/sec:157345.62
step 8307, loss: 3.309901, norm:0.2368, lr:3.1135e-04 dt: 3332.20ms, tok/sec:157340.13
step 8308, loss: 3.313880, norm:0.2733, lr:3.1129e-04 dt: 3332.11ms, tok/sec:157344.23
step 8309, loss: 3.333976, norm:0.2455, lr:3.1124e-04 dt: 3332.01ms, tok/sec:157348.65
step 8310, loss: 3.310544, norm:0.2874, lr:3.1118e-04 dt: 3332.18ms, tok/sec:157340.95
step 8311, loss: 3.262565, norm:0.2546, lr:3.1112e-04 dt: 3331.97ms, tok/sec:157350.71
step 8312, loss: 3.343760, norm:0.2851, lr:3.1106e-04 dt: 3332.63ms, tok/sec:157319.46
step 8313, loss: 3.385816, norm:0.3072, lr:3.1100e-04 dt: 3332.11ms, tok/sec:157343.91
step 8314, loss: 3.343165, norm:0.2836, lr:3.1094e-04 dt: 3332.06ms, tok/sec:157346.40
step 8315, loss: 3.326067, norm:0.2632, lr:3.1089e-04 dt: 3332.10ms, tok/sec:157344.57
step 8316, loss: 3.291982, norm:0.2561, lr:3.1083e-04 dt: 3332.37ms, tok/sec:157331.79
step 8317, loss: 3.328137, norm:0.2847, lr:3.1077e-04 dt: 3332.19ms, tok/sec:157340.48
step 8318, loss: 3.302418, norm:0.2543, lr:3.1071e-04 dt: 3332.06ms, tok/sec:157346.56
step 8319, loss: 3.301740, norm:0.2663, lr:3.1065e-04 dt: 3332.42ms, tok/sec:157329.44
step 8320, loss: 3.379865, norm:0.2728, lr:3.1060e-04 dt: 3332.35ms, tok/sec:157332.61
step 8321, loss: 3.434622, norm:0.2658, lr:3.1054e-04 dt: 3332.10ms, tok/sec:157344.48
step 8322, loss: 3.358845, norm:0.2843, lr:3.1048e-04 dt: 3332.12ms, tok/sec:157343.72
step 8323, loss: 3.366131, norm:0.2609, lr:3.1042e-04 dt: 3332.00ms, tok/sec:157349.27
step 8324, loss: 3.353172, norm:0.2760, lr:3.1036e-04 dt: 3331.99ms, tok/sec:157349.88
step 8325, loss: 3.403427, norm:0.2856, lr:3.1030e-04 dt: 3332.08ms, tok/sec:157345.68
step 8326, loss: 3.334090, norm:0.2681, lr:3.1025e-04 dt: 3332.03ms, tok/sec:157348.00
step 8327, loss: 3.367615, norm:0.2817, lr:3.1019e-04 dt: 3332.12ms, tok/sec:157343.80
step 8328, loss: 3.416647, norm:0.2753, lr:3.1013e-04 dt: 3332.04ms, tok/sec:157347.39
step 8329, loss: 3.316936, norm:0.2582, lr:3.1007e-04 dt: 3332.31ms, tok/sec:157334.74
step 8330, loss: 3.389996, norm:0.2632, lr:3.1001e-04 dt: 3331.85ms, tok/sec:157356.30
step 8331, loss: 3.310976, norm:0.2566, lr:3.0996e-04 dt: 3332.08ms, tok/sec:157345.35
step 8332, loss: 3.241802, norm:0.2723, lr:3.0990e-04 dt: 3331.98ms, tok/sec:157350.27
step 8333, loss: 3.337979, norm:0.2707, lr:3.0984e-04 dt: 3332.46ms, tok/sec:157327.80
step 8334, loss: 3.292541, norm:0.2630, lr:3.0978e-04 dt: 3332.16ms, tok/sec:157341.71
step 8335, loss: 3.289161, norm:0.2686, lr:3.0972e-04 dt: 3332.28ms, tok/sec:157336.23
step 8336, loss: 3.288549, norm:0.2652, lr:3.0967e-04 dt: 3332.56ms, tok/sec:157322.67
step 8337, loss: 3.248507, norm:0.2348, lr:3.0961e-04 dt: 3331.95ms, tok/sec:157351.76
step 8338, loss: 3.282527, norm:0.2623, lr:3.0955e-04 dt: 3332.50ms, tok/sec:157325.78
step 8339, loss: 3.305720, norm:0.2500, lr:3.0949e-04 dt: 3332.00ms, tok/sec:157349.23
step 8340, loss: 3.370672, norm:0.2433, lr:3.0943e-04 dt: 3331.93ms, tok/sec:157352.62
step 8341, loss: 3.312282, norm:0.2536, lr:3.0937e-04 dt: 3331.92ms, tok/sec:157353.03
step 8342, loss: 3.276179, norm:0.2422, lr:3.0932e-04 dt: 3332.26ms, tok/sec:157337.14
step 8343, loss: 3.355790, norm:0.2869, lr:3.0926e-04 dt: 3332.16ms, tok/sec:157341.69
step 8344, loss: 3.320359, norm:0.2489, lr:3.0920e-04 dt: 3332.10ms, tok/sec:157344.63
step 8345, loss: 3.359382, norm:0.3080, lr:3.0914e-04 dt: 3332.26ms, tok/sec:157337.27
step 8346, loss: 3.374300, norm:0.3067, lr:3.0908e-04 dt: 3331.79ms, tok/sec:157359.08
step 8347, loss: 3.333964, norm:0.2989, lr:3.0903e-04 dt: 3332.58ms, tok/sec:157321.89
step 8348, loss: 3.353669, norm:0.2779, lr:3.0897e-04 dt: 3332.09ms, tok/sec:157344.98
step 8349, loss: 3.287180, norm:0.2868, lr:3.0891e-04 dt: 3332.04ms, tok/sec:157347.65
HellaSwag accuracy:2361086498177221713/-2=-1180543249088610816.0000
rank 1 sample 0: Hello, I'm a language model, just want to know how to use each letter of each color in the word.
Here, if you are using a
rank 1 sample 1: Hello, I'm a language model, which I'm developing to help with programming code. I really like writing code with only 3 characters (I'm not sure
rank 1 sample 2: Hello, I'm a language model, so learning a language is a lot easier than learning a second language." (2)
- ixis-freerank 0 sample 0: Hello, I'm a language model, and I'll be talking about this at first.
Another example
What are the basic structures?
I'm just

rank 1 sample 3: Hello, I'm a language model, and I'm pretty good at coding languages anyway. I wrote about in previous publications, it appears that the software used in
rank 0 sample 1: Hello, I'm a language model, so there's a lot of work into it and when I get my writing to a computer, it seems like I'm
rank 0 sample 2: Hello, I'm a language model, but I could't work, because there's something that I'd be hard to see in my language model. I also
rank 0 sample 3: Hello, I'm a language model, so i'm trying to do something and it all is a little bit too complicated which's very nice and it was written
step 8350, loss: 3.432142, norm:0.2828, lr:3.0885e-04 dt: 48526.07ms, tok/sec:10804.25
step 8351, loss: 3.376523, norm:0.2610, lr:3.0879e-04 dt: 3332.17ms, tok/sec:157341.48
step 8352, loss: 3.313181, norm:0.2829, lr:3.0873e-04 dt: 3332.01ms, tok/sec:157348.82
step 8353, loss: 3.363595, norm:0.2644, lr:3.0868e-04 dt: 3332.19ms, tok/sec:157340.57
step 8354, loss: 3.300225, norm:0.2706, lr:3.0862e-04 dt: 3332.16ms, tok/sec:157341.87
step 8355, loss: 3.356695, norm:0.3238, lr:3.0856e-04 dt: 3332.25ms, tok/sec:157337.75
step 8356, loss: 3.384826, norm:0.3539, lr:3.0850e-04 dt: 3332.06ms, tok/sec:157346.70
step 8357, loss: 3.361222, norm:0.2787, lr:3.0844e-04 dt: 3332.16ms, tok/sec:157341.83
step 8358, loss: 3.374436, norm:0.2886, lr:3.0839e-04 dt: 3332.13ms, tok/sec:157343.05
step 8359, loss: 3.364818, norm:0.2741, lr:3.0833e-04 dt: 3332.22ms, tok/sec:157338.76
step 8360, loss: 3.391091, norm:0.2575, lr:3.0827e-04 dt: 3332.09ms, tok/sec:157345.03
step 8361, loss: 3.338406, norm:0.2653, lr:3.0821e-04 dt: 3332.03ms, tok/sec:157347.86
step 8362, loss: 3.434651, norm:0.2596, lr:3.0815e-04 dt: 3332.22ms, tok/sec:157338.80
step 8363, loss: 3.360970, norm:0.2729, lr:3.0810e-04 dt: 3331.97ms, tok/sec:157350.71
step 8364, loss: 3.361426, norm:0.2512, lr:3.0804e-04 dt: 3331.99ms, tok/sec:157349.91
step 8365, loss: 3.331594, norm:0.2524, lr:3.0798e-04 dt: 3332.16ms, tok/sec:157341.72
step 8366, loss: 3.406481, norm:0.2687, lr:3.0792e-04 dt: 3332.51ms, tok/sec:157325.26
step 8367, loss: 3.365307, norm:0.2418, lr:3.0786e-04 dt: 3331.94ms, tok/sec:157351.97
step 8368, loss: 3.320410, norm:0.2626, lr:3.0780e-04 dt: 3332.09ms, tok/sec:157345.05
step 8369, loss: 3.364790, norm:0.2717, lr:3.0775e-04 dt: 3332.15ms, tok/sec:157342.04
step 8370, loss: 3.239632, norm:0.2559, lr:3.0769e-04 dt: 3332.03ms, tok/sec:157347.85
step 8371, loss: 3.327771, norm:0.2687, lr:3.0763e-04 dt: 3331.90ms, tok/sec:157353.89
step 8372, loss: 3.281480, norm:0.2474, lr:3.0757e-04 dt: 3332.19ms, tok/sec:157340.26
step 8373, loss: 3.370528, norm:0.2585, lr:3.0751e-04 dt: 3332.21ms, tok/sec:157339.31
step 8374, loss: 3.322929, norm:0.2499, lr:3.0746e-04 dt: 3332.59ms, tok/sec:157321.61
step 8375, loss: 3.283653, norm:0.2647, lr:3.0740e-04 dt: 3332.02ms, tok/sec:157348.49
step 8376, loss: 3.259679, norm:0.2349, lr:3.0734e-04 dt: 3332.11ms, tok/sec:157343.99
step 8377, loss: 3.279989, norm:0.2736, lr:3.0728e-04 dt: 3332.40ms, tok/sec:157330.39
step 8378, loss: 3.249946, norm:0.2409, lr:3.0722e-04 dt: 3332.70ms, tok/sec:157316.09
step 8379, loss: 3.256368, norm:0.2519, lr:3.0717e-04 dt: 3332.32ms, tok/sec:157334.10
step 8380, loss: 3.334454, norm:0.2675, lr:3.0711e-04 dt: 3332.09ms, tok/sec:157345.08
step 8381, loss: 3.303487, norm:0.2573, lr:3.0705e-04 dt: 3334.06ms, tok/sec:157252.19
step 8382, loss: 3.347620, norm:0.2525, lr:3.0699e-04 dt: 3332.32ms, tok/sec:157334.04
step 8383, loss: 3.290573, norm:0.3422, lr:3.0693e-04 dt: 3331.91ms, tok/sec:157353.82
step 8384, loss: 3.343044, norm:0.2996, lr:3.0687e-04 dt: 3332.20ms, tok/sec:157339.84
step 8385, loss: 3.343794, norm:0.3248, lr:3.0682e-04 dt: 3332.14ms, tok/sec:157342.94
step 8386, loss: 3.349325, norm:0.3094, lr:3.0676e-04 dt: 3332.19ms, tok/sec:157340.29
step 8387, loss: 3.283107, norm:0.2862, lr:3.0670e-04 dt: 3332.23ms, tok/sec:157338.27
step 8388, loss: 3.329832, norm:0.2624, lr:3.0664e-04 dt: 3332.35ms, tok/sec:157332.77
step 8389, loss: 3.364809, norm:0.2920, lr:3.0658e-04 dt: 3332.15ms, tok/sec:157342.04
step 8390, loss: 3.312937, norm:0.3232, lr:3.0653e-04 dt: 3332.10ms, tok/sec:157344.79
step 8391, loss: 3.381152, norm:0.2900, lr:3.0647e-04 dt: 3332.10ms, tok/sec:157344.82
step 8392, loss: 3.419326, norm:0.2898, lr:3.0641e-04 dt: 3331.97ms, tok/sec:157350.62
step 8393, loss: 3.355357, norm:0.2658, lr:3.0635e-04 dt: 3332.27ms, tok/sec:157336.67
step 8394, loss: 3.383290, norm:0.2867, lr:3.0629e-04 dt: 3331.96ms, tok/sec:157351.43
step 8395, loss: 3.393308, norm:0.2948, lr:3.0624e-04 dt: 3331.99ms, tok/sec:157349.97
step 8396, loss: 3.337690, norm:0.2764, lr:3.0618e-04 dt: 3332.20ms, tok/sec:157339.83
step 8397, loss: 3.387600, norm:0.2682, lr:3.0612e-04 dt: 3331.94ms, tok/sec:157352.40
step 8398, loss: 3.326361, norm:0.2841, lr:3.0606e-04 dt: 3332.08ms, tok/sec:157345.51
step 8399, loss: 3.371502, norm:0.2549, lr:3.0600e-04 dt: 3332.20ms, tok/sec:157340.04
validation loss: 3.3223
Model and optimizer state saved.
HellaSwag accuracy:-2286610690723314607/-2=1143305345361657344.0000
rank 1 sample 0: Hello, I'm a language model, this is my first project, and will give you an idea of what the program is all about!
What is the
rank 1 sample 1: Hello, I'm a language model, which I have started in my school at T.Ig/W/UEP since 2015.
I'm a
rank 1 sample 2: Hello, I'm a language model, but because we can't do it without the help of some computer programs and you can't use them to create anything -
rank 1 sample 3: Hello, I'm a language model, and I'm really excited to see a long-term history of human languages such as Thai, Chinese, and Russian.
rank 0 sample 0: Hello, I'm a language model, and I'll be writing code on an even lower level. I like building and writing my own script and the code.
rank 0 sample 1: Hello, I'm a language model, so how do you do that? When you talk that's what they say, they mean they mean language.
There
rank 0 sample 2: Hello, I'm a language model, but I just don't remember every grammar I'll ever use." The idea is to use the language to express the sense
rank 0 sample 3: Hello, I'm a language model, so in my experience, I've made the word "language" so simple that is the "official" word for languages
step 8400, loss: 3.332613, norm:0.2507, lr:3.0595e-04 dt: 56275.70ms, tok/sec:9316.42
step 8401, loss: 3.366042, norm:0.2920, lr:3.0589e-04 dt: 3332.21ms, tok/sec:157339.29
step 8402, loss: 3.283924, norm:0.2377, lr:3.0583e-04 dt: 3332.13ms, tok/sec:157343.25
step 8403, loss: 3.322587, norm:0.2558, lr:3.0577e-04 dt: 3332.24ms, tok/sec:157337.90
step 8404, loss: 3.295493, norm:0.2420, lr:3.0571e-04 dt: 3332.43ms, tok/sec:157328.95
step 8405, loss: 3.268919, norm:0.2497, lr:3.0565e-04 dt: 3332.09ms, tok/sec:157345.18
step 8406, loss: 3.292389, norm:0.2456, lr:3.0560e-04 dt: 3331.96ms, tok/sec:157351.38
step 8407, loss: 3.342039, norm:0.2574, lr:3.0554e-04 dt: 3332.12ms, tok/sec:157343.55
step 8408, loss: 3.382605, norm:0.2409, lr:3.0548e-04 dt: 3332.14ms, tok/sec:157342.94
step 8409, loss: 3.303743, norm:0.2498, lr:3.0542e-04 dt: 3332.10ms, tok/sec:157344.70
step 8410, loss: 3.323690, norm:0.2344, lr:3.0536e-04 dt: 3332.43ms, tok/sec:157328.92
step 8411, loss: 3.343745, norm:0.2403, lr:3.0531e-04 dt: 3332.42ms, tok/sec:157329.73
step 8412, loss: 3.343406, norm:0.2447, lr:3.0525e-04 dt: 3332.04ms, tok/sec:157347.58
step 8413, loss: 3.351597, norm:0.2488, lr:3.0519e-04 dt: 3332.14ms, tok/sec:157342.80
step 8414, loss: 3.298524, norm:0.2823, lr:3.0513e-04 dt: 3332.12ms, tok/sec:157343.46
step 8415, loss: 3.440401, norm:0.2701, lr:3.0507e-04 dt: 3332.03ms, tok/sec:157347.97
step 8416, loss: 3.280723, norm:0.2636, lr:3.0502e-04 dt: 3332.26ms, tok/sec:157337.14
step 8417, loss: 3.317491, norm:0.2445, lr:3.0496e-04 dt: 3332.27ms, tok/sec:157336.45
step 8418, loss: 3.266786, norm:0.2612, lr:3.0490e-04 dt: 3331.89ms, tok/sec:157354.74
step 8419, loss: 3.271141, norm:0.2536, lr:3.0484e-04 dt: 3332.34ms, tok/sec:157333.15
step 8420, loss: 3.323834, norm:0.2635, lr:3.0478e-04 dt: 3332.30ms, tok/sec:157335.29
step 8421, loss: 3.312807, norm:0.2653, lr:3.0473e-04 dt: 3332.10ms, tok/sec:157344.59
step 8422, loss: 3.317515, norm:0.2541, lr:3.0467e-04 dt: 3332.17ms, tok/sec:157341.54
step 8423, loss: 3.331239, norm:0.2913, lr:3.0461e-04 dt: 3331.96ms, tok/sec:157351.43
step 8424, loss: 3.319329, norm:0.2563, lr:3.0455e-04 dt: 3332.13ms, tok/sec:157342.98
step 8425, loss: 3.313874, norm:0.2547, lr:3.0449e-04 dt: 3332.08ms, tok/sec:157345.49
step 8426, loss: 3.334624, norm:0.2579, lr:3.0444e-04 dt: 3331.96ms, tok/sec:157351.41
step 8427, loss: 3.343928, norm:0.2445, lr:3.0438e-04 dt: 3332.00ms, tok/sec:157349.20
step 8428, loss: 3.349833, norm:0.2776, lr:3.0432e-04 dt: 3332.21ms, tok/sec:157339.29
step 8429, loss: 3.406176, norm:0.2444, lr:3.0426e-04 dt: 3332.49ms, tok/sec:157326.06
step 8430, loss: 3.371535, norm:0.2790, lr:3.0420e-04 dt: 3332.19ms, tok/sec:157340.14
step 8431, loss: 3.379436, norm:0.2613, lr:3.0414e-04 dt: 3332.11ms, tok/sec:157344.37
step 8432, loss: 3.372865, norm:0.2683, lr:3.0409e-04 dt: 3332.04ms, tok/sec:157347.38
step 8433, loss: 3.380687, norm:0.2678, lr:3.0403e-04 dt: 3332.21ms, tok/sec:157339.41
step 8434, loss: 3.394631, norm:0.2602, lr:3.0397e-04 dt: 3332.11ms, tok/sec:157343.92
step 8435, loss: 3.294781, norm:0.2583, lr:3.0391e-04 dt: 3332.03ms, tok/sec:157347.89
step 8436, loss: 3.305104, norm:0.2680, lr:3.0385e-04 dt: 3332.25ms, tok/sec:157337.64
step 8437, loss: 3.293031, norm:0.2662, lr:3.0380e-04 dt: 3331.94ms, tok/sec:157352.15
step 8438, loss: 3.294143, norm:0.2681, lr:3.0374e-04 dt: 3332.35ms, tok/sec:157332.94
step 8439, loss: 3.274537, norm:0.2572, lr:3.0368e-04 dt: 3332.07ms, tok/sec:157345.81
step 8440, loss: 3.364738, norm:0.2710, lr:3.0362e-04 dt: 3331.96ms, tok/sec:157351.08
step 8441, loss: 3.293041, norm:0.2310, lr:3.0356e-04 dt: 3332.15ms, tok/sec:157342.04
step 8442, loss: 3.311409, norm:0.2596, lr:3.0351e-04 dt: 3332.11ms, tok/sec:157343.95
step 8443, loss: 3.370411, norm:0.3080, lr:3.0345e-04 dt: 3331.81ms, tok/sec:157358.25
step 8444, loss: 3.278110, norm:0.2582, lr:3.0339e-04 dt: 3332.28ms, tok/sec:157336.00
step 8445, loss: 3.338804, norm:0.2779, lr:3.0333e-04 dt: 3332.51ms, tok/sec:157325.43
step 8446, loss: 3.318107, norm:0.2599, lr:3.0327e-04 dt: 3332.07ms, tok/sec:157346.24
step 8447, loss: 3.304406, norm:0.2606, lr:3.0322e-04 dt: 3331.82ms, tok/sec:157357.65
step 8448, loss: 3.301031, norm:0.2692, lr:3.0316e-04 dt: 3332.03ms, tok/sec:157347.83
step 8449, loss: 3.279117, norm:0.2770, lr:3.0310e-04 dt: 3332.01ms, tok/sec:157348.69
HellaSwag accuracy:-2286628282909096879/-2=1143314141454548480.0000
rank 1 sample 0: Hello, I'm a language model, one that's based on a human agent and it can be said that it's a perfect metaphor, but the term is
rank 1 sample 1: Hello, I'm a language model, which I've seen in the literature in school, and how it looks like a science topic, let's say, a
rank 1 sample 2: Hello, I'm a language model, but then there's a lot of programming that's so hard: you're adding a new function in Java that is written
rank 1 sample 3: Hello, I'm a language model, and I'm using the whole language language platform. I’m writing here for both iOS and Android. You can
rank 0 sample 0: Hello, I'm a language model, and I don't have to speak Spanish any faster than I can tell you that. It's my favorite language. But
rank 0 sample 1: Hello, I'm a language model, so to speak, we need a "language." English to communicate needs all kinds of words that are easy to understand and
rank 0 sample 2: Hello, I'm a language model, so I thought that people from all around the world could use some basic programming language in the language.
I think it
rank 0 sample 3: Hello, I'm a language model, and like any other language, it can help ESL students learn a lot about how words relate to each other. It means
step 8450, loss: 3.271350, norm:0.3048, lr:3.0304e-04 dt: 48523.58ms, tok/sec:10804.81
step 8451, loss: 3.307225, norm:0.2741, lr:3.0298e-04 dt: 3332.12ms, tok/sec:157343.73
step 8452, loss: 3.284351, norm:0.2890, lr:3.0293e-04 dt: 3332.25ms, tok/sec:157337.68
step 8453, loss: 3.312485, norm:0.2788, lr:3.0287e-04 dt: 3332.24ms, tok/sec:157337.79
step 8454, loss: 3.269840, norm:0.2855, lr:3.0281e-04 dt: 3332.23ms, tok/sec:157338.38
step 8455, loss: 3.295562, norm:0.2634, lr:3.0275e-04 dt: 3332.04ms, tok/sec:157347.48
step 8456, loss: 3.364782, norm:0.2575, lr:3.0269e-04 dt: 3332.36ms, tok/sec:157332.49
step 8457, loss: 3.354143, norm:0.2963, lr:3.0264e-04 dt: 3331.87ms, tok/sec:157355.36
step 8458, loss: 3.306142, norm:0.2779, lr:3.0258e-04 dt: 3332.13ms, tok/sec:157343.30
step 8459, loss: 3.348904, norm:0.2882, lr:3.0252e-04 dt: 3332.04ms, tok/sec:157347.48
step 8460, loss: 3.360853, norm:0.2861, lr:3.0246e-04 dt: 3332.24ms, tok/sec:157337.78
step 8461, loss: 3.355027, norm:0.2842, lr:3.0240e-04 dt: 3331.99ms, tok/sec:157349.62
step 8462, loss: 3.341891, norm:0.2687, lr:3.0235e-04 dt: 3332.30ms, tok/sec:157335.02
step 8463, loss: 3.333292, norm:0.2620, lr:3.0229e-04 dt: 3332.27ms, tok/sec:157336.65
step 8464, loss: 3.325435, norm:0.2703, lr:3.0223e-04 dt: 3332.15ms, tok/sec:157342.05
step 8465, loss: 3.304725, norm:0.2472, lr:3.0217e-04 dt: 3331.92ms, tok/sec:157352.92
step 8466, loss: 3.294895, norm:0.2617, lr:3.0211e-04 dt: 3331.98ms, tok/sec:157350.06
step 8467, loss: 3.389369, norm:0.2734, lr:3.0206e-04 dt: 3332.16ms, tok/sec:157341.80
step 8468, loss: 3.343672, norm:0.2332, lr:3.0200e-04 dt: 3332.11ms, tok/sec:157344.11
step 8469, loss: 3.339429, norm:0.2649, lr:3.0194e-04 dt: 3331.90ms, tok/sec:157354.17
step 8470, loss: 3.364968, norm:0.2801, lr:3.0188e-04 dt: 3331.96ms, tok/sec:157351.39
step 8471, loss: 3.325197, norm:0.2641, lr:3.0182e-04 dt: 3332.03ms, tok/sec:157347.76
step 8472, loss: 3.290972, norm:0.2592, lr:3.0177e-04 dt: 3332.33ms, tok/sec:157333.97
step 8473, loss: 3.356661, norm:0.2894, lr:3.0171e-04 dt: 3332.29ms, tok/sec:157335.58
step 8474, loss: 3.317765, norm:0.2527, lr:3.0165e-04 dt: 3332.37ms, tok/sec:157332.00
step 8475, loss: 3.288710, norm:0.2568, lr:3.0159e-04 dt: 3332.17ms, tok/sec:157341.35
step 8476, loss: 3.344196, norm:0.2483, lr:3.0153e-04 dt: 3332.04ms, tok/sec:157347.49
step 8477, loss: 3.306309, norm:0.2875, lr:3.0148e-04 dt: 3332.13ms, tok/sec:157343.23
step 8478, loss: 3.328285, norm:0.2410, lr:3.0142e-04 dt: 3332.11ms, tok/sec:157344.31
step 8479, loss: 3.370952, norm:0.2457, lr:3.0136e-04 dt: 3332.31ms, tok/sec:157334.79
step 8480, loss: 3.290003, norm:0.2501, lr:3.0130e-04 dt: 3332.26ms, tok/sec:157336.83
step 8481, loss: 3.343252, norm:0.2478, lr:3.0124e-04 dt: 3332.43ms, tok/sec:157329.18
step 8482, loss: 3.276677, norm:0.2432, lr:3.0119e-04 dt: 3331.99ms, tok/sec:157349.79
step 8483, loss: 3.295113, norm:0.2411, lr:3.0113e-04 dt: 3331.97ms, tok/sec:157350.94
step 8484, loss: 3.328147, norm:0.2647, lr:3.0107e-04 dt: 3332.25ms, tok/sec:157337.68
step 8485, loss: 3.330771, norm:0.2769, lr:3.0101e-04 dt: 3332.00ms, tok/sec:157349.55
step 8486, loss: 3.327623, norm:0.2696, lr:3.0095e-04 dt: 3332.13ms, tok/sec:157343.16
step 8487, loss: 3.407191, norm:0.3184, lr:3.0090e-04 dt: 3332.31ms, tok/sec:157334.92
step 8488, loss: 3.306077, norm:0.2733, lr:3.0084e-04 dt: 3332.52ms, tok/sec:157324.98
step 8489, loss: 3.318270, norm:0.2834, lr:3.0078e-04 dt: 3331.99ms, tok/sec:157349.90
step 8490, loss: 3.310622, norm:0.2468, lr:3.0072e-04 dt: 3331.91ms, tok/sec:157353.82
step 8491, loss: 3.327595, norm:0.2497, lr:3.0066e-04 dt: 3332.11ms, tok/sec:157344.05
step 8492, loss: 3.301948, norm:0.2749, lr:3.0061e-04 dt: 3332.29ms, tok/sec:157335.47
step 8493, loss: 3.299362, norm:0.2579, lr:3.0055e-04 dt: 3331.99ms, tok/sec:157349.92
step 8494, loss: 3.266183, norm:0.2562, lr:3.0049e-04 dt: 3331.87ms, tok/sec:157355.36
step 8495, loss: 3.295336, norm:0.2692, lr:3.0043e-04 dt: 3332.02ms, tok/sec:157348.45
step 8496, loss: 3.380602, norm:0.2568, lr:3.0037e-04 dt: 3332.24ms, tok/sec:157338.01
step 8497, loss: 3.311970, norm:0.2549, lr:3.0032e-04 dt: 3332.31ms, tok/sec:157334.76
step 8498, loss: 3.316617, norm:0.2668, lr:3.0026e-04 dt: 3331.96ms, tok/sec:157351.26
step 8499, loss: 3.314066, norm:0.2494, lr:3.0020e-04 dt: 3332.15ms, tok/sec:157342.19
validation loss: 3.3164
Model and optimizer state saved.
HellaSwag accuracy:2325057736055161937/-2=-1162528868027580928.0000
rank 1 sample 0: Hello, I'm a language model, this is the first part of the introduction to C language - let's start thinking about how it was developed by the late
rank 1 sample 1: Hello, I'm a language model, I've learned more about it before, but I'd need to work on more words -- something no other language models can
rank 1 sample 2: Hello, I'm a language model, so whatever is written in it is the same as the rest of a computer.
I have an old friend's "
rank 1 sample 3: Hello, I'm a language model, and I'm interested to look at things within the language hierarchy. How should you use or ignore language? How would you
rank 0 sample 0: Hello, I'm a language model, and I'll be writing to help you at an early age. For older learners, you can start out by learning some
rank 0 sample 1: Hello, I'm a language model, but when I first started there were 30,000-50,000 and they're just trying to start out. But
rank 0 sample 2: Hello, I'm a language model, I'm fluent and that way I have a great perspective on things I can do at home, and I'm a cultural
rank 0 sample 3: Hello, I'm a language model, not what I'd like to say a little for my language. I want my ESL teachers to know it's the stuff
step 8500, loss: 3.314463, norm:0.2570, lr:3.0014e-04 dt: 56276.25ms, tok/sec:9316.33
step 8501, loss: 3.430143, norm:0.2392, lr:3.0008e-04 dt: 3333.22ms, tok/sec:157291.74
step 8502, loss: 3.306587, norm:0.2727, lr:3.0003e-04 dt: 3332.26ms, tok/sec:157337.28
step 8503, loss: 3.327036, norm:0.2394, lr:2.9997e-04 dt: 3332.12ms, tok/sec:157343.74
step 8504, loss: 3.362225, norm:0.2685, lr:2.9991e-04 dt: 3332.11ms, tok/sec:157344.35
step 8505, loss: 3.325601, norm:0.2517, lr:2.9985e-04 dt: 3332.03ms, tok/sec:157348.13
step 8506, loss: 3.308197, norm:0.3027, lr:2.9979e-04 dt: 3332.19ms, tok/sec:157340.48
step 8507, loss: 3.370341, norm:0.2827, lr:2.9974e-04 dt: 3332.32ms, tok/sec:157334.31
step 8508, loss: 3.374742, norm:0.3361, lr:2.9968e-04 dt: 3332.37ms, tok/sec:157331.97
step 8509, loss: 3.299912, norm:0.2502, lr:2.9962e-04 dt: 3331.98ms, tok/sec:157350.17
step 8510, loss: 3.306325, norm:0.2850, lr:2.9956e-04 dt: 3331.98ms, tok/sec:157350.35
step 8511, loss: 3.366099, norm:0.2766, lr:2.9950e-04 dt: 3332.14ms, tok/sec:157342.91
step 8512, loss: 3.287781, norm:0.2808, lr:2.9945e-04 dt: 3332.10ms, tok/sec:157344.49
step 8513, loss: 3.253494, norm:0.2483, lr:2.9939e-04 dt: 3332.20ms, tok/sec:157340.12
step 8514, loss: 3.299912, norm:0.2551, lr:2.9933e-04 dt: 3332.04ms, tok/sec:157347.31
step 8515, loss: 3.280612, norm:0.2571, lr:2.9927e-04 dt: 3332.21ms, tok/sec:157339.42
step 8516, loss: 3.284114, norm:0.2540, lr:2.9921e-04 dt: 3332.13ms, tok/sec:157343.38
step 8517, loss: 3.309306, norm:0.2407, lr:2.9916e-04 dt: 3332.45ms, tok/sec:157328.03
step 8518, loss: 3.274791, norm:0.2586, lr:2.9910e-04 dt: 3332.06ms, tok/sec:157346.34
step 8519, loss: 3.299539, norm:0.2557, lr:2.9904e-04 dt: 3332.14ms, tok/sec:157342.85
step 8520, loss: 3.308920, norm:0.2711, lr:2.9898e-04 dt: 3331.96ms, tok/sec:157351.06
step 8521, loss: 3.313380, norm:0.2694, lr:2.9893e-04 dt: 3332.04ms, tok/sec:157347.26
step 8522, loss: 3.308374, norm:0.2711, lr:2.9887e-04 dt: 3332.33ms, tok/sec:157333.90
step 8523, loss: 3.297640, norm:0.2732, lr:2.9881e-04 dt: 3332.06ms, tok/sec:157346.67
step 8524, loss: 3.308697, norm:0.2537, lr:2.9875e-04 dt: 3332.08ms, tok/sec:157345.79
step 8525, loss: 3.250431, norm:0.3064, lr:2.9869e-04 dt: 3332.23ms, tok/sec:157338.71
step 8526, loss: 3.320518, norm:0.2827, lr:2.9864e-04 dt: 3332.46ms, tok/sec:157327.47
step 8527, loss: 3.330773, norm:0.2681, lr:2.9858e-04 dt: 3332.20ms, tok/sec:157339.67
step 8528, loss: 3.285748, norm:0.2592, lr:2.9852e-04 dt: 3332.09ms, tok/sec:157345.16
step 8529, loss: 3.312291, norm:0.2783, lr:2.9846e-04 dt: 3332.20ms, tok/sec:157339.78
step 8530, loss: 3.345994, norm:0.2597, lr:2.9840e-04 dt: 3332.18ms, tok/sec:157340.88
step 8531, loss: 3.335447, norm:0.2745, lr:2.9835e-04 dt: 3332.12ms, tok/sec:157343.89
step 8532, loss: 3.360042, norm:0.2845, lr:2.9829e-04 dt: 3332.08ms, tok/sec:157345.67
step 8533, loss: 3.358096, norm:0.2617, lr:2.9823e-04 dt: 3332.20ms, tok/sec:157339.68
step 8534, loss: 3.344654, norm:0.2622, lr:2.9817e-04 dt: 3332.19ms, tok/sec:157340.25
step 8535, loss: 3.322337, norm:0.2579, lr:2.9811e-04 dt: 3332.02ms, tok/sec:157348.26
step 8536, loss: 3.310472, norm:0.2488, lr:2.9806e-04 dt: 3331.96ms, tok/sec:157351.41
step 8537, loss: 3.325041, norm:0.2655, lr:2.9800e-04 dt: 3331.95ms, tok/sec:157351.65
step 8538, loss: 3.302452, norm:0.2642, lr:2.9794e-04 dt: 3332.12ms, tok/sec:157343.89
step 8539, loss: 3.343682, norm:0.2719, lr:2.9788e-04 dt: 3332.41ms, tok/sec:157329.98
step 8540, loss: 3.288951, norm:0.2431, lr:2.9782e-04 dt: 3332.11ms, tok/sec:157344.02
step 8541, loss: 3.391034, norm:0.2406, lr:2.9777e-04 dt: 3332.57ms, tok/sec:157322.25
step 8542, loss: 3.329556, norm:0.2764, lr:2.9771e-04 dt: 3332.26ms, tok/sec:157337.09
step 8543, loss: 3.298261, norm:0.2511, lr:2.9765e-04 dt: 3332.12ms, tok/sec:157343.56
step 8544, loss: 3.390468, norm:0.2671, lr:2.9759e-04 dt: 3331.86ms, tok/sec:157355.79
step 8545, loss: 3.312385, norm:0.2466, lr:2.9754e-04 dt: 3332.22ms, tok/sec:157339.05
step 8546, loss: 3.280392, norm:0.2531, lr:2.9748e-04 dt: 3332.15ms, tok/sec:157342.31
step 8547, loss: 3.292833, norm:0.2540, lr:2.9742e-04 dt: 3332.04ms, tok/sec:157347.54
step 8548, loss: 3.299240, norm:0.2449, lr:2.9736e-04 dt: 3332.04ms, tok/sec:157347.68
step 8549, loss: 3.304755, norm:0.2490, lr:2.9730e-04 dt: 3332.19ms, tok/sec:157340.51
HellaSwag accuracy:-6898314301344873391/-2=3449157150672436736.0000
rank 1 sample 0: Hello, I'm a language model, just think about it. I'm making my programming languages easy. I'm developing my software. We're not necessarily going
rank 1 sample 1: Hello, I'm a language model, a language for me. I want to say, "If you didn't understand me right then in my case, I
rank 1 sample 2: Hello, I'm a language model, but its syntax is the same as that of a text-line interface (where the syntax is different, and not dependent
rank 1 sample 3: Hello, I'm a language model, and I'm using a combination of programming technology and my expertise in languages (Python, Microsoft®, Python, R,
rank 0 sample 0: Hello, I'm a language model, and I'll be writing that out soon . :-)
All this work isn't the most usual and the simplest. So
rank 0 sample 1: Hello, I'm a language model, but also a language. I think he's right by "diversity", which means "unity", with an organization and
rank 0 sample 2: Hello, I'm a language model, so I like it. In this course, this is a really useful way to develop a language.
I'll take
rank 0 sample 3: Hello, I'm a language model, you say, the language model, and I also have a collection of grammar-compiler classes that I have been looking
step 8550, loss: 3.261024, norm:0.2502, lr:2.9725e-04 dt: 48525.64ms, tok/sec:10804.35
step 8551, loss: 3.333261, norm:0.2488, lr:2.9719e-04 dt: 3332.16ms, tok/sec:157341.58
step 8552, loss: 3.297747, norm:0.2420, lr:2.9713e-04 dt: 3332.03ms, tok/sec:157347.98
step 8553, loss: 3.340094, norm:0.2456, lr:2.9707e-04 dt: 3332.09ms, tok/sec:157345.11
step 8554, loss: 3.347885, norm:0.2418, lr:2.9701e-04 dt: 3332.31ms, tok/sec:157334.68
step 8555, loss: 3.317376, norm:0.2440, lr:2.9696e-04 dt: 3331.89ms, tok/sec:157354.39
step 8556, loss: 3.394448, norm:0.2641, lr:2.9690e-04 dt: 3332.33ms, tok/sec:157333.74
step 8557, loss: 3.327127, norm:0.2457, lr:2.9684e-04 dt: 3332.30ms, tok/sec:157335.23
step 8558, loss: 3.316513, norm:0.2598, lr:2.9678e-04 dt: 3332.12ms, tok/sec:157343.53
step 8559, loss: 3.331631, norm:0.2544, lr:2.9672e-04 dt: 3332.32ms, tok/sec:157334.25
step 8560, loss: 3.301884, norm:0.2687, lr:2.9667e-04 dt: 3332.04ms, tok/sec:157347.63
step 8561, loss: 3.356560, norm:0.2732, lr:2.9661e-04 dt: 3332.27ms, tok/sec:157336.67
step 8562, loss: 3.284308, norm:0.2616, lr:2.9655e-04 dt: 3332.19ms, tok/sec:157340.16
step 8563, loss: 3.349924, norm:0.2715, lr:2.9649e-04 dt: 3331.91ms, tok/sec:157353.65
step 8564, loss: 3.352538, norm:0.2608, lr:2.9644e-04 dt: 3332.25ms, tok/sec:157337.61
step 8565, loss: 3.349736, norm:0.2531, lr:2.9638e-04 dt: 3332.78ms, tok/sec:157312.59
step 8566, loss: 3.357814, norm:0.2595, lr:2.9632e-04 dt: 3331.99ms, tok/sec:157349.70
step 8567, loss: 3.319109, norm:0.2580, lr:2.9626e-04 dt: 3332.29ms, tok/sec:157335.80
step 8568, loss: 3.378005, norm:0.2698, lr:2.9620e-04 dt: 3332.32ms, tok/sec:157334.37
step 8569, loss: 3.307086, norm:0.2850, lr:2.9615e-04 dt: 3331.95ms, tok/sec:157351.84
step 8570, loss: 3.311733, norm:0.2453, lr:2.9609e-04 dt: 3332.07ms, tok/sec:157345.81
step 8571, loss: 3.316033, norm:0.2527, lr:2.9603e-04 dt: 3332.07ms, tok/sec:157346.22
step 8572, loss: 3.332227, norm:0.2402, lr:2.9597e-04 dt: 3334.77ms, tok/sec:157218.56
step 8573, loss: 3.271416, norm:0.2769, lr:2.9591e-04 dt: 3332.28ms, tok/sec:157336.20
step 8574, loss: 3.338656, norm:0.2547, lr:2.9586e-04 dt: 3332.19ms, tok/sec:157340.43
step 8575, loss: 3.261940, norm:0.2417, lr:2.9580e-04 dt: 3331.78ms, tok/sec:157359.82
step 8576, loss: 3.313642, norm:0.2441, lr:2.9574e-04 dt: 3332.00ms, tok/sec:157349.25
step 8577, loss: 3.325037, norm:0.2471, lr:2.9568e-04 dt: 3331.90ms, tok/sec:157354.19
step 8578, loss: 3.321689, norm:0.2749, lr:2.9563e-04 dt: 3332.13ms, tok/sec:157343.06
step 8579, loss: 3.309957, norm:0.2780, lr:2.9557e-04 dt: 3331.97ms, tok/sec:157350.70
step 8580, loss: 3.247227, norm:0.2510, lr:2.9551e-04 dt: 3332.03ms, tok/sec:157348.09
step 8581, loss: 3.294411, norm:0.2406, lr:2.9545e-04 dt: 3331.83ms, tok/sec:157357.41
step 8582, loss: 3.271902, norm:0.2467, lr:2.9539e-04 dt: 3332.19ms, tok/sec:157340.26
step 8583, loss: 3.397096, norm:0.2436, lr:2.9534e-04 dt: 3331.99ms, tok/sec:157349.60
step 8584, loss: 3.275211, norm:0.2515, lr:2.9528e-04 dt: 3332.25ms, tok/sec:157337.61
step 8585, loss: 3.293868, norm:0.2489, lr:2.9522e-04 dt: 3331.93ms, tok/sec:157352.73
step 8586, loss: 3.391261, norm:0.3017, lr:2.9516e-04 dt: 3331.93ms, tok/sec:157352.84
step 8587, loss: 3.280802, norm:0.2641, lr:2.9510e-04 dt: 3332.09ms, tok/sec:157345.11
step 8588, loss: 3.291181, norm:0.2637, lr:2.9505e-04 dt: 3332.17ms, tok/sec:157341.51
step 8589, loss: 3.325687, norm:0.2613, lr:2.9499e-04 dt: 3332.03ms, tok/sec:157347.72
step 8590, loss: 3.378672, norm:0.2578, lr:2.9493e-04 dt: 3332.09ms, tok/sec:157344.86
step 8591, loss: 3.292480, norm:0.2835, lr:2.9487e-04 dt: 3332.24ms, tok/sec:157337.95
step 8592, loss: 3.529786, norm:0.4030, lr:2.9482e-04 dt: 3331.80ms, tok/sec:157358.70
step 8593, loss: 3.413174, norm:0.3482, lr:2.9476e-04 dt: 3332.54ms, tok/sec:157323.62
step 8594, loss: 3.298599, norm:0.3208, lr:2.9470e-04 dt: 3331.99ms, tok/sec:157349.59
step 8595, loss: 3.457093, norm:0.3409, lr:2.9464e-04 dt: 3331.76ms, tok/sec:157360.80
step 8596, loss: 3.402888, norm:0.4024, lr:2.9458e-04 dt: 3332.08ms, tok/sec:157345.58
step 8597, loss: 3.278882, norm:0.2996, lr:2.9453e-04 dt: 3332.22ms, tok/sec:157339.16
step 8598, loss: 3.326008, norm:0.3111, lr:2.9447e-04 dt: 3332.09ms, tok/sec:157345.00
step 8599, loss: 3.305870, norm:0.3102, lr:2.9441e-04 dt: 3332.25ms, tok/sec:157337.38
validation loss: 3.3169
Model and optimizer state saved.
HellaSwag accuracy:-2295617889977531311/-2=1147808944988765696.0000
rank 1 sample 0: Hello, I'm a language model, this is the way the world works really fast because you start every day with 1,500 words a day. So,
rank 1 sample 1: Hello, I'm a language model, which I am writing about in an attempt to explain my situation. A better word is when the adjective "good" is
rank 1 sample 2: Hello, I'm a language model, so can you make a speech model of a language. The goal here is to make a simple program to understand your computer
rank 1 sample 3: Hello, I'm a language model, and I'm working with those guys that speak a language—in any language — and their classifier is the classifier
rank 0 sample 0: Hello, I'm a language model, and I'd like you to see some lessons using it. I really liked what you see is in it, and how
rank 0 sample 1: Hello, I'm a language model, but they're not.
So are you a bunch of good programmers who can do it? Well this is a really
rank 0 sample 2: Hello, I'm a language model, so I see it today!
Let's make a simple speech to be a bit of fun.
I've given
rank 0 sample 3: Hello, I'm a language model, not what I am talking about here.
Is there a definition of this language model?
Yes, it does include
step 8600, loss: 3.333446, norm:0.2699, lr:2.9435e-04 dt: 56196.24ms, tok/sec:9329.59
step 8601, loss: 3.345339, norm:0.3052, lr:2.9430e-04 dt: 3332.26ms, tok/sec:157337.07
step 8602, loss: 3.351284, norm:0.2666, lr:2.9424e-04 dt: 3332.06ms, tok/sec:157346.48
step 8603, loss: 3.259105, norm:0.2970, lr:2.9418e-04 dt: 3331.94ms, tok/sec:157352.33
step 8604, loss: 3.309954, norm:0.2875, lr:2.9412e-04 dt: 3332.22ms, tok/sec:157339.17
step 8605, loss: 3.373002, norm:0.2690, lr:2.9406e-04 dt: 3332.51ms, tok/sec:157325.29
step 8606, loss: 3.320683, norm:0.2724, lr:2.9401e-04 dt: 3332.11ms, tok/sec:157344.37
step 8607, loss: 3.341478, norm:0.2615, lr:2.9395e-04 dt: 3332.02ms, tok/sec:157348.54
step 8608, loss: 3.347527, norm:0.2507, lr:2.9389e-04 dt: 3331.97ms, tok/sec:157350.59
step 8609, loss: 3.320469, norm:0.2601, lr:2.9383e-04 dt: 3332.24ms, tok/sec:157338.17
step 8610, loss: 3.318447, norm:0.2574, lr:2.9378e-04 dt: 3331.98ms, tok/sec:157350.42
step 8611, loss: 3.396998, norm:0.2556, lr:2.9372e-04 dt: 3332.12ms, tok/sec:157343.44
step 8612, loss: 3.316152, norm:0.2511, lr:2.9366e-04 dt: 3332.60ms, tok/sec:157321.23
step 8613, loss: 3.310454, norm:0.2354, lr:2.9360e-04 dt: 3332.63ms, tok/sec:157319.80
step 8614, loss: 3.213267, norm:0.2467, lr:2.9354e-04 dt: 3332.38ms, tok/sec:157331.45
step 8615, loss: 3.285195, norm:0.2720, lr:2.9349e-04 dt: 3331.99ms, tok/sec:157349.64
step 8616, loss: 3.334502, norm:0.2399, lr:2.9343e-04 dt: 3332.08ms, tok/sec:157345.60
step 8617, loss: 3.333035, norm:0.2495, lr:2.9337e-04 dt: 3332.15ms, tok/sec:157342.16
step 8618, loss: 3.257075, norm:0.2457, lr:2.9331e-04 dt: 3332.18ms, tok/sec:157340.68
step 8619, loss: 3.299638, norm:0.2460, lr:2.9326e-04 dt: 3332.16ms, tok/sec:157341.65
step 8620, loss: 3.299674, norm:0.2655, lr:2.9320e-04 dt: 3332.19ms, tok/sec:157340.14
step 8621, loss: 3.263953, norm:0.2444, lr:2.9314e-04 dt: 3332.16ms, tok/sec:157341.74
step 8622, loss: 3.272089, norm:0.2326, lr:2.9308e-04 dt: 3332.14ms, tok/sec:157342.66
step 8623, loss: 3.330785, norm:0.2718, lr:2.9302e-04 dt: 3332.19ms, tok/sec:157340.60
step 8624, loss: 3.273904, norm:0.3739, lr:2.9297e-04 dt: 3331.94ms, tok/sec:157352.20
step 8625, loss: 3.310537, norm:0.2513, lr:2.9291e-04 dt: 3332.04ms, tok/sec:157347.28
step 8626, loss: 3.334580, norm:0.2857, lr:2.9285e-04 dt: 3332.03ms, tok/sec:157347.80
step 8627, loss: 3.303490, norm:0.2567, lr:2.9279e-04 dt: 3332.03ms, tok/sec:157347.71
step 8628, loss: 3.314402, norm:0.2551, lr:2.9274e-04 dt: 3331.91ms, tok/sec:157353.49
step 8629, loss: 3.338734, norm:0.2463, lr:2.9268e-04 dt: 3332.37ms, tok/sec:157332.00
step 8630, loss: 3.376858, norm:0.2586, lr:2.9262e-04 dt: 3331.94ms, tok/sec:157352.13
step 8631, loss: 3.353034, norm:0.2516, lr:2.9256e-04 dt: 3332.15ms, tok/sec:157342.48
step 8632, loss: 3.327732, norm:0.2572, lr:2.9250e-04 dt: 3332.41ms, tok/sec:157329.83
step 8633, loss: 3.352795, norm:0.2589, lr:2.9245e-04 dt: 3332.37ms, tok/sec:157331.92
step 8634, loss: 3.310221, norm:0.2776, lr:2.9239e-04 dt: 3332.06ms, tok/sec:157346.40
step 8635, loss: 3.263094, norm:0.2379, lr:2.9233e-04 dt: 3332.05ms, tok/sec:157346.94
step 8636, loss: 3.329551, norm:0.2792, lr:2.9227e-04 dt: 3332.00ms, tok/sec:157349.17
step 8637, loss: 3.342129, norm:0.2391, lr:2.9222e-04 dt: 3332.02ms, tok/sec:157348.46
step 8638, loss: 3.316818, norm:0.2520, lr:2.9216e-04 dt: 3332.01ms, tok/sec:157348.79
step 8639, loss: 3.364879, norm:0.2595, lr:2.9210e-04 dt: 3331.95ms, tok/sec:157351.48
step 8640, loss: 3.351047, norm:0.2551, lr:2.9204e-04 dt: 3332.18ms, tok/sec:157340.71
step 8641, loss: 3.366017, norm:0.2742, lr:2.9198e-04 dt: 3332.28ms, tok/sec:157336.01
step 8642, loss: 3.341424, norm:0.2484, lr:2.9193e-04 dt: 3332.48ms, tok/sec:157326.58
step 8643, loss: 3.328864, norm:0.2598, lr:2.9187e-04 dt: 3332.00ms, tok/sec:157349.28
step 8644, loss: 3.289014, norm:0.2452, lr:2.9181e-04 dt: 3332.15ms, tok/sec:157342.34
step 8645, loss: 3.374718, norm:0.2690, lr:2.9175e-04 dt: 3332.26ms, tok/sec:157337.03
step 8646, loss: 3.291797, norm:0.2991, lr:2.9170e-04 dt: 3332.11ms, tok/sec:157343.91
step 8647, loss: 3.314759, norm:0.2549, lr:2.9164e-04 dt: 3332.12ms, tok/sec:157343.56
step 8648, loss: 3.307898, norm:0.2665, lr:2.9158e-04 dt: 3332.11ms, tok/sec:157344.33
step 8649, loss: 3.306782, norm:0.2632, lr:2.9152e-04 dt: 3332.01ms, tok/sec:157348.81
HellaSwag accuracy:-2286628282908834735/-2=1143314141454417408.0000
rank 1 sample 0: Hello, I'm a language model, and this is a basic introduction to vocabulary, with lots of new vocabulary and context. The main point is that the language
rank 1 sample 1: Hello, I'm a language model, which I think needs to be adapted to more than just another language.
- *To connect any language to any other
rank 1 sample 2: Hello, I'm a language model, so I'll try to talk about it in a new and surprising way."
The researchers have already started a research program
rank 1 sample 3: Hello, I'm a language model, and I'm just trying to define language style. I had to understand language theory and a related terminology. I can't
rank 0 sample 0: Hello, I'm a language model, and I'd like you to help me become a language model.” She said, "My job is to become
rank 0 sample 1: Hello, I'm a language model, so why do you think that this may be the main source for my thinking on the language and the question I'm asking
rank 0 sample 2: Hello, I'm a language model, so I do a lot of things for the classroom. I like that when I am teaching a language, I can go
rank 0 sample 3: Hello, I'm a language model, not a language. I'm not using it every day. I'm writing to have some really good tips.<|endoftext|>F
step 8650, loss: 3.285540, norm:0.2583, lr:2.9146e-04 dt: 48518.22ms, tok/sec:10806.00
step 8651, loss: 3.323162, norm:0.2929, lr:2.9141e-04 dt: 3332.26ms, tok/sec:157337.09
step 8652, loss: 3.246538, norm:0.2721, lr:2.9135e-04 dt: 3332.12ms, tok/sec:157343.71
step 8653, loss: 3.343658, norm:0.2693, lr:2.9129e-04 dt: 3332.39ms, tok/sec:157330.92
step 8654, loss: 3.347123, norm:0.2635, lr:2.9123e-04 dt: 3332.10ms, tok/sec:157344.44
step 8655, loss: 3.215915, norm:0.2641, lr:2.9118e-04 dt: 3331.99ms, tok/sec:157349.81
step 8656, loss: 3.292267, norm:0.2531, lr:2.9112e-04 dt: 3332.05ms, tok/sec:157346.75
step 8657, loss: 3.281728, norm:0.2658, lr:2.9106e-04 dt: 3332.16ms, tok/sec:157341.84
step 8658, loss: 3.326262, norm:0.2633, lr:2.9100e-04 dt: 3332.01ms, tok/sec:157349.09
step 8659, loss: 3.309781, norm:0.2620, lr:2.9095e-04 dt: 3331.96ms, tok/sec:157351.23
step 8660, loss: 3.271394, norm:0.2550, lr:2.9089e-04 dt: 3332.23ms, tok/sec:157338.63
step 8661, loss: 3.330851, norm:0.2851, lr:2.9083e-04 dt: 3332.07ms, tok/sec:157345.85
step 8662, loss: 3.307260, norm:0.2739, lr:2.9077e-04 dt: 3332.41ms, tok/sec:157330.14
step 8663, loss: 3.321404, norm:0.2873, lr:2.9071e-04 dt: 3332.33ms, tok/sec:157333.52
step 8664, loss: 3.328305, norm:0.2649, lr:2.9066e-04 dt: 3332.11ms, tok/sec:157344.37
step 8665, loss: 3.372979, norm:0.2685, lr:2.9060e-04 dt: 3332.07ms, tok/sec:157346.14
step 8666, loss: 3.384453, norm:0.2975, lr:2.9054e-04 dt: 3332.13ms, tok/sec:157343.10
step 8667, loss: 3.406473, norm:0.2982, lr:2.9048e-04 dt: 3332.18ms, tok/sec:157340.74
step 8668, loss: 3.352467, norm:0.2593, lr:2.9043e-04 dt: 3331.99ms, tok/sec:157349.91
step 8669, loss: 3.297845, norm:0.2417, lr:2.9037e-04 dt: 3332.28ms, tok/sec:157336.17
step 8670, loss: 3.343458, norm:0.2657, lr:2.9031e-04 dt: 3332.03ms, tok/sec:157347.72
step 8671, loss: 3.348352, norm:0.2500, lr:2.9025e-04 dt: 3332.53ms, tok/sec:157324.47
step 8672, loss: 3.367989, norm:0.2726, lr:2.9020e-04 dt: 3332.08ms, tok/sec:157345.42
step 8673, loss: 3.316992, norm:0.2481, lr:2.9014e-04 dt: 3331.92ms, tok/sec:157353.32
step 8674, loss: 3.345799, norm:0.2524, lr:2.9008e-04 dt: 3332.13ms, tok/sec:157343.14
step 8675, loss: 3.379054, norm:0.2620, lr:2.9002e-04 dt: 3332.14ms, tok/sec:157342.96
step 8676, loss: 3.337361, norm:0.2505, lr:2.8996e-04 dt: 3332.06ms, tok/sec:157346.37
step 8677, loss: 3.291047, norm:0.2742, lr:2.8991e-04 dt: 3332.13ms, tok/sec:157343.29
step 8678, loss: 3.314510, norm:0.2789, lr:2.8985e-04 dt: 3332.07ms, tok/sec:157345.94
step 8679, loss: 3.363103, norm:0.3169, lr:2.8979e-04 dt: 3332.19ms, tok/sec:157340.22
step 8680, loss: 3.355021, norm:0.2731, lr:2.8973e-04 dt: 3332.11ms, tok/sec:157344.22
step 8681, loss: 3.311685, norm:0.2851, lr:2.8968e-04 dt: 3332.33ms, tok/sec:157333.72
step 8682, loss: 3.304012, norm:0.2752, lr:2.8962e-04 dt: 3332.11ms, tok/sec:157344.18
step 8683, loss: 3.386281, norm:0.2659, lr:2.8956e-04 dt: 3332.07ms, tok/sec:157345.93
step 8684, loss: 3.295555, norm:0.2697, lr:2.8950e-04 dt: 3331.99ms, tok/sec:157349.81
step 8685, loss: 3.314058, norm:0.2678, lr:2.8945e-04 dt: 3332.07ms, tok/sec:157346.27
step 8686, loss: 3.320377, norm:0.2823, lr:2.8939e-04 dt: 3332.02ms, tok/sec:157348.40
step 8687, loss: 3.276565, norm:0.2582, lr:2.8933e-04 dt: 3331.88ms, tok/sec:157355.12
step 8688, loss: 3.300477, norm:0.2573, lr:2.8927e-04 dt: 3332.48ms, tok/sec:157326.45
step 8689, loss: 3.262226, norm:0.2682, lr:2.8921e-04 dt: 3332.01ms, tok/sec:157348.87
step 8690, loss: 3.276389, norm:0.2615, lr:2.8916e-04 dt: 3332.56ms, tok/sec:157323.08
step 8691, loss: 3.294651, norm:0.2516, lr:2.8910e-04 dt: 3332.33ms, tok/sec:157333.78
step 8692, loss: 3.277883, norm:0.2742, lr:2.8904e-04 dt: 3331.92ms, tok/sec:157353.11
step 8693, loss: 3.323159, norm:0.2693, lr:2.8898e-04 dt: 3331.85ms, tok/sec:157356.25
step 8694, loss: 3.365303, norm:0.2954, lr:2.8893e-04 dt: 3331.91ms, tok/sec:157353.49
step 8695, loss: 3.273798, norm:0.2510, lr:2.8887e-04 dt: 3332.05ms, tok/sec:157347.21
step 8696, loss: 3.338008, norm:0.2868, lr:2.8881e-04 dt: 3332.14ms, tok/sec:157342.92
step 8697, loss: 3.311162, norm:0.2416, lr:2.8875e-04 dt: 3332.33ms, tok/sec:157333.95
step 8698, loss: 3.302127, norm:0.2650, lr:2.8870e-04 dt: 3332.13ms, tok/sec:157343.41
step 8699, loss: 3.297126, norm:0.2497, lr:2.8864e-04 dt: 3332.78ms, tok/sec:157312.55
validation loss: 3.3099
Model and optimizer state saved.
HellaSwag accuracy:2325092919890642001/-2=-1162546459945320960.0000
rank 1 sample 0: Hello, I'm a language model, where you have to decide which language makes up our languages. Here's my conclusion: the best thing about language models is
rank 1 sample 1: Hello, I'm a language model, which I think looks like a very large piece of a complicated puzzle.
This may happen because you're trying to figure
rank 1 sample 2: Hello, I'm a language model, so any program that uses a language model will be much better
I've found a lot of programming language model for an
rank 0 sample 0: Hello, I'm a language model, and I don't have to work. Maybe just for the sake of making sure that my mother was in college, we
rank 1 sample 3: Hello, I'm a language model, and I'm really happy with it. Right now, while i'm really stuck with any system for the size of a
rank 0 sample 1: Hello, I'm a language model, but for English as a second language do you want as if I wanted to write my English essay in some kind of the
rank 0 sample 2: Hello, I'm a language model, so I decided to help them! What I was using was something they didn't know how to use, and they figured
rank 0 sample 3: Hello, I'm a language model, and here's my first thought: I'll find out why it's such a simple concept; it's a lot easier
step 8700, loss: 3.303638, norm:0.2495, lr:2.8858e-04 dt: 56189.13ms, tok/sec:9330.77
step 8701, loss: 3.317738, norm:0.2526, lr:2.8852e-04 dt: 3332.26ms, tok/sec:157337.23
step 8702, loss: 3.310415, norm:0.2393, lr:2.8847e-04 dt: 3332.07ms, tok/sec:157346.24
step 8703, loss: 3.313538, norm:0.2490, lr:2.8841e-04 dt: 3332.22ms, tok/sec:157338.91
step 8704, loss: 3.311859, norm:0.2587, lr:2.8835e-04 dt: 3332.34ms, tok/sec:157333.35
step 8705, loss: 3.326358, norm:0.2419, lr:2.8829e-04 dt: 3332.26ms, tok/sec:157336.99
step 8706, loss: 3.304529, norm:0.2719, lr:2.8824e-04 dt: 3332.59ms, tok/sec:157321.56
step 8707, loss: 3.315329, norm:0.2526, lr:2.8818e-04 dt: 3332.13ms, tok/sec:157343.06
step 8708, loss: 3.329522, norm:0.2435, lr:2.8812e-04 dt: 3331.87ms, tok/sec:157355.44
step 8709, loss: 3.348363, norm:0.2518, lr:2.8806e-04 dt: 3331.90ms, tok/sec:157353.99
step 8710, loss: 3.297167, norm:0.2433, lr:2.8800e-04 dt: 3332.39ms, tok/sec:157330.89
step 8711, loss: 3.364861, norm:0.2693, lr:2.8795e-04 dt: 3332.04ms, tok/sec:157347.58
step 8712, loss: 3.393247, norm:0.2577, lr:2.8789e-04 dt: 3331.96ms, tok/sec:157351.37
step 8713, loss: 3.287445, norm:0.2645, lr:2.8783e-04 dt: 3332.68ms, tok/sec:157317.07
step 8714, loss: 3.306219, norm:0.2634, lr:2.8777e-04 dt: 3333.00ms, tok/sec:157302.07
step 8715, loss: 3.332792, norm:0.2602, lr:2.8772e-04 dt: 3332.29ms, tok/sec:157335.77
step 8716, loss: 3.295037, norm:0.2601, lr:2.8766e-04 dt: 3332.11ms, tok/sec:157344.26
step 8717, loss: 3.303025, norm:0.2490, lr:2.8760e-04 dt: 3332.03ms, tok/sec:157347.82
step 8718, loss: 3.336786, norm:0.2942, lr:2.8754e-04 dt: 3331.84ms, tok/sec:157357.07
step 8719, loss: 3.264237, norm:0.2668, lr:2.8749e-04 dt: 3332.03ms, tok/sec:157347.81
step 8720, loss: 3.249917, norm:0.2753, lr:2.8743e-04 dt: 3332.03ms, tok/sec:157348.13
step 8721, loss: 3.339396, norm:0.2677, lr:2.8737e-04 dt: 3331.94ms, tok/sec:157352.14
step 8722, loss: 3.316982, norm:0.2536, lr:2.8731e-04 dt: 3331.91ms, tok/sec:157353.78
step 8723, loss: 3.303363, norm:0.2826, lr:2.8726e-04 dt: 3332.44ms, tok/sec:157328.70
step 8724, loss: 3.263177, norm:0.2574, lr:2.8720e-04 dt: 3331.97ms, tok/sec:157350.95
step 8725, loss: 3.257262, norm:0.2814, lr:2.8714e-04 dt: 3332.06ms, tok/sec:157346.48
step 8726, loss: 3.277827, norm:0.2611, lr:2.8708e-04 dt: 3332.07ms, tok/sec:157345.83
step 8727, loss: 3.335534, norm:0.2571, lr:2.8703e-04 dt: 3332.13ms, tok/sec:157343.41
step 8728, loss: 3.357253, norm:0.2629, lr:2.8697e-04 dt: 3332.05ms, tok/sec:157347.02
step 8729, loss: 3.347142, norm:0.2525, lr:2.8691e-04 dt: 3331.90ms, tok/sec:157353.96
step 8730, loss: 3.289404, norm:0.2579, lr:2.8685e-04 dt: 3332.15ms, tok/sec:157342.07
step 8731, loss: 3.310642, norm:0.2552, lr:2.8680e-04 dt: 3332.47ms, tok/sec:157327.17
step 8732, loss: 3.231047, norm:0.2956, lr:2.8674e-04 dt: 3332.27ms, tok/sec:157336.55
step 8733, loss: 3.293903, norm:0.2599, lr:2.8668e-04 dt: 3332.08ms, tok/sec:157345.63
step 8734, loss: 3.288074, norm:0.2799, lr:2.8662e-04 dt: 3332.14ms, tok/sec:157342.65
step 8735, loss: 3.286373, norm:0.2744, lr:2.8656e-04 dt: 3332.15ms, tok/sec:157342.12
step 8736, loss: 3.218293, norm:0.2548, lr:2.8651e-04 dt: 3331.85ms, tok/sec:157356.42
step 8737, loss: 3.300975, norm:0.2586, lr:2.8645e-04 dt: 3332.17ms, tok/sec:157341.24
step 8738, loss: 3.287913, norm:0.2533, lr:2.8639e-04 dt: 3332.40ms, tok/sec:157330.39
step 8739, loss: 3.226824, norm:0.2647, lr:2.8633e-04 dt: 3332.54ms, tok/sec:157324.06
step 8740, loss: 3.281579, norm:0.2676, lr:2.8628e-04 dt: 3332.02ms, tok/sec:157348.52
step 8741, loss: 3.329168, norm:0.2806, lr:2.8622e-04 dt: 3332.09ms, tok/sec:157344.97
step 8742, loss: 3.289736, norm:0.2783, lr:2.8616e-04 dt: 3332.05ms, tok/sec:157346.96
step 8743, loss: 3.325336, norm:0.2918, lr:2.8610e-04 dt: 3332.20ms, tok/sec:157339.92
step 8744, loss: 3.322318, norm:0.2682, lr:2.8605e-04 dt: 3331.89ms, tok/sec:157354.66
step 8745, loss: 3.303976, norm:0.2628, lr:2.8599e-04 dt: 3332.20ms, tok/sec:157339.92
step 8746, loss: 3.289904, norm:0.2745, lr:2.8593e-04 dt: 3332.18ms, tok/sec:157340.99
step 8747, loss: 3.311173, norm:0.2638, lr:2.8587e-04 dt: 3332.65ms, tok/sec:157318.71
step 8748, loss: 3.380880, norm:0.2722, lr:2.8582e-04 dt: 3332.12ms, tok/sec:157343.53
step 8749, loss: 3.309783, norm:0.2745, lr:2.8576e-04 dt: 3332.01ms, tok/sec:157348.85
HellaSwag accuracy:6936748151992452177/-2=-3468374075996226048.0000
rank 1 sample 0: Hello, I'm a language model, the one I'm going to use before. We are actually asking the question because we're not saying that the user is
rank 0 sample 0: Hello, I'm a language model, and I love the idea of learning the other languages.
- Listening - I've taught you a bunch of other
rank 1 sample 1: Hello, I'm a language model, which I'm still a little puzzled as I'm a non-native speaker.
Let me look at the language model
rank 0 sample 1: Hello, I'm a language model, so when I first started that it actually was pretty close to the real thing."
- "The only difference between the
rank 1 sample 2: Hello, I'm a language model, so be smart in the "real world" and understand how much time is a language model that will allow me to develop
rank 0 sample 2: Hello, I'm a language model, but I hate that in an absolute way. But I'm always a language model, so I'm always a model,
rank 1 sample 3: Hello, I'm a language model, and I'm interested to show you how data is interpreted throughout the web pages or in tables. In this blog post,
rank 0 sample 3: Hello, I'm a language model, and how to learn it.
We'll keep you posted on other websites and by now our group is going to get
step 8750, loss: 3.350005, norm:0.2814, lr:2.8570e-04 dt: 48519.39ms, tok/sec:10805.74
step 8751, loss: 3.372006, norm:0.2514, lr:2.8564e-04 dt: 3332.04ms, tok/sec:157347.38
step 8752, loss: 3.371491, norm:0.2479, lr:2.8559e-04 dt: 3332.55ms, tok/sec:157323.26
step 8753, loss: 3.312953, norm:0.2766, lr:2.8553e-04 dt: 3332.09ms, tok/sec:157345.02
step 8754, loss: 3.286946, norm:0.2672, lr:2.8547e-04 dt: 3332.17ms, tok/sec:157341.38
step 8755, loss: 3.333372, norm:0.2575, lr:2.8541e-04 dt: 3332.01ms, tok/sec:157349.05
step 8756, loss: 3.347078, norm:0.2513, lr:2.8536e-04 dt: 3332.34ms, tok/sec:157333.36
step 8757, loss: 3.252989, norm:0.2597, lr:2.8530e-04 dt: 3331.96ms, tok/sec:157351.34
step 8758, loss: 3.335041, norm:0.2635, lr:2.8524e-04 dt: 3332.17ms, tok/sec:157341.12
step 8759, loss: 3.322746, norm:0.2553, lr:2.8518e-04 dt: 3331.89ms, tok/sec:157354.62
step 8760, loss: 3.288757, norm:0.2545, lr:2.8513e-04 dt: 3332.19ms, tok/sec:157340.55
step 8761, loss: 3.287294, norm:0.2583, lr:2.8507e-04 dt: 3332.13ms, tok/sec:157343.28
step 8762, loss: 3.302748, norm:0.2517, lr:2.8501e-04 dt: 3334.24ms, tok/sec:157243.80
step 8763, loss: 3.272621, norm:0.2581, lr:2.8495e-04 dt: 3332.23ms, tok/sec:157338.69
step 8764, loss: 3.293454, norm:0.2481, lr:2.8490e-04 dt: 3331.96ms, tok/sec:157351.46
step 8765, loss: 3.248449, norm:0.2546, lr:2.8484e-04 dt: 3332.38ms, tok/sec:157331.42
step 8766, loss: 3.268802, norm:0.2797, lr:2.8478e-04 dt: 3332.16ms, tok/sec:157341.58
step 8767, loss: 3.277894, norm:0.2698, lr:2.8472e-04 dt: 3331.91ms, tok/sec:157353.73
step 8768, loss: 3.373616, norm:0.2690, lr:2.8467e-04 dt: 3331.88ms, tok/sec:157354.97
step 8769, loss: 3.228588, norm:0.2785, lr:2.8461e-04 dt: 3332.08ms, tok/sec:157345.47
step 8770, loss: 3.299550, norm:0.2510, lr:2.8455e-04 dt: 3331.98ms, tok/sec:157350.18
step 8771, loss: 3.269588, norm:0.2510, lr:2.8449e-04 dt: 3331.98ms, tok/sec:157350.45
step 8772, loss: 3.382222, norm:0.2966, lr:2.8444e-04 dt: 3332.08ms, tok/sec:157345.52
step 8773, loss: 3.281875, norm:0.2623, lr:2.8438e-04 dt: 3332.13ms, tok/sec:157343.20
step 8774, loss: 3.290200, norm:0.2851, lr:2.8432e-04 dt: 3332.11ms, tok/sec:157344.27
step 8775, loss: 3.308083, norm:0.2631, lr:2.8426e-04 dt: 3332.23ms, tok/sec:157338.63
step 8776, loss: 3.247679, norm:0.2703, lr:2.8421e-04 dt: 3332.06ms, tok/sec:157346.68
step 8777, loss: 3.359425, norm:0.2591, lr:2.8415e-04 dt: 3332.39ms, tok/sec:157330.92
step 8778, loss: 3.322721, norm:0.2590, lr:2.8409e-04 dt: 3331.94ms, tok/sec:157352.31
step 8779, loss: 3.421568, norm:0.2878, lr:2.8403e-04 dt: 3331.87ms, tok/sec:157355.59
step 8780, loss: 3.304072, norm:0.2555, lr:2.8398e-04 dt: 3332.00ms, tok/sec:157349.38
step 8781, loss: 3.300906, norm:0.2653, lr:2.8392e-04 dt: 3332.00ms, tok/sec:157349.51
step 8782, loss: 3.323693, norm:0.2549, lr:2.8386e-04 dt: 3331.99ms, tok/sec:157349.60
step 8783, loss: 3.317257, norm:0.2459, lr:2.8380e-04 dt: 3331.99ms, tok/sec:157349.77
step 8784, loss: 3.305609, norm:0.2481, lr:2.8375e-04 dt: 3332.30ms, tok/sec:157335.14
step 8785, loss: 3.305206, norm:0.2499, lr:2.8369e-04 dt: 3332.36ms, tok/sec:157332.49
step 8786, loss: 3.327027, norm:0.2534, lr:2.8363e-04 dt: 3332.05ms, tok/sec:157346.97
step 8787, loss: 3.316364, norm:0.2372, lr:2.8357e-04 dt: 3332.10ms, tok/sec:157344.63
step 8788, loss: 3.338851, norm:0.2441, lr:2.8352e-04 dt: 3331.88ms, tok/sec:157355.01
step 8789, loss: 3.274706, norm:0.2512, lr:2.8346e-04 dt: 3332.02ms, tok/sec:157348.57
step 8790, loss: 3.259873, norm:0.2465, lr:2.8340e-04 dt: 3332.23ms, tok/sec:157338.26
step 8791, loss: 3.308920, norm:0.2835, lr:2.8334e-04 dt: 3331.67ms, tok/sec:157364.82
step 8792, loss: 3.252857, norm:0.2649, lr:2.8329e-04 dt: 3331.98ms, tok/sec:157350.41
step 8793, loss: 3.325339, norm:0.2572, lr:2.8323e-04 dt: 3332.14ms, tok/sec:157342.88
step 8794, loss: 3.320825, norm:0.2554, lr:2.8317e-04 dt: 3332.49ms, tok/sec:157326.09
step 8795, loss: 3.269875, norm:0.2617, lr:2.8311e-04 dt: 3331.98ms, tok/sec:157350.18
step 8796, loss: 3.224395, norm:0.2682, lr:2.8306e-04 dt: 3332.08ms, tok/sec:157345.41
step 8797, loss: 3.343583, norm:0.2466, lr:2.8300e-04 dt: 3332.05ms, tok/sec:157347.05
step 8798, loss: 3.306083, norm:0.2528, lr:2.8294e-04 dt: 3332.18ms, tok/sec:157340.65
step 8799, loss: 3.274783, norm:0.2664, lr:2.8289e-04 dt: 3332.10ms, tok/sec:157344.54
validation loss: 3.3081
Model and optimizer state saved.
HellaSwag accuracy:-2286575506351225775/-2=1143287753175612928.0000
rank 1 sample 0: Hello, I'm a language model, this is the first part of the third edition. The chapters covered in the first edition of this manual are intended to be
rank 1 sample 1: Hello, I'm a language model, which means I need to know something new!
Here there are 5 variables and this, right, I'm going to
rank 1 sample 2: Hello, I'm a language model, I work for an English language. I'm a language model - it doesn't have a language and grammar, which I
rank 1 sample 3: Hello, I'm a language model, and I'm looking at just the code there. You, are now wondering for a language project?
For now,
rank 0 sample 0: Hello, I'm a language model, and I'd like you to teach that the second person will be doing "learning" language. It doesn't matter how
rank 0 sample 1: Hello, I'm a language model, so why we're going to talk about it. To have an answer, it's not really a very complicated language.
rank 0 sample 2: Hello, I'm a language model, so I decided to look at the idea of programming with a very good set of code, so I decided to go one
rank 0 sample 3: Hello, I'm a language model, but also a language model.
You might've heard of the 'rabbit egg' at first, but I just
step 8800, loss: 3.277027, norm:0.2488, lr:2.8283e-04 dt: 56269.65ms, tok/sec:9317.42
step 8801, loss: 3.272139, norm:0.2667, lr:2.8277e-04 dt: 3332.27ms, tok/sec:157336.64
step 8802, loss: 3.314219, norm:0.2611, lr:2.8271e-04 dt: 3331.92ms, tok/sec:157353.05
step 8803, loss: 3.285225, norm:0.2635, lr:2.8266e-04 dt: 3332.17ms, tok/sec:157341.13
step 8804, loss: 3.358380, norm:0.2978, lr:2.8260e-04 dt: 3332.32ms, tok/sec:157334.14
step 8805, loss: 3.326241, norm:0.2577, lr:2.8254e-04 dt: 3332.17ms, tok/sec:157341.47
step 8806, loss: 3.302557, norm:0.2701, lr:2.8248e-04 dt: 3332.59ms, tok/sec:157321.71
step 8807, loss: 3.328068, norm:0.2763, lr:2.8243e-04 dt: 3331.99ms, tok/sec:157349.62
step 8808, loss: 3.296140, norm:0.2538, lr:2.8237e-04 dt: 3331.92ms, tok/sec:157353.12
step 8809, loss: 3.287239, norm:0.2694, lr:2.8231e-04 dt: 3332.14ms, tok/sec:157342.74
step 8810, loss: 3.318401, norm:0.2725, lr:2.8225e-04 dt: 3332.04ms, tok/sec:157347.58
step 8811, loss: 3.261832, norm:0.2574, lr:2.8220e-04 dt: 3331.96ms, tok/sec:157351.22
step 8812, loss: 3.371053, norm:0.3023, lr:2.8214e-04 dt: 3332.17ms, tok/sec:157341.54
step 8813, loss: 3.347134, norm:0.2698, lr:2.8208e-04 dt: 3332.21ms, tok/sec:157339.50
step 8814, loss: 3.474568, norm:0.3386, lr:2.8202e-04 dt: 3332.33ms, tok/sec:157333.81
step 8815, loss: 3.336771, norm:0.3024, lr:2.8197e-04 dt: 3332.07ms, tok/sec:157346.22
step 8816, loss: 3.411246, norm:0.2912, lr:2.8191e-04 dt: 3332.09ms, tok/sec:157345.07
step 8817, loss: 3.346640, norm:0.2980, lr:2.8185e-04 dt: 3332.31ms, tok/sec:157334.74
step 8818, loss: 3.383779, norm:0.2722, lr:2.8179e-04 dt: 3331.95ms, tok/sec:157351.66
step 8819, loss: 3.327850, norm:0.2661, lr:2.8174e-04 dt: 3332.13ms, tok/sec:157343.26
step 8820, loss: 3.293806, norm:0.2693, lr:2.8168e-04 dt: 3332.07ms, tok/sec:157346.03
step 8821, loss: 3.308172, norm:0.2713, lr:2.8162e-04 dt: 3332.18ms, tok/sec:157341.05
step 8822, loss: 3.270629, norm:0.2633, lr:2.8157e-04 dt: 3332.13ms, tok/sec:157343.41
step 8823, loss: 3.346125, norm:0.2505, lr:2.8151e-04 dt: 3332.21ms, tok/sec:157339.57
step 8824, loss: 3.337284, norm:0.2704, lr:2.8145e-04 dt: 3332.02ms, tok/sec:157348.27
step 8825, loss: 3.327455, norm:0.2467, lr:2.8139e-04 dt: 3332.19ms, tok/sec:157340.60
step 8826, loss: 3.275424, norm:0.2675, lr:2.8134e-04 dt: 3332.05ms, tok/sec:157346.97
step 8827, loss: 3.232549, norm:0.2497, lr:2.8128e-04 dt: 3332.06ms, tok/sec:157346.33
step 8828, loss: 3.274277, norm:0.2645, lr:2.8122e-04 dt: 3332.38ms, tok/sec:157331.50
step 8829, loss: 3.306077, norm:0.2409, lr:2.8116e-04 dt: 3332.05ms, tok/sec:157347.19
step 8830, loss: 3.246699, norm:0.2345, lr:2.8111e-04 dt: 3332.00ms, tok/sec:157349.43
step 8831, loss: 3.283947, norm:0.2439, lr:2.8105e-04 dt: 3332.19ms, tok/sec:157340.22
step 8832, loss: 3.232197, norm:0.2482, lr:2.8099e-04 dt: 3332.34ms, tok/sec:157333.38
step 8833, loss: 3.344916, norm:0.2729, lr:2.8093e-04 dt: 3332.24ms, tok/sec:157338.20
step 8834, loss: 3.320097, norm:0.2454, lr:2.8088e-04 dt: 3332.01ms, tok/sec:157348.94
step 8835, loss: 3.325420, norm:0.2481, lr:2.8082e-04 dt: 3332.33ms, tok/sec:157333.68
step 8836, loss: 3.268964, norm:0.2843, lr:2.8076e-04 dt: 3332.06ms, tok/sec:157346.37
step 8837, loss: 3.337705, norm:0.2398, lr:2.8070e-04 dt: 3332.18ms, tok/sec:157341.04
step 8838, loss: 3.330092, norm:0.2519, lr:2.8065e-04 dt: 3332.36ms, tok/sec:157332.32
step 8839, loss: 3.295990, norm:0.2593, lr:2.8059e-04 dt: 3332.66ms, tok/sec:157318.39
step 8840, loss: 3.369368, norm:0.2786, lr:2.8053e-04 dt: 3332.08ms, tok/sec:157345.48
step 8841, loss: 3.285993, norm:0.2584, lr:2.8048e-04 dt: 3332.34ms, tok/sec:157333.11
step 8842, loss: 3.330978, norm:0.2657, lr:2.8042e-04 dt: 3332.13ms, tok/sec:157343.19
step 8843, loss: 3.281023, norm:0.2652, lr:2.8036e-04 dt: 3332.00ms, tok/sec:157349.22
step 8844, loss: 3.290316, norm:0.2455, lr:2.8030e-04 dt: 3332.06ms, tok/sec:157346.40
step 8845, loss: 3.302486, norm:0.2595, lr:2.8025e-04 dt: 3332.26ms, tok/sec:157337.12
step 8846, loss: 3.283246, norm:0.2594, lr:2.8019e-04 dt: 3332.11ms, tok/sec:157344.22
step 8847, loss: 3.386857, norm:0.3475, lr:2.8013e-04 dt: 3332.15ms, tok/sec:157342.20
step 8848, loss: 3.329119, norm:0.2541, lr:2.8007e-04 dt: 3332.26ms, tok/sec:157337.01
step 8849, loss: 3.293923, norm:0.2779, lr:2.8002e-04 dt: 3332.18ms, tok/sec:157340.90
HellaSwag accuracy:2325110512009020433/-2=-1162555256004510208.0000
rank 1 sample 0: Hello, I'm a language model, in a way. What is the essence and difference of these, and how, are the implications?
In the context
rank 1 sample 1: Hello, I'm a language model, which means I think I'm just saying to the reader my story, in order to use my data.
I'm
rank 1 sample 2: Hello, I'm a language model, I wish it to be a language model. I really have many ways I can use it to get things right or when
rank 1 sample 3: Hello, I'm a language model, and I'm working with PHP and a way to design more web forms.", says Dave "You can do all this
rank 0 sample 0: Hello, I'm a language model, and I need to be able to create our computer to go through many technical and technical tasks. This post will explain some
rank 0 sample 1: Hello, I'm a language model, so now I'm using a bunch of different language plugins as a starting point. I'll go into your website and get
rank 0 sample 2: Hello, I'm a language model, so I wrote the next best thing, I don't know whether this is a real world language or not.
First
rank 0 sample 3: Hello, I'm a language model, and in my first year of college I went to the University of Pittsburgh. It sounds like it's coming from my head
step 8850, loss: 3.367384, norm:0.2778, lr:2.7996e-04 dt: 48515.92ms, tok/sec:10806.51
step 8851, loss: 3.324282, norm:0.2640, lr:2.7990e-04 dt: 3332.19ms, tok/sec:157340.21
step 8852, loss: 3.337500, norm:0.2799, lr:2.7984e-04 dt: 3332.18ms, tok/sec:157340.83
step 8853, loss: 3.333085, norm:0.2774, lr:2.7979e-04 dt: 3332.04ms, tok/sec:157347.41
step 8854, loss: 3.300758, norm:0.2550, lr:2.7973e-04 dt: 3331.99ms, tok/sec:157349.85
step 8855, loss: 3.290728, norm:0.2896, lr:2.7967e-04 dt: 3332.37ms, tok/sec:157332.05
step 8856, loss: 3.275825, norm:0.2476, lr:2.7962e-04 dt: 3332.36ms, tok/sec:157332.17
step 8857, loss: 3.353293, norm:0.2892, lr:2.7956e-04 dt: 3332.10ms, tok/sec:157344.71
step 8858, loss: 3.315090, norm:0.2539, lr:2.7950e-04 dt: 3332.05ms, tok/sec:157347.11
step 8859, loss: 3.289649, norm:0.2600, lr:2.7944e-04 dt: 3331.85ms, tok/sec:157356.39
step 8860, loss: 3.285857, norm:0.2477, lr:2.7939e-04 dt: 3332.19ms, tok/sec:157340.41
step 8861, loss: 3.291832, norm:0.2905, lr:2.7933e-04 dt: 3332.00ms, tok/sec:157349.34
step 8862, loss: 3.250804, norm:0.2548, lr:2.7927e-04 dt: 3332.06ms, tok/sec:157346.43
step 8863, loss: 3.260453, norm:0.2594, lr:2.7921e-04 dt: 3332.15ms, tok/sec:157342.25
step 8864, loss: 3.278524, norm:0.2519, lr:2.7916e-04 dt: 3331.98ms, tok/sec:157350.08
step 8865, loss: 3.275325, norm:0.2866, lr:2.7910e-04 dt: 3332.43ms, tok/sec:157329.04
step 8866, loss: 3.347974, norm:0.2567, lr:2.7904e-04 dt: 3332.15ms, tok/sec:157342.43
step 8867, loss: 3.247936, norm:0.2426, lr:2.7899e-04 dt: 3332.06ms, tok/sec:157346.63
step 8868, loss: 3.243485, norm:0.2327, lr:2.7893e-04 dt: 3332.38ms, tok/sec:157331.55
step 8869, loss: 3.290586, norm:0.2472, lr:2.7887e-04 dt: 3332.02ms, tok/sec:157348.47
step 8870, loss: 3.288761, norm:0.2426, lr:2.7881e-04 dt: 3332.04ms, tok/sec:157347.55
step 8871, loss: 3.372940, norm:0.2524, lr:2.7876e-04 dt: 3332.17ms, tok/sec:157341.12
step 8872, loss: 3.347563, norm:0.2428, lr:2.7870e-04 dt: 3332.54ms, tok/sec:157323.63
step 8873, loss: 3.257640, norm:0.2606, lr:2.7864e-04 dt: 3332.04ms, tok/sec:157347.56
step 8874, loss: 3.276866, norm:0.2531, lr:2.7858e-04 dt: 3331.93ms, tok/sec:157352.42
step 8875, loss: 3.270338, norm:0.2635, lr:2.7853e-04 dt: 3331.88ms, tok/sec:157354.87
step 8876, loss: 3.340394, norm:0.2727, lr:2.7847e-04 dt: 3332.17ms, tok/sec:157341.48
step 8877, loss: 3.213368, norm:0.2531, lr:2.7841e-04 dt: 3332.28ms, tok/sec:157335.89
step 8878, loss: 3.313161, norm:0.2828, lr:2.7836e-04 dt: 3332.04ms, tok/sec:157347.42
step 8879, loss: 3.289249, norm:0.2640, lr:2.7830e-04 dt: 3332.09ms, tok/sec:157345.14
step 8880, loss: 3.368484, norm:0.2840, lr:2.7824e-04 dt: 3332.09ms, tok/sec:157345.13
step 8881, loss: 3.375568, norm:0.2557, lr:2.7818e-04 dt: 3332.58ms, tok/sec:157321.78
step 8882, loss: 3.362575, norm:0.3597, lr:2.7813e-04 dt: 3332.01ms, tok/sec:157348.81
step 8883, loss: 3.332120, norm:0.2910, lr:2.7807e-04 dt: 3332.25ms, tok/sec:157337.63
step 8884, loss: 3.373931, norm:0.2914, lr:2.7801e-04 dt: 3332.11ms, tok/sec:157344.35
step 8885, loss: 3.325133, norm:0.2767, lr:2.7795e-04 dt: 3332.05ms, tok/sec:157347.05
step 8886, loss: 3.298545, norm:0.2730, lr:2.7790e-04 dt: 3332.17ms, tok/sec:157341.51
step 8887, loss: 3.260527, norm:0.3080, lr:2.7784e-04 dt: 3332.33ms, tok/sec:157333.53
step 8888, loss: 3.410112, norm:0.2761, lr:2.7778e-04 dt: 3332.44ms, tok/sec:157328.70
step 8889, loss: 3.352388, norm:0.2622, lr:2.7773e-04 dt: 3332.05ms, tok/sec:157347.12
step 8890, loss: 3.317045, norm:0.2615, lr:2.7767e-04 dt: 3331.73ms, tok/sec:157362.05
step 8891, loss: 3.313042, norm:0.2605, lr:2.7761e-04 dt: 3332.08ms, tok/sec:157345.44
step 8892, loss: 3.328151, norm:0.2632, lr:2.7755e-04 dt: 3332.12ms, tok/sec:157343.71
step 8893, loss: 3.374855, norm:0.2684, lr:2.7750e-04 dt: 3332.04ms, tok/sec:157347.64
step 8894, loss: 3.365859, norm:0.2659, lr:2.7744e-04 dt: 3332.02ms, tok/sec:157348.43
step 8895, loss: 3.353030, norm:0.2611, lr:2.7738e-04 dt: 3332.10ms, tok/sec:157344.80
step 8896, loss: 3.345941, norm:0.2507, lr:2.7733e-04 dt: 3332.17ms, tok/sec:157341.31
step 8897, loss: 3.247406, norm:0.2499, lr:2.7727e-04 dt: 3332.37ms, tok/sec:157332.10
step 8898, loss: 3.330631, norm:0.2671, lr:2.7721e-04 dt: 3332.01ms, tok/sec:157348.72
step 8899, loss: 3.294046, norm:0.2630, lr:2.7715e-04 dt: 3332.10ms, tok/sec:157344.44
validation loss: 3.3046
Model and optimizer state saved.
HellaSwag accuracy:2325092919890086993/-2=-1162546459945043456.0000
rank 1 sample 0: Hello, I'm a language model, as we will see if we can call a variable the first two arguments of the variable name: '\', '\
rank 0 sample 0: Hello, I'm a language model, and I need to be able to find any meaningful object in my class without using the C++. So I'm workingrank 1 sample 1: Hello, I'm a language model, which I think does not have an adequate framework for understanding these concepts.
A Language That Meets Us
The first

rank 1 sample 2: Hello, I'm a language model, I had the best of it. I'm not sure about whether you should use it in the right way, as it
rank 0 sample 1: Hello, I'm a language model, I was pretty good at it, when I got myself there. And while that was my only problem that I know I
rank 1 sample 3: Hello, I'm a language model, and I'm looking at many of the possibilities.
|0||Literal modeling/Mathematical modeling|
rank 0 sample 2: Hello, I'm a language model, so I like this website (though, I haven't really made that out of it). I'm a language model.
rank 0 sample 3: Hello, I'm a language model, you get the idea.
The question of learning a language is really the first. A language is only one of hundreds
step 8900, loss: 3.279009, norm:0.2591, lr:2.7710e-04 dt: 56275.29ms, tok/sec:9316.49
step 8901, loss: 3.338042, norm:0.2499, lr:2.7704e-04 dt: 3332.26ms, tok/sec:157336.89
step 8902, loss: 3.257432, norm:0.2377, lr:2.7698e-04 dt: 3332.14ms, tok/sec:157342.75
step 8903, loss: 3.294771, norm:0.2551, lr:2.7693e-04 dt: 3332.62ms, tok/sec:157320.05
step 8904, loss: 3.281324, norm:0.2524, lr:2.7687e-04 dt: 3331.98ms, tok/sec:157350.48
step 8905, loss: 3.344604, norm:0.2552, lr:2.7681e-04 dt: 3331.94ms, tok/sec:157351.98
step 8906, loss: 3.269637, norm:0.2471, lr:2.7675e-04 dt: 3332.00ms, tok/sec:157349.43
step 8907, loss: 3.283973, norm:0.2481, lr:2.7670e-04 dt: 3332.19ms, tok/sec:157340.30
step 8908, loss: 3.313387, norm:0.2543, lr:2.7664e-04 dt: 3332.29ms, tok/sec:157335.77
step 8909, loss: 3.276032, norm:0.2564, lr:2.7658e-04 dt: 3332.14ms, tok/sec:157342.62
step 8910, loss: 3.297828, norm:0.2453, lr:2.7653e-04 dt: 3332.60ms, tok/sec:157320.81
step 8911, loss: 3.293229, norm:0.2472, lr:2.7647e-04 dt: 3332.11ms, tok/sec:157344.24
step 8912, loss: 3.355373, norm:0.2593, lr:2.7641e-04 dt: 3332.06ms, tok/sec:157346.52
step 8913, loss: 3.360841, norm:0.3083, lr:2.7635e-04 dt: 3331.95ms, tok/sec:157351.64
step 8914, loss: 3.313293, norm:0.2675, lr:2.7630e-04 dt: 3332.07ms, tok/sec:157346.06
step 8915, loss: 3.262271, norm:0.2660, lr:2.7624e-04 dt: 3332.01ms, tok/sec:157348.97
step 8916, loss: 3.255853, norm:0.2710, lr:2.7618e-04 dt: 3332.31ms, tok/sec:157334.77
step 8917, loss: 3.374024, norm:0.2624, lr:2.7612e-04 dt: 3332.54ms, tok/sec:157323.82
step 8918, loss: 3.296027, norm:0.2866, lr:2.7607e-04 dt: 3332.09ms, tok/sec:157344.89
step 8919, loss: 3.312190, norm:0.2975, lr:2.7601e-04 dt: 3332.12ms, tok/sec:157343.69
step 8920, loss: 3.409495, norm:0.2825, lr:2.7595e-04 dt: 3331.97ms, tok/sec:157350.66
step 8921, loss: 3.376276, norm:0.2933, lr:2.7590e-04 dt: 3332.00ms, tok/sec:157349.29
step 8922, loss: 3.320055, norm:0.2492, lr:2.7584e-04 dt: 3332.20ms, tok/sec:157339.97
step 8923, loss: 3.274139, norm:0.2694, lr:2.7578e-04 dt: 3332.38ms, tok/sec:157331.51
step 8924, loss: 3.304857, norm:0.3550, lr:2.7572e-04 dt: 3332.09ms, tok/sec:157345.31
step 8925, loss: 3.444354, norm:0.2772, lr:2.7567e-04 dt: 3332.62ms, tok/sec:157320.12
step 8926, loss: 3.279128, norm:0.2651, lr:2.7561e-04 dt: 3332.99ms, tok/sec:157302.67
step 8927, loss: 3.329550, norm:0.2657, lr:2.7555e-04 dt: 3332.34ms, tok/sec:157333.24
step 8928, loss: 3.392990, norm:0.2859, lr:2.7550e-04 dt: 3332.07ms, tok/sec:157346.23
step 8929, loss: 3.348681, norm:0.2540, lr:2.7544e-04 dt: 3332.19ms, tok/sec:157340.30
step 8930, loss: 3.307360, norm:0.2453, lr:2.7538e-04 dt: 3332.07ms, tok/sec:157345.80
step 8931, loss: 3.284004, norm:0.2648, lr:2.7533e-04 dt: 3332.21ms, tok/sec:157339.58
step 8932, loss: 3.291232, norm:0.2439, lr:2.7527e-04 dt: 3332.28ms, tok/sec:157336.16
step 8933, loss: 3.282251, norm:0.2677, lr:2.7521e-04 dt: 3332.33ms, tok/sec:157333.96
step 8934, loss: 3.309955, norm:0.2774, lr:2.7515e-04 dt: 3332.16ms, tok/sec:157341.60
step 8935, loss: 3.271284, norm:0.2742, lr:2.7510e-04 dt: 3332.25ms, tok/sec:157337.46
step 8936, loss: 3.291904, norm:0.2624, lr:2.7504e-04 dt: 3332.17ms, tok/sec:157341.15
step 8937, loss: 3.339087, norm:0.2581, lr:2.7498e-04 dt: 3331.96ms, tok/sec:157351.26
step 8938, loss: 3.282397, norm:0.2457, lr:2.7493e-04 dt: 3332.16ms, tok/sec:157341.72
step 8939, loss: 3.307594, norm:0.2609, lr:2.7487e-04 dt: 3332.36ms, tok/sec:157332.29
step 8940, loss: 3.274348, norm:0.2829, lr:2.7481e-04 dt: 3331.99ms, tok/sec:157349.62
step 8941, loss: 3.252541, norm:0.2503, lr:2.7475e-04 dt: 3332.42ms, tok/sec:157329.53
step 8942, loss: 3.299016, norm:0.2546, lr:2.7470e-04 dt: 3332.15ms, tok/sec:157342.41
step 8943, loss: 3.309588, norm:0.2504, lr:2.7464e-04 dt: 3332.26ms, tok/sec:157337.18
step 8944, loss: 3.297302, norm:0.2522, lr:2.7458e-04 dt: 3332.27ms, tok/sec:157336.47
step 8945, loss: 3.348007, norm:0.2475, lr:2.7453e-04 dt: 3332.20ms, tok/sec:157339.74
step 8946, loss: 3.311775, norm:0.2473, lr:2.7447e-04 dt: 3332.03ms, tok/sec:157347.89
step 8947, loss: 3.280633, norm:0.2810, lr:2.7441e-04 dt: 3332.38ms, tok/sec:157331.29
step 8948, loss: 3.350552, norm:0.2682, lr:2.7435e-04 dt: 3332.28ms, tok/sec:157336.30
step 8949, loss: 3.297460, norm:0.2671, lr:2.7430e-04 dt: 3332.12ms, tok/sec:157343.65
HellaSwag accuracy:2325216065192395857/-2=-1162608032596197888.0000
rank 1 sample 0: Hello, I'm a language model, with some parameters, a function, this code was not even very useful. (I can't see how that works,
rank 1 sample 1: Hello, I'm a language model, which I think can be used with an increasing number of programs. Now, you know an interpreter on a computer, and
rank 1 sample 2: Hello, I'm a language model, I haven't got a lot of time to do that and just have the browser to do the math to it for you
rank 1 sample 3: Hello, I'm a language model, and I'm looking at ways to solve specific problems. Of course, my friends are teaching me about the structure of languages
rank 0 sample 0: Hello, I'm a language model, and I was really surprised! But now...how does the language structure represent what you do? Well I'll tell you
rank 0 sample 1: Hello, I'm a language model, so there's a lot of language evolution in this one... I mean, it's the thing you don't want to
rank 0 sample 2: Hello, I'm a language model, but I believe I're using a much more technical language.<|endoftext|>|Name: _________________________||Period: 
rank 0 sample 3: Hello, I'm a language model, and want to do something, and to make money. I'm the language that i can run on. I have found
step 8950, loss: 3.292401, norm:0.2670, lr:2.7424e-04 dt: 48520.21ms, tok/sec:10805.56
step 8951, loss: 3.322929, norm:0.2652, lr:2.7418e-04 dt: 3332.14ms, tok/sec:157342.65
step 8952, loss: 3.305384, norm:0.2618, lr:2.7413e-04 dt: 3332.30ms, tok/sec:157335.18
step 8953, loss: 3.296845, norm:0.2553, lr:2.7407e-04 dt: 3334.50ms, tok/sec:157231.57
step 8954, loss: 3.429869, norm:0.2615, lr:2.7401e-04 dt: 3332.25ms, tok/sec:157337.65
step 8955, loss: 3.333685, norm:0.2554, lr:2.7396e-04 dt: 3332.17ms, tok/sec:157341.28
step 8956, loss: 3.459485, norm:0.2715, lr:2.7390e-04 dt: 3332.23ms, tok/sec:157338.28
step 8957, loss: 3.349727, norm:0.2542, lr:2.7384e-04 dt: 3332.12ms, tok/sec:157343.74
step 8958, loss: 3.312151, norm:0.2560, lr:2.7378e-04 dt: 3332.08ms, tok/sec:157345.41
step 8959, loss: 3.287668, norm:0.2981, lr:2.7373e-04 dt: 3332.06ms, tok/sec:157346.56
step 8960, loss: 3.352558, norm:0.2690, lr:2.7367e-04 dt: 3331.92ms, tok/sec:157353.34
step 8961, loss: 3.317203, norm:0.3286, lr:2.7361e-04 dt: 3331.94ms, tok/sec:157352.40
step 8962, loss: 3.345735, norm:0.2678, lr:2.7356e-04 dt: 3332.03ms, tok/sec:157347.94
step 8963, loss: 3.366756, norm:0.2620, lr:2.7350e-04 dt: 3332.10ms, tok/sec:157344.62
step 8964, loss: 3.353341, norm:0.2733, lr:2.7344e-04 dt: 3332.22ms, tok/sec:157339.06
step 8965, loss: 3.316279, norm:0.2672, lr:2.7338e-04 dt: 3332.00ms, tok/sec:157349.31
step 8966, loss: 3.299784, norm:0.2827, lr:2.7333e-04 dt: 3332.32ms, tok/sec:157334.07
step 8967, loss: 3.252280, norm:0.2682, lr:2.7327e-04 dt: 3332.15ms, tok/sec:157342.47
step 8968, loss: 3.271609, norm:0.2624, lr:2.7321e-04 dt: 3332.39ms, tok/sec:157331.12
step 8969, loss: 3.257804, norm:0.2698, lr:2.7316e-04 dt: 3332.37ms, tok/sec:157331.78
step 8970, loss: 3.308219, norm:0.2492, lr:2.7310e-04 dt: 3332.25ms, tok/sec:157337.41
step 8971, loss: 3.285862, norm:0.2800, lr:2.7304e-04 dt: 3331.87ms, tok/sec:157355.42
step 8972, loss: 3.340709, norm:0.3039, lr:2.7299e-04 dt: 3332.17ms, tok/sec:157341.22
step 8973, loss: 3.303868, norm:0.2801, lr:2.7293e-04 dt: 3332.00ms, tok/sec:157349.23
step 8974, loss: 3.318555, norm:0.2711, lr:2.7287e-04 dt: 3332.09ms, tok/sec:157344.97
step 8975, loss: 3.266119, norm:0.2434, lr:2.7281e-04 dt: 3332.19ms, tok/sec:157340.15
step 8976, loss: 3.349770, norm:0.2557, lr:2.7276e-04 dt: 3332.06ms, tok/sec:157346.41
step 8977, loss: 3.285743, norm:0.2464, lr:2.7270e-04 dt: 3332.69ms, tok/sec:157316.81
step 8978, loss: 3.242075, norm:0.3061, lr:2.7264e-04 dt: 3331.93ms, tok/sec:157352.48
step 8979, loss: 3.281531, norm:0.2840, lr:2.7259e-04 dt: 3331.97ms, tok/sec:157350.61
step 8980, loss: 3.302388, norm:0.2935, lr:2.7253e-04 dt: 3332.10ms, tok/sec:157344.59
step 8981, loss: 3.253541, norm:0.2800, lr:2.7247e-04 dt: 3332.15ms, tok/sec:157342.17
step 8982, loss: 3.279848, norm:0.2594, lr:2.7242e-04 dt: 3332.18ms, tok/sec:157340.92
step 8983, loss: 3.293755, norm:0.2655, lr:2.7236e-04 dt: 3332.06ms, tok/sec:157346.66
step 8984, loss: 3.322616, norm:0.2907, lr:2.7230e-04 dt: 3332.25ms, tok/sec:157337.36
step 8985, loss: 3.344721, norm:0.2701, lr:2.7224e-04 dt: 3332.31ms, tok/sec:157334.78
step 8986, loss: 3.338055, norm:0.2755, lr:2.7219e-04 dt: 3332.24ms, tok/sec:157338.11
step 8987, loss: 3.342490, norm:0.2730, lr:2.7213e-04 dt: 3332.20ms, tok/sec:157339.90
step 8988, loss: 3.261486, norm:0.2606, lr:2.7207e-04 dt: 3331.96ms, tok/sec:157351.12
step 8989, loss: 3.317872, norm:0.2641, lr:2.7202e-04 dt: 3332.05ms, tok/sec:157346.81
step 8990, loss: 3.371437, norm:0.2510, lr:2.7196e-04 dt: 3332.40ms, tok/sec:157330.39
step 8991, loss: 3.335232, norm:0.2720, lr:2.7190e-04 dt: 3332.08ms, tok/sec:157345.47
step 8992, loss: 3.285397, norm:0.2355, lr:2.7185e-04 dt: 3332.10ms, tok/sec:157344.82
step 8993, loss: 3.338000, norm:0.2415, lr:2.7179e-04 dt: 3332.13ms, tok/sec:157343.26
step 8994, loss: 3.262276, norm:0.2602, lr:2.7173e-04 dt: 3332.38ms, tok/sec:157331.26
step 8995, loss: 3.340154, norm:0.2641, lr:2.7168e-04 dt: 3332.25ms, tok/sec:157337.43
step 8996, loss: 3.318553, norm:0.2577, lr:2.7162e-04 dt: 3332.18ms, tok/sec:157340.79
step 8997, loss: 3.330410, norm:0.2605, lr:2.7156e-04 dt: 3332.07ms, tok/sec:157345.86
step 8998, loss: 3.261142, norm:0.2938, lr:2.7150e-04 dt: 3332.07ms, tok/sec:157346.25
step 8999, loss: 3.285403, norm:0.2683, lr:2.7145e-04 dt: 3332.18ms, tok/sec:157340.82
validation loss: 3.3005
Model and optimizer state saved.
HellaSwag accuracy:-6898155971661855663/-2=3449077985830927872.0000
rank 1 sample 0: Hello, I'm a language model, no. 12. This is a wonderful way to introduce concepts:
- Learn about your country: How do you know
rank 1 sample 1: Hello, I'm a language model, a teacher who speaks the language of teaching people to talk more. They speak their language; they explain the meaning of the
rank 1 sample 2: Hello, I'm a language model, but only the people who are interested in learning it. The goal here is to help you to make a language better.
rank 1 sample 3: Hello, I'm a language model, and I'm looking at one of the worlds most influential social studies teacher in today's US at home. I want to
rank 0 sample 0: Hello, I'm a language model, and I'll be writing it down." To see why he was interested in language, "Oh no. That's why
rank 0 sample 1: Hello, I'm a language model, but for me, the main focus is the idea – this is not always how we use a word bank to achieve that
rank 0 sample 2: Hello, I'm a language model, so I won't explain you stuff that's on the Internet but is really useful for that.
I'm going to
rank 0 sample 3: Hello, I'm a language model, you will need to know how you are using SQL.
2. Create the cursor with the .w_invas
step 9000, loss: 3.321123, norm:0.2445, lr:2.7139e-04 dt: 56210.47ms, tok/sec:9327.23
step 9001, loss: 3.292240, norm:0.2569, lr:2.7133e-04 dt: 3332.16ms, tok/sec:157341.58
step 9002, loss: 3.342559, norm:0.2553, lr:2.7128e-04 dt: 3332.15ms, tok/sec:157342.37
step 9003, loss: 3.285955, norm:0.2639, lr:2.7122e-04 dt: 3332.07ms, tok/sec:157346.09
step 9004, loss: 3.256928, norm:0.2775, lr:2.7116e-04 dt: 3332.05ms, tok/sec:157346.85
step 9005, loss: 3.214357, norm:0.2351, lr:2.7111e-04 dt: 3332.00ms, tok/sec:157349.17
step 9006, loss: 3.270873, norm:0.2603, lr:2.7105e-04 dt: 3331.99ms, tok/sec:157349.80
step 9007, loss: 3.344087, norm:0.2599, lr:2.7099e-04 dt: 3332.04ms, tok/sec:157347.28
step 9008, loss: 3.283998, norm:0.2720, lr:2.7094e-04 dt: 3332.18ms, tok/sec:157341.02
step 9009, loss: 3.248162, norm:0.2462, lr:2.7088e-04 dt: 3332.05ms, tok/sec:157346.84
step 9010, loss: 3.355227, norm:0.2546, lr:2.7082e-04 dt: 3332.34ms, tok/sec:157333.21
step 9011, loss: 3.287362, norm:0.2511, lr:2.7076e-04 dt: 3331.98ms, tok/sec:157350.21
step 9012, loss: 3.346591, norm:0.2412, lr:2.7071e-04 dt: 3331.87ms, tok/sec:157355.55
step 9013, loss: 3.328682, norm:0.2653, lr:2.7065e-04 dt: 3332.29ms, tok/sec:157335.78
step 9014, loss: 3.332799, norm:0.2818, lr:2.7059e-04 dt: 3332.10ms, tok/sec:157344.81
step 9015, loss: 3.306259, norm:0.2878, lr:2.7054e-04 dt: 3331.89ms, tok/sec:157354.71
step 9016, loss: 3.330779, norm:0.2714, lr:2.7048e-04 dt: 3332.21ms, tok/sec:157339.43
step 9017, loss: 3.381279, norm:0.2940, lr:2.7042e-04 dt: 3332.20ms, tok/sec:157339.86
step 9018, loss: 3.326900, norm:0.2650, lr:2.7037e-04 dt: 3331.96ms, tok/sec:157351.08
step 9019, loss: 3.282532, norm:0.2525, lr:2.7031e-04 dt: 3332.48ms, tok/sec:157326.69
step 9020, loss: 3.304782, norm:0.2521, lr:2.7025e-04 dt: 3332.09ms, tok/sec:157345.23
step 9021, loss: 3.304145, norm:0.2508, lr:2.7020e-04 dt: 3332.14ms, tok/sec:157342.57
step 9022, loss: 3.381802, norm:0.2730, lr:2.7014e-04 dt: 3331.81ms, tok/sec:157358.35
step 9023, loss: 3.333848, norm:0.2889, lr:2.7008e-04 dt: 3332.22ms, tok/sec:157339.15
step 9024, loss: 3.352456, norm:0.2981, lr:2.7002e-04 dt: 3332.12ms, tok/sec:157343.47
step 9025, loss: 3.325002, norm:0.2727, lr:2.6997e-04 dt: 3332.00ms, tok/sec:157349.24
step 9026, loss: 3.249731, norm:0.2806, lr:2.6991e-04 dt: 3332.14ms, tok/sec:157342.60
step 9027, loss: 3.270759, norm:0.2466, lr:2.6985e-04 dt: 3332.09ms, tok/sec:157345.27
step 9028, loss: 3.321211, norm:0.2600, lr:2.6980e-04 dt: 3332.57ms, tok/sec:157322.55
step 9029, loss: 3.327480, norm:0.2480, lr:2.6974e-04 dt: 3332.11ms, tok/sec:157344.04
step 9030, loss: 3.355428, norm:0.2501, lr:2.6968e-04 dt: 3332.04ms, tok/sec:157347.51
step 9031, loss: 3.294304, norm:0.2677, lr:2.6963e-04 dt: 3332.02ms, tok/sec:157348.43
step 9032, loss: 3.298616, norm:0.2553, lr:2.6957e-04 dt: 3332.19ms, tok/sec:157340.41
step 9033, loss: 3.319352, norm:0.2415, lr:2.6951e-04 dt: 3332.25ms, tok/sec:157337.43
step 9034, loss: 3.287420, norm:0.2514, lr:2.6946e-04 dt: 3332.14ms, tok/sec:157342.87
step 9035, loss: 3.236654, norm:0.2815, lr:2.6940e-04 dt: 3332.23ms, tok/sec:157338.51
step 9036, loss: 3.276569, norm:0.2619, lr:2.6934e-04 dt: 3332.21ms, tok/sec:157339.57
step 9037, loss: 3.319507, norm:0.2521, lr:2.6929e-04 dt: 3332.27ms, tok/sec:157336.47
step 9038, loss: 3.257536, norm:0.2601, lr:2.6923e-04 dt: 3332.06ms, tok/sec:157346.64
step 9039, loss: 3.255430, norm:0.2481, lr:2.6917e-04 dt: 3331.81ms, tok/sec:157358.31
step 9040, loss: 3.274051, norm:0.2779, lr:2.6912e-04 dt: 3332.11ms, tok/sec:157344.08
step 9041, loss: 3.296480, norm:0.2483, lr:2.6906e-04 dt: 3332.27ms, tok/sec:157336.71
step 9042, loss: 3.272710, norm:0.2727, lr:2.6900e-04 dt: 3331.94ms, tok/sec:157351.96
step 9043, loss: 3.258124, norm:0.2605, lr:2.6895e-04 dt: 3332.14ms, tok/sec:157342.53
step 9044, loss: 3.297667, norm:0.2428, lr:2.6889e-04 dt: 3332.21ms, tok/sec:157339.53
step 9045, loss: 3.282233, norm:0.2526, lr:2.6883e-04 dt: 3332.67ms, tok/sec:157317.88
step 9046, loss: 3.309489, norm:0.2678, lr:2.6877e-04 dt: 3332.02ms, tok/sec:157348.35
step 9047, loss: 3.270680, norm:0.2740, lr:2.6872e-04 dt: 3332.20ms, tok/sec:157339.84
step 9048, loss: 3.309954, norm:0.2711, lr:2.6866e-04 dt: 3332.06ms, tok/sec:157346.47
step 9049, loss: 3.297141, norm:0.2692, lr:2.6860e-04 dt: 3332.10ms, tok/sec:157344.49
HellaSwag accuracy:-2286610690723314607/-2=1143305345361657344.0000
rank 1 sample 0: Hello, I'm a language model, just one:
And, when's that enough, my wife is a software engineer and we work together as a team
rank 1 sample 1: Hello, I'm a language model, a tool for managing a language that doesn't have a strong language foundation. And although if I thought about it, I
rank 1 sample 2: Hello, I'm a language model, I already learned that language. I'm not sure if you think language was different from the one that was taught there,
rank 1 sample 3: Hello, I'm a language model, and I'm writing this because I need
to teach what I just learnt when I teach how to use the language.
rank 0 sample 0: Hello, I'm a language model, and I love it, right?”
He tried to stop thinking he was just starting, “I know
rank 0 sample 1: Hello, I'm a language model, so when I started my career, something was different I found my passion and the desire. My first name was Mr.
rank 0 sample 2: Hello, I'm a language model, so I’m an ambassador to the field of language models for a lot of people.
I’ve
rank 0 sample 3: Hello, I'm a language model, not in the same way as a book or piece of paper. That's what language models like the classic case, however
step 9050, loss: 3.340212, norm:0.2678, lr:2.6855e-04 dt: 48521.69ms, tok/sec:10805.23
step 9051, loss: 3.336784, norm:0.2972, lr:2.6849e-04 dt: 3332.18ms, tok/sec:157340.82
step 9052, loss: 3.284141, norm:0.2566, lr:2.6843e-04 dt: 3332.58ms, tok/sec:157321.79
step 9053, loss: 3.315887, norm:0.2512, lr:2.6838e-04 dt: 3332.00ms, tok/sec:157349.14
step 9054, loss: 3.321074, norm:0.2716, lr:2.6832e-04 dt: 3332.03ms, tok/sec:157347.75
step 9055, loss: 3.403706, norm:0.3083, lr:2.6826e-04 dt: 3332.01ms, tok/sec:157348.82
step 9056, loss: 3.287333, norm:0.2694, lr:2.6821e-04 dt: 3332.30ms, tok/sec:157335.14
step 9057, loss: 3.358891, norm:0.2726, lr:2.6815e-04 dt: 3332.08ms, tok/sec:157345.78
step 9058, loss: 3.343460, norm:0.2609, lr:2.6809e-04 dt: 3332.11ms, tok/sec:157344.22
step 9059, loss: 3.273752, norm:0.2853, lr:2.6804e-04 dt: 3332.41ms, tok/sec:157330.21
step 9060, loss: 3.382637, norm:0.2932, lr:2.6798e-04 dt: 3332.26ms, tok/sec:157337.27
step 9061, loss: 3.352988, norm:0.2602, lr:2.6792e-04 dt: 3331.97ms, tok/sec:157350.96
step 9062, loss: 3.320378, norm:0.3089, lr:2.6787e-04 dt: 3332.16ms, tok/sec:157341.58
step 9063, loss: 3.331326, norm:0.2597, lr:2.6781e-04 dt: 3331.96ms, tok/sec:157351.08
step 9064, loss: 3.321037, norm:0.2786, lr:2.6775e-04 dt: 3332.06ms, tok/sec:157346.50
step 9065, loss: 3.256177, norm:0.2612, lr:2.6770e-04 dt: 3332.15ms, tok/sec:157342.03
step 9066, loss: 3.281916, norm:0.2583, lr:2.6764e-04 dt: 3332.01ms, tok/sec:157348.84
step 9067, loss: 3.329868, norm:0.2749, lr:2.6758e-04 dt: 3331.96ms, tok/sec:157351.46
step 9068, loss: 3.348284, norm:0.2637, lr:2.6753e-04 dt: 3332.45ms, tok/sec:157328.06
step 9069, loss: 3.312200, norm:0.2646, lr:2.6747e-04 dt: 3332.31ms, tok/sec:157334.52
step 9070, loss: 3.295914, norm:0.2631, lr:2.6741e-04 dt: 3332.05ms, tok/sec:157347.17
step 9071, loss: 3.311835, norm:0.2603, lr:2.6736e-04 dt: 3332.26ms, tok/sec:157337.09
step 9072, loss: 3.272037, norm:0.2676, lr:2.6730e-04 dt: 3332.21ms, tok/sec:157339.26
step 9073, loss: 3.272533, norm:0.2677, lr:2.6724e-04 dt: 3331.97ms, tok/sec:157350.67
step 9074, loss: 3.390728, norm:0.2864, lr:2.6719e-04 dt: 3332.22ms, tok/sec:157339.06
step 9075, loss: 3.371496, norm:0.2742, lr:2.6713e-04 dt: 3332.20ms, tok/sec:157339.92
step 9076, loss: 3.397663, norm:0.2855, lr:2.6707e-04 dt: 3332.11ms, tok/sec:157343.92
step 9077, loss: 3.239650, norm:0.2683, lr:2.6702e-04 dt: 3332.15ms, tok/sec:157342.34
step 9078, loss: 3.258593, norm:0.2797, lr:2.6696e-04 dt: 3332.49ms, tok/sec:157326.31
step 9079, loss: 3.302882, norm:0.2548, lr:2.6690e-04 dt: 3332.15ms, tok/sec:157342.08
step 9080, loss: 3.262321, norm:0.2532, lr:2.6684e-04 dt: 3332.12ms, tok/sec:157343.69
step 9081, loss: 3.265159, norm:0.2726, lr:2.6679e-04 dt: 3331.94ms, tok/sec:157352.21
step 9082, loss: 3.267739, norm:0.2541, lr:2.6673e-04 dt: 3332.10ms, tok/sec:157344.42
step 9083, loss: 3.338596, norm:0.2556, lr:2.6667e-04 dt: 3331.99ms, tok/sec:157349.82
step 9084, loss: 3.315946, norm:0.2811, lr:2.6662e-04 dt: 3332.11ms, tok/sec:157344.07
step 9085, loss: 3.272660, norm:0.2869, lr:2.6656e-04 dt: 3332.18ms, tok/sec:157340.97
step 9086, loss: 3.306097, norm:0.2732, lr:2.6650e-04 dt: 3332.03ms, tok/sec:157347.86
step 9087, loss: 3.346137, norm:0.2606, lr:2.6645e-04 dt: 3332.58ms, tok/sec:157322.11
step 9088, loss: 3.364228, norm:0.3109, lr:2.6639e-04 dt: 3332.38ms, tok/sec:157331.46
step 9089, loss: 3.362647, norm:0.2765, lr:2.6633e-04 dt: 3332.09ms, tok/sec:157345.11
step 9090, loss: 3.266952, norm:0.2970, lr:2.6628e-04 dt: 3332.14ms, tok/sec:157342.74
step 9091, loss: 3.302841, norm:0.2706, lr:2.6622e-04 dt: 3332.15ms, tok/sec:157342.09
step 9092, loss: 3.326783, norm:0.2839, lr:2.6616e-04 dt: 3332.10ms, tok/sec:157344.51
step 9093, loss: 3.321193, norm:0.2733, lr:2.6611e-04 dt: 3332.04ms, tok/sec:157347.64
step 9094, loss: 3.285714, norm:0.2709, lr:2.6605e-04 dt: 3332.48ms, tok/sec:157326.80
step 9095, loss: 3.423403, norm:0.2971, lr:2.6599e-04 dt: 3331.90ms, tok/sec:157354.09
step 9096, loss: 3.316722, norm:0.2696, lr:2.6594e-04 dt: 3332.66ms, tok/sec:157318.37
step 9097, loss: 3.291901, norm:0.2689, lr:2.6588e-04 dt: 3331.92ms, tok/sec:157353.18
step 9098, loss: 3.325993, norm:0.2587, lr:2.6582e-04 dt: 3332.00ms, tok/sec:157349.54
step 9099, loss: 3.373690, norm:0.2562, lr:2.6577e-04 dt: 3332.19ms, tok/sec:157340.20
validation loss: 3.3002
Model and optimizer state saved.
HellaSwag accuracy:6936761346131723345/-2=-3468380673065861632.0000
rank 1 sample 0: Hello, I'm a language model,
so I'm looking at the bottom, because the whole argument is not equal to 1:4.
I'm
rank 1 sample 1: Hello, I'm a language model, you're probably referring to a class which talks about itself when you refer to other languages without much attention.
A class
rank 0 sample 0: Hello, I'm a language model, and I need to learn all those new terminology of the language, or is my language.
- The basics of how
rank 1 sample 2: Hello, I'm a language model, but like most, I'm not. I'm not even thinking, but I'm not saying to the language modeling companies
rank 0 sample 1: Hello, I'm a language model, I was actually doing this stuff myself? I was being put in front of this, it was just one of the thingsrank 1 sample 3: Hello, I'm a language model, and I'm talking to those people who read the original textbook that will go. I’m not going to be

rank 0 sample 2: Hello, I'm a language model, so I wanted to define your approach so that everyone could understand my own language. I don't know how to define it
rank 0 sample 3: Hello, I'm a language model, and for the language model, it's not pretty. I didn't really think for some time, but I believe at
step 9100, loss: 3.330879, norm:0.2636, lr:2.6571e-04 dt: 56153.33ms, tok/sec:9336.72
step 9101, loss: 3.298526, norm:0.2528, lr:2.6565e-04 dt: 3332.24ms, tok/sec:157338.23
step 9102, loss: 3.296011, norm:0.2565, lr:2.6560e-04 dt: 3332.16ms, tok/sec:157342.00
step 9103, loss: 3.359229, norm:0.2790, lr:2.6554e-04 dt: 3332.45ms, tok/sec:157328.13
step 9104, loss: 3.276865, norm:0.2437, lr:2.6548e-04 dt: 3332.22ms, tok/sec:157339.15
step 9105, loss: 3.270656, norm:0.2667, lr:2.6543e-04 dt: 3331.97ms, tok/sec:157350.83
step 9106, loss: 3.366847, norm:0.3377, lr:2.6537e-04 dt: 3332.20ms, tok/sec:157339.89
step 9107, loss: 3.283434, norm:0.2444, lr:2.6532e-04 dt: 3332.44ms, tok/sec:157328.51
step 9108, loss: 3.284949, norm:0.2596, lr:2.6526e-04 dt: 3332.06ms, tok/sec:157346.31
step 9109, loss: 3.270250, norm:0.2468, lr:2.6520e-04 dt: 3332.17ms, tok/sec:157341.15
step 9110, loss: 3.254430, norm:0.2682, lr:2.6515e-04 dt: 3332.42ms, tok/sec:157329.55
step 9111, loss: 3.249061, norm:0.2655, lr:2.6509e-04 dt: 3332.41ms, tok/sec:157330.09
step 9112, loss: 3.301478, norm:0.2487, lr:2.6503e-04 dt: 3332.13ms, tok/sec:157343.15
step 9113, loss: 3.304158, norm:0.2367, lr:2.6498e-04 dt: 3331.87ms, tok/sec:157355.41
step 9114, loss: 3.303824, norm:0.2783, lr:2.6492e-04 dt: 3331.99ms, tok/sec:157349.63
step 9115, loss: 3.257304, norm:0.2563, lr:2.6486e-04 dt: 3332.13ms, tok/sec:157343.12
step 9116, loss: 3.314573, norm:0.2489, lr:2.6481e-04 dt: 3332.01ms, tok/sec:157348.90
step 9117, loss: 3.289990, norm:0.2549, lr:2.6475e-04 dt: 3332.13ms, tok/sec:157343.34
step 9118, loss: 3.335274, norm:0.2622, lr:2.6469e-04 dt: 3332.13ms, tok/sec:157343.43
step 9119, loss: 3.239160, norm:0.2503, lr:2.6464e-04 dt: 3332.64ms, tok/sec:157319.05
step 9120, loss: 3.305590, norm:0.2644, lr:2.6458e-04 dt: 3332.03ms, tok/sec:157347.80
step 9121, loss: 3.331602, norm:0.2645, lr:2.6452e-04 dt: 3332.08ms, tok/sec:157345.48
step 9122, loss: 3.410476, norm:0.2569, lr:2.6447e-04 dt: 3332.23ms, tok/sec:157338.27
step 9123, loss: 3.342728, norm:0.2621, lr:2.6441e-04 dt: 3332.34ms, tok/sec:157333.50
step 9124, loss: 3.277039, norm:0.2655, lr:2.6435e-04 dt: 3331.95ms, tok/sec:157351.62
step 9125, loss: 3.285138, norm:0.2593, lr:2.6430e-04 dt: 3332.23ms, tok/sec:157338.56
step 9126, loss: 3.350153, norm:0.2760, lr:2.6424e-04 dt: 3332.37ms, tok/sec:157332.06
step 9127, loss: 3.306858, norm:0.2737, lr:2.6418e-04 dt: 3332.07ms, tok/sec:157345.84
step 9128, loss: 3.289237, norm:0.2906, lr:2.6413e-04 dt: 3332.04ms, tok/sec:157347.35
step 9129, loss: 3.330956, norm:0.2558, lr:2.6407e-04 dt: 3332.06ms, tok/sec:157346.33
step 9130, loss: 3.377067, norm:0.3105, lr:2.6401e-04 dt: 3331.85ms, tok/sec:157356.30
step 9131, loss: 3.306191, norm:0.2728, lr:2.6396e-04 dt: 3332.30ms, tok/sec:157335.22
step 9132, loss: 3.288016, norm:0.2771, lr:2.6390e-04 dt: 3332.03ms, tok/sec:157348.12
step 9133, loss: 3.270523, norm:0.2685, lr:2.6384e-04 dt: 3332.44ms, tok/sec:157328.48
step 9134, loss: 3.294984, norm:0.2647, lr:2.6379e-04 dt: 3332.08ms, tok/sec:157345.33
step 9135, loss: 3.315609, norm:0.2873, lr:2.6373e-04 dt: 3332.08ms, tok/sec:157345.51
step 9136, loss: 3.306474, norm:0.2674, lr:2.6367e-04 dt: 3332.24ms, tok/sec:157338.13
step 9137, loss: 3.295492, norm:0.2898, lr:2.6362e-04 dt: 3332.23ms, tok/sec:157338.41
step 9138, loss: 3.325940, norm:0.2729, lr:2.6356e-04 dt: 3333.04ms, tok/sec:157300.35
step 9139, loss: 3.348561, norm:0.2842, lr:2.6350e-04 dt: 3332.44ms, tok/sec:157328.39
step 9140, loss: 3.298079, norm:0.2739, lr:2.6345e-04 dt: 3332.10ms, tok/sec:157344.71
step 9141, loss: 3.279254, norm:0.2658, lr:2.6339e-04 dt: 3332.40ms, tok/sec:157330.68
step 9142, loss: 3.351760, norm:0.2613, lr:2.6334e-04 dt: 3332.18ms, tok/sec:157340.64
step 9143, loss: 3.281330, norm:0.2674, lr:2.6328e-04 dt: 3334.40ms, tok/sec:157236.11
step 9144, loss: 3.329444, norm:0.2527, lr:2.6322e-04 dt: 3332.24ms, tok/sec:157338.05
step 9145, loss: 3.337353, norm:0.2715, lr:2.6317e-04 dt: 3332.06ms, tok/sec:157346.61
step 9146, loss: 3.281209, norm:0.2401, lr:2.6311e-04 dt: 3331.97ms, tok/sec:157350.53
step 9147, loss: 3.393234, norm:0.2637, lr:2.6305e-04 dt: 3332.11ms, tok/sec:157344.32
step 9148, loss: 3.325248, norm:0.2530, lr:2.6300e-04 dt: 3332.29ms, tok/sec:157335.72
step 9149, loss: 3.363188, norm:0.2730, lr:2.6294e-04 dt: 3332.04ms, tok/sec:157347.30
HellaSwag accuracy:-6898296709150210991/-2=3449148354575105536.0000
rank 0 sample 0: Hello, I'm a language model, and I need to know if we'll end up with a new programming language and a language that we'll all need for
rank 0 sample 1: Hello, I'm a language model, but when I use it, it would be the better representation. And language learning is the art and hobby of learning and
rank 0 sample 2: Hello, I'm a language model, so I will make any questions you need to understand it. It can be a word or a phrase.
The words
rank 1 sample 0: Hello, I'm a language model, for a basic understanding of language. Every single concept has a corresponding definition. To understand that idea properly, I need to
rank 0 sample 3: Hello, I'm a language model, but have you ever wondered what language you are.
I wish I could say as much with the concept of the system
rank 1 sample 1: Hello, I'm a language model, which means it's a language model which does not rely or any thing that has been a language model, but a language
rank 1 sample 2: Hello, I'm a language model, so while we're talking about the same thing, we're only talking about the same thing. It's a lot more
rank 1 sample 3: Hello, I'm a language model, and I'm just a Java developer. Even though I would spend an entire life learning your concepts, I'm only a
step 9150, loss: 3.307784, norm:0.2566, lr:2.6288e-04 dt: 48523.82ms, tok/sec:10804.75
step 9151, loss: 3.285898, norm:0.2533, lr:2.6283e-04 dt: 3331.90ms, tok/sec:157354.28
step 9152, loss: 3.283005, norm:0.2636, lr:2.6277e-04 dt: 3331.87ms, tok/sec:157355.31
step 9153, loss: 3.297017, norm:0.2521, lr:2.6271e-04 dt: 3332.20ms, tok/sec:157339.68
step 9154, loss: 3.329597, norm:0.2503, lr:2.6266e-04 dt: 3332.08ms, tok/sec:157345.76
step 9155, loss: 3.238374, norm:0.2763, lr:2.6260e-04 dt: 3332.11ms, tok/sec:157344.14
step 9156, loss: 3.254511, norm:0.2770, lr:2.6254e-04 dt: 3332.03ms, tok/sec:157347.86
step 9157, loss: 3.246105, norm:0.2819, lr:2.6249e-04 dt: 3332.15ms, tok/sec:157342.10
step 9158, loss: 3.189841, norm:0.2628, lr:2.6243e-04 dt: 3331.99ms, tok/sec:157349.61
step 9159, loss: 3.231508, norm:0.2520, lr:2.6237e-04 dt: 3332.07ms, tok/sec:157345.98
step 9160, loss: 3.235211, norm:0.3384, lr:2.6232e-04 dt: 3332.24ms, tok/sec:157337.95
step 9161, loss: 3.245082, norm:0.2603, lr:2.6226e-04 dt: 3332.17ms, tok/sec:157341.35
step 9162, loss: 3.364024, norm:0.2763, lr:2.6221e-04 dt: 3331.90ms, tok/sec:157354.08
step 9163, loss: 3.331501, norm:0.2836, lr:2.6215e-04 dt: 3331.81ms, tok/sec:157358.44
step 9164, loss: 3.268946, norm:0.2697, lr:2.6209e-04 dt: 3332.02ms, tok/sec:157348.42
step 9165, loss: 3.268449, norm:0.2789, lr:2.6204e-04 dt: 3332.07ms, tok/sec:157345.88
step 9166, loss: 3.268839, norm:0.2798, lr:2.6198e-04 dt: 3331.86ms, tok/sec:157356.06
step 9167, loss: 3.264538, norm:0.2928, lr:2.6192e-04 dt: 3331.84ms, tok/sec:157357.02
step 9168, loss: 3.305203, norm:0.2502, lr:2.6187e-04 dt: 3332.09ms, tok/sec:157345.17
step 9169, loss: 3.311533, norm:0.2899, lr:2.6181e-04 dt: 3332.31ms, tok/sec:157334.49
step 9170, loss: 3.312663, norm:0.2420, lr:2.6175e-04 dt: 3332.12ms, tok/sec:157343.77
step 9171, loss: 3.352984, norm:0.3303, lr:2.6170e-04 dt: 3331.85ms, tok/sec:157356.35
step 9172, loss: 3.290566, norm:0.2660, lr:2.6164e-04 dt: 3331.97ms, tok/sec:157350.86
step 9173, loss: 3.344140, norm:0.2877, lr:2.6158e-04 dt: 3331.95ms, tok/sec:157351.89
step 9174, loss: 3.346525, norm:0.2709, lr:2.6153e-04 dt: 3332.26ms, tok/sec:157337.28
step 9175, loss: 3.252635, norm:0.3220, lr:2.6147e-04 dt: 3332.04ms, tok/sec:157347.68
step 9176, loss: 3.299681, norm:0.3326, lr:2.6142e-04 dt: 3332.19ms, tok/sec:157340.22
step 9177, loss: 3.319835, norm:0.2874, lr:2.6136e-04 dt: 3332.02ms, tok/sec:157348.27
step 9178, loss: 3.307617, norm:0.2724, lr:2.6130e-04 dt: 3332.14ms, tok/sec:157342.60
step 9179, loss: 3.249063, norm:0.3102, lr:2.6125e-04 dt: 3331.83ms, tok/sec:157357.54
step 9180, loss: 3.242325, norm:0.2962, lr:2.6119e-04 dt: 3331.94ms, tok/sec:157351.97
step 9181, loss: 3.282954, norm:0.3024, lr:2.6113e-04 dt: 3332.17ms, tok/sec:157341.31
step 9182, loss: 3.311419, norm:0.2915, lr:2.6108e-04 dt: 3331.96ms, tok/sec:157351.23
step 9183, loss: 3.273081, norm:0.3044, lr:2.6102e-04 dt: 3332.07ms, tok/sec:157346.05
step 9184, loss: 3.260870, norm:0.2801, lr:2.6096e-04 dt: 3332.12ms, tok/sec:157343.52
step 9185, loss: 3.312232, norm:0.2827, lr:2.6091e-04 dt: 3332.38ms, tok/sec:157331.21
step 9186, loss: 3.300397, norm:0.2684, lr:2.6085e-04 dt: 3332.30ms, tok/sec:157335.11
step 9187, loss: 3.328093, norm:0.2707, lr:2.6080e-04 dt: 3332.22ms, tok/sec:157338.96
step 9188, loss: 3.304511, norm:0.2563, lr:2.6074e-04 dt: 3332.01ms, tok/sec:157348.72
step 9189, loss: 3.310259, norm:0.2669, lr:2.6068e-04 dt: 3331.98ms, tok/sec:157350.50
step 9190, loss: 3.274729, norm:0.2473, lr:2.6063e-04 dt: 3332.06ms, tok/sec:157346.59
step 9191, loss: 3.290571, norm:0.3854, lr:2.6057e-04 dt: 3332.06ms, tok/sec:157346.58
step 9192, loss: 3.306258, norm:0.2951, lr:2.6051e-04 dt: 3331.89ms, tok/sec:157354.52
step 9193, loss: 3.280646, norm:0.3036, lr:2.6046e-04 dt: 3332.51ms, tok/sec:157325.25
step 9194, loss: 3.237395, norm:0.2862, lr:2.6040e-04 dt: 3332.09ms, tok/sec:157345.21
step 9195, loss: 3.296034, norm:0.2864, lr:2.6034e-04 dt: 3331.94ms, tok/sec:157352.37
step 9196, loss: 3.243327, norm:0.2664, lr:2.6029e-04 dt: 3332.06ms, tok/sec:157346.41
step 9197, loss: 3.267761, norm:0.2693, lr:2.6023e-04 dt: 3332.19ms, tok/sec:157340.17
step 9198, loss: 3.234621, norm:0.2601, lr:2.6018e-04 dt: 3332.03ms, tok/sec:157347.97
step 9199, loss: 3.295517, norm:0.2717, lr:2.6012e-04 dt: 3332.21ms, tok/sec:157339.26
validation loss: 3.2953
Model and optimizer state saved.
HellaSwag accuracy:2325075327704335441/-2=-1162537663852167680.0000
rank 1 sample 0: Hello, I'm a language model, this is the first time I've posted an open language-level description of programming. The second of these is the first
rank 1 sample 1: Hello, I'm a language model, a model for designing a language and software framework for human users. I'm writing an A+ on a computer and I
rank 1 sample 2: Hello, I'm a language model, but could be the next step.
I'm getting ready by using this idea.
The following example is part of
rank 1 sample 3: Hello, I'm a language model, and I'm an engineer or engineer. Most of the jobs of many engineering are done after me, and I'm a
rank 0 sample 0: Hello, I'm a language model, and I'll be talking about how my programming might work. I would look into the programming language itself to try to get
rank 0 sample 1: Hello, I'm a language model, but for our purposes we're trying to make sure its easy for kids to learn. We're going to do a bit
rank 0 sample 2: Hello, I'm a language model, but I would be too far away from the English language. If it weren't for my English, I'd have been
rank 0 sample 3: Hello, I'm a language model, and one of the things I've used is pretty much the same. But it basically tells me how to sort things (
step 9200, loss: 3.303716, norm:0.2688, lr:2.6006e-04 dt: 56309.56ms, tok/sec:9310.82
step 9201, loss: 3.347787, norm:0.2813, lr:2.6001e-04 dt: 3332.20ms, tok/sec:157339.87
step 9202, loss: 3.254708, norm:0.2603, lr:2.5995e-04 dt: 3331.96ms, tok/sec:157351.15
step 9203, loss: 3.309537, norm:0.2733, lr:2.5989e-04 dt: 3332.15ms, tok/sec:157342.21
step 9204, loss: 3.302393, norm:0.2601, lr:2.5984e-04 dt: 3332.62ms, tok/sec:157320.00
step 9205, loss: 3.293028, norm:0.2760, lr:2.5978e-04 dt: 3332.11ms, tok/sec:157343.93
step 9206, loss: 3.285522, norm:0.2659, lr:2.5972e-04 dt: 3332.24ms, tok/sec:157338.15
step 9207, loss: 3.293059, norm:0.2798, lr:2.5967e-04 dt: 3332.09ms, tok/sec:157345.32
step 9208, loss: 3.291271, norm:0.2536, lr:2.5961e-04 dt: 3332.12ms, tok/sec:157343.78
step 9209, loss: 3.344576, norm:0.2632, lr:2.5956e-04 dt: 3332.15ms, tok/sec:157342.39
step 9210, loss: 3.304724, norm:0.2547, lr:2.5950e-04 dt: 3332.08ms, tok/sec:157345.67
step 9211, loss: 3.300728, norm:0.2593, lr:2.5944e-04 dt: 3332.26ms, tok/sec:157337.15
step 9212, loss: 3.376090, norm:0.2552, lr:2.5939e-04 dt: 3332.03ms, tok/sec:157348.15
step 9213, loss: 3.261183, norm:0.2914, lr:2.5933e-04 dt: 3332.52ms, tok/sec:157324.82
step 9214, loss: 3.342304, norm:0.2552, lr:2.5927e-04 dt: 3331.92ms, tok/sec:157353.08
step 9215, loss: 3.298521, norm:0.2744, lr:2.5922e-04 dt: 3332.03ms, tok/sec:157347.83
step 9216, loss: 3.277125, norm:0.2475, lr:2.5916e-04 dt: 3332.20ms, tok/sec:157340.10
step 9217, loss: 3.282565, norm:0.2628, lr:2.5911e-04 dt: 3332.14ms, tok/sec:157342.80
step 9218, loss: 3.471149, norm:0.3747, lr:2.5905e-04 dt: 3331.87ms, tok/sec:157355.55
step 9219, loss: 3.308329, norm:0.2755, lr:2.5899e-04 dt: 3332.20ms, tok/sec:157339.87
step 9220, loss: 3.277969, norm:0.2598, lr:2.5894e-04 dt: 3332.29ms, tok/sec:157335.63
step 9221, loss: 3.274376, norm:0.2585, lr:2.5888e-04 dt: 3332.30ms, tok/sec:157334.96
step 9222, loss: 3.281128, norm:0.2571, lr:2.5882e-04 dt: 3331.98ms, tok/sec:157350.15
step 9223, loss: 3.252577, norm:0.2574, lr:2.5877e-04 dt: 3332.03ms, tok/sec:157348.00
step 9224, loss: 3.250628, norm:0.2453, lr:2.5871e-04 dt: 3331.96ms, tok/sec:157351.10
step 9225, loss: 3.278350, norm:0.2548, lr:2.5866e-04 dt: 3332.24ms, tok/sec:157338.04
step 9226, loss: 3.302542, norm:0.2581, lr:2.5860e-04 dt: 3332.16ms, tok/sec:157341.74
step 9227, loss: 3.319430, norm:0.2731, lr:2.5854e-04 dt: 3332.36ms, tok/sec:157332.56
step 9228, loss: 3.262190, norm:0.2914, lr:2.5849e-04 dt: 3332.13ms, tok/sec:157343.39
step 9229, loss: 3.317212, norm:0.2711, lr:2.5843e-04 dt: 3332.43ms, tok/sec:157329.06
step 9230, loss: 3.231938, norm:0.2843, lr:2.5837e-04 dt: 3332.07ms, tok/sec:157345.97
step 9231, loss: 3.289181, norm:0.2603, lr:2.5832e-04 dt: 3332.18ms, tok/sec:157340.77
step 9232, loss: 3.262563, norm:0.2866, lr:2.5826e-04 dt: 3332.07ms, tok/sec:157345.94
step 9233, loss: 3.328926, norm:0.2589, lr:2.5821e-04 dt: 3332.26ms, tok/sec:157337.29
step 9234, loss: 3.299184, norm:0.2840, lr:2.5815e-04 dt: 3332.00ms, tok/sec:157349.50
step 9235, loss: 3.252753, norm:0.2717, lr:2.5809e-04 dt: 3332.22ms, tok/sec:157338.91
step 9236, loss: 3.314857, norm:0.2815, lr:2.5804e-04 dt: 3332.59ms, tok/sec:157321.67
step 9237, loss: 3.342098, norm:0.2682, lr:2.5798e-04 dt: 3332.04ms, tok/sec:157347.65
step 9238, loss: 3.301735, norm:0.2550, lr:2.5792e-04 dt: 3332.07ms, tok/sec:157346.25
step 9239, loss: 3.314291, norm:0.2776, lr:2.5787e-04 dt: 3332.02ms, tok/sec:157348.44
step 9240, loss: 3.273272, norm:0.2424, lr:2.5781e-04 dt: 3331.92ms, tok/sec:157353.30
step 9241, loss: 3.268867, norm:0.2600, lr:2.5776e-04 dt: 3332.32ms, tok/sec:157334.41
step 9242, loss: 3.292430, norm:0.2501, lr:2.5770e-04 dt: 3332.33ms, tok/sec:157333.59
step 9243, loss: 3.287827, norm:0.2457, lr:2.5764e-04 dt: 3332.08ms, tok/sec:157345.54
step 9244, loss: 3.281687, norm:0.2652, lr:2.5759e-04 dt: 3332.11ms, tok/sec:157344.10
step 9245, loss: 3.260828, norm:0.2450, lr:2.5753e-04 dt: 3332.64ms, tok/sec:157319.32
step 9246, loss: 3.306220, norm:0.2480, lr:2.5747e-04 dt: 3332.18ms, tok/sec:157340.96
step 9247, loss: 3.308578, norm:0.2639, lr:2.5742e-04 dt: 3332.28ms, tok/sec:157336.22
step 9248, loss: 3.391569, norm:0.2451, lr:2.5736e-04 dt: 3332.35ms, tok/sec:157332.60
step 9249, loss: 3.318052, norm:0.2605, lr:2.5731e-04 dt: 3332.35ms, tok/sec:157332.89
HellaSwag accuracy:-2286575506351225775/-2=1143287753175612928.0000
rank 1 sample 0: Hello, I'm a language model, as the way to look at it becomes more relevant in the new context.
I don't understand why this is so
rank 0 sample 0: Hello, I'm a language model, and I love to use your voice model every day.
What kind of environment do I need to learn and how does
rank 1 sample 1: Hello, I'm a language model, a model for languages. I am trying to understand this for the sake of argument. Can't ask, "What is
rank 1 sample 2: Hello, I'm a language model, so is that's what I'm trying to do. But now that we're going to look at what's behind that
rank 0 sample 1: Hello, I'm a language model, but i'm not really sure i found a good name/case definition there, i have never had many friends.

rank 1 sample 3: Hello, I'm a language model, and I'm using the grammar to create "language models". Let's talk that way on it.
So how do
rank 0 sample 2: Hello, I'm a language model, but I haven't thought.
One of the things I really enjoyed the most was that I had a lot of interactions
rank 0 sample 3: Hello, I'm a language model, and when I get to work on a project right now, I really don't like language."
There are two aspects
step 9250, loss: 3.271236, norm:0.3155, lr:2.5725e-04 dt: 48523.61ms, tok/sec:10804.80
step 9251, loss: 3.248584, norm:0.2528, lr:2.5719e-04 dt: 3332.15ms, tok/sec:157342.26
step 9252, loss: 3.274097, norm:0.2599, lr:2.5714e-04 dt: 3332.20ms, tok/sec:157339.79
step 9253, loss: 3.368594, norm:0.3098, lr:2.5708e-04 dt: 3332.18ms, tok/sec:157340.92
step 9254, loss: 3.289200, norm:0.2713, lr:2.5703e-04 dt: 3332.45ms, tok/sec:157328.05
step 9255, loss: 3.319020, norm:0.2617, lr:2.5697e-04 dt: 3332.20ms, tok/sec:157339.97
step 9256, loss: 3.298228, norm:0.2517, lr:2.5691e-04 dt: 3332.29ms, tok/sec:157335.75
step 9257, loss: 3.295627, norm:0.2859, lr:2.5686e-04 dt: 3332.07ms, tok/sec:157345.96
step 9258, loss: 3.198278, norm:0.2440, lr:2.5680e-04 dt: 3332.00ms, tok/sec:157349.33
step 9259, loss: 3.282526, norm:0.2804, lr:2.5674e-04 dt: 3332.11ms, tok/sec:157344.08
step 9260, loss: 3.242749, norm:0.2717, lr:2.5669e-04 dt: 3332.38ms, tok/sec:157331.41
step 9261, loss: 3.271844, norm:0.2567, lr:2.5663e-04 dt: 3332.21ms, tok/sec:157339.38
step 9262, loss: 3.264822, norm:0.2494, lr:2.5658e-04 dt: 3332.59ms, tok/sec:157321.28
step 9263, loss: 3.303462, norm:0.2978, lr:2.5652e-04 dt: 3332.24ms, tok/sec:157337.81
step 9264, loss: 3.289886, norm:0.2590, lr:2.5646e-04 dt: 3332.21ms, tok/sec:157339.40
step 9265, loss: 3.213173, norm:0.2665, lr:2.5641e-04 dt: 3332.12ms, tok/sec:157343.79
step 9266, loss: 3.308686, norm:0.2719, lr:2.5635e-04 dt: 3332.08ms, tok/sec:157345.57
step 9267, loss: 3.308516, norm:0.2705, lr:2.5630e-04 dt: 3332.02ms, tok/sec:157348.53
step 9268, loss: 3.296496, norm:0.2545, lr:2.5624e-04 dt: 3332.30ms, tok/sec:157335.38
step 9269, loss: 3.295449, norm:0.2662, lr:2.5618e-04 dt: 3332.49ms, tok/sec:157326.10
step 9270, loss: 3.376285, norm:0.2840, lr:2.5613e-04 dt: 3332.08ms, tok/sec:157345.65
step 9271, loss: 3.265650, norm:0.2761, lr:2.5607e-04 dt: 3332.15ms, tok/sec:157342.22
step 9272, loss: 3.310216, norm:0.2664, lr:2.5602e-04 dt: 3332.25ms, tok/sec:157337.30
step 9273, loss: 3.273648, norm:0.2835, lr:2.5596e-04 dt: 3332.11ms, tok/sec:157344.19
step 9274, loss: 3.278018, norm:0.2815, lr:2.5590e-04 dt: 3331.89ms, tok/sec:157354.66
step 9275, loss: 3.289026, norm:0.2941, lr:2.5585e-04 dt: 3332.13ms, tok/sec:157343.25
step 9276, loss: 3.327598, norm:0.2473, lr:2.5579e-04 dt: 3332.36ms, tok/sec:157332.47
step 9277, loss: 3.308579, norm:0.2764, lr:2.5573e-04 dt: 3331.88ms, tok/sec:157354.87
step 9278, loss: 3.266167, norm:0.2454, lr:2.5568e-04 dt: 3332.58ms, tok/sec:157322.09
step 9279, loss: 3.244074, norm:0.2568, lr:2.5562e-04 dt: 3332.16ms, tok/sec:157341.65
step 9280, loss: 3.280316, norm:0.2593, lr:2.5557e-04 dt: 3332.14ms, tok/sec:157342.52
step 9281, loss: 3.282513, norm:0.2444, lr:2.5551e-04 dt: 3332.08ms, tok/sec:157345.76
step 9282, loss: 3.313272, norm:0.2511, lr:2.5545e-04 dt: 3332.07ms, tok/sec:157346.13
step 9283, loss: 3.279824, norm:0.2529, lr:2.5540e-04 dt: 3331.90ms, tok/sec:157354.07
step 9284, loss: 3.317094, norm:0.2426, lr:2.5534e-04 dt: 3332.20ms, tok/sec:157339.97
step 9285, loss: 3.289679, norm:0.2638, lr:2.5529e-04 dt: 3332.33ms, tok/sec:157333.92
step 9286, loss: 3.292734, norm:0.2456, lr:2.5523e-04 dt: 3332.02ms, tok/sec:157348.27
step 9287, loss: 3.279766, norm:0.2835, lr:2.5517e-04 dt: 3332.58ms, tok/sec:157321.95
step 9288, loss: 3.298262, norm:0.2641, lr:2.5512e-04 dt: 3332.22ms, tok/sec:157338.89
step 9289, loss: 3.266855, norm:0.2691, lr:2.5506e-04 dt: 3332.10ms, tok/sec:157344.78
step 9290, loss: 3.281748, norm:0.2707, lr:2.5501e-04 dt: 3332.19ms, tok/sec:157340.47
step 9291, loss: 3.269792, norm:0.2442, lr:2.5495e-04 dt: 3332.13ms, tok/sec:157343.09
step 9292, loss: 3.274694, norm:0.2585, lr:2.5489e-04 dt: 3332.09ms, tok/sec:157345.30
step 9293, loss: 3.246677, norm:0.2531, lr:2.5484e-04 dt: 3331.98ms, tok/sec:157350.10
step 9294, loss: 3.315342, norm:0.2614, lr:2.5478e-04 dt: 3332.35ms, tok/sec:157332.59
step 9295, loss: 3.238393, norm:0.2814, lr:2.5473e-04 dt: 3331.95ms, tok/sec:157351.49
step 9296, loss: 3.235737, norm:0.2469, lr:2.5467e-04 dt: 3332.38ms, tok/sec:157331.56
step 9297, loss: 3.310287, norm:0.2784, lr:2.5461e-04 dt: 3332.09ms, tok/sec:157345.20
step 9298, loss: 3.276549, norm:0.2749, lr:2.5456e-04 dt: 3332.08ms, tok/sec:157345.44
step 9299, loss: 3.232795, norm:0.2470, lr:2.5450e-04 dt: 3332.18ms, tok/sec:157341.00
validation loss: 3.2921
Model and optimizer state saved.
HellaSwag accuracy:2325075327701943377/-2=-1162537663850971648.0000
rank 1 sample 0: Hello, I'm a language model, one that is more intuitive than the original, the second in more advanced languages.
But, it's still a good
rank 1 sample 1: Hello, I'm a language model, a computer programmer, a programmer, a Web designer, computer programmer, a developer. Now I have to write a program
rank 1 sample 2: Hello, I'm a language model, but like all languages, it's a lot more complicated, if I have a good understanding of them and the semantics.
rank 1 sample 3: Hello, I'm a language model, and I'm really good at writing, like I'd wanted to see you work with "Growth" and "C
rank 0 sample 0: Hello, I'm a language model, and I don't have a way to look at it.
A nice, cool idea. Thanks,<|endoftext|>The U
rank 0 sample 1: Hello, I'm a language model, so here's a link to this week's episode [laughs].
That's a good day to celebrate my school,
rank 0 sample 2: Hello, I'm a language model, so I had a number of good experiences with different languages. In a lot of languages, the language is a set of
rank 0 sample 3: Hello, I'm a language model, and not a language. I have to say
"I'm a language, even if I don't know how we
step 9300, loss: 3.303889, norm:0.2702, lr:2.5445e-04 dt: 56214.27ms, tok/sec:9326.60
step 9301, loss: 3.333597, norm:0.2860, lr:2.5439e-04 dt: 3332.07ms, tok/sec:157345.96
step 9302, loss: 3.297211, norm:0.2719, lr:2.5433e-04 dt: 3332.21ms, tok/sec:157339.59
step 9303, loss: 3.282636, norm:0.2843, lr:2.5428e-04 dt: 3332.13ms, tok/sec:157343.05
step 9304, loss: 3.290322, norm:0.2621, lr:2.5422e-04 dt: 3331.92ms, tok/sec:157353.12
step 9305, loss: 3.306913, norm:0.2532, lr:2.5417e-04 dt: 3332.22ms, tok/sec:157338.95
step 9306, loss: 3.291342, norm:0.2741, lr:2.5411e-04 dt: 3332.03ms, tok/sec:157347.95
step 9307, loss: 3.285839, norm:0.2650, lr:2.5405e-04 dt: 3332.74ms, tok/sec:157314.39
step 9308, loss: 3.332249, norm:0.2645, lr:2.5400e-04 dt: 3332.00ms, tok/sec:157349.49
step 9309, loss: 3.278232, norm:0.2503, lr:2.5394e-04 dt: 3331.91ms, tok/sec:157353.69
step 9310, loss: 3.464446, norm:0.2895, lr:2.5389e-04 dt: 3332.12ms, tok/sec:157343.82
step 9311, loss: 3.294983, norm:0.2719, lr:2.5383e-04 dt: 3332.07ms, tok/sec:157345.89
step 9312, loss: 3.331484, norm:0.2679, lr:2.5377e-04 dt: 3331.81ms, tok/sec:157358.27
step 9313, loss: 3.275352, norm:0.3393, lr:2.5372e-04 dt: 3331.97ms, tok/sec:157350.80
step 9314, loss: 3.363415, norm:0.2857, lr:2.5366e-04 dt: 3332.58ms, tok/sec:157322.09
step 9315, loss: 3.281950, norm:0.2618, lr:2.5361e-04 dt: 3332.14ms, tok/sec:157342.79
step 9316, loss: 3.347964, norm:0.2655, lr:2.5355e-04 dt: 3332.09ms, tok/sec:157345.11
step 9317, loss: 3.344327, norm:0.2528, lr:2.5349e-04 dt: 3332.23ms, tok/sec:157338.54
step 9318, loss: 3.325330, norm:0.2639, lr:2.5344e-04 dt: 3332.19ms, tok/sec:157340.26
step 9319, loss: 3.327511, norm:0.2610, lr:2.5338e-04 dt: 3332.07ms, tok/sec:157346.04
step 9320, loss: 3.297078, norm:0.2772, lr:2.5333e-04 dt: 3332.24ms, tok/sec:157338.20
step 9321, loss: 3.283016, norm:0.2439, lr:2.5327e-04 dt: 3332.11ms, tok/sec:157344.04
step 9322, loss: 3.271749, norm:0.2511, lr:2.5322e-04 dt: 3332.56ms, tok/sec:157322.68
step 9323, loss: 3.272632, norm:0.2659, lr:2.5316e-04 dt: 3332.10ms, tok/sec:157344.59
step 9324, loss: 3.314702, norm:0.2386, lr:2.5310e-04 dt: 3332.41ms, tok/sec:157330.02
step 9325, loss: 3.211213, norm:0.2533, lr:2.5305e-04 dt: 3332.15ms, tok/sec:157342.21
step 9326, loss: 3.230935, norm:0.2515, lr:2.5299e-04 dt: 3332.10ms, tok/sec:157344.64
step 9327, loss: 3.301820, norm:0.2448, lr:2.5294e-04 dt: 3332.47ms, tok/sec:157327.33
step 9328, loss: 3.303434, norm:0.2447, lr:2.5288e-04 dt: 3331.86ms, tok/sec:157355.77
step 9329, loss: 3.256178, norm:0.2528, lr:2.5282e-04 dt: 3332.50ms, tok/sec:157325.62
step 9330, loss: 3.249942, norm:0.2601, lr:2.5277e-04 dt: 3332.01ms, tok/sec:157348.97
step 9331, loss: 3.247891, norm:0.2317, lr:2.5271e-04 dt: 3332.11ms, tok/sec:157344.11
step 9332, loss: 3.273327, norm:0.2544, lr:2.5266e-04 dt: 3332.21ms, tok/sec:157339.42
step 9333, loss: 3.314022, norm:0.2555, lr:2.5260e-04 dt: 3332.22ms, tok/sec:157339.05
step 9334, loss: 3.301717, norm:0.2713, lr:2.5254e-04 dt: 3334.23ms, tok/sec:157244.31
step 9335, loss: 3.298779, norm:0.2614, lr:2.5249e-04 dt: 3332.44ms, tok/sec:157328.41
step 9336, loss: 3.383312, norm:0.2640, lr:2.5243e-04 dt: 3332.36ms, tok/sec:157332.32
step 9337, loss: 3.289946, norm:0.3042, lr:2.5238e-04 dt: 3332.04ms, tok/sec:157347.58
step 9338, loss: 3.247859, norm:0.2581, lr:2.5232e-04 dt: 3332.31ms, tok/sec:157334.87
step 9339, loss: 3.282901, norm:0.2969, lr:2.5227e-04 dt: 3332.08ms, tok/sec:157345.39
step 9340, loss: 3.305259, norm:0.2523, lr:2.5221e-04 dt: 3331.94ms, tok/sec:157352.31
step 9341, loss: 3.332026, norm:0.2928, lr:2.5215e-04 dt: 3332.16ms, tok/sec:157341.73
step 9342, loss: 3.322874, norm:0.2889, lr:2.5210e-04 dt: 3331.90ms, tok/sec:157354.24
step 9343, loss: 3.205506, norm:0.2634, lr:2.5204e-04 dt: 3332.15ms, tok/sec:157342.04
step 9344, loss: 3.246785, norm:0.2798, lr:2.5199e-04 dt: 3332.10ms, tok/sec:157344.68
step 9345, loss: 3.304310, norm:0.2815, lr:2.5193e-04 dt: 3331.94ms, tok/sec:157352.18
step 9346, loss: 3.329683, norm:0.2842, lr:2.5187e-04 dt: 3332.23ms, tok/sec:157338.51
step 9347, loss: 3.281610, norm:0.2585, lr:2.5182e-04 dt: 3332.15ms, tok/sec:157342.11
step 9348, loss: 3.358539, norm:0.2858, lr:2.5176e-04 dt: 3332.26ms, tok/sec:157336.94
step 9349, loss: 3.346673, norm:0.2687, lr:2.5171e-04 dt: 3332.16ms, tok/sec:157341.59
HellaSwag accuracy:2325075327704073297/-2=-1162537663852036608.0000
rank 1 sample 0: Hello, I'm a language model, with lots of variables.
Here, the "key" (key) that is a value that is in the range
rank 1 sample 1: Hello, I'm a language model, which means I think you're just talking about the basics you'll need to know to talk to someone.
I'm
rank 1 sample 2: Hello, I'm a language model, but where I have to do it. I'm a globalizing model and I'm a model user and a host;
rank 1 sample 3: Hello, I'm a language model, and I'm using a grammar structure of mine. I actually do my math problem, like. It's a great way
rank 0 sample 0: Hello, I'm a language model, and I'll be writing for that next meeting.
On this talk, a lot of other topics were discussed, as
rank 0 sample 1: Hello, I'm a language model, I was never told how to program there. But they used it and had the ability to make a big deal out of
rank 0 sample 2: Hello, I'm a language model, but I hate the kind of language models I see in the books that make me feel like I'm a language model,
rank 0 sample 3: Hello, I'm a language model, and one of the most important languages for us
is the official national language for Ireland. Our language will be English because
step 9350, loss: 3.370242, norm:0.2581, lr:2.5165e-04 dt: 48518.51ms, tok/sec:10805.94
step 9351, loss: 3.366126, norm:0.3003, lr:2.5160e-04 dt: 3331.97ms, tok/sec:157350.74
step 9352, loss: 3.352701, norm:0.2484, lr:2.5154e-04 dt: 3332.06ms, tok/sec:157346.57
step 9353, loss: 3.304469, norm:0.2471, lr:2.5148e-04 dt: 3332.12ms, tok/sec:157343.82
step 9354, loss: 3.320989, norm:0.2578, lr:2.5143e-04 dt: 3332.18ms, tok/sec:157340.95
step 9355, loss: 3.359452, norm:0.2514, lr:2.5137e-04 dt: 3331.85ms, tok/sec:157356.51
step 9356, loss: 3.310163, norm:0.2457, lr:2.5132e-04 dt: 3332.07ms, tok/sec:157346.22
step 9357, loss: 3.260357, norm:0.2735, lr:2.5126e-04 dt: 3332.26ms, tok/sec:157337.25
step 9358, loss: 3.274957, norm:0.2520, lr:2.5120e-04 dt: 3332.19ms, tok/sec:157340.39
step 9359, loss: 3.301837, norm:0.2539, lr:2.5115e-04 dt: 3332.58ms, tok/sec:157322.16
step 9360, loss: 3.267869, norm:0.2474, lr:2.5109e-04 dt: 3331.83ms, tok/sec:157357.31
step 9361, loss: 3.260224, norm:0.2440, lr:2.5104e-04 dt: 3332.03ms, tok/sec:157348.12
step 9362, loss: 3.211210, norm:0.2463, lr:2.5098e-04 dt: 3332.12ms, tok/sec:157343.84
step 9363, loss: 3.278635, norm:0.2619, lr:2.5093e-04 dt: 3332.05ms, tok/sec:157347.17
step 9364, loss: 3.289163, norm:0.2572, lr:2.5087e-04 dt: 3332.03ms, tok/sec:157347.81
step 9365, loss: 3.247391, norm:0.2500, lr:2.5081e-04 dt: 3332.03ms, tok/sec:157348.03
step 9366, loss: 3.305569, norm:0.2525, lr:2.5076e-04 dt: 3332.37ms, tok/sec:157331.80
step 9367, loss: 3.327740, norm:0.2590, lr:2.5070e-04 dt: 3332.09ms, tok/sec:157344.90
step 9368, loss: 3.280269, norm:0.2489, lr:2.5065e-04 dt: 3332.48ms, tok/sec:157326.52
step 9369, loss: 3.313081, norm:0.2723, lr:2.5059e-04 dt: 3331.89ms, tok/sec:157354.50
step 9370, loss: 3.282917, norm:0.2682, lr:2.5054e-04 dt: 3332.10ms, tok/sec:157344.84
step 9371, loss: 3.292840, norm:0.2625, lr:2.5048e-04 dt: 3331.92ms, tok/sec:157353.28
step 9372, loss: 3.307628, norm:0.2659, lr:2.5042e-04 dt: 3332.23ms, tok/sec:157338.42
step 9373, loss: 3.335511, norm:0.2978, lr:2.5037e-04 dt: 3332.20ms, tok/sec:157340.02
step 9374, loss: 3.242733, norm:0.2858, lr:2.5031e-04 dt: 3331.93ms, tok/sec:157352.72
step 9375, loss: 3.320692, norm:0.2735, lr:2.5026e-04 dt: 3332.39ms, tok/sec:157330.74
step 9376, loss: 3.282128, norm:0.2778, lr:2.5020e-04 dt: 3332.29ms, tok/sec:157335.54
step 9377, loss: 3.266167, norm:0.2744, lr:2.5015e-04 dt: 3332.16ms, tok/sec:157341.67
step 9378, loss: 3.287216, norm:0.2526, lr:2.5009e-04 dt: 3332.24ms, tok/sec:157337.96
step 9379, loss: 3.345452, norm:0.2787, lr:2.5003e-04 dt: 3331.97ms, tok/sec:157350.90
step 9380, loss: 3.323093, norm:0.2723, lr:2.4998e-04 dt: 3332.05ms, tok/sec:157346.90
step 9381, loss: 3.302528, norm:0.2752, lr:2.4992e-04 dt: 3332.44ms, tok/sec:157328.69
step 9382, loss: 3.428089, norm:0.2706, lr:2.4987e-04 dt: 3332.02ms, tok/sec:157348.19
step 9383, loss: 3.350156, norm:0.2625, lr:2.4981e-04 dt: 3332.07ms, tok/sec:157345.84
step 9384, loss: 3.318844, norm:0.2694, lr:2.4976e-04 dt: 3332.24ms, tok/sec:157337.98
step 9385, loss: 3.326451, norm:0.2658, lr:2.4970e-04 dt: 3332.38ms, tok/sec:157331.37
step 9386, loss: 3.282673, norm:0.2413, lr:2.4964e-04 dt: 3332.06ms, tok/sec:157346.60
step 9387, loss: 3.253345, norm:0.2609, lr:2.4959e-04 dt: 3332.04ms, tok/sec:157347.59
step 9388, loss: 3.290915, norm:0.2711, lr:2.4953e-04 dt: 3332.16ms, tok/sec:157341.81
step 9389, loss: 3.300685, norm:0.2753, lr:2.4948e-04 dt: 3331.93ms, tok/sec:157352.69
step 9390, loss: 3.303673, norm:0.2588, lr:2.4942e-04 dt: 3332.28ms, tok/sec:157336.30
step 9391, loss: 3.321906, norm:0.2423, lr:2.4937e-04 dt: 3331.96ms, tok/sec:157351.40
step 9392, loss: 3.276424, norm:0.2702, lr:2.4931e-04 dt: 3332.00ms, tok/sec:157349.25
step 9393, loss: 3.255876, norm:0.2659, lr:2.4925e-04 dt: 3332.17ms, tok/sec:157341.51
step 9394, loss: 3.321723, norm:0.2840, lr:2.4920e-04 dt: 3332.17ms, tok/sec:157341.17
step 9395, loss: 3.311927, norm:0.2811, lr:2.4914e-04 dt: 3332.18ms, tok/sec:157340.82
step 9396, loss: 3.272822, norm:0.2856, lr:2.4909e-04 dt: 3332.02ms, tok/sec:157348.62
step 9397, loss: 3.262733, norm:0.2743, lr:2.4903e-04 dt: 3332.05ms, tok/sec:157347.03
step 9398, loss: 3.314601, norm:0.2947, lr:2.4898e-04 dt: 3332.04ms, tok/sec:157347.33
step 9399, loss: 3.402236, norm:0.3019, lr:2.4892e-04 dt: 3332.11ms, tok/sec:157344.06
validation loss: 3.2920
Model and optimizer state saved.
HellaSwag accuracy:2325092919881729105/-2=-1162546459940864512.0000
rank 1 sample 0: Hello, I'm a language model,” said the head of the BIO of C++ workshop. “I can tell what I mean by the
rank 1 sample 1: Hello, I'm a language model, which means I need to understand and learn this language. When I went to class I looked up different ways to teach it
rank 1 sample 2: Hello, I'm a language model, but at this point I'm not sure how to read. My language models are pretty simple and can be read using the
rank 1 sample 3: Hello, I'm a language model, and I'm also interested in learning language games.
Why would you write some code based on some of the basic principles
rank 0 sample 0: Hello, I'm a language model, and I'll be writing in it (because I'll be writing as I want to do an action in Java). The
rank 0 sample 1: Hello, I'm a language model, but with a lot of variables and multiple variables that a developer needs to add a new function to. There's a lot
rank 0 sample 2: Hello, I'm a language model, so I wanted to give myself a bit more reason to be kind and think about a language that I'm not.

rank 0 sample 3: Hello, I'm a language model, not native to the world.
That’s why I like to write one here! I've been working around
step 9400, loss: 3.278671, norm:0.3264, lr:2.4887e-04 dt: 56338.10ms, tok/sec:9306.10
step 9401, loss: 3.366611, norm:0.2802, lr:2.4881e-04 dt: 3332.66ms, tok/sec:157318.38
step 9402, loss: 3.274940, norm:0.2833, lr:2.4875e-04 dt: 3332.05ms, tok/sec:157347.12
step 9403, loss: 3.297810, norm:0.2822, lr:2.4870e-04 dt: 3331.97ms, tok/sec:157350.63
step 9404, loss: 3.283599, norm:0.2816, lr:2.4864e-04 dt: 3332.13ms, tok/sec:157343.02
step 9405, loss: 3.306139, norm:0.2817, lr:2.4859e-04 dt: 3332.25ms, tok/sec:157337.33
step 9406, loss: 3.287558, norm:0.2521, lr:2.4853e-04 dt: 3332.01ms, tok/sec:157348.85
step 9407, loss: 3.284434, norm:0.2558, lr:2.4848e-04 dt: 3332.01ms, tok/sec:157348.99
step 9408, loss: 3.307568, norm:0.2558, lr:2.4842e-04 dt: 3332.37ms, tok/sec:157331.79
step 9409, loss: 3.315578, norm:0.3006, lr:2.4836e-04 dt: 3331.89ms, tok/sec:157354.60
step 9410, loss: 3.328875, norm:0.2954, lr:2.4831e-04 dt: 3332.57ms, tok/sec:157322.65
step 9411, loss: 3.333144, norm:0.2656, lr:2.4825e-04 dt: 3331.90ms, tok/sec:157353.98
step 9412, loss: 3.260961, norm:0.2742, lr:2.4820e-04 dt: 3331.92ms, tok/sec:157353.14
step 9413, loss: 3.247708, norm:0.2513, lr:2.4814e-04 dt: 3332.14ms, tok/sec:157342.93
step 9414, loss: 3.300149, norm:0.2720, lr:2.4809e-04 dt: 3332.09ms, tok/sec:157345.04
step 9415, loss: 3.343175, norm:0.2813, lr:2.4803e-04 dt: 3332.07ms, tok/sec:157345.97
step 9416, loss: 3.264043, norm:0.2669, lr:2.4798e-04 dt: 3332.12ms, tok/sec:157343.73
step 9417, loss: 3.315509, norm:0.2709, lr:2.4792e-04 dt: 3332.38ms, tok/sec:157331.45
step 9418, loss: 3.270379, norm:0.2569, lr:2.4786e-04 dt: 3331.99ms, tok/sec:157349.89
step 9419, loss: 3.275455, norm:0.2588, lr:2.4781e-04 dt: 3332.43ms, tok/sec:157328.99
step 9420, loss: 3.328244, norm:0.2595, lr:2.4775e-04 dt: 3332.24ms, tok/sec:157337.95
step 9421, loss: 3.303874, norm:0.2510, lr:2.4770e-04 dt: 3332.20ms, tok/sec:157339.70
step 9422, loss: 3.310478, norm:0.2497, lr:2.4764e-04 dt: 3331.93ms, tok/sec:157352.56
step 9423, loss: 3.271809, norm:0.2429, lr:2.4759e-04 dt: 3332.16ms, tok/sec:157341.87
step 9424, loss: 3.289663, norm:0.2682, lr:2.4753e-04 dt: 3332.16ms, tok/sec:157341.73
step 9425, loss: 3.344298, norm:0.2629, lr:2.4748e-04 dt: 3332.15ms, tok/sec:157342.23
step 9426, loss: 3.309509, norm:0.2524, lr:2.4742e-04 dt: 3332.37ms, tok/sec:157331.68
step 9427, loss: 3.320427, norm:0.2511, lr:2.4736e-04 dt: 3332.49ms, tok/sec:157325.98
step 9428, loss: 3.305405, norm:0.2772, lr:2.4731e-04 dt: 3332.01ms, tok/sec:157348.93
step 9429, loss: 3.225963, norm:0.2622, lr:2.4725e-04 dt: 3331.99ms, tok/sec:157349.95
step 9430, loss: 3.268074, norm:0.2490, lr:2.4720e-04 dt: 3331.91ms, tok/sec:157353.36
step 9431, loss: 3.278989, norm:0.2662, lr:2.4714e-04 dt: 3331.91ms, tok/sec:157353.45
step 9432, loss: 3.278787, norm:0.2737, lr:2.4709e-04 dt: 3332.04ms, tok/sec:157347.49
step 9433, loss: 3.254993, norm:0.2592, lr:2.4703e-04 dt: 3332.15ms, tok/sec:157342.23
step 9434, loss: 3.257099, norm:0.2587, lr:2.4698e-04 dt: 3332.12ms, tok/sec:157343.66
step 9435, loss: 3.235824, norm:0.2529, lr:2.4692e-04 dt: 3331.85ms, tok/sec:157356.33
step 9436, loss: 3.319533, norm:0.2532, lr:2.4686e-04 dt: 3332.03ms, tok/sec:157347.76
step 9437, loss: 3.246735, norm:0.2453, lr:2.4681e-04 dt: 3332.28ms, tok/sec:157336.08
step 9438, loss: 3.271749, norm:0.2502, lr:2.4675e-04 dt: 3332.40ms, tok/sec:157330.56
step 9439, loss: 3.259140, norm:0.2436, lr:2.4670e-04 dt: 3332.13ms, tok/sec:157343.24
step 9440, loss: 3.343318, norm:0.2934, lr:2.4664e-04 dt: 3331.93ms, tok/sec:157352.75
step 9441, loss: 3.326257, norm:0.2612, lr:2.4659e-04 dt: 3332.19ms, tok/sec:157340.59
step 9442, loss: 3.283036, norm:0.2645, lr:2.4653e-04 dt: 3332.09ms, tok/sec:157345.13
step 9443, loss: 3.319320, norm:0.2739, lr:2.4648e-04 dt: 3332.01ms, tok/sec:157348.75
step 9444, loss: 3.250043, norm:0.2423, lr:2.4642e-04 dt: 3332.31ms, tok/sec:157334.82
step 9445, loss: 3.309250, norm:0.2484, lr:2.4637e-04 dt: 3332.19ms, tok/sec:157340.59
step 9446, loss: 3.261105, norm:0.2549, lr:2.4631e-04 dt: 3332.05ms, tok/sec:157347.09
step 9447, loss: 3.288245, norm:0.2548, lr:2.4625e-04 dt: 3332.26ms, tok/sec:157337.16
step 9448, loss: 3.227067, norm:0.2606, lr:2.4620e-04 dt: 3332.49ms, tok/sec:157326.28
step 9449, loss: 3.359084, norm:0.2561, lr:2.4614e-04 dt: 3332.24ms, tok/sec:157338.22
HellaSwag accuracy:2325251249564484689/-2=-1162625624782242304.0000
rank 1 sample 0: Hello, I'm a language model, meaning you can't go into a culture, it just does happen.
As a native speaker I'm very familiar with
rank 1 sample 1: Hello, I'm a language model, which means I need to know my language rules. I hope I was able to take off my hands, and I'm
rank 1 sample 2: Hello, I'm a language model, but all you really need to know is that you can also take advantage of the language model, or the model's documentation
rank 1 sample 3: Hello, I'm a language model, and I'm also interested in learning language systems.
At the University of South Florida [WU] we're working
rank 0 sample 0: Hello, I'm a language model, and I'll be using it, please know....
Thing. For some reason you forgot what the Thing tool
rank 0 sample 1: Hello, I'm a language model, so my advice is not to write everything in one time only, so keep reading to see if I use that.

rank 0 sample 2: Hello, I'm a language model, so I like that so, just be sure there are some nice stuff to do about that.
I'm going to
rank 0 sample 3: Hello, I'm a language model, you get a model that works well, you try to find the patterns of words on your target language, and that'll
step 9450, loss: 3.310386, norm:0.2777, lr:2.4609e-04 dt: 48521.84ms, tok/sec:10805.20
step 9451, loss: 3.321642, norm:0.2799, lr:2.4603e-04 dt: 3332.44ms, tok/sec:157328.44
step 9452, loss: 3.269765, norm:0.3201, lr:2.4598e-04 dt: 3332.04ms, tok/sec:157347.24
step 9453, loss: 3.326187, norm:0.3202, lr:2.4592e-04 dt: 3332.07ms, tok/sec:157345.85
step 9454, loss: 3.303660, norm:0.2594, lr:2.4587e-04 dt: 3331.98ms, tok/sec:157350.13
step 9455, loss: 3.305581, norm:0.2956, lr:2.4581e-04 dt: 3332.07ms, tok/sec:157346.21
step 9456, loss: 3.331550, norm:0.2835, lr:2.4576e-04 dt: 3332.27ms, tok/sec:157336.38
step 9457, loss: 3.308874, norm:0.2779, lr:2.4570e-04 dt: 3332.04ms, tok/sec:157347.57
step 9458, loss: 3.307242, norm:0.2689, lr:2.4565e-04 dt: 3332.01ms, tok/sec:157348.85
step 9459, loss: 3.297192, norm:0.2630, lr:2.4559e-04 dt: 3332.31ms, tok/sec:157334.58
step 9460, loss: 3.285650, norm:0.2626, lr:2.4553e-04 dt: 3332.44ms, tok/sec:157328.37
step 9461, loss: 3.314818, norm:0.2854, lr:2.4548e-04 dt: 3332.28ms, tok/sec:157335.95
step 9462, loss: 3.326311, norm:0.2731, lr:2.4542e-04 dt: 3332.16ms, tok/sec:157341.56
step 9463, loss: 3.332484, norm:0.2478, lr:2.4537e-04 dt: 3332.08ms, tok/sec:157345.72
step 9464, loss: 3.303460, norm:0.2542, lr:2.4531e-04 dt: 3332.09ms, tok/sec:157344.98
step 9465, loss: 3.287732, norm:0.2686, lr:2.4526e-04 dt: 3332.58ms, tok/sec:157321.74
step 9466, loss: 3.246739, norm:0.2452, lr:2.4520e-04 dt: 3332.25ms, tok/sec:157337.50
step 9467, loss: 3.331629, norm:0.2696, lr:2.4515e-04 dt: 3332.26ms, tok/sec:157336.84
step 9468, loss: 3.297998, norm:0.2597, lr:2.4509e-04 dt: 3332.15ms, tok/sec:157342.45
step 9469, loss: 3.253843, norm:0.2457, lr:2.4504e-04 dt: 3332.36ms, tok/sec:157332.14
step 9470, loss: 3.285635, norm:0.2492, lr:2.4498e-04 dt: 3332.18ms, tok/sec:157340.85
step 9471, loss: 3.303778, norm:0.2570, lr:2.4493e-04 dt: 3332.05ms, tok/sec:157347.00
step 9472, loss: 3.314360, norm:0.2561, lr:2.4487e-04 dt: 3332.26ms, tok/sec:157337.23
step 9473, loss: 3.274240, norm:0.2442, lr:2.4481e-04 dt: 3332.12ms, tok/sec:157343.89
step 9474, loss: 3.260130, norm:0.2817, lr:2.4476e-04 dt: 3331.95ms, tok/sec:157351.77
step 9475, loss: 3.267517, norm:0.2503, lr:2.4470e-04 dt: 3332.11ms, tok/sec:157344.37
step 9476, loss: 3.273400, norm:0.2504, lr:2.4465e-04 dt: 3332.31ms, tok/sec:157334.59
step 9477, loss: 3.307989, norm:0.2687, lr:2.4459e-04 dt: 3332.03ms, tok/sec:157347.74
step 9478, loss: 3.218403, norm:0.2611, lr:2.4454e-04 dt: 3332.33ms, tok/sec:157333.87
step 9479, loss: 3.280418, norm:0.2979, lr:2.4448e-04 dt: 3332.08ms, tok/sec:157345.67
step 9480, loss: 3.263210, norm:0.2717, lr:2.4443e-04 dt: 3332.17ms, tok/sec:157341.37
step 9481, loss: 3.292027, norm:0.2531, lr:2.4437e-04 dt: 3332.08ms, tok/sec:157345.38
step 9482, loss: 3.264790, norm:0.2723, lr:2.4432e-04 dt: 3332.05ms, tok/sec:157346.97
step 9483, loss: 3.276937, norm:0.2507, lr:2.4426e-04 dt: 3332.18ms, tok/sec:157340.75
step 9484, loss: 3.294932, norm:0.2593, lr:2.4421e-04 dt: 3332.08ms, tok/sec:157345.53
step 9485, loss: 3.245404, norm:0.2499, lr:2.4415e-04 dt: 3332.25ms, tok/sec:157337.38
step 9486, loss: 3.345853, norm:0.2583, lr:2.4410e-04 dt: 3332.05ms, tok/sec:157346.86
step 9487, loss: 3.297555, norm:0.2542, lr:2.4404e-04 dt: 3332.79ms, tok/sec:157311.88
step 9488, loss: 3.312380, norm:0.2699, lr:2.4398e-04 dt: 3332.19ms, tok/sec:157340.25
step 9489, loss: 3.300467, norm:0.2437, lr:2.4393e-04 dt: 3331.99ms, tok/sec:157349.92
step 9490, loss: 3.315727, norm:0.2507, lr:2.4387e-04 dt: 3331.99ms, tok/sec:157349.69
step 9491, loss: 3.303139, norm:0.2533, lr:2.4382e-04 dt: 3332.17ms, tok/sec:157341.40
step 9492, loss: 3.306270, norm:0.2456, lr:2.4376e-04 dt: 3332.04ms, tok/sec:157347.67
step 9493, loss: 3.303073, norm:0.2516, lr:2.4371e-04 dt: 3332.21ms, tok/sec:157339.53
step 9494, loss: 3.304782, norm:0.2487, lr:2.4365e-04 dt: 3332.57ms, tok/sec:157322.46
step 9495, loss: 3.298456, norm:0.2383, lr:2.4360e-04 dt: 3332.00ms, tok/sec:157349.18
step 9496, loss: 3.318016, norm:0.2419, lr:2.4354e-04 dt: 3332.31ms, tok/sec:157334.71
step 9497, loss: 3.309547, norm:0.2485, lr:2.4349e-04 dt: 3332.05ms, tok/sec:157347.08
step 9498, loss: 3.312011, norm:0.2614, lr:2.4343e-04 dt: 3332.11ms, tok/sec:157344.30
step 9499, loss: 3.251323, norm:0.2429, lr:2.4338e-04 dt: 3331.99ms, tok/sec:157349.82
validation loss: 3.2857
Model and optimizer state saved.
HellaSwag accuracy:-2286575506350701487/-2=1143287753175350784.0000
rank 1 sample 0: Hello, I'm a language model, i know a lot of languages, too many and they’re not like that too, it’s not
rank 1 sample 1: Hello, I'm a language model, which means I think we're doing all the right things because we're looking at 'an' terms, and I'm
rank 1 sample 2: Hello, I'm a language model, but its use of the word 'I' is rare and only the most relevant to me, the other languages being named
rank 1 sample 3: Hello, I'm a language model, and I'm pretty good at modeling what data is to create that image. One thing to have in mind is that it
rank 0 sample 0: Hello, I'm a language model, and I'll be working on my writing first," said Edith Pheme, one of his classmates.
He
rank 0 sample 1: Hello, I'm a language model, but for each of you, there are a lot of useful tools, along with some basic tips. These are some of
rank 0 sample 2: Hello, I'm a language model, so I like it. There are three different methods I use every day:
- Use a dictionary
- Use the
rank 0 sample 3: Hello, I'm a language model, but here's what I need:
I write a program that makes all kinds of requests into the console. The system
step 9500, loss: 3.266171, norm:0.2535, lr:2.4332e-04 dt: 56240.27ms, tok/sec:9322.29
step 9501, loss: 3.313982, norm:0.2595, lr:2.4327e-04 dt: 3332.07ms, tok/sec:157346.02
step 9502, loss: 3.279625, norm:0.2490, lr:2.4321e-04 dt: 3332.10ms, tok/sec:157344.59
step 9503, loss: 3.263450, norm:0.2355, lr:2.4316e-04 dt: 3331.84ms, tok/sec:157356.98
step 9504, loss: 3.276729, norm:0.2592, lr:2.4310e-04 dt: 3331.93ms, tok/sec:157352.85
step 9505, loss: 3.194955, norm:0.2678, lr:2.4305e-04 dt: 3332.23ms, tok/sec:157338.38
step 9506, loss: 3.257698, norm:0.2563, lr:2.4299e-04 dt: 3332.09ms, tok/sec:157345.09
step 9507, loss: 3.243844, norm:0.2515, lr:2.4294e-04 dt: 3332.65ms, tok/sec:157318.62
step 9508, loss: 3.288221, norm:0.2679, lr:2.4288e-04 dt: 3332.04ms, tok/sec:157347.49
step 9509, loss: 3.264078, norm:0.2792, lr:2.4282e-04 dt: 3332.00ms, tok/sec:157349.36
step 9510, loss: 3.234448, norm:0.2887, lr:2.4277e-04 dt: 3331.94ms, tok/sec:157352.01
step 9511, loss: 3.284236, norm:0.2991, lr:2.4271e-04 dt: 3332.31ms, tok/sec:157334.80
step 9512, loss: 3.386414, norm:0.2820, lr:2.4266e-04 dt: 3332.09ms, tok/sec:157345.13
step 9513, loss: 3.299268, norm:0.3136, lr:2.4260e-04 dt: 3332.24ms, tok/sec:157337.87
step 9514, loss: 3.339777, norm:0.2748, lr:2.4255e-04 dt: 3332.25ms, tok/sec:157337.73
step 9515, loss: 3.367208, norm:0.2954, lr:2.4249e-04 dt: 3332.21ms, tok/sec:157339.24
step 9516, loss: 3.257159, norm:0.2816, lr:2.4244e-04 dt: 3332.47ms, tok/sec:157327.26
step 9517, loss: 3.313674, norm:0.2836, lr:2.4238e-04 dt: 3331.98ms, tok/sec:157350.39
step 9518, loss: 3.303857, norm:0.2843, lr:2.4233e-04 dt: 3332.04ms, tok/sec:157347.53
step 9519, loss: 3.249541, norm:0.2976, lr:2.4227e-04 dt: 3332.20ms, tok/sec:157339.72
step 9520, loss: 3.305005, norm:0.2726, lr:2.4222e-04 dt: 3332.21ms, tok/sec:157339.47
step 9521, loss: 3.278149, norm:0.2935, lr:2.4216e-04 dt: 3332.12ms, tok/sec:157343.90
step 9522, loss: 3.295387, norm:0.2621, lr:2.4211e-04 dt: 3332.05ms, tok/sec:157346.87
step 9523, loss: 3.300016, norm:0.2673, lr:2.4205e-04 dt: 3332.55ms, tok/sec:157323.24
step 9524, loss: 3.240055, norm:0.2665, lr:2.4200e-04 dt: 3334.17ms, tok/sec:157246.75
step 9525, loss: 3.291821, norm:0.3041, lr:2.4194e-04 dt: 3332.22ms, tok/sec:157338.97
step 9526, loss: 3.324302, norm:0.2599, lr:2.4189e-04 dt: 3332.16ms, tok/sec:157341.67
step 9527, loss: 3.419169, norm:0.2957, lr:2.4183e-04 dt: 3331.95ms, tok/sec:157351.86
step 9528, loss: 3.290254, norm:0.2767, lr:2.4178e-04 dt: 3332.00ms, tok/sec:157349.27
step 9529, loss: 3.258658, norm:0.2720, lr:2.4172e-04 dt: 3332.42ms, tok/sec:157329.57
step 9530, loss: 3.257521, norm:0.2693, lr:2.4167e-04 dt: 3332.32ms, tok/sec:157334.43
step 9531, loss: 3.334745, norm:0.2973, lr:2.4161e-04 dt: 3332.26ms, tok/sec:157337.19
step 9532, loss: 3.304383, norm:0.2718, lr:2.4156e-04 dt: 3331.82ms, tok/sec:157357.95
step 9533, loss: 3.285970, norm:0.2559, lr:2.4150e-04 dt: 3331.95ms, tok/sec:157351.50
step 9534, loss: 3.252626, norm:0.2586, lr:2.4145e-04 dt: 3332.21ms, tok/sec:157339.59
step 9535, loss: 3.267942, norm:0.2863, lr:2.4139e-04 dt: 3331.82ms, tok/sec:157357.81
step 9536, loss: 3.253698, norm:0.2506, lr:2.4134e-04 dt: 3332.04ms, tok/sec:157347.42
step 9537, loss: 3.264715, norm:0.2494, lr:2.4128e-04 dt: 3331.83ms, tok/sec:157357.37
step 9538, loss: 3.260571, norm:0.2630, lr:2.4123e-04 dt: 3332.30ms, tok/sec:157335.03
step 9539, loss: 3.247961, norm:0.2598, lr:2.4117e-04 dt: 3332.19ms, tok/sec:157340.38
step 9540, loss: 3.293854, norm:0.2483, lr:2.4112e-04 dt: 3332.00ms, tok/sec:157349.52
step 9541, loss: 3.257648, norm:0.2631, lr:2.4106e-04 dt: 3332.53ms, tok/sec:157324.26
step 9542, loss: 3.269431, norm:0.2621, lr:2.4101e-04 dt: 3332.53ms, tok/sec:157324.45
step 9543, loss: 3.264709, norm:0.2665, lr:2.4095e-04 dt: 3332.00ms, tok/sec:157349.24
step 9544, loss: 3.253963, norm:0.2656, lr:2.4090e-04 dt: 3331.97ms, tok/sec:157350.98
step 9545, loss: 3.277683, norm:0.2524, lr:2.4084e-04 dt: 3332.08ms, tok/sec:157345.79
step 9546, loss: 3.302741, norm:0.2630, lr:2.4079e-04 dt: 3331.93ms, tok/sec:157352.67
step 9547, loss: 3.321368, norm:0.2611, lr:2.4073e-04 dt: 3332.24ms, tok/sec:157338.20
step 9548, loss: 3.265640, norm:0.2492, lr:2.4068e-04 dt: 3332.05ms, tok/sec:157346.88
step 9549, loss: 3.327691, norm:0.2780, lr:2.4062e-04 dt: 3332.03ms, tok/sec:157347.99
HellaSwag accuracy:-6898261524778613679/-2=3449130762389306880.0000
rank 1 sample 0: Hello, I'm a language model, and my instructor is a linguistics assistant for students with Language Sciences. I wanted to get students who are struggling with language
rank 1 sample 1: Hello, I'm a language model, which means you've got to use all the language to produce a nice visual language model based on other language models.

rank 1 sample 2: Hello, I'm a language model, so much like an English teacher. I'm a language model so far from the language model I think.
In this
rank 1 sample 3: Hello, I'm a language model, and I'm pretty good at explaining my meaning.
PATMOSYSTEM READ_TESTS ABOUT

rank 0 sample 0: Hello, I'm a language model, and I know that's pretty well in mind-set. So this can make a great programming assignment for your students or
rank 0 sample 1: Hello, I'm a language model, so here's a quick note. Why is this and exactly how you implement it? I mean I should say that language
rank 0 sample 2: Hello, I'm a language model, but I thought about things for a minute. Here's a short explanation about the way language is used in the classroom:
rank 0 sample 3: Hello, I'm a language model, and here's what I did.
- Get a list of all the language elements (not all possible ones): words
step 9550, loss: 3.348060, norm:0.2557, lr:2.4057e-04 dt: 48526.38ms, tok/sec:10804.19
step 9551, loss: 3.301635, norm:0.2716, lr:2.4051e-04 dt: 3332.23ms, tok/sec:157338.54
step 9552, loss: 3.446982, norm:0.2748, lr:2.4046e-04 dt: 3332.03ms, tok/sec:157347.82
step 9553, loss: 3.293289, norm:0.2953, lr:2.4040e-04 dt: 3332.15ms, tok/sec:157342.27
step 9554, loss: 3.274172, norm:0.2908, lr:2.4035e-04 dt: 3332.05ms, tok/sec:157346.77
step 9555, loss: 3.348710, norm:0.2864, lr:2.4029e-04 dt: 3331.89ms, tok/sec:157354.54
step 9556, loss: 3.290687, norm:0.2798, lr:2.4024e-04 dt: 3331.94ms, tok/sec:157352.31
step 9557, loss: 3.388638, norm:0.2932, lr:2.4018e-04 dt: 3332.18ms, tok/sec:157340.69
step 9558, loss: 3.273274, norm:0.2952, lr:2.4013e-04 dt: 3332.12ms, tok/sec:157343.46
step 9559, loss: 3.235372, norm:0.2626, lr:2.4007e-04 dt: 3332.49ms, tok/sec:157326.13
step 9560, loss: 3.260249, norm:0.2709, lr:2.4002e-04 dt: 3332.05ms, tok/sec:157346.76
step 9561, loss: 3.253345, norm:0.2714, lr:2.3996e-04 dt: 3332.03ms, tok/sec:157347.91
step 9562, loss: 3.247132, norm:0.2456, lr:2.3991e-04 dt: 3332.29ms, tok/sec:157335.48
step 9563, loss: 3.267214, norm:0.2544, lr:2.3985e-04 dt: 3332.18ms, tok/sec:157340.82
step 9564, loss: 3.263359, norm:0.2621, lr:2.3980e-04 dt: 3331.95ms, tok/sec:157351.79
step 9565, loss: 3.265428, norm:0.2446, lr:2.3974e-04 dt: 3332.03ms, tok/sec:157348.10
step 9566, loss: 3.233145, norm:0.2509, lr:2.3969e-04 dt: 3332.24ms, tok/sec:157337.91
step 9567, loss: 3.236572, norm:0.2447, lr:2.3963e-04 dt: 3332.12ms, tok/sec:157343.87
step 9568, loss: 3.248101, norm:0.2450, lr:2.3958e-04 dt: 3332.54ms, tok/sec:157323.98
step 9569, loss: 3.243083, norm:0.2470, lr:2.3952e-04 dt: 3332.28ms, tok/sec:157336.10
step 9570, loss: 3.273429, norm:0.2518, lr:2.3947e-04 dt: 3332.10ms, tok/sec:157344.85
step 9571, loss: 3.349909, norm:0.2466, lr:2.3941e-04 dt: 3332.06ms, tok/sec:157346.72
step 9572, loss: 3.285389, norm:0.2636, lr:2.3936e-04 dt: 3331.96ms, tok/sec:157351.32
step 9573, loss: 3.273375, norm:0.2851, lr:2.3930e-04 dt: 3332.23ms, tok/sec:157338.62
step 9574, loss: 3.262849, norm:0.2658, lr:2.3925e-04 dt: 3331.96ms, tok/sec:157351.01
step 9575, loss: 3.234654, norm:0.2621, lr:2.3919e-04 dt: 3332.27ms, tok/sec:157336.71
step 9576, loss: 3.232408, norm:0.2410, lr:2.3914e-04 dt: 3332.01ms, tok/sec:157348.80
step 9577, loss: 3.319160, norm:0.2645, lr:2.3908e-04 dt: 3332.01ms, tok/sec:157349.01
step 9578, loss: 3.311808, norm:0.2537, lr:2.3903e-04 dt: 3331.94ms, tok/sec:157352.01
step 9579, loss: 3.264383, norm:0.2490, lr:2.3897e-04 dt: 3332.26ms, tok/sec:157337.12
step 9580, loss: 3.367236, norm:0.3543, lr:2.3892e-04 dt: 3331.87ms, tok/sec:157355.51
step 9581, loss: 3.286970, norm:0.2917, lr:2.3886e-04 dt: 3332.04ms, tok/sec:157347.38
step 9582, loss: 3.337608, norm:0.3148, lr:2.3881e-04 dt: 3332.33ms, tok/sec:157333.60
step 9583, loss: 3.294304, norm:0.2731, lr:2.3875e-04 dt: 3332.10ms, tok/sec:157344.61
step 9584, loss: 3.266293, norm:0.3153, lr:2.3870e-04 dt: 3332.48ms, tok/sec:157326.54
step 9585, loss: 3.305256, norm:0.3014, lr:2.3864e-04 dt: 3332.19ms, tok/sec:157340.44
step 9586, loss: 3.386740, norm:0.2775, lr:2.3859e-04 dt: 3332.22ms, tok/sec:157338.82
step 9587, loss: 3.365511, norm:0.2768, lr:2.3853e-04 dt: 3331.97ms, tok/sec:157350.90
step 9588, loss: 3.253563, norm:0.2871, lr:2.3848e-04 dt: 3332.00ms, tok/sec:157349.18
step 9589, loss: 3.274950, norm:0.2786, lr:2.3842e-04 dt: 3332.13ms, tok/sec:157343.35
step 9590, loss: 3.242227, norm:0.2817, lr:2.3837e-04 dt: 3332.23ms, tok/sec:157338.33
step 9591, loss: 3.275524, norm:0.2486, lr:2.3831e-04 dt: 3332.32ms, tok/sec:157334.15
step 9592, loss: 3.288823, norm:0.2812, lr:2.3826e-04 dt: 3331.92ms, tok/sec:157353.01
step 9593, loss: 3.371460, norm:0.2704, lr:2.3820e-04 dt: 3332.47ms, tok/sec:157326.97
step 9594, loss: 3.268810, norm:0.2532, lr:2.3815e-04 dt: 3332.12ms, tok/sec:157343.87
step 9595, loss: 3.262378, norm:0.2499, lr:2.3809e-04 dt: 3331.93ms, tok/sec:157352.71
step 9596, loss: 3.309279, norm:0.2613, lr:2.3804e-04 dt: 3331.94ms, tok/sec:157352.38
step 9597, loss: 3.299836, norm:0.2503, lr:2.3798e-04 dt: 3332.19ms, tok/sec:157340.30
step 9598, loss: 3.225832, norm:0.2464, lr:2.3793e-04 dt: 3332.38ms, tok/sec:157331.18
step 9599, loss: 3.264522, norm:0.2370, lr:2.3787e-04 dt: 3332.01ms, tok/sec:157348.76
validation loss: 3.2833
Model and optimizer state saved.
HellaSwag accuracy:2361156866919334993/-2=-1180578433459667456.0000
rank 1 sample 0: Hello, I'm a language model, what's a language model?
No one yet thinks for whom the language model is right or wrong. The language model
rank 1 sample 1: Hello, I'm a language model, which is what most people think of when they think of them. When you read a Wikipedia article about the language, you
rank 1 sample 2: Hello, I'm a language model, but learning it myself is a challenge. I'm an online English learner, and I want to make it happen withoutrank 0 sample 0: Hello, I'm a language model, and I'll be writing to this soon at 4:48pm<|endoftext|>Lipstick (1st Edition)
by

rank 1 sample 3: Hello, I'm a language model, and I'm sure that every language model tends to assume (on, by.)
At the beginning of the year,
rank 0 sample 1: Hello, I'm a language model, so how do I know if it uses the word train if it does not have a verb. I would use the word
rank 0 sample 2: Hello, I'm a language model, so I haven't gone from a traditional language standpoint to a world that doesn't allow us to speak English.
Well
rank 0 sample 3: Hello, I'm a language model, and here's what I want you to know.....
I'm a language model, and here's what I want him
step 9600, loss: 3.270360, norm:0.2460, lr:2.3782e-04 dt: 56183.48ms, tok/sec:9331.71
step 9601, loss: 3.276306, norm:0.2491, lr:2.3776e-04 dt: 3332.28ms, tok/sec:157336.21
step 9602, loss: 3.276486, norm:0.2468, lr:2.3771e-04 dt: 3331.97ms, tok/sec:157350.69
step 9603, loss: 3.228211, norm:0.2509, lr:2.3765e-04 dt: 3332.32ms, tok/sec:157334.03
step 9604, loss: 3.224845, norm:0.2380, lr:2.3760e-04 dt: 3332.64ms, tok/sec:157319.26
step 9605, loss: 3.274045, norm:0.2429, lr:2.3754e-04 dt: 3332.35ms, tok/sec:157332.62
step 9606, loss: 3.278502, norm:0.2465, lr:2.3749e-04 dt: 3332.05ms, tok/sec:157347.14
step 9607, loss: 3.216446, norm:0.2378, lr:2.3744e-04 dt: 3331.99ms, tok/sec:157349.99
step 9608, loss: 3.259676, norm:0.2395, lr:2.3738e-04 dt: 3332.14ms, tok/sec:157342.63
step 9609, loss: 3.293379, norm:0.2551, lr:2.3733e-04 dt: 3332.11ms, tok/sec:157344.25
step 9610, loss: 3.250320, norm:0.2488, lr:2.3727e-04 dt: 3332.04ms, tok/sec:157347.57
step 9611, loss: 3.226398, norm:0.2536, lr:2.3722e-04 dt: 3332.23ms, tok/sec:157338.59
step 9612, loss: 3.256126, norm:0.2535, lr:2.3716e-04 dt: 3332.12ms, tok/sec:157343.59
step 9613, loss: 3.213184, norm:0.2659, lr:2.3711e-04 dt: 3332.48ms, tok/sec:157326.79
step 9614, loss: 3.322667, norm:0.2800, lr:2.3705e-04 dt: 3332.15ms, tok/sec:157342.28
step 9615, loss: 3.339542, norm:0.2610, lr:2.3700e-04 dt: 3331.94ms, tok/sec:157352.23
step 9616, loss: 3.264295, norm:0.2642, lr:2.3694e-04 dt: 3332.12ms, tok/sec:157343.80
step 9617, loss: 3.246053, norm:0.2713, lr:2.3689e-04 dt: 3332.20ms, tok/sec:157339.88
step 9618, loss: 3.261845, norm:0.2854, lr:2.3683e-04 dt: 3332.11ms, tok/sec:157344.35
step 9619, loss: 3.222297, norm:0.2879, lr:2.3678e-04 dt: 3332.17ms, tok/sec:157341.35
step 9620, loss: 3.164828, norm:0.2687, lr:2.3672e-04 dt: 3332.32ms, tok/sec:157334.40
step 9621, loss: 3.308579, norm:0.3394, lr:2.3667e-04 dt: 3332.11ms, tok/sec:157344.25
step 9622, loss: 3.281236, norm:0.2928, lr:2.3661e-04 dt: 3332.23ms, tok/sec:157338.54
step 9623, loss: 3.331238, norm:0.3068, lr:2.3656e-04 dt: 3332.06ms, tok/sec:157346.29
step 9624, loss: 3.311031, norm:0.2750, lr:2.3650e-04 dt: 3332.35ms, tok/sec:157333.03
step 9625, loss: 3.223798, norm:0.2662, lr:2.3645e-04 dt: 3332.23ms, tok/sec:157338.60
step 9626, loss: 3.263414, norm:0.2636, lr:2.3640e-04 dt: 3332.07ms, tok/sec:157346.12
step 9627, loss: 3.257643, norm:0.2899, lr:2.3634e-04 dt: 3332.19ms, tok/sec:157340.48
step 9628, loss: 3.216562, norm:0.2910, lr:2.3629e-04 dt: 3332.20ms, tok/sec:157339.67
step 9629, loss: 3.222715, norm:0.2714, lr:2.3623e-04 dt: 3332.49ms, tok/sec:157326.13
step 9630, loss: 3.279664, norm:0.2657, lr:2.3618e-04 dt: 3332.07ms, tok/sec:157346.10
step 9631, loss: 3.280234, norm:0.2761, lr:2.3612e-04 dt: 3332.12ms, tok/sec:157343.80
step 9632, loss: 3.243198, norm:0.2653, lr:2.3607e-04 dt: 3332.06ms, tok/sec:157346.31
step 9633, loss: 3.223262, norm:0.2505, lr:2.3601e-04 dt: 3332.13ms, tok/sec:157343.03
step 9634, loss: 3.251465, norm:0.2914, lr:2.3596e-04 dt: 3332.08ms, tok/sec:157345.51
step 9635, loss: 3.261563, norm:0.2552, lr:2.3590e-04 dt: 3332.07ms, tok/sec:157346.03
step 9636, loss: 3.211216, norm:0.2904, lr:2.3585e-04 dt: 3332.31ms, tok/sec:157334.79
step 9637, loss: 3.262870, norm:0.2699, lr:2.3579e-04 dt: 3332.19ms, tok/sec:157340.15
step 9638, loss: 3.259187, norm:0.2786, lr:2.3574e-04 dt: 3332.31ms, tok/sec:157334.53
step 9639, loss: 3.239901, norm:0.2503, lr:2.3568e-04 dt: 3332.28ms, tok/sec:157335.92
step 9640, loss: 3.263647, norm:0.2722, lr:2.3563e-04 dt: 3332.11ms, tok/sec:157343.97
step 9641, loss: 3.232156, norm:0.2798, lr:2.3558e-04 dt: 3332.12ms, tok/sec:157343.69
step 9642, loss: 3.252930, norm:0.2487, lr:2.3552e-04 dt: 3331.98ms, tok/sec:157350.28
step 9643, loss: 3.247422, norm:0.2495, lr:2.3547e-04 dt: 3331.86ms, tok/sec:157355.76
step 9644, loss: 3.289845, norm:0.2764, lr:2.3541e-04 dt: 3332.22ms, tok/sec:157338.82
step 9645, loss: 3.269200, norm:0.2590, lr:2.3536e-04 dt: 3332.36ms, tok/sec:157332.28
step 9646, loss: 3.229511, norm:0.2552, lr:2.3530e-04 dt: 3332.04ms, tok/sec:157347.64
step 9647, loss: 3.232791, norm:0.2613, lr:2.3525e-04 dt: 3332.16ms, tok/sec:157341.68
step 9648, loss: 3.303612, norm:0.2637, lr:2.3519e-04 dt: 3331.96ms, tok/sec:157351.19
step 9649, loss: 3.268856, norm:0.2575, lr:2.3514e-04 dt: 3332.07ms, tok/sec:157346.15
HellaSwag accuracy:2325110512067773521/-2=-1162555256033886720.0000
rank 1 sample 0: Hello, I'm a language model, as a trainer, teacher and writer doing my assignments for myself all the time. I've got this blog post from a
rank 1 sample 1: Hello, I'm a language model, which means it's a language, you may have to make a sentence a different language you teach, or you may have
rank 1 sample 2: Hello, I'm a language model, but so what about the ones that I've been working on?"<|endoftext|>In the case of the American Revolution, when people
rank 1 sample 3: Hello, I'm a language model, and I'm working with C++ programming projects. I’m still running a simple example of a C++ compiler
rank 0 sample 0: Hello, I'm a language model, and I'll be writing more about these because I don't have one wrong, but you can see how this is actually
rank 0 sample 1: Hello, I'm a language model, so why speak to someone who doesn't speak the ICT industry and would never be allowed to use another language?

rank 0 sample 2: Hello, I'm a language model, so I should say here you can say that this is a way of doing things differently, and that's what you think
rank 0 sample 3: Hello, I'm a language model, so please show me how to do this, why don't you give me an illustration for my particular dialect?<|endoftext|>An
step 9650, loss: 3.273852, norm:0.2653, lr:2.3508e-04 dt: 48523.66ms, tok/sec:10804.79
step 9651, loss: 3.254524, norm:0.2546, lr:2.3503e-04 dt: 3331.99ms, tok/sec:157350.04
step 9652, loss: 3.296429, norm:0.2618, lr:2.3497e-04 dt: 3332.27ms, tok/sec:157336.79
step 9653, loss: 3.243468, norm:0.2434, lr:2.3492e-04 dt: 3332.06ms, tok/sec:157346.47
step 9654, loss: 3.280389, norm:0.2605, lr:2.3487e-04 dt: 3332.35ms, tok/sec:157333.05
step 9655, loss: 3.231568, norm:0.2516, lr:2.3481e-04 dt: 3332.12ms, tok/sec:157343.89
step 9656, loss: 3.233288, norm:0.2614, lr:2.3476e-04 dt: 3332.56ms, tok/sec:157323.11
step 9657, loss: 3.221040, norm:0.2493, lr:2.3470e-04 dt: 3332.02ms, tok/sec:157348.34
step 9658, loss: 3.231520, norm:0.2645, lr:2.3465e-04 dt: 3332.08ms, tok/sec:157345.68
step 9659, loss: 3.298198, norm:0.2606, lr:2.3459e-04 dt: 3332.10ms, tok/sec:157344.62
step 9660, loss: 3.298111, norm:0.2642, lr:2.3454e-04 dt: 3332.29ms, tok/sec:157335.59
step 9661, loss: 3.289933, norm:0.2766, lr:2.3448e-04 dt: 3332.01ms, tok/sec:157348.88
step 9662, loss: 3.277800, norm:0.2723, lr:2.3443e-04 dt: 3332.04ms, tok/sec:157347.47
step 9663, loss: 3.300051, norm:0.2720, lr:2.3437e-04 dt: 3332.22ms, tok/sec:157338.93
step 9664, loss: 3.245923, norm:0.2592, lr:2.3432e-04 dt: 3332.15ms, tok/sec:157342.21
step 9665, loss: 3.338684, norm:0.2535, lr:2.3427e-04 dt: 3332.69ms, tok/sec:157316.96
step 9666, loss: 3.268998, norm:0.2497, lr:2.3421e-04 dt: 3332.11ms, tok/sec:157344.22
step 9667, loss: 3.222806, norm:0.2571, lr:2.3416e-04 dt: 3332.01ms, tok/sec:157349.09
step 9668, loss: 3.326649, norm:0.2479, lr:2.3410e-04 dt: 3332.08ms, tok/sec:157345.50
step 9669, loss: 3.299453, norm:0.2530, lr:2.3405e-04 dt: 3332.08ms, tok/sec:157345.65
step 9670, loss: 3.304798, norm:0.2746, lr:2.3399e-04 dt: 3331.86ms, tok/sec:157355.77
step 9671, loss: 3.249645, norm:0.2555, lr:2.3394e-04 dt: 3332.14ms, tok/sec:157342.78
step 9672, loss: 3.266741, norm:0.2706, lr:2.3388e-04 dt: 3332.27ms, tok/sec:157336.49
step 9673, loss: 3.307092, norm:0.2763, lr:2.3383e-04 dt: 3332.47ms, tok/sec:157327.12
step 9674, loss: 3.281181, norm:0.2649, lr:2.3377e-04 dt: 3332.04ms, tok/sec:157347.31
step 9675, loss: 3.230016, norm:0.2480, lr:2.3372e-04 dt: 3332.07ms, tok/sec:157345.98
step 9676, loss: 3.239180, norm:0.2674, lr:2.3367e-04 dt: 3332.09ms, tok/sec:157345.03
step 9677, loss: 3.259883, norm:0.2541, lr:2.3361e-04 dt: 3332.06ms, tok/sec:157346.31
step 9678, loss: 3.248314, norm:0.2528, lr:2.3356e-04 dt: 3332.19ms, tok/sec:157340.53
step 9679, loss: 3.257749, norm:0.2484, lr:2.3350e-04 dt: 3332.07ms, tok/sec:157346.19
step 9680, loss: 3.294485, norm:0.2682, lr:2.3345e-04 dt: 3332.03ms, tok/sec:157347.85
step 9681, loss: 3.293883, norm:0.2673, lr:2.3339e-04 dt: 3332.40ms, tok/sec:157330.55
step 9682, loss: 3.309813, norm:0.2653, lr:2.3334e-04 dt: 3332.46ms, tok/sec:157327.63
step 9683, loss: 3.241771, norm:0.2446, lr:2.3328e-04 dt: 3332.02ms, tok/sec:157348.29
step 9684, loss: 3.246149, norm:0.2720, lr:2.3323e-04 dt: 3332.14ms, tok/sec:157342.81
step 9685, loss: 3.220171, norm:0.2476, lr:2.3318e-04 dt: 3331.97ms, tok/sec:157350.68
step 9686, loss: 3.244033, norm:0.2662, lr:2.3312e-04 dt: 3332.14ms, tok/sec:157342.96
step 9687, loss: 3.251163, norm:0.2739, lr:2.3307e-04 dt: 3332.29ms, tok/sec:157335.81
step 9688, loss: 3.256985, norm:0.2847, lr:2.3301e-04 dt: 3331.97ms, tok/sec:157350.77
step 9689, loss: 3.314581, norm:0.2777, lr:2.3296e-04 dt: 3332.08ms, tok/sec:157345.33
step 9690, loss: 3.286677, norm:0.2960, lr:2.3290e-04 dt: 3332.16ms, tok/sec:157341.57
step 9691, loss: 3.249301, norm:0.2899, lr:2.3285e-04 dt: 3332.53ms, tok/sec:157324.42
step 9692, loss: 3.210507, norm:0.2682, lr:2.3279e-04 dt: 3332.19ms, tok/sec:157340.24
step 9693, loss: 3.232968, norm:0.2922, lr:2.3274e-04 dt: 3331.89ms, tok/sec:157354.64
step 9694, loss: 3.359707, norm:0.2745, lr:2.3269e-04 dt: 3332.24ms, tok/sec:157338.09
step 9695, loss: 3.279649, norm:0.2786, lr:2.3263e-04 dt: 3332.22ms, tok/sec:157338.81
step 9696, loss: 3.223324, norm:0.2677, lr:2.3258e-04 dt: 3332.08ms, tok/sec:157345.48
step 9697, loss: 3.261045, norm:0.2823, lr:2.3252e-04 dt: 3332.28ms, tok/sec:157336.05
step 9698, loss: 3.282175, norm:0.2947, lr:2.3247e-04 dt: 3332.17ms, tok/sec:157341.36
step 9699, loss: 3.285634, norm:0.2608, lr:2.3241e-04 dt: 3332.17ms, tok/sec:157341.21
validation loss: 3.2793
Model and optimizer state saved.
HellaSwag accuracy:-2286487545421003695/-2=1143243772710501888.0000
rank 1 sample 0: Hello, I'm a language model, the most valuable thing to me is looking into something and how I can help!
This week I'm writing a paper
rank 1 sample 1: Hello, I'm a language model, which means I need to understand what is about to happen there. Now, what is, I might be able to do
rank 1 sample 2: Hello, I'm a language model, but since the world is a lot more complex than that, so let's move on to another part of this story:
rank 1 sample 3: Hello, I'm a language model, and I'm sure you guys aren't writing a proper answer for our assignment tomorrow. To complete it, you should have
rank 0 sample 0: Hello, I'm a language model, and I need to be able to learn anything (the data is) - in the context of what the machine is and
rank 0 sample 1: Hello, I'm a language model, so how do I know what a tool is, my background is not necessarily an example, but rather another way of describing
rank 0 sample 2: Hello, I'm a language model, so I wanted to set aside a single word so I could make my own.
You can use the word to "
rank 0 sample 3: Hello, I'm a language model, and here's how I use the tool to solve this problem.
So what are the results of your modeling and analyzing
step 9700, loss: 3.341284, norm:0.3112, lr:2.3236e-04 dt: 56156.09ms, tok/sec:9336.26
step 9701, loss: 3.234396, norm:0.2656, lr:2.3230e-04 dt: 3332.34ms, tok/sec:157333.26
step 9702, loss: 3.263136, norm:0.2877, lr:2.3225e-04 dt: 3332.37ms, tok/sec:157331.86
step 9703, loss: 3.264883, norm:0.2657, lr:2.3220e-04 dt: 3332.20ms, tok/sec:157339.86
step 9704, loss: 3.280504, norm:0.2811, lr:2.3214e-04 dt: 3331.96ms, tok/sec:157351.02
step 9705, loss: 3.290975, norm:0.2608, lr:2.3209e-04 dt: 3332.26ms, tok/sec:157337.00
step 9706, loss: 3.264476, norm:0.2631, lr:2.3203e-04 dt: 3332.07ms, tok/sec:157346.18
step 9707, loss: 3.283951, norm:0.2885, lr:2.3198e-04 dt: 3332.16ms, tok/sec:157341.95
step 9708, loss: 3.266439, norm:0.2678, lr:2.3192e-04 dt: 3332.17ms, tok/sec:157341.10
step 9709, loss: 3.291571, norm:0.2697, lr:2.3187e-04 dt: 3332.04ms, tok/sec:157347.61
step 9710, loss: 3.265828, norm:0.2642, lr:2.3182e-04 dt: 3332.66ms, tok/sec:157318.32
step 9711, loss: 3.287894, norm:0.2551, lr:2.3176e-04 dt: 3332.22ms, tok/sec:157338.81
step 9712, loss: 3.240083, norm:0.2672, lr:2.3171e-04 dt: 3332.02ms, tok/sec:157348.52
step 9713, loss: 3.201281, norm:0.2499, lr:2.3165e-04 dt: 3332.23ms, tok/sec:157338.34
step 9714, loss: 3.209905, norm:0.2480, lr:2.3160e-04 dt: 3332.08ms, tok/sec:157345.43
step 9715, loss: 3.266693, norm:0.2577, lr:2.3154e-04 dt: 3334.35ms, tok/sec:157238.26
step 9716, loss: 3.338652, norm:0.2659, lr:2.3149e-04 dt: 3331.94ms, tok/sec:157352.09
step 9717, loss: 3.285671, norm:0.2469, lr:2.3144e-04 dt: 3332.23ms, tok/sec:157338.29
step 9718, loss: 3.218538, norm:0.2449, lr:2.3138e-04 dt: 3332.04ms, tok/sec:157347.53
step 9719, loss: 3.307343, norm:0.2852, lr:2.3133e-04 dt: 3332.06ms, tok/sec:157346.66
step 9720, loss: 3.226598, norm:0.2918, lr:2.3127e-04 dt: 3331.86ms, tok/sec:157356.06
step 9721, loss: 3.271164, norm:0.2911, lr:2.3122e-04 dt: 3332.03ms, tok/sec:157348.02
step 9722, loss: 3.301723, norm:0.2911, lr:2.3116e-04 dt: 3332.21ms, tok/sec:157339.35
step 9723, loss: 3.280035, norm:0.2605, lr:2.3111e-04 dt: 3332.07ms, tok/sec:157346.18
step 9724, loss: 3.268445, norm:0.2945, lr:2.3106e-04 dt: 3332.20ms, tok/sec:157340.06
step 9725, loss: 3.274759, norm:0.2741, lr:2.3100e-04 dt: 3331.97ms, tok/sec:157350.68
step 9726, loss: 3.278500, norm:0.2779, lr:2.3095e-04 dt: 3332.13ms, tok/sec:157343.23
step 9727, loss: 3.257408, norm:0.2740, lr:2.3089e-04 dt: 3332.46ms, tok/sec:157327.70
step 9728, loss: 3.301631, norm:0.2819, lr:2.3084e-04 dt: 3331.99ms, tok/sec:157349.87
step 9729, loss: 3.264060, norm:0.2932, lr:2.3078e-04 dt: 3332.41ms, tok/sec:157329.82
step 9730, loss: 3.246896, norm:0.2999, lr:2.3073e-04 dt: 3332.05ms, tok/sec:157347.00
step 9731, loss: 3.250165, norm:0.2745, lr:2.3068e-04 dt: 3332.02ms, tok/sec:157348.31
step 9732, loss: 3.311446, norm:0.3071, lr:2.3062e-04 dt: 3331.99ms, tok/sec:157349.94
step 9733, loss: 3.306116, norm:0.2531, lr:2.3057e-04 dt: 3332.09ms, tok/sec:157345.02
step 9734, loss: 3.272880, norm:0.2560, lr:2.3051e-04 dt: 3332.05ms, tok/sec:157347.18
step 9735, loss: 3.282401, norm:0.2697, lr:2.3046e-04 dt: 3332.18ms, tok/sec:157340.65
step 9736, loss: 3.223699, norm:0.2445, lr:2.3040e-04 dt: 3331.99ms, tok/sec:157349.62
step 9737, loss: 3.265258, norm:0.2553, lr:2.3035e-04 dt: 3332.40ms, tok/sec:157330.28
step 9738, loss: 3.276066, norm:0.2742, lr:2.3030e-04 dt: 3332.64ms, tok/sec:157319.07
step 9739, loss: 3.275244, norm:0.2364, lr:2.3024e-04 dt: 3331.95ms, tok/sec:157351.66
step 9740, loss: 3.233726, norm:0.2650, lr:2.3019e-04 dt: 3332.09ms, tok/sec:157345.32
step 9741, loss: 3.253523, norm:0.2646, lr:2.3013e-04 dt: 3332.03ms, tok/sec:157348.03
step 9742, loss: 3.268674, norm:0.2418, lr:2.3008e-04 dt: 3332.11ms, tok/sec:157344.02
step 9743, loss: 3.265302, norm:0.2637, lr:2.3003e-04 dt: 3332.05ms, tok/sec:157347.11
step 9744, loss: 3.253104, norm:0.2504, lr:2.2997e-04 dt: 3331.94ms, tok/sec:157352.31
step 9745, loss: 3.325047, norm:0.2569, lr:2.2992e-04 dt: 3332.21ms, tok/sec:157339.62
step 9746, loss: 3.299619, norm:0.2665, lr:2.2986e-04 dt: 3332.47ms, tok/sec:157327.05
step 9747, loss: 3.280364, norm:0.2441, lr:2.2981e-04 dt: 3332.14ms, tok/sec:157342.93
step 9748, loss: 3.259599, norm:0.2355, lr:2.2975e-04 dt: 3332.05ms, tok/sec:157346.81
step 9749, loss: 3.213397, norm:0.2525, lr:2.2970e-04 dt: 3331.91ms, tok/sec:157353.47
HellaSwag accuracy:-2250441156215995311/-2=1125220578107997696.0000
rank 1 sample 0: Hello, I'm a language model, as well as a way to help developers build on a big database. I used to think it worked, but I'm
rank 1 sample 1: Hello, I'm a language model, a model that will help you understand some of the key characteristics of a language. I do a word search on the internet
rank 1 sample 2: Hello, I'm a language model, but because you don't have to write a program to run my language. I'm not going to use a native CPU
rank 1 sample 3: Hello, I'm a language model, and I'm looking at three different languages now. There's about 25% chance that to ask the question, or you
rank 0 sample 0: Hello, I'm a language model, and I'll be using all kinds of models so I can make an English version of that work."
As a student
rank 0 sample 1: Hello, I'm a language model, so how do I do that? Like, you use our own, let's say we've been building your own language
rank 0 sample 2: Hello, I'm a language model, so I understand how to set up this. In my case here it's a "native" language, but it will
rank 0 sample 3: Hello, I'm a language model, so in the language model, we can't count the number of instances to have such a result, unless we want such
step 9750, loss: 3.326122, norm:0.2606, lr:2.2965e-04 dt: 48521.77ms, tok/sec:10805.21
step 9751, loss: 3.348655, norm:0.2478, lr:2.2959e-04 dt: 3332.21ms, tok/sec:157339.62
step 9752, loss: 3.347693, norm:0.2913, lr:2.2954e-04 dt: 3332.05ms, tok/sec:157346.97
step 9753, loss: 3.264789, norm:0.2614, lr:2.2948e-04 dt: 3331.86ms, tok/sec:157355.73
step 9754, loss: 3.301732, norm:0.2569, lr:2.2943e-04 dt: 3332.19ms, tok/sec:157340.46
step 9755, loss: 3.271866, norm:0.2688, lr:2.2938e-04 dt: 3331.87ms, tok/sec:157355.35
step 9756, loss: 3.252459, norm:0.2612, lr:2.2932e-04 dt: 3332.11ms, tok/sec:157344.13
step 9757, loss: 3.211336, norm:0.2937, lr:2.2927e-04 dt: 3332.46ms, tok/sec:157327.42
step 9758, loss: 3.293896, norm:0.2702, lr:2.2921e-04 dt: 3332.26ms, tok/sec:157336.90
step 9759, loss: 3.416632, norm:0.2806, lr:2.2916e-04 dt: 3332.67ms, tok/sec:157317.75
step 9760, loss: 3.270246, norm:0.2884, lr:2.2911e-04 dt: 3332.22ms, tok/sec:157338.95
step 9761, loss: 3.276186, norm:0.2614, lr:2.2905e-04 dt: 3333.07ms, tok/sec:157298.68
step 9762, loss: 3.304164, norm:0.2685, lr:2.2900e-04 dt: 3332.25ms, tok/sec:157337.74
step 9763, loss: 3.225350, norm:0.2749, lr:2.2894e-04 dt: 3332.20ms, tok/sec:157339.69
step 9764, loss: 3.278524, norm:0.2650, lr:2.2889e-04 dt: 3332.11ms, tok/sec:157344.28
step 9765, loss: 3.307095, norm:0.2551, lr:2.2883e-04 dt: 3332.11ms, tok/sec:157344.16
step 9766, loss: 3.303402, norm:0.2814, lr:2.2878e-04 dt: 3332.40ms, tok/sec:157330.50
step 9767, loss: 3.208390, norm:0.2979, lr:2.2873e-04 dt: 3331.98ms, tok/sec:157350.37
step 9768, loss: 3.309503, norm:0.2779, lr:2.2867e-04 dt: 3332.30ms, tok/sec:157334.95
step 9769, loss: 3.289618, norm:0.2898, lr:2.2862e-04 dt: 3332.14ms, tok/sec:157342.65
step 9770, loss: 3.365109, norm:0.2922, lr:2.2856e-04 dt: 3332.26ms, tok/sec:157337.02
step 9771, loss: 3.264939, norm:0.2692, lr:2.2851e-04 dt: 3332.11ms, tok/sec:157343.99
step 9772, loss: 3.250713, norm:0.2676, lr:2.2846e-04 dt: 3332.12ms, tok/sec:157343.54
step 9773, loss: 3.270096, norm:0.2571, lr:2.2840e-04 dt: 3332.10ms, tok/sec:157344.60
step 9774, loss: 3.286516, norm:0.2627, lr:2.2835e-04 dt: 3332.20ms, tok/sec:157339.90
step 9775, loss: 3.294129, norm:0.3219, lr:2.2829e-04 dt: 3331.99ms, tok/sec:157349.92
step 9776, loss: 3.278731, norm:0.2684, lr:2.2824e-04 dt: 3332.26ms, tok/sec:157336.90
step 9777, loss: 3.243952, norm:0.2560, lr:2.2819e-04 dt: 3332.07ms, tok/sec:157346.25
step 9778, loss: 3.275738, norm:0.3178, lr:2.2813e-04 dt: 3332.74ms, tok/sec:157314.64
step 9779, loss: 3.272439, norm:0.2761, lr:2.2808e-04 dt: 3332.10ms, tok/sec:157344.41
step 9780, loss: 3.257108, norm:0.2485, lr:2.2802e-04 dt: 3332.07ms, tok/sec:157345.92
step 9781, loss: 3.323641, norm:0.2845, lr:2.2797e-04 dt: 3332.12ms, tok/sec:157343.69
step 9782, loss: 3.264490, norm:0.2534, lr:2.2792e-04 dt: 3332.23ms, tok/sec:157338.38
step 9783, loss: 3.286234, norm:0.2689, lr:2.2786e-04 dt: 3332.10ms, tok/sec:157344.41
step 9784, loss: 3.276462, norm:0.2522, lr:2.2781e-04 dt: 3332.10ms, tok/sec:157344.60
step 9785, loss: 3.302553, norm:0.2735, lr:2.2775e-04 dt: 3332.15ms, tok/sec:157342.21
step 9786, loss: 3.250532, norm:0.2711, lr:2.2770e-04 dt: 3332.11ms, tok/sec:157344.33
step 9787, loss: 3.285446, norm:0.2443, lr:2.2765e-04 dt: 3332.35ms, tok/sec:157333.02
step 9788, loss: 3.217733, norm:0.2636, lr:2.2759e-04 dt: 3332.09ms, tok/sec:157345.00
step 9789, loss: 3.277679, norm:0.3010, lr:2.2754e-04 dt: 3332.09ms, tok/sec:157345.27
step 9790, loss: 3.293534, norm:0.2566, lr:2.2748e-04 dt: 3332.24ms, tok/sec:157338.10
step 9791, loss: 3.286916, norm:0.2896, lr:2.2743e-04 dt: 3332.20ms, tok/sec:157340.12
step 9792, loss: 3.294111, norm:0.2594, lr:2.2738e-04 dt: 3332.24ms, tok/sec:157337.99
step 9793, loss: 3.244124, norm:0.2765, lr:2.2732e-04 dt: 3332.07ms, tok/sec:157346.18
step 9794, loss: 3.211950, norm:0.2513, lr:2.2727e-04 dt: 3332.28ms, tok/sec:157336.03
step 9795, loss: 3.286108, norm:0.2684, lr:2.2721e-04 dt: 3332.26ms, tok/sec:157337.18
step 9796, loss: 3.315133, norm:0.2798, lr:2.2716e-04 dt: 3332.12ms, tok/sec:157343.75
step 9797, loss: 3.291572, norm:0.2637, lr:2.2711e-04 dt: 3332.07ms, tok/sec:157345.97
step 9798, loss: 3.282664, norm:0.2605, lr:2.2705e-04 dt: 3332.18ms, tok/sec:157340.78
step 9799, loss: 3.271777, norm:0.3314, lr:2.2700e-04 dt: 3332.14ms, tok/sec:157342.78
validation loss: 3.2755
Model and optimizer state saved.
HellaSwag accuracy:-2286610690723314607/-2=1143305345361657344.0000
rank 1 sample 0: Hello, I'm a language model, an online software developer, a blogger's blog writer and a speaker. I'm a computer model guru, so I'm
rank 1 sample 1: Hello, I'm a language model, which means I think there's a lot of stuff here as well. I guess that they actually build the language, but
rank 1 sample 2: Hello, I'm a language model, but some languages aren't. I'm not a linguist so I can't understand the rules or concepts. The second
rank 0 sample 0: Hello, I'm a language model, and I need to be able to make more people's native language for language.
"As for my question, 'rank 1 sample 3: Hello, I'm a language model, and I'm pretty much learning new languages
- My hope is that there's a fun site on the internet for people

rank 0 sample 1: Hello, I'm a language model, but when I see that I have forgotten to write: http://www.mycourses.org/language-models
rank 0 sample 2: Hello, I'm a language model, so I wrote my programming with python as a beginner. I wrote code using Python for the first time, and then python
rank 0 sample 3: Hello, I'm a language model, and how to create a language model and implement objects.
Some languages use an inherited constructor with a type "isEmpty
step 9800, loss: 3.215192, norm:0.2694, lr:2.2694e-04 dt: 56364.44ms, tok/sec:9301.75
step 9801, loss: 3.260958, norm:0.2671, lr:2.2689e-04 dt: 3332.61ms, tok/sec:157320.51
step 9802, loss: 3.358327, norm:0.2745, lr:2.2684e-04 dt: 3332.10ms, tok/sec:157344.79
step 9803, loss: 3.304146, norm:0.2491, lr:2.2678e-04 dt: 3332.03ms, tok/sec:157347.85
step 9804, loss: 3.288074, norm:0.2699, lr:2.2673e-04 dt: 3332.12ms, tok/sec:157343.45
step 9805, loss: 3.305540, norm:0.2636, lr:2.2668e-04 dt: 3332.33ms, tok/sec:157333.85
step 9806, loss: 3.291340, norm:0.2493, lr:2.2662e-04 dt: 3332.04ms, tok/sec:157347.42
step 9807, loss: 3.291919, norm:0.2544, lr:2.2657e-04 dt: 3332.00ms, tok/sec:157349.24
step 9808, loss: 3.283067, norm:0.2474, lr:2.2651e-04 dt: 3332.33ms, tok/sec:157333.70
step 9809, loss: 3.276992, norm:0.2490, lr:2.2646e-04 dt: 3332.05ms, tok/sec:157347.06
step 9810, loss: 3.319369, norm:0.3118, lr:2.2641e-04 dt: 3332.37ms, tok/sec:157331.95
step 9811, loss: 3.246549, norm:0.2860, lr:2.2635e-04 dt: 3331.97ms, tok/sec:157350.71
step 9812, loss: 3.234489, norm:0.3116, lr:2.2630e-04 dt: 3331.85ms, tok/sec:157356.41
step 9813, loss: 3.229922, norm:0.2631, lr:2.2624e-04 dt: 3332.11ms, tok/sec:157344.08
step 9814, loss: 3.322297, norm:0.2791, lr:2.2619e-04 dt: 3331.95ms, tok/sec:157351.73
step 9815, loss: 3.233221, norm:0.2823, lr:2.2614e-04 dt: 3332.01ms, tok/sec:157348.65
step 9816, loss: 3.256020, norm:0.2684, lr:2.2608e-04 dt: 3332.16ms, tok/sec:157341.78
step 9817, loss: 3.260827, norm:0.2992, lr:2.2603e-04 dt: 3332.15ms, tok/sec:157342.37
step 9818, loss: 3.266468, norm:0.2632, lr:2.2598e-04 dt: 3332.07ms, tok/sec:157346.15
step 9819, loss: 3.252265, norm:0.2614, lr:2.2592e-04 dt: 3332.14ms, tok/sec:157342.69
step 9820, loss: 3.247793, norm:0.2472, lr:2.2587e-04 dt: 3332.64ms, tok/sec:157319.01
step 9821, loss: 3.260754, norm:0.2547, lr:2.2581e-04 dt: 3332.29ms, tok/sec:157335.58
step 9822, loss: 3.281399, norm:0.2521, lr:2.2576e-04 dt: 3332.07ms, tok/sec:157346.19
step 9823, loss: 3.298339, norm:0.2643, lr:2.2571e-04 dt: 3332.09ms, tok/sec:157345.08
step 9824, loss: 3.279669, norm:0.2646, lr:2.2565e-04 dt: 3332.29ms, tok/sec:157335.78
step 9825, loss: 3.268939, norm:0.2636, lr:2.2560e-04 dt: 3332.23ms, tok/sec:157338.65
step 9826, loss: 3.279730, norm:0.2768, lr:2.2555e-04 dt: 3332.09ms, tok/sec:157344.94
step 9827, loss: 3.300145, norm:0.2547, lr:2.2549e-04 dt: 3332.18ms, tok/sec:157340.88
step 9828, loss: 3.311999, norm:0.3326, lr:2.2544e-04 dt: 3332.03ms, tok/sec:157347.77
step 9829, loss: 3.254886, norm:0.2719, lr:2.2538e-04 dt: 3332.74ms, tok/sec:157314.39
step 9830, loss: 3.280641, norm:0.2942, lr:2.2533e-04 dt: 3332.08ms, tok/sec:157345.63
step 9831, loss: 3.254589, norm:0.2735, lr:2.2528e-04 dt: 3332.11ms, tok/sec:157344.35
step 9832, loss: 3.276668, norm:0.2784, lr:2.2522e-04 dt: 3332.03ms, tok/sec:157348.00
step 9833, loss: 3.345549, norm:0.2545, lr:2.2517e-04 dt: 3332.10ms, tok/sec:157344.70
step 9834, loss: 3.263389, norm:0.2766, lr:2.2511e-04 dt: 3331.92ms, tok/sec:157352.99
step 9835, loss: 3.292059, norm:0.2867, lr:2.2506e-04 dt: 3332.09ms, tok/sec:157345.22
step 9836, loss: 3.230222, norm:0.2523, lr:2.2501e-04 dt: 3332.28ms, tok/sec:157336.05
step 9837, loss: 3.299126, norm:0.2546, lr:2.2495e-04 dt: 3332.27ms, tok/sec:157336.72
step 9838, loss: 3.211721, norm:0.2525, lr:2.2490e-04 dt: 3332.50ms, tok/sec:157325.71
step 9839, loss: 3.358434, norm:0.2650, lr:2.2485e-04 dt: 3332.21ms, tok/sec:157339.52
step 9840, loss: 3.316279, norm:0.2768, lr:2.2479e-04 dt: 3332.13ms, tok/sec:157343.28
step 9841, loss: 3.269105, norm:0.3521, lr:2.2474e-04 dt: 3331.96ms, tok/sec:157351.11
step 9842, loss: 3.296665, norm:0.2706, lr:2.2469e-04 dt: 3332.23ms, tok/sec:157338.47
step 9843, loss: 3.310800, norm:0.2624, lr:2.2463e-04 dt: 3332.21ms, tok/sec:157339.25
step 9844, loss: 3.234696, norm:0.2638, lr:2.2458e-04 dt: 3332.13ms, tok/sec:157343.00
step 9845, loss: 3.298998, norm:0.2751, lr:2.2452e-04 dt: 3332.55ms, tok/sec:157323.58
step 9846, loss: 3.270250, norm:0.2573, lr:2.2447e-04 dt: 3332.45ms, tok/sec:157328.29
step 9847, loss: 3.236865, norm:0.2642, lr:2.2442e-04 dt: 3332.20ms, tok/sec:157339.67
step 9848, loss: 3.303765, norm:0.2490, lr:2.2436e-04 dt: 3332.02ms, tok/sec:157348.31
step 9849, loss: 3.278442, norm:0.2535, lr:2.2431e-04 dt: 3332.12ms, tok/sec:157343.55
HellaSwag accuracy:2325057735518030929/-2=-1162528867759015424.0000
rank 1 sample 0: Hello, I'm a language model, and what is it? I'm familiar. How many languages make up a language? How many is a language?

rank 1 sample 1: Hello, I'm a language model, which means I use a language model as I'm studying how to model English syntax and not with students, but with students
rank 1 sample 2: Hello, I'm a language model, but all the time I'm not sure what I'm trying to teach it. I'm just hoping it will come up
rank 1 sample 3: Hello, I'm a language model, and I'm looking at that. If and only if-then can get right. With, if and only if,
rank 0 sample 0: Hello, I'm a language model, and I love to learn languages that speak like we do.
It should make sense if you know what your language needs
rank 0 sample 1: Hello, I'm a language model, so now I can use it, where I have lots of different kinds of functions, so I'm working with a few
rank 0 sample 2: Hello, I'm a language model, so I didn't worry because I'd like to start learning this one.
"So I'm going to do this
rank 0 sample 3: Hello, I'm a language model, and what I can do is I'm a verb. I'm doing an activity to be my friend here. So a
step 9850, loss: 3.207145, norm:0.2636, lr:2.2426e-04 dt: 48521.54ms, tok/sec:10805.26
step 9851, loss: 3.295858, norm:0.2486, lr:2.2420e-04 dt: 3332.20ms, tok/sec:157339.86
step 9852, loss: 3.301492, norm:0.2654, lr:2.2415e-04 dt: 3331.88ms, tok/sec:157354.79
step 9853, loss: 3.282139, norm:0.2548, lr:2.2409e-04 dt: 3332.10ms, tok/sec:157344.51
step 9854, loss: 3.304430, norm:0.2490, lr:2.2404e-04 dt: 3332.19ms, tok/sec:157340.35
step 9855, loss: 3.266814, norm:0.2407, lr:2.2399e-04 dt: 3332.03ms, tok/sec:157348.00
step 9856, loss: 3.227449, norm:0.2508, lr:2.2393e-04 dt: 3331.95ms, tok/sec:157351.80
step 9857, loss: 3.291402, norm:0.2535, lr:2.2388e-04 dt: 3332.19ms, tok/sec:157340.23
step 9858, loss: 3.298120, norm:0.2780, lr:2.2383e-04 dt: 3331.97ms, tok/sec:157350.58
step 9859, loss: 3.224281, norm:0.2643, lr:2.2377e-04 dt: 3332.40ms, tok/sec:157330.29
step 9860, loss: 3.243995, norm:0.2820, lr:2.2372e-04 dt: 3332.18ms, tok/sec:157341.02
step 9861, loss: 3.332447, norm:0.2633, lr:2.2367e-04 dt: 3332.03ms, tok/sec:157347.82
step 9862, loss: 3.297418, norm:0.2737, lr:2.2361e-04 dt: 3332.05ms, tok/sec:157346.88
step 9863, loss: 3.270030, norm:0.2602, lr:2.2356e-04 dt: 3332.07ms, tok/sec:157346.15
step 9864, loss: 3.302630, norm:0.4104, lr:2.2350e-04 dt: 3332.21ms, tok/sec:157339.38
step 9865, loss: 3.312294, norm:0.3537, lr:2.2345e-04 dt: 3332.11ms, tok/sec:157344.08
step 9866, loss: 3.298916, norm:0.2835, lr:2.2340e-04 dt: 3332.18ms, tok/sec:157340.84
step 9867, loss: 3.264960, norm:0.2988, lr:2.2334e-04 dt: 3331.98ms, tok/sec:157350.05
step 9868, loss: 3.271571, norm:0.2887, lr:2.2329e-04 dt: 3332.55ms, tok/sec:157323.46
step 9869, loss: 3.311652, norm:0.2822, lr:2.2324e-04 dt: 3332.03ms, tok/sec:157348.02
step 9870, loss: 3.329665, norm:0.2802, lr:2.2318e-04 dt: 3332.19ms, tok/sec:157340.60
step 9871, loss: 3.307818, norm:0.2628, lr:2.2313e-04 dt: 3331.84ms, tok/sec:157356.93
step 9872, loss: 3.229715, norm:0.2664, lr:2.2308e-04 dt: 3332.11ms, tok/sec:157344.13
step 9873, loss: 3.248900, norm:0.2579, lr:2.2302e-04 dt: 3332.13ms, tok/sec:157343.05
step 9874, loss: 3.306753, norm:0.2633, lr:2.2297e-04 dt: 3332.14ms, tok/sec:157342.93
step 9875, loss: 3.233048, norm:0.2647, lr:2.2292e-04 dt: 3332.33ms, tok/sec:157333.72
step 9876, loss: 3.308285, norm:0.2620, lr:2.2286e-04 dt: 3332.10ms, tok/sec:157344.78
step 9877, loss: 3.238755, norm:0.2670, lr:2.2281e-04 dt: 3332.10ms, tok/sec:157344.71
step 9878, loss: 3.253201, norm:0.2539, lr:2.2276e-04 dt: 3332.09ms, tok/sec:157345.21
step 9879, loss: 3.264575, norm:0.2571, lr:2.2270e-04 dt: 3332.22ms, tok/sec:157338.96
step 9880, loss: 3.197537, norm:0.2499, lr:2.2265e-04 dt: 3332.24ms, tok/sec:157337.83
step 9881, loss: 3.196389, norm:0.2496, lr:2.2259e-04 dt: 3332.20ms, tok/sec:157339.79
step 9882, loss: 3.216959, norm:0.2571, lr:2.2254e-04 dt: 3332.39ms, tok/sec:157330.80
step 9883, loss: 3.257537, norm:0.2501, lr:2.2249e-04 dt: 3332.21ms, tok/sec:157339.27
step 9884, loss: 3.216643, norm:0.2539, lr:2.2243e-04 dt: 3332.53ms, tok/sec:157324.29
step 9885, loss: 3.223821, norm:0.2480, lr:2.2238e-04 dt: 3331.98ms, tok/sec:157350.48
step 9886, loss: 3.247403, norm:0.2657, lr:2.2233e-04 dt: 3331.90ms, tok/sec:157354.02
step 9887, loss: 3.258218, norm:0.2593, lr:2.2227e-04 dt: 3332.05ms, tok/sec:157346.96
step 9888, loss: 3.240157, norm:0.2640, lr:2.2222e-04 dt: 3332.22ms, tok/sec:157338.88
step 9889, loss: 3.201612, norm:0.2381, lr:2.2217e-04 dt: 3331.88ms, tok/sec:157354.97
step 9890, loss: 3.275878, norm:0.2673, lr:2.2211e-04 dt: 3332.12ms, tok/sec:157343.46
step 9891, loss: 3.252379, norm:0.2828, lr:2.2206e-04 dt: 3332.39ms, tok/sec:157331.08
step 9892, loss: 3.307250, norm:0.2736, lr:2.2201e-04 dt: 3332.16ms, tok/sec:157341.74
step 9893, loss: 3.262071, norm:0.2735, lr:2.2195e-04 dt: 3332.34ms, tok/sec:157333.44
step 9894, loss: 3.280568, norm:0.2584, lr:2.2190e-04 dt: 3332.28ms, tok/sec:157335.98
step 9895, loss: 3.278964, norm:0.2781, lr:2.2185e-04 dt: 3332.05ms, tok/sec:157347.12
step 9896, loss: 3.284463, norm:0.2674, lr:2.2179e-04 dt: 3332.11ms, tok/sec:157344.02
step 9897, loss: 3.277928, norm:0.2678, lr:2.2174e-04 dt: 3332.33ms, tok/sec:157333.88
step 9898, loss: 3.299792, norm:0.2465, lr:2.2169e-04 dt: 3332.70ms, tok/sec:157316.41
step 9899, loss: 3.354612, norm:0.2854, lr:2.2163e-04 dt: 3332.30ms, tok/sec:157335.39
validation loss: 3.2721
Model and optimizer state saved.
HellaSwag accuracy:2325198473006384145/-2=-1162599236503192064.0000
rank 1 sample 0: Hello, I'm a language model, in my home country, I'm used as a computer lab user. I also use my friends.
So, I
rank 1 sample 1: Hello, I'm a language model, a computer system, a computer system and data processing system or a computer system (it might be for example, a computer
rank 1 sample 2: Hello, I'm a language model, so where is all the data?
I'm not really an engineer or a programmer, I have some data on a
rank 0 sample 0: Hello, I'm a language model, and I'll be writing something to write up next week. I love getting it. I wanted to be a language model
rank 1 sample 3: Hello, I'm a language model, and I'm looking at one of the projects I'm proposing in this class—the One Simple Program.
One Simple
rank 0 sample 1: Hello, I'm a language model, so here's a link to an early version: My model of speech shows us the language models I found, and then
rank 0 sample 2: Hello, I'm a language model, but I also use what I've already learned as a language designer: how to learn and understand a language. I used
rank 0 sample 3: Hello, I'm a language model, so just click the link and I'm ready to start.
A good example for your application to show you the model
step 9900, loss: 3.258461, norm:0.2575, lr:2.2158e-04 dt: 56240.53ms, tok/sec:9322.24
step 9901, loss: 3.236717, norm:0.2679, lr:2.2153e-04 dt: 3332.20ms, tok/sec:157339.88
step 9902, loss: 3.241030, norm:0.2559, lr:2.2147e-04 dt: 3332.30ms, tok/sec:157335.08
step 9903, loss: 3.298840, norm:0.2874, lr:2.2142e-04 dt: 3331.97ms, tok/sec:157350.80
step 9904, loss: 3.273697, norm:0.3392, lr:2.2137e-04 dt: 3332.49ms, tok/sec:157326.14
step 9905, loss: 3.306364, norm:0.2522, lr:2.2131e-04 dt: 3334.24ms, tok/sec:157243.42
step 9906, loss: 3.265289, norm:0.2840, lr:2.2126e-04 dt: 3332.11ms, tok/sec:157344.13
step 9907, loss: 3.274738, norm:0.2630, lr:2.2121e-04 dt: 3332.06ms, tok/sec:157346.52
step 9908, loss: 3.264211, norm:0.2683, lr:2.2115e-04 dt: 3332.08ms, tok/sec:157345.54
step 9909, loss: 3.285110, norm:0.2664, lr:2.2110e-04 dt: 3332.11ms, tok/sec:157343.96
step 9910, loss: 3.227671, norm:0.2689, lr:2.2104e-04 dt: 3332.16ms, tok/sec:157341.82
step 9911, loss: 3.290403, norm:0.2627, lr:2.2099e-04 dt: 3332.26ms, tok/sec:157337.07
step 9912, loss: 3.297182, norm:0.2692, lr:2.2094e-04 dt: 3332.01ms, tok/sec:157349.01
step 9913, loss: 3.263192, norm:0.2561, lr:2.2088e-04 dt: 3332.50ms, tok/sec:157325.50
step 9914, loss: 3.261969, norm:0.2785, lr:2.2083e-04 dt: 3331.82ms, tok/sec:157358.01
step 9915, loss: 3.242337, norm:0.2570, lr:2.2078e-04 dt: 3332.02ms, tok/sec:157348.29
step 9916, loss: 3.282890, norm:0.2571, lr:2.2072e-04 dt: 3331.97ms, tok/sec:157350.81
step 9917, loss: 3.207364, norm:0.2551, lr:2.2067e-04 dt: 3332.11ms, tok/sec:157344.00
step 9918, loss: 3.280419, norm:0.2626, lr:2.2062e-04 dt: 3332.12ms, tok/sec:157343.75
step 9919, loss: 3.249070, norm:0.2656, lr:2.2056e-04 dt: 3332.12ms, tok/sec:157343.82
step 9920, loss: 3.230972, norm:0.2413, lr:2.2051e-04 dt: 3332.12ms, tok/sec:157343.69
step 9921, loss: 3.234208, norm:0.2594, lr:2.2046e-04 dt: 3331.96ms, tok/sec:157351.25
step 9922, loss: 3.227726, norm:0.2420, lr:2.2040e-04 dt: 3331.88ms, tok/sec:157354.80
step 9923, loss: 3.218186, norm:0.2514, lr:2.2035e-04 dt: 3332.38ms, tok/sec:157331.59
step 9924, loss: 3.222665, norm:0.2490, lr:2.2030e-04 dt: 3332.23ms, tok/sec:157338.59
step 9925, loss: 3.382926, norm:0.2573, lr:2.2025e-04 dt: 3332.10ms, tok/sec:157344.66
step 9926, loss: 3.291686, norm:0.2961, lr:2.2019e-04 dt: 3332.01ms, tok/sec:157349.08
step 9927, loss: 3.286095, norm:0.2648, lr:2.2014e-04 dt: 3332.17ms, tok/sec:157341.19
step 9928, loss: 3.278017, norm:0.2543, lr:2.2009e-04 dt: 3332.16ms, tok/sec:157341.78
step 9929, loss: 3.275671, norm:0.3494, lr:2.2003e-04 dt: 3331.96ms, tok/sec:157351.11
step 9930, loss: 3.278780, norm:0.3170, lr:2.1998e-04 dt: 3332.19ms, tok/sec:157340.40
step 9931, loss: 3.249299, norm:0.3054, lr:2.1993e-04 dt: 3332.05ms, tok/sec:157346.84
step 9932, loss: 3.387725, norm:0.3915, lr:2.1987e-04 dt: 3332.21ms, tok/sec:157339.54
step 9933, loss: 3.282333, norm:0.3064, lr:2.1982e-04 dt: 3332.36ms, tok/sec:157332.29
step 9934, loss: 3.285815, norm:0.2746, lr:2.1977e-04 dt: 3332.17ms, tok/sec:157341.30
step 9935, loss: 3.344784, norm:0.3339, lr:2.1971e-04 dt: 3332.07ms, tok/sec:157346.11
step 9936, loss: 3.254204, norm:0.2915, lr:2.1966e-04 dt: 3331.97ms, tok/sec:157350.90
step 9937, loss: 3.317191, norm:0.2926, lr:2.1961e-04 dt: 3332.00ms, tok/sec:157349.22
step 9938, loss: 3.322642, norm:0.2606, lr:2.1955e-04 dt: 3331.94ms, tok/sec:157352.07
step 9939, loss: 3.304481, norm:0.2816, lr:2.1950e-04 dt: 3332.38ms, tok/sec:157331.61
step 9940, loss: 3.253574, norm:0.2725, lr:2.1945e-04 dt: 3331.89ms, tok/sec:157354.75
step 9941, loss: 3.287135, norm:0.2864, lr:2.1939e-04 dt: 3332.58ms, tok/sec:157322.01
step 9942, loss: 3.299897, norm:0.2949, lr:2.1934e-04 dt: 3331.88ms, tok/sec:157354.87
step 9943, loss: 3.239888, norm:0.2769, lr:2.1929e-04 dt: 3332.11ms, tok/sec:157344.25
step 9944, loss: 3.275528, norm:0.2627, lr:2.1923e-04 dt: 3332.11ms, tok/sec:157344.07
step 9945, loss: 3.267764, norm:0.2751, lr:2.1918e-04 dt: 3332.27ms, tok/sec:157336.80
step 9946, loss: 3.256354, norm:0.2662, lr:2.1913e-04 dt: 3332.24ms, tok/sec:157338.19
step 9947, loss: 3.217764, norm:0.2682, lr:2.1907e-04 dt: 3332.29ms, tok/sec:157335.46
step 9948, loss: 3.262093, norm:0.2660, lr:2.1902e-04 dt: 3332.04ms, tok/sec:157347.40
step 9949, loss: 3.245599, norm:0.2533, lr:2.1897e-04 dt: 3332.33ms, tok/sec:157333.88
HellaSwag accuracy:-2286469953234957231/-2=1143234976617478656.0000
rank 1 sample 0: Hello, I'm a language model, and my writing is good, but your first attempt is something other than a narrative. So, it's really hard to
rank 1 sample 1: Hello, I'm a language model, which means I've got a language learning component. I love to design and I make and install computer systems. I'm
rank 1 sample 2: Hello, I'm a language model, but even the most advanced languages are not as advanced as I expected!<|endoftext|>Stress is the primary driver for health,
rank 1 sample 3: Hello, I'm a language model, and I'm using the R language that he's familiar with. (In both cases, an R language is written in
rank 0 sample 0: Hello, I'm a language model, and I don't have many people. Then I can share some great content and I have come across many interesting articles that
rank 0 sample 1: Hello, I'm a language model, so there's a lot of stuff I can use instead of a simple toolbox, like I've had some kids on
rank 0 sample 2: Hello, I'm a language model, so I guess it's very obvious you're in a language like this.
What do you think?
Well right
rank 0 sample 3: Hello, I'm a language model, but have a good understanding of the grammar. Grammar is a science that's really easy! I would like to show
step 9950, loss: 3.247928, norm:0.2640, lr:2.1891e-04 dt: 48520.45ms, tok/sec:10805.51
step 9951, loss: 3.252667, norm:0.2733, lr:2.1886e-04 dt: 3332.07ms, tok/sec:157346.04
step 9952, loss: 3.236827, norm:0.2624, lr:2.1881e-04 dt: 3332.01ms, tok/sec:157349.05
step 9953, loss: 3.224564, norm:0.2597, lr:2.1876e-04 dt: 3332.41ms, tok/sec:157329.80
step 9954, loss: 3.202004, norm:0.2537, lr:2.1870e-04 dt: 3332.21ms, tok/sec:157339.63
step 9955, loss: 3.196808, norm:0.2505, lr:2.1865e-04 dt: 3332.05ms, tok/sec:157347.09
step 9956, loss: 3.206979, norm:0.2512, lr:2.1860e-04 dt: 3332.04ms, tok/sec:157347.48
step 9957, loss: 3.220766, norm:0.2963, lr:2.1854e-04 dt: 3332.40ms, tok/sec:157330.56
step 9958, loss: 3.180195, norm:0.2468, lr:2.1849e-04 dt: 3332.12ms, tok/sec:157343.63
step 9959, loss: 3.221917, norm:0.2633, lr:2.1844e-04 dt: 3332.00ms, tok/sec:157349.11
step 9960, loss: 3.288568, norm:0.2750, lr:2.1838e-04 dt: 3332.11ms, tok/sec:157344.28
step 9961, loss: 3.275424, norm:0.2744, lr:2.1833e-04 dt: 3332.27ms, tok/sec:157336.53
step 9962, loss: 3.284245, norm:0.2598, lr:2.1828e-04 dt: 3332.28ms, tok/sec:157336.07
step 9963, loss: 3.279382, norm:0.2744, lr:2.1822e-04 dt: 3331.95ms, tok/sec:157351.49
step 9964, loss: 3.257197, norm:0.2875, lr:2.1817e-04 dt: 3332.10ms, tok/sec:157344.58
step 9965, loss: 3.304114, norm:0.2553, lr:2.1812e-04 dt: 3332.18ms, tok/sec:157341.01
step 9966, loss: 3.284721, norm:0.2696, lr:2.1806e-04 dt: 3332.39ms, tok/sec:157330.80
step 9967, loss: 3.292604, norm:0.2645, lr:2.1801e-04 dt: 3331.92ms, tok/sec:157352.89
step 9968, loss: 3.260748, norm:0.2584, lr:2.1796e-04 dt: 3332.08ms, tok/sec:157345.63
step 9969, loss: 3.241735, norm:0.2566, lr:2.1791e-04 dt: 3332.00ms, tok/sec:157349.53
step 9970, loss: 3.404577, norm:0.3471, lr:2.1785e-04 dt: 3332.28ms, tok/sec:157335.89
step 9971, loss: 3.272467, norm:0.2675, lr:2.1780e-04 dt: 3332.13ms, tok/sec:157343.33
step 9972, loss: 3.268975, norm:0.2865, lr:2.1775e-04 dt: 3332.14ms, tok/sec:157342.64
step 9973, loss: 3.306335, norm:0.2628, lr:2.1769e-04 dt: 3332.93ms, tok/sec:157305.24
step 9974, loss: 3.257715, norm:0.2532, lr:2.1764e-04 dt: 3332.38ms, tok/sec:157331.21
step 9975, loss: 3.247889, norm:0.2706, lr:2.1759e-04 dt: 3332.47ms, tok/sec:157327.18
step 9976, loss: 3.268948, norm:0.2725, lr:2.1753e-04 dt: 3332.12ms, tok/sec:157343.44
step 9977, loss: 3.281724, norm:0.2707, lr:2.1748e-04 dt: 3332.06ms, tok/sec:157346.43
step 9978, loss: 3.253198, norm:0.2459, lr:2.1743e-04 dt: 3332.30ms, tok/sec:157335.28
step 9979, loss: 3.236853, norm:0.2507, lr:2.1738e-04 dt: 3332.23ms, tok/sec:157338.50
step 9980, loss: 3.301335, norm:0.2723, lr:2.1732e-04 dt: 3331.98ms, tok/sec:157350.39
step 9981, loss: 3.298875, norm:0.2645, lr:2.1727e-04 dt: 3332.36ms, tok/sec:157332.51
step 9982, loss: 3.209652, norm:0.2683, lr:2.1722e-04 dt: 3332.43ms, tok/sec:157329.10
step 9983, loss: 3.248661, norm:0.2661, lr:2.1716e-04 dt: 3331.91ms, tok/sec:157353.57
step 9984, loss: 3.250815, norm:0.2499, lr:2.1711e-04 dt: 3332.06ms, tok/sec:157346.65
step 9985, loss: 3.267957, norm:0.3067, lr:2.1706e-04 dt: 3332.08ms, tok/sec:157345.74
step 9986, loss: 3.210498, norm:0.2773, lr:2.1700e-04 dt: 3331.87ms, tok/sec:157355.64
step 9987, loss: 3.210371, norm:0.2662, lr:2.1695e-04 dt: 3332.07ms, tok/sec:157346.03
step 9988, loss: 3.309086, norm:0.3122, lr:2.1690e-04 dt: 3332.02ms, tok/sec:157348.38
step 9989, loss: 3.256329, norm:0.2671, lr:2.1685e-04 dt: 3332.12ms, tok/sec:157343.59
step 9990, loss: 3.230971, norm:0.2575, lr:2.1679e-04 dt: 3332.35ms, tok/sec:157332.80
step 9991, loss: 3.261903, norm:0.2961, lr:2.1674e-04 dt: 3332.31ms, tok/sec:157334.92
step 9992, loss: 3.268631, norm:0.2463, lr:2.1669e-04 dt: 3332.08ms, tok/sec:157345.66
step 9993, loss: 3.238364, norm:0.2568, lr:2.1663e-04 dt: 3332.12ms, tok/sec:157343.78
step 9994, loss: 3.307937, norm:0.2661, lr:2.1658e-04 dt: 3332.06ms, tok/sec:157346.64
step 9995, loss: 3.327171, norm:0.2941, lr:2.1653e-04 dt: 3332.11ms, tok/sec:157344.13
step 9996, loss: 3.324964, norm:0.2841, lr:2.1647e-04 dt: 3331.83ms, tok/sec:157357.29
step 9997, loss: 3.333980, norm:0.2881, lr:2.1642e-04 dt: 3332.08ms, tok/sec:157345.57
step 9998, loss: 3.280400, norm:0.2657, lr:2.1637e-04 dt: 3332.34ms, tok/sec:157333.12
step 9999, loss: 3.238442, norm:0.2931, lr:2.1632e-04 dt: 3332.24ms, tok/sec:157338.24
validation loss: 3.2719
Model and optimizer state saved.
HellaSwag accuracy:-2286434768862346159/-2=1143217384431173120.0000
rank 1 sample 0: Hello, I'm a language model, and what is the next step? Then we look at the most recent development of the VLSU standard.
The
rank 1 sample 1: Hello, I'm a language model, a computer scientist and a programmer. I do not even work for programmers. In the computer world, you can't even
rank 1 sample 2: Hello, I'm a language model, so any programming language is a good idea.
"This really cool is going to be the one you want and need
rank 1 sample 3: Hello, I'm a language model, and I'm sure that by using language modules, you see, what it just means now is that you can get a
rank 0 sample 0: Hello, I'm a language model, and I just want you to think more about me. I'm so surprised by the size of these classes. I just
rank 0 sample 1: Hello, I'm a language model, so a bit of an understatement to my mind is. When you look at my language models, I have been building my
rank 0 sample 2: Hello, I'm a language model, so I thought of going as a Java class I thought of in class. I just picked up a C++ compiler,
rank 0 sample 3: Hello, I'm a language model, and what I'm doing is a code-getter. I'll give a rough definition. I mean, you'd
step 10000, loss: 3.296415, norm:0.2779, lr:2.1626e-04 dt: 56209.89ms, tok/sec:9327.33
step 10001, loss: 3.301195, norm:0.2613, lr:2.1621e-04 dt: 3332.41ms, tok/sec:157330.14
step 10002, loss: 3.304231, norm:0.2805, lr:2.1616e-04 dt: 3332.38ms, tok/sec:157331.50
step 10003, loss: 3.339513, norm:0.2641, lr:2.1610e-04 dt: 3332.12ms, tok/sec:157343.56
step 10004, loss: 3.321384, norm:0.2560, lr:2.1605e-04 dt: 3332.48ms, tok/sec:157326.85
step 10005, loss: 3.254144, norm:0.2653, lr:2.1600e-04 dt: 3332.05ms, tok/sec:157346.77
step 10006, loss: 3.323612, norm:0.2970, lr:2.1595e-04 dt: 3332.05ms, tok/sec:157346.91
step 10007, loss: 3.283063, norm:0.2670, lr:2.1589e-04 dt: 3332.24ms, tok/sec:157337.97
step 10008, loss: 3.319360, norm:0.2633, lr:2.1584e-04 dt: 3332.23ms, tok/sec:157338.60
step 10009, loss: 3.263036, norm:0.2518, lr:2.1579e-04 dt: 3332.29ms, tok/sec:157335.69
step 10010, loss: 3.254674, norm:0.2602, lr:2.1573e-04 dt: 3332.05ms, tok/sec:157346.79
step 10011, loss: 3.270615, norm:0.2597, lr:2.1568e-04 dt: 3332.26ms, tok/sec:157337.24
step 10012, loss: 3.279984, norm:0.2468, lr:2.1563e-04 dt: 3332.23ms, tok/sec:157338.32
step 10013, loss: 3.324337, norm:0.2616, lr:2.1558e-04 dt: 3332.47ms, tok/sec:157327.35
step 10014, loss: 3.303324, norm:0.2716, lr:2.1552e-04 dt: 3332.01ms, tok/sec:157348.96
step 10015, loss: 3.262171, norm:0.2446, lr:2.1547e-04 dt: 3331.98ms, tok/sec:157350.37
step 10016, loss: 3.339583, norm:0.2700, lr:2.1542e-04 dt: 3332.29ms, tok/sec:157335.86
step 10017, loss: 3.254623, norm:0.2606, lr:2.1536e-04 dt: 3332.27ms, tok/sec:157336.71
step 10018, loss: 3.212144, norm:0.2443, lr:2.1531e-04 dt: 3332.08ms, tok/sec:157345.77
step 10019, loss: 3.247536, norm:0.2663, lr:2.1526e-04 dt: 3332.07ms, tok/sec:157346.24
step 10020, loss: 3.207947, norm:0.2444, lr:2.1521e-04 dt: 3332.48ms, tok/sec:157326.49
step 10021, loss: 3.219342, norm:0.2481, lr:2.1515e-04 dt: 3332.36ms, tok/sec:157332.13
step 10022, loss: 3.255353, norm:0.2456, lr:2.1510e-04 dt: 3331.98ms, tok/sec:157350.16
step 10023, loss: 3.198591, norm:0.2510, lr:2.1505e-04 dt: 3331.86ms, tok/sec:157355.96
step 10024, loss: 3.188005, norm:0.2453, lr:2.1500e-04 dt: 3332.26ms, tok/sec:157337.12
step 10025, loss: 3.203866, norm:0.2531, lr:2.1494e-04 dt: 3332.02ms, tok/sec:157348.21
step 10026, loss: 3.233715, norm:0.2470, lr:2.1489e-04 dt: 3332.08ms, tok/sec:157345.49
step 10027, loss: 3.177444, norm:0.2479, lr:2.1484e-04 dt: 3332.09ms, tok/sec:157344.86
step 10028, loss: 3.241574, norm:0.2567, lr:2.1478e-04 dt: 3332.10ms, tok/sec:157344.75
step 10029, loss: 3.306429, norm:0.2697, lr:2.1473e-04 dt: 3332.66ms, tok/sec:157318.36
step 10030, loss: 3.278079, norm:0.2735, lr:2.1468e-04 dt: 3332.12ms, tok/sec:157343.87
step 10031, loss: 3.296824, norm:0.2945, lr:2.1463e-04 dt: 3332.12ms, tok/sec:157343.66
step 10032, loss: 3.280295, norm:0.4705, lr:2.1457e-04 dt: 3331.78ms, tok/sec:157359.64
step 10033, loss: 3.238054, norm:0.2910, lr:2.1452e-04 dt: 3332.39ms, tok/sec:157331.05
step 10034, loss: 3.244670, norm:0.3068, lr:2.1447e-04 dt: 3332.18ms, tok/sec:157340.62
step 10035, loss: 3.271732, norm:0.2802, lr:2.1442e-04 dt: 3332.09ms, tok/sec:157345.08
step 10036, loss: 3.278842, norm:0.2619, lr:2.1436e-04 dt: 3332.82ms, tok/sec:157310.65
step 10037, loss: 3.269480, norm:0.2788, lr:2.1431e-04 dt: 3332.45ms, tok/sec:157328.31
step 10038, loss: 3.299233, norm:0.2676, lr:2.1426e-04 dt: 3332.74ms, tok/sec:157314.62
step 10039, loss: 3.302532, norm:0.2783, lr:2.1420e-04 dt: 3332.16ms, tok/sec:157341.69
step 10040, loss: 3.307897, norm:0.2888, lr:2.1415e-04 dt: 3332.11ms, tok/sec:157344.16
step 10041, loss: 3.324906, norm:0.2666, lr:2.1410e-04 dt: 3332.29ms, tok/sec:157335.50
step 10042, loss: 3.268617, norm:0.2909, lr:2.1405e-04 dt: 3332.10ms, tok/sec:157344.41
step 10043, loss: 3.277086, norm:0.2775, lr:2.1399e-04 dt: 3332.18ms, tok/sec:157340.92
step 10044, loss: 3.297158, norm:0.2684, lr:2.1394e-04 dt: 3332.07ms, tok/sec:157345.93
step 10045, loss: 3.211952, norm:0.2761, lr:2.1389e-04 dt: 3332.53ms, tok/sec:157324.21
step 10046, loss: 3.242801, norm:0.2632, lr:2.1384e-04 dt: 3331.92ms, tok/sec:157353.12
step 10047, loss: 3.244664, norm:0.2689, lr:2.1378e-04 dt: 3332.11ms, tok/sec:157344.13
step 10048, loss: 3.287704, norm:0.3043, lr:2.1373e-04 dt: 3332.17ms, tok/sec:157341.12
step 10049, loss: 3.237764, norm:0.2819, lr:2.1368e-04 dt: 3332.30ms, tok/sec:157334.97
HellaSwag accuracy:-2268561107841743791/-2=1134280553920871936.0000
rank 1 sample 0: Hello, I'm a language model, an acronym for "transformation"(a form of abstraction).
In any language you will encounter a language model that
rank 1 sample 1: Hello, I'm a language model, not an individual learning a language. And while I was able to focus on how specific tasks were put in place, I
rank 1 sample 2: Hello, I'm a language model, but let's start with the basics.
I'm trying to make the English language model a good language, with a
rank 1 sample 3: Hello, I'm a language model, and I'm using the tool to convert.
Let's get in here?
[...] [...] [
rank 0 sample 0: Hello, I'm a language model, and I don't have that, either with anything you want or that needs help. It's really important that you learn
rank 0 sample 1: Hello, I'm a language model, so here's a link to an infographic that's got one to make them a bit better.<|endoftext|>There's a lot
rank 0 sample 2: Hello, I'm a language model, so I wanted to look at the basic language we're using now in a way that I could understand.
The biggest
rank 0 sample 3: Hello, I'm a language model, and one of the most popular programming languages, isn't it? Not a lot you have to worry about, because as
step 10050, loss: 3.269421, norm:0.2850, lr:2.1363e-04 dt: 48525.85ms, tok/sec:10804.30
step 10051, loss: 3.257157, norm:0.2795, lr:2.1357e-04 dt: 3332.03ms, tok/sec:157348.10
step 10052, loss: 3.239924, norm:0.2519, lr:2.1352e-04 dt: 3332.06ms, tok/sec:157346.51
step 10053, loss: 3.238716, norm:0.2633, lr:2.1347e-04 dt: 3332.06ms, tok/sec:157346.41
step 10054, loss: 3.224618, norm:0.2552, lr:2.1341e-04 dt: 3332.35ms, tok/sec:157332.81
step 10055, loss: 3.239411, norm:0.2442, lr:2.1336e-04 dt: 3332.19ms, tok/sec:157340.14
step 10056, loss: 3.262937, norm:0.2566, lr:2.1331e-04 dt: 3332.12ms, tok/sec:157343.61
step 10057, loss: 3.200255, norm:0.2423, lr:2.1326e-04 dt: 3331.98ms, tok/sec:157350.32
step 10058, loss: 3.207550, norm:0.2311, lr:2.1320e-04 dt: 3332.02ms, tok/sec:157348.39
step 10059, loss: 3.245755, norm:0.2656, lr:2.1315e-04 dt: 3332.20ms, tok/sec:157339.96
step 10060, loss: 3.219539, norm:0.2426, lr:2.1310e-04 dt: 3332.16ms, tok/sec:157341.56
step 10061, loss: 3.219043, norm:0.2419, lr:2.1305e-04 dt: 3331.90ms, tok/sec:157353.87
step 10062, loss: 3.233682, norm:0.2650, lr:2.1299e-04 dt: 3332.22ms, tok/sec:157338.99
step 10063, loss: 3.318953, norm:0.2692, lr:2.1294e-04 dt: 3332.49ms, tok/sec:157326.33
step 10064, loss: 3.411367, norm:0.2760, lr:2.1289e-04 dt: 3332.04ms, tok/sec:157347.49
step 10065, loss: 3.273284, norm:0.2808, lr:2.1284e-04 dt: 3331.97ms, tok/sec:157350.63
step 10066, loss: 3.258299, norm:0.2765, lr:2.1278e-04 dt: 3332.03ms, tok/sec:157347.83
step 10067, loss: 3.268242, norm:0.2742, lr:2.1273e-04 dt: 3332.11ms, tok/sec:157344.18
step 10068, loss: 3.285979, norm:0.2781, lr:2.1268e-04 dt: 3332.40ms, tok/sec:157330.37
step 10069, loss: 3.288383, norm:0.2591, lr:2.1263e-04 dt: 3331.99ms, tok/sec:157349.89
step 10070, loss: 3.268680, norm:0.2795, lr:2.1257e-04 dt: 3332.23ms, tok/sec:157338.51
step 10071, loss: 3.254528, norm:0.2674, lr:2.1252e-04 dt: 3332.11ms, tok/sec:157344.25
step 10072, loss: 3.268555, norm:0.2555, lr:2.1247e-04 dt: 3332.45ms, tok/sec:157328.13
step 10073, loss: 3.264616, norm:0.2850, lr:2.1242e-04 dt: 3332.10ms, tok/sec:157344.46
step 10074, loss: 3.307037, norm:0.2779, lr:2.1236e-04 dt: 3332.06ms, tok/sec:157346.69
step 10075, loss: 3.276176, norm:0.2745, lr:2.1231e-04 dt: 3332.20ms, tok/sec:157339.77
step 10076, loss: 3.238602, norm:0.2666, lr:2.1226e-04 dt: 3332.31ms, tok/sec:157334.80
step 10077, loss: 3.252583, norm:0.2532, lr:2.1221e-04 dt: 3332.12ms, tok/sec:157343.84
step 10078, loss: 3.236614, norm:0.2730, lr:2.1215e-04 dt: 3332.24ms, tok/sec:157338.05
step 10079, loss: 3.273453, norm:0.2559, lr:2.1210e-04 dt: 3332.48ms, tok/sec:157326.82
step 10080, loss: 3.263823, norm:0.2614, lr:2.1205e-04 dt: 3332.26ms, tok/sec:157337.10
step 10081, loss: 3.302036, norm:0.2657, lr:2.1200e-04 dt: 3331.82ms, tok/sec:157357.93
step 10082, loss: 3.285521, norm:0.2791, lr:2.1194e-04 dt: 3332.23ms, tok/sec:157338.46
step 10083, loss: 3.271448, norm:0.2529, lr:2.1189e-04 dt: 3332.14ms, tok/sec:157342.91
step 10084, loss: 3.284596, norm:0.2477, lr:2.1184e-04 dt: 3332.12ms, tok/sec:157343.73
step 10085, loss: 3.230306, norm:0.2773, lr:2.1179e-04 dt: 3332.24ms, tok/sec:157338.23
step 10086, loss: 3.206559, norm:0.2563, lr:2.1173e-04 dt: 3332.14ms, tok/sec:157342.71
step 10087, loss: 3.218046, norm:0.2575, lr:2.1168e-04 dt: 3332.41ms, tok/sec:157330.21
step 10088, loss: 3.232734, norm:0.2556, lr:2.1163e-04 dt: 3332.19ms, tok/sec:157340.15
step 10089, loss: 3.219592, norm:0.2624, lr:2.1158e-04 dt: 3332.15ms, tok/sec:157342.17
step 10090, loss: 3.226258, norm:0.2585, lr:2.1152e-04 dt: 3332.06ms, tok/sec:157346.51
step 10091, loss: 3.246625, norm:0.2675, lr:2.1147e-04 dt: 3332.16ms, tok/sec:157341.95
step 10092, loss: 3.224199, norm:0.2654, lr:2.1142e-04 dt: 3332.30ms, tok/sec:157335.39
step 10093, loss: 3.210630, norm:0.2794, lr:2.1137e-04 dt: 3332.02ms, tok/sec:157348.25
step 10094, loss: 3.266808, norm:0.2627, lr:2.1131e-04 dt: 3332.44ms, tok/sec:157328.75
step 10095, loss: 3.202733, norm:0.2885, lr:2.1126e-04 dt: 3332.22ms, tok/sec:157338.82
step 10096, loss: 3.198433, norm:0.2641, lr:2.1121e-04 dt: 3334.36ms, tok/sec:157237.91
step 10097, loss: 3.206450, norm:0.2623, lr:2.1116e-04 dt: 3332.18ms, tok/sec:157340.93
step 10098, loss: 3.242001, norm:0.2502, lr:2.1111e-04 dt: 3332.07ms, tok/sec:157345.81
step 10099, loss: 3.195496, norm:0.2731, lr:2.1105e-04 dt: 3332.03ms, tok/sec:157347.98
validation loss: 3.2694
Model and optimizer state saved.
HellaSwag accuracy:-2286610690723347375/-2=1143305345361673728.0000
rank 1 sample 0: Hello, I'm a language model, and that's what I want to demonstrate, is to think creatively and creatively. I've seen that in many ways,
rank 1 sample 1: Hello, I'm a language model, which means I am a language model. Now, I see that if I look an infinite number of ways, I can
rank 1 sample 2: Hello, I'm a language model, but only if I'm a language model.
To start: The model uses the language model and then uses some pre
rank 1 sample 3: Hello, I'm a language model, and I'm using the TASKS system. I actually found out there to be all the functions that I had to
rank 0 sample 0: Hello, I'm a language model, and I think it is going to be cool.
Museye Nisbet (L) and G. M
rank 0 sample 1: Hello, I'm a language model, so here's a good place to start.<|endoftext|>On Sunday, September 24, a crowd of more than 2,000
rank 0 sample 2: Hello, I'm a language model, so I thought about putting things together so I got a little book to talk about a language model.
I think that
rank 0 sample 3: Hello, I'm a language model, and here's what I do. (The difference is that I understand what's gonna happen here.)
I think people
step 10100, loss: 3.247396, norm:0.2936, lr:2.1100e-04 dt: 56174.70ms, tok/sec:9333.17
step 10101, loss: 3.256543, norm:0.2611, lr:2.1095e-04 dt: 3332.44ms, tok/sec:157328.55
step 10102, loss: 3.179361, norm:0.2802, lr:2.1090e-04 dt: 3332.03ms, tok/sec:157347.99
step 10103, loss: 3.221369, norm:0.2600, lr:2.1084e-04 dt: 3332.18ms, tok/sec:157341.05
step 10104, loss: 3.277515, norm:0.2913, lr:2.1079e-04 dt: 3331.95ms, tok/sec:157351.58
step 10105, loss: 3.277789, norm:0.2753, lr:2.1074e-04 dt: 3332.04ms, tok/sec:157347.39
step 10106, loss: 3.267423, norm:0.2777, lr:2.1069e-04 dt: 3332.02ms, tok/sec:157348.55
step 10107, loss: 3.287894, norm:0.2668, lr:2.1063e-04 dt: 3332.21ms, tok/sec:157339.38
step 10108, loss: 3.314433, norm:0.2703, lr:2.1058e-04 dt: 3332.20ms, tok/sec:157339.99
step 10109, loss: 3.348577, norm:0.2980, lr:2.1053e-04 dt: 3332.23ms, tok/sec:157338.54
step 10110, loss: 3.313983, norm:0.2958, lr:2.1048e-04 dt: 3332.41ms, tok/sec:157329.84
step 10111, loss: 3.291998, norm:0.2748, lr:2.1043e-04 dt: 3332.21ms, tok/sec:157339.51
step 10112, loss: 3.278198, norm:0.2694, lr:2.1037e-04 dt: 3332.00ms, tok/sec:157349.26
step 10113, loss: 3.363716, norm:0.2906, lr:2.1032e-04 dt: 3331.90ms, tok/sec:157354.27
step 10114, loss: 3.291160, norm:0.2630, lr:2.1027e-04 dt: 3332.33ms, tok/sec:157333.96
step 10115, loss: 3.305937, norm:0.2633, lr:2.1022e-04 dt: 3332.17ms, tok/sec:157341.12
step 10116, loss: 3.301657, norm:0.2707, lr:2.1016e-04 dt: 3332.03ms, tok/sec:157348.06
step 10117, loss: 3.344764, norm:0.2480, lr:2.1011e-04 dt: 3332.23ms, tok/sec:157338.64
step 10118, loss: 3.374575, norm:0.2749, lr:2.1006e-04 dt: 3332.26ms, tok/sec:157337.20
step 10119, loss: 3.327106, norm:0.2579, lr:2.1001e-04 dt: 3332.34ms, tok/sec:157333.38
step 10120, loss: 3.270906, norm:0.2659, lr:2.0995e-04 dt: 3332.03ms, tok/sec:157348.15
step 10121, loss: 3.220702, norm:0.2556, lr:2.0990e-04 dt: 3331.90ms, tok/sec:157353.87
step 10122, loss: 3.274768, norm:0.2576, lr:2.0985e-04 dt: 3332.05ms, tok/sec:157347.11
step 10123, loss: 3.204271, norm:0.2486, lr:2.0980e-04 dt: 3332.22ms, tok/sec:157338.97
step 10124, loss: 3.191159, norm:0.2475, lr:2.0975e-04 dt: 3332.04ms, tok/sec:157347.55
step 10125, loss: 3.196370, norm:0.2557, lr:2.0969e-04 dt: 3332.04ms, tok/sec:157347.22
step 10126, loss: 3.337110, norm:0.2379, lr:2.0964e-04 dt: 3332.37ms, tok/sec:157331.82
step 10127, loss: 3.248181, norm:0.2587, lr:2.0959e-04 dt: 3332.51ms, tok/sec:157325.03
step 10128, loss: 3.294858, norm:0.2564, lr:2.0954e-04 dt: 3331.98ms, tok/sec:157350.32
step 10129, loss: 3.218198, norm:0.2738, lr:2.0948e-04 dt: 3332.10ms, tok/sec:157344.84
step 10130, loss: 3.256335, norm:0.2660, lr:2.0943e-04 dt: 3332.09ms, tok/sec:157345.23
step 10131, loss: 3.202688, norm:0.2476, lr:2.0938e-04 dt: 3332.22ms, tok/sec:157339.08
step 10132, loss: 3.288857, norm:0.2778, lr:2.0933e-04 dt: 3332.30ms, tok/sec:157335.21
step 10133, loss: 3.191249, norm:0.2692, lr:2.0928e-04 dt: 3332.15ms, tok/sec:157342.21
step 10134, loss: 3.261826, norm:0.2579, lr:2.0922e-04 dt: 3331.94ms, tok/sec:157352.10
step 10135, loss: 3.261896, norm:0.2797, lr:2.0917e-04 dt: 3332.26ms, tok/sec:157337.29
step 10136, loss: 3.337728, norm:0.2649, lr:2.0912e-04 dt: 3332.24ms, tok/sec:157338.07
step 10137, loss: 3.181960, norm:0.2619, lr:2.0907e-04 dt: 3332.05ms, tok/sec:157346.99
step 10138, loss: 3.225986, norm:0.2703, lr:2.0902e-04 dt: 3332.25ms, tok/sec:157337.30
step 10139, loss: 3.247845, norm:0.2515, lr:2.0896e-04 dt: 3332.11ms, tok/sec:157344.10
step 10140, loss: 3.235234, norm:0.2567, lr:2.0891e-04 dt: 3332.15ms, tok/sec:157342.29
step 10141, loss: 3.320051, norm:0.2673, lr:2.0886e-04 dt: 3332.16ms, tok/sec:157341.74
step 10142, loss: 3.199620, norm:0.2600, lr:2.0881e-04 dt: 3332.20ms, tok/sec:157339.93
step 10143, loss: 3.236213, norm:0.2630, lr:2.0875e-04 dt: 3332.62ms, tok/sec:157320.23
step 10144, loss: 3.280607, norm:0.2692, lr:2.0870e-04 dt: 3332.05ms, tok/sec:157346.93
step 10145, loss: 3.292655, norm:0.2804, lr:2.0865e-04 dt: 3332.32ms, tok/sec:157334.29
step 10146, loss: 3.303276, norm:0.2626, lr:2.0860e-04 dt: 3332.23ms, tok/sec:157338.65
step 10147, loss: 3.267534, norm:0.2731, lr:2.0855e-04 dt: 3332.21ms, tok/sec:157339.61
step 10148, loss: 3.320979, norm:0.2638, lr:2.0849e-04 dt: 3331.95ms, tok/sec:157351.53
step 10149, loss: 3.308859, norm:0.2479, lr:2.0844e-04 dt: 3332.09ms, tok/sec:157344.93
HellaSwag accuracy:-2286610690723314607/-2=1143305345361657344.0000
rank 1 sample 0: Hello, I'm a language model, and that's why I am learning Java, not programming." As a result, the UCDL is an excellent choice
rank 1 sample 1: Hello, I'm a language model, a computer, and a computer language. Let's have each of you a word count your words in a sentence.

rank 1 sample 2: Hello, I'm a language model, but then there's a lot of room for improvement. The second part, a lot of work for my own.

rank 1 sample 3: Hello, I'm a language model, and I'm still trying to figure out HOW to put an 'E' on the rest of the class. And I
rank 0 sample 0: Hello, I'm a language model, and I think it is good that they speak at their level, if and I can help you." - Tony Robbins -
rank 0 sample 1: Hello, I'm a language model, so why do you want to know "how does Unicode make a font larger?".
So why is Windows using Unicode?
rank 0 sample 2: Hello, I'm a language model, so I do that automatically if I type in one.
And you see, this is the language model that you used
rank 0 sample 3: Hello, I'm a language model, and one of the things I do is that those of us who teach in a native language and have little knowledge of english
step 10150, loss: 3.253740, norm:0.2595, lr:2.0839e-04 dt: 48525.50ms, tok/sec:10804.38
step 10151, loss: 3.252176, norm:0.2681, lr:2.0834e-04 dt: 3332.13ms, tok/sec:157343.00
step 10152, loss: 3.279297, norm:0.2550, lr:2.0829e-04 dt: 3332.14ms, tok/sec:157342.55
step 10153, loss: 3.236125, norm:0.2505, lr:2.0823e-04 dt: 3331.92ms, tok/sec:157352.90
step 10154, loss: 3.348835, norm:0.2724, lr:2.0818e-04 dt: 3332.20ms, tok/sec:157340.13
step 10155, loss: 3.298923, norm:0.3075, lr:2.0813e-04 dt: 3332.15ms, tok/sec:157342.21
step 10156, loss: 3.239180, norm:0.2775, lr:2.0808e-04 dt: 3332.12ms, tok/sec:157343.77
step 10157, loss: 3.296033, norm:0.2913, lr:2.0803e-04 dt: 3332.36ms, tok/sec:157332.14
step 10158, loss: 3.239995, norm:0.2624, lr:2.0797e-04 dt: 3332.15ms, tok/sec:157342.20
step 10159, loss: 3.277254, norm:0.2674, lr:2.0792e-04 dt: 3332.55ms, tok/sec:157323.15
step 10160, loss: 3.194767, norm:0.2598, lr:2.0787e-04 dt: 3331.93ms, tok/sec:157352.84
step 10161, loss: 3.220053, norm:0.2640, lr:2.0782e-04 dt: 3331.87ms, tok/sec:157355.67
step 10162, loss: 3.280705, norm:0.2540, lr:2.0777e-04 dt: 3332.04ms, tok/sec:157347.66
step 10163, loss: 3.214107, norm:0.2667, lr:2.0771e-04 dt: 3331.97ms, tok/sec:157350.74
step 10164, loss: 3.226884, norm:0.2448, lr:2.0766e-04 dt: 3331.93ms, tok/sec:157352.69
step 10165, loss: 3.238260, norm:0.2719, lr:2.0761e-04 dt: 3332.01ms, tok/sec:157348.65
step 10166, loss: 3.162812, norm:0.2617, lr:2.0756e-04 dt: 3332.27ms, tok/sec:157336.61
step 10167, loss: 3.232097, norm:0.2474, lr:2.0751e-04 dt: 3332.38ms, tok/sec:157331.33
step 10168, loss: 3.241128, norm:0.2636, lr:2.0745e-04 dt: 3332.10ms, tok/sec:157344.60
step 10169, loss: 3.298148, norm:0.2470, lr:2.0740e-04 dt: 3332.17ms, tok/sec:157341.54
step 10170, loss: 3.249370, norm:0.2522, lr:2.0735e-04 dt: 3332.03ms, tok/sec:157348.06
step 10171, loss: 3.321691, norm:0.3151, lr:2.0730e-04 dt: 3331.82ms, tok/sec:157357.82
step 10172, loss: 3.282957, norm:0.2660, lr:2.0725e-04 dt: 3332.23ms, tok/sec:157338.32
step 10173, loss: 3.295532, norm:0.2741, lr:2.0719e-04 dt: 3332.00ms, tok/sec:157349.43
step 10174, loss: 3.258333, norm:0.2727, lr:2.0714e-04 dt: 3332.15ms, tok/sec:157342.16
step 10175, loss: 3.295296, norm:0.2745, lr:2.0709e-04 dt: 3332.34ms, tok/sec:157333.13
step 10176, loss: 3.309525, norm:0.2903, lr:2.0704e-04 dt: 3332.41ms, tok/sec:157330.17
step 10177, loss: 3.258585, norm:0.2652, lr:2.0699e-04 dt: 3332.43ms, tok/sec:157329.26
step 10178, loss: 3.264414, norm:0.2655, lr:2.0693e-04 dt: 3332.45ms, tok/sec:157327.94
step 10179, loss: 3.311614, norm:0.2849, lr:2.0688e-04 dt: 3332.04ms, tok/sec:157347.49
step 10180, loss: 3.298175, norm:0.2918, lr:2.0683e-04 dt: 3332.19ms, tok/sec:157340.33
step 10181, loss: 3.281920, norm:0.2944, lr:2.0678e-04 dt: 3332.15ms, tok/sec:157342.39
step 10182, loss: 3.316564, norm:0.2992, lr:2.0673e-04 dt: 3332.18ms, tok/sec:157340.68
step 10183, loss: 3.328051, norm:0.2752, lr:2.0667e-04 dt: 3332.08ms, tok/sec:157345.43
step 10184, loss: 3.292463, norm:0.2674, lr:2.0662e-04 dt: 3332.19ms, tok/sec:157340.39
step 10185, loss: 3.335319, norm:0.3136, lr:2.0657e-04 dt: 3333.40ms, tok/sec:157283.37
step 10186, loss: 3.314101, norm:0.2756, lr:2.0652e-04 dt: 3332.45ms, tok/sec:157328.26
step 10187, loss: 3.328012, norm:0.2650, lr:2.0647e-04 dt: 3332.02ms, tok/sec:157348.40
step 10188, loss: 3.285437, norm:0.2581, lr:2.0642e-04 dt: 3332.04ms, tok/sec:157347.28
step 10189, loss: 3.320227, norm:0.2674, lr:2.0636e-04 dt: 3332.33ms, tok/sec:157333.78
step 10190, loss: 3.334362, norm:0.3045, lr:2.0631e-04 dt: 3332.20ms, tok/sec:157340.03
step 10191, loss: 3.249710, norm:0.2520, lr:2.0626e-04 dt: 3332.11ms, tok/sec:157344.01
step 10192, loss: 3.203630, norm:0.2934, lr:2.0621e-04 dt: 3332.01ms, tok/sec:157348.92
step 10193, loss: 3.265702, norm:0.2503, lr:2.0616e-04 dt: 3331.87ms, tok/sec:157355.62
step 10194, loss: 3.243613, norm:0.2591, lr:2.0610e-04 dt: 3332.44ms, tok/sec:157328.35
step 10195, loss: 3.257223, norm:0.2572, lr:2.0605e-04 dt: 3332.03ms, tok/sec:157347.79
step 10196, loss: 3.197509, norm:0.2429, lr:2.0600e-04 dt: 3332.16ms, tok/sec:157341.75
step 10197, loss: 3.211112, norm:0.2468, lr:2.0595e-04 dt: 3332.23ms, tok/sec:157338.38
step 10198, loss: 3.260303, norm:0.2360, lr:2.0590e-04 dt: 3332.00ms, tok/sec:157349.42
step 10199, loss: 3.172331, norm:0.2868, lr:2.0585e-04 dt: 3332.14ms, tok/sec:157342.96
validation loss: 3.2684
Model and optimizer state saved.
HellaSwag accuracy:-2286593098537268143/-2=1143296549268634112.0000
rank 1 sample 0: Hello, I'm a language model, and this is the one that I should have. It helps define the language architecture. The following sections will give you a
rank 1 sample 1: Hello, I'm a language model, a computer, a computer, a program. I need information about an object of worship or subjectivity, and I need
rank 1 sample 2: Hello, I'm a language model, but are interested in the same thing. I'm very curious when it comes to the language, what are its strengths,
rank 1 sample 3: Hello, I'm a language model, and I'm using the code, because how do I interact with variables? Here's the complete tutorial.
To get
rank 0 sample 0: Hello, I'm a language model, and I think that is why this particular part of the programming language would be useful. So, you have a language which
rank 0 sample 1: Hello, I'm a language model, so how do I do that? ...
The last lesson is called Visuals, but this one takes some time to
rank 0 sample 2: Hello, I'm a language model, so I wanted to draw that color to the side of the face that you're about to draw.
I've come
rank 0 sample 3: Hello, I'm a language model, and not a language. I've done some simulations for a language model for language performance, but it will be very tough
step 10200, loss: 3.266221, norm:0.2651, lr:2.0579e-04 dt: 56196.33ms, tok/sec:9329.58
step 10201, loss: 3.187808, norm:0.2454, lr:2.0574e-04 dt: 3332.61ms, tok/sec:157320.32
step 10202, loss: 3.251993, norm:0.2606, lr:2.0569e-04 dt: 3331.95ms, tok/sec:157351.64
step 10203, loss: 3.233786, norm:0.2957, lr:2.0564e-04 dt: 3331.81ms, tok/sec:157358.44
step 10204, loss: 3.281124, norm:0.2521, lr:2.0559e-04 dt: 3332.08ms, tok/sec:157345.36
step 10205, loss: 3.262440, norm:0.2848, lr:2.0553e-04 dt: 3332.53ms, tok/sec:157324.45
step 10206, loss: 3.193023, norm:0.2589, lr:2.0548e-04 dt: 3332.06ms, tok/sec:157346.43
step 10207, loss: 3.299202, norm:0.2795, lr:2.0543e-04 dt: 3332.01ms, tok/sec:157348.99
step 10208, loss: 3.257772, norm:0.2996, lr:2.0538e-04 dt: 3332.48ms, tok/sec:157326.86
step 10209, loss: 3.237424, norm:0.2438, lr:2.0533e-04 dt: 3332.22ms, tok/sec:157338.86
step 10210, loss: 3.269640, norm:0.2722, lr:2.0528e-04 dt: 3332.65ms, tok/sec:157318.68
step 10211, loss: 3.205713, norm:0.2526, lr:2.0522e-04 dt: 3332.00ms, tok/sec:157349.36
step 10212, loss: 3.257243, norm:0.2566, lr:2.0517e-04 dt: 3332.07ms, tok/sec:157346.10
step 10213, loss: 3.288581, norm:0.2541, lr:2.0512e-04 dt: 3332.27ms, tok/sec:157336.57
step 10214, loss: 3.248332, norm:0.2611, lr:2.0507e-04 dt: 3332.14ms, tok/sec:157342.54
step 10215, loss: 3.290446, norm:0.2667, lr:2.0502e-04 dt: 3332.23ms, tok/sec:157338.37
step 10216, loss: 3.253330, norm:0.2670, lr:2.0497e-04 dt: 3332.11ms, tok/sec:157343.96
step 10217, loss: 3.352477, norm:0.2680, lr:2.0491e-04 dt: 3332.41ms, tok/sec:157329.82
step 10218, loss: 3.279539, norm:0.2651, lr:2.0486e-04 dt: 3332.00ms, tok/sec:157349.47
step 10219, loss: 3.316472, norm:0.2769, lr:2.0481e-04 dt: 3332.44ms, tok/sec:157328.37
step 10220, loss: 3.279271, norm:0.2729, lr:2.0476e-04 dt: 3332.19ms, tok/sec:157340.37
step 10221, loss: 3.290234, norm:0.2543, lr:2.0471e-04 dt: 3332.09ms, tok/sec:157345.27
step 10222, loss: 3.229885, norm:0.2674, lr:2.0466e-04 dt: 3332.08ms, tok/sec:157345.79
step 10223, loss: 3.357183, norm:0.2699, lr:2.0460e-04 dt: 3332.09ms, tok/sec:157345.05
step 10224, loss: 3.260415, norm:0.2589, lr:2.0455e-04 dt: 3332.22ms, tok/sec:157338.80
step 10225, loss: 3.312150, norm:0.2630, lr:2.0450e-04 dt: 3331.96ms, tok/sec:157351.24
step 10226, loss: 3.257899, norm:0.2649, lr:2.0445e-04 dt: 3332.41ms, tok/sec:157329.93
step 10227, loss: 3.171243, norm:0.2706, lr:2.0440e-04 dt: 3332.28ms, tok/sec:157336.17
step 10228, loss: 3.198169, norm:0.2486, lr:2.0435e-04 dt: 3331.95ms, tok/sec:157351.86
step 10229, loss: 3.219025, norm:0.2681, lr:2.0429e-04 dt: 3332.13ms, tok/sec:157343.37
step 10230, loss: 3.198230, norm:0.2484, lr:2.0424e-04 dt: 3331.97ms, tok/sec:157350.84
step 10231, loss: 3.239880, norm:0.2625, lr:2.0419e-04 dt: 3332.22ms, tok/sec:157338.97
step 10232, loss: 3.192834, norm:0.2431, lr:2.0414e-04 dt: 3332.31ms, tok/sec:157334.64
step 10233, loss: 3.214814, norm:0.2506, lr:2.0409e-04 dt: 3332.22ms, tok/sec:157339.00
step 10234, loss: 3.308710, norm:0.2580, lr:2.0404e-04 dt: 3332.16ms, tok/sec:157341.94
step 10235, loss: 3.235932, norm:0.2479, lr:2.0398e-04 dt: 3332.32ms, tok/sec:157334.29
step 10236, loss: 3.240059, norm:0.2505, lr:2.0393e-04 dt: 3332.50ms, tok/sec:157325.59
step 10237, loss: 3.221013, norm:0.2471, lr:2.0388e-04 dt: 3332.09ms, tok/sec:157345.11
step 10238, loss: 3.234164, norm:0.2506, lr:2.0383e-04 dt: 3332.07ms, tok/sec:157346.00
step 10239, loss: 3.257361, norm:0.2580, lr:2.0378e-04 dt: 3331.94ms, tok/sec:157352.09
step 10240, loss: 3.294287, norm:0.2550, lr:2.0373e-04 dt: 3332.15ms, tok/sec:157342.38
step 10241, loss: 3.345119, norm:0.3068, lr:2.0368e-04 dt: 3332.16ms, tok/sec:157341.90
step 10242, loss: 3.219836, norm:0.2690, lr:2.0362e-04 dt: 3332.06ms, tok/sec:157346.28
step 10243, loss: 3.256974, norm:0.2743, lr:2.0357e-04 dt: 3332.16ms, tok/sec:157341.60
step 10244, loss: 3.285890, norm:0.3635, lr:2.0352e-04 dt: 3332.18ms, tok/sec:157340.87
step 10245, loss: 3.235186, norm:0.2838, lr:2.0347e-04 dt: 3332.62ms, tok/sec:157319.85
step 10246, loss: 3.262917, norm:0.2579, lr:2.0342e-04 dt: 3331.94ms, tok/sec:157352.09
step 10247, loss: 3.265919, norm:0.3596, lr:2.0337e-04 dt: 3331.84ms, tok/sec:157356.92
step 10248, loss: 3.198656, norm:0.2893, lr:2.0331e-04 dt: 3332.27ms, tok/sec:157336.47
step 10249, loss: 3.266814, norm:0.2610, lr:2.0326e-04 dt: 3332.24ms, tok/sec:157338.15
HellaSwag accuracy:-2286593098537008047/-2=1143296549268504064.0000
rank 1 sample 0: Hello, I'm a language model, and my grammar is so simple that is easy for a beginner.
I used to write this as a joke, but
rank 1 sample 1: Hello, I'm a language model, which means I have a language model with that language model at the language level.
One of the most important things about
rank 1 sample 2: Hello, I'm a language model, but only one way to test it.
I like that in my code.
I don't like that much,
rank 1 sample 3: Hello, I'm a language model, and I'm working with two languages for more than a year. To find which languages come our way, I just need
rank 0 sample 0: Hello, I'm a language model, and I think it is interesting and worthwhile to present to you. Thanks in advance.<|endoftext|>You know what happens when someone
rank 0 sample 1: Hello, I'm a language model, but here's a quick list to come up: some features of C++:
1. The Language Model
2
rank 0 sample 2: Hello, I'm a language model, so I wanted to look at a particular language's main features for its use. If I were to move on, say
rank 0 sample 3: Hello, I'm a language model, and what I'm doing is trying to build such a language model as well, even when we don't have any training
step 10250, loss: 3.323406, norm:0.3297, lr:2.0321e-04 dt: 48527.63ms, tok/sec:10803.91
step 10251, loss: 3.349188, norm:0.2783, lr:2.0316e-04 dt: 3332.29ms, tok/sec:157335.56
step 10252, loss: 3.328525, norm:0.2950, lr:2.0311e-04 dt: 3331.98ms, tok/sec:157350.36
step 10253, loss: 3.282509, norm:0.2762, lr:2.0306e-04 dt: 3332.02ms, tok/sec:157348.30
step 10254, loss: 3.346159, norm:0.2861, lr:2.0301e-04 dt: 3332.15ms, tok/sec:157342.22
step 10255, loss: 3.313369, norm:0.2891, lr:2.0295e-04 dt: 3332.15ms, tok/sec:157342.16
step 10256, loss: 3.297592, norm:0.3036, lr:2.0290e-04 dt: 3332.17ms, tok/sec:157341.46
step 10257, loss: 3.332639, norm:0.2576, lr:2.0285e-04 dt: 3332.11ms, tok/sec:157344.04
step 10258, loss: 3.306834, norm:0.2786, lr:2.0280e-04 dt: 3332.13ms, tok/sec:157343.37
step 10259, loss: 3.303111, norm:0.2680, lr:2.0275e-04 dt: 3332.69ms, tok/sec:157316.81
step 10260, loss: 3.213902, norm:0.2613, lr:2.0270e-04 dt: 3331.90ms, tok/sec:157354.28
step 10261, loss: 3.279314, norm:0.2668, lr:2.0265e-04 dt: 3332.03ms, tok/sec:157347.98
step 10262, loss: 3.313033, norm:0.2830, lr:2.0259e-04 dt: 3332.06ms, tok/sec:157346.43
step 10263, loss: 3.184528, norm:0.2559, lr:2.0254e-04 dt: 3332.08ms, tok/sec:157345.45
step 10264, loss: 3.174445, norm:0.2620, lr:2.0249e-04 dt: 3332.13ms, tok/sec:157343.38
step 10265, loss: 3.296989, norm:0.2911, lr:2.0244e-04 dt: 3332.04ms, tok/sec:157347.44
step 10266, loss: 3.202579, norm:0.2593, lr:2.0239e-04 dt: 3332.33ms, tok/sec:157333.94
step 10267, loss: 3.245079, norm:0.2518, lr:2.0234e-04 dt: 3332.47ms, tok/sec:157326.96
step 10268, loss: 3.224869, norm:0.2627, lr:2.0229e-04 dt: 3332.17ms, tok/sec:157341.41
step 10269, loss: 3.235074, norm:0.2491, lr:2.0223e-04 dt: 3332.23ms, tok/sec:157338.56
step 10270, loss: 3.273132, norm:0.2607, lr:2.0218e-04 dt: 3331.97ms, tok/sec:157350.89
step 10271, loss: 3.227341, norm:0.2751, lr:2.0213e-04 dt: 3331.98ms, tok/sec:157350.15
step 10272, loss: 3.256320, norm:0.2472, lr:2.0208e-04 dt: 3332.11ms, tok/sec:157343.91
step 10273, loss: 3.232357, norm:0.2559, lr:2.0203e-04 dt: 3331.93ms, tok/sec:157352.63
step 10274, loss: 3.270459, norm:0.2641, lr:2.0198e-04 dt: 3332.05ms, tok/sec:157346.82
step 10275, loss: 3.276796, norm:0.2978, lr:2.0193e-04 dt: 3332.29ms, tok/sec:157335.47
step 10276, loss: 3.233890, norm:0.2726, lr:2.0188e-04 dt: 3332.67ms, tok/sec:157317.80
step 10277, loss: 3.263846, norm:0.2752, lr:2.0182e-04 dt: 3332.05ms, tok/sec:157346.84
step 10278, loss: 3.281597, norm:0.2664, lr:2.0177e-04 dt: 3331.97ms, tok/sec:157350.74
step 10279, loss: 3.279860, norm:0.2714, lr:2.0172e-04 dt: 3332.36ms, tok/sec:157332.11
step 10280, loss: 3.248579, norm:0.2633, lr:2.0167e-04 dt: 3332.26ms, tok/sec:157337.23
step 10281, loss: 3.266045, norm:0.2661, lr:2.0162e-04 dt: 3332.15ms, tok/sec:157342.22
step 10282, loss: 3.263026, norm:0.2895, lr:2.0157e-04 dt: 3332.07ms, tok/sec:157345.80
step 10283, loss: 3.281558, norm:0.2771, lr:2.0152e-04 dt: 3332.05ms, tok/sec:157346.75
step 10284, loss: 3.293617, norm:0.2675, lr:2.0146e-04 dt: 3332.41ms, tok/sec:157330.09
step 10285, loss: 3.300638, norm:0.2881, lr:2.0141e-04 dt: 3332.38ms, tok/sec:157331.27
step 10286, loss: 3.302987, norm:0.2754, lr:2.0136e-04 dt: 3334.12ms, tok/sec:157249.37
step 10287, loss: 3.267462, norm:0.2575, lr:2.0131e-04 dt: 3332.32ms, tok/sec:157334.33
step 10288, loss: 3.296601, norm:0.2816, lr:2.0126e-04 dt: 3332.32ms, tok/sec:157334.22
step 10289, loss: 3.298831, norm:0.2688, lr:2.0121e-04 dt: 3331.90ms, tok/sec:157353.96
step 10290, loss: 3.284169, norm:0.3654, lr:2.0116e-04 dt: 3332.09ms, tok/sec:157344.89
step 10291, loss: 3.299968, norm:0.2785, lr:2.0111e-04 dt: 3332.16ms, tok/sec:157341.66
step 10292, loss: 3.267928, norm:0.2596, lr:2.0105e-04 dt: 3332.04ms, tok/sec:157347.68
step 10293, loss: 3.318354, norm:0.2825, lr:2.0100e-04 dt: 3332.69ms, tok/sec:157316.54
step 10294, loss: 3.272567, norm:0.2516, lr:2.0095e-04 dt: 3332.02ms, tok/sec:157348.30
step 10295, loss: 3.287172, norm:0.2736, lr:2.0090e-04 dt: 3331.97ms, tok/sec:157350.54
step 10296, loss: 3.353557, norm:0.2602, lr:2.0085e-04 dt: 3332.17ms, tok/sec:157341.17
step 10297, loss: 3.230172, norm:0.2628, lr:2.0080e-04 dt: 3332.11ms, tok/sec:157344.23
step 10298, loss: 3.222960, norm:0.2824, lr:2.0075e-04 dt: 3331.89ms, tok/sec:157354.72
step 10299, loss: 3.224335, norm:0.2597, lr:2.0070e-04 dt: 3332.22ms, tok/sec:157339.15
validation loss: 3.2641
Model and optimizer state saved.
HellaSwag accuracy:-2286628282909094831/-2=1143314141454547456.0000
rank 1 sample 0: Hello, I'm a language model, and that's what I want to change, which is exactly the way I talk to the language and the way I talk
rank 1 sample 1: Hello, I'm a language model, which means I love writing. And so now, I hope you could really appreciate it at a cultural and historical level.
rank 1 sample 2: Hello, I'm a language model, but maybe not quite as well.
I'm in this group. It's a little difficult and the first couple days
rank 1 sample 3: Hello, I'm a language model, and I'm really excited to see your face.
Titanii Yayto’s video shows a
rank 0 sample 0: Hello, I'm a language model, and I think I can say this a lot on my own. We did our first graders write the following sentences to
rank 0 sample 1: Hello, I'm a language model, so please allow me to see it live.<|endoftext|>"Dot is almost as old as a person was."
—
rank 0 sample 2: Hello, I'm a language model, so I see it sometimes very well."
What is the point to note that "no" is not a verb,
rank 0 sample 3: Hello, I'm a language model, and like language models, I can get the
language model the same way I'm, just by getting the same answer
step 10300, loss: 3.242857, norm:0.2713, lr:2.0064e-04 dt: 56270.30ms, tok/sec:9317.31
step 10301, loss: 3.210872, norm:0.2608, lr:2.0059e-04 dt: 3332.09ms, tok/sec:157344.98
step 10302, loss: 3.252750, norm:0.2653, lr:2.0054e-04 dt: 3332.14ms, tok/sec:157342.66
step 10303, loss: 3.235698, norm:0.2619, lr:2.0049e-04 dt: 3332.07ms, tok/sec:157346.19
step 10304, loss: 3.189411, norm:0.2617, lr:2.0044e-04 dt: 3332.02ms, tok/sec:157348.53
step 10305, loss: 3.223148, norm:0.2465, lr:2.0039e-04 dt: 3332.15ms, tok/sec:157342.41
step 10306, loss: 3.252329, norm:0.2629, lr:2.0034e-04 dt: 3332.01ms, tok/sec:157348.97
step 10307, loss: 3.242188, norm:0.2773, lr:2.0029e-04 dt: 3332.51ms, tok/sec:157325.05
step 10308, loss: 3.172030, norm:0.2625, lr:2.0024e-04 dt: 3331.90ms, tok/sec:157353.96
step 10309, loss: 3.282187, norm:0.2937, lr:2.0018e-04 dt: 3332.05ms, tok/sec:157346.84
step 10310, loss: 3.248887, norm:0.2756, lr:2.0013e-04 dt: 3332.16ms, tok/sec:157341.59
step 10311, loss: 3.233313, norm:0.2631, lr:2.0008e-04 dt: 3332.48ms, tok/sec:157326.66
step 10312, loss: 3.245310, norm:0.2682, lr:2.0003e-04 dt: 3332.05ms, tok/sec:157346.82
step 10313, loss: 3.278159, norm:0.2577, lr:1.9998e-04 dt: 3332.03ms, tok/sec:157347.94
step 10314, loss: 3.237792, norm:0.2617, lr:1.9993e-04 dt: 3332.26ms, tok/sec:157336.93
step 10315, loss: 3.252247, norm:0.2957, lr:1.9988e-04 dt: 3332.35ms, tok/sec:157332.94
step 10316, loss: 3.254786, norm:0.2607, lr:1.9983e-04 dt: 3332.84ms, tok/sec:157309.57
step 10317, loss: 3.278752, norm:0.2641, lr:1.9978e-04 dt: 3332.15ms, tok/sec:157342.37
step 10318, loss: 3.289944, norm:0.2586, lr:1.9972e-04 dt: 3332.18ms, tok/sec:157340.97
step 10319, loss: 3.245316, norm:0.2642, lr:1.9967e-04 dt: 3332.07ms, tok/sec:157346.07
step 10320, loss: 3.314325, norm:0.2873, lr:1.9962e-04 dt: 3331.94ms, tok/sec:157351.98
step 10321, loss: 3.283648, norm:0.2587, lr:1.9957e-04 dt: 3332.10ms, tok/sec:157344.66
step 10322, loss: 3.295562, norm:0.2607, lr:1.9952e-04 dt: 3332.00ms, tok/sec:157349.49
step 10323, loss: 3.258813, norm:0.2703, lr:1.9947e-04 dt: 3332.26ms, tok/sec:157337.23
step 10324, loss: 3.299816, norm:0.2529, lr:1.9942e-04 dt: 3332.63ms, tok/sec:157319.72
step 10325, loss: 3.206269, norm:0.2567, lr:1.9937e-04 dt: 3331.96ms, tok/sec:157351.30
step 10326, loss: 3.320400, norm:0.2946, lr:1.9932e-04 dt: 3332.12ms, tok/sec:157343.80
step 10327, loss: 3.296062, norm:0.2537, lr:1.9926e-04 dt: 3332.21ms, tok/sec:157339.27
step 10328, loss: 3.307780, norm:0.2643, lr:1.9921e-04 dt: 3331.91ms, tok/sec:157353.46
step 10329, loss: 3.301013, norm:0.2494, lr:1.9916e-04 dt: 3332.06ms, tok/sec:157346.74
step 10330, loss: 3.279447, norm:0.2952, lr:1.9911e-04 dt: 3332.04ms, tok/sec:157347.46
step 10331, loss: 3.219186, norm:0.2437, lr:1.9906e-04 dt: 3332.21ms, tok/sec:157339.22
step 10332, loss: 3.207582, norm:0.3447, lr:1.9901e-04 dt: 3332.01ms, tok/sec:157348.74
step 10333, loss: 3.232201, norm:0.2726, lr:1.9896e-04 dt: 3332.35ms, tok/sec:157332.74
step 10334, loss: 3.181780, norm:0.2723, lr:1.9891e-04 dt: 3331.92ms, tok/sec:157353.07
step 10335, loss: 3.232858, norm:0.2713, lr:1.9886e-04 dt: 3332.19ms, tok/sec:157340.14
step 10336, loss: 3.240175, norm:0.2513, lr:1.9881e-04 dt: 3331.96ms, tok/sec:157351.28
step 10337, loss: 3.232358, norm:0.2704, lr:1.9875e-04 dt: 3331.97ms, tok/sec:157350.58
step 10338, loss: 3.217936, norm:0.2773, lr:1.9870e-04 dt: 3332.11ms, tok/sec:157344.11
step 10339, loss: 3.241002, norm:0.2510, lr:1.9865e-04 dt: 3332.30ms, tok/sec:157335.39
step 10340, loss: 3.255888, norm:0.2652, lr:1.9860e-04 dt: 3331.84ms, tok/sec:157357.06
step 10341, loss: 3.223397, norm:0.2687, lr:1.9855e-04 dt: 3332.06ms, tok/sec:157346.61
step 10342, loss: 3.237432, norm:0.2419, lr:1.9850e-04 dt: 3332.34ms, tok/sec:157333.13
step 10343, loss: 3.243078, norm:0.2926, lr:1.9845e-04 dt: 3332.15ms, tok/sec:157342.46
step 10344, loss: 3.268909, norm:0.2874, lr:1.9840e-04 dt: 3332.00ms, tok/sec:157349.19
step 10345, loss: 3.290809, norm:0.2681, lr:1.9835e-04 dt: 3332.16ms, tok/sec:157341.67
step 10346, loss: 3.264861, norm:0.2753, lr:1.9830e-04 dt: 3332.20ms, tok/sec:157339.78
step 10347, loss: 3.377448, norm:0.2735, lr:1.9825e-04 dt: 3332.21ms, tok/sec:157339.51
step 10348, loss: 3.317863, norm:0.2701, lr:1.9819e-04 dt: 3332.23ms, tok/sec:157338.36
step 10349, loss: 3.267663, norm:0.2805, lr:1.9814e-04 dt: 3332.40ms, tok/sec:157330.67
HellaSwag accuracy:-2295477152489733039/-2=1147738576244866560.0000
rank 1 sample 0: Hello, I'm a language model, and a machine learning tutorial. I actually like being an AIO, and working with machine learning algorithms. But I'm
rank 1 sample 1: Hello, I'm a language model, I can say from the beginning that a very important idea must be reached. This topic started to fascinate me, and
rank 1 sample 2: Hello, I'm a language model, but its syntax is a bit different than the language model that uses it to code.
A new generation is the process
rank 1 sample 3: Hello, I'm a language model, and I'm an engineer that does it too. Now because I'm talking mostly about working with language, I have to
rank 0 sample 0: Hello, I'm a language model, and I love to teach a little vocabulary before starting this unit. That's what I want to get to learn. The
rank 0 sample 1: Hello, I'm a language model, so to speak. It's like reading a newspaper about its time or what a child would see, say, a book
rank 0 sample 2: Hello, I'm a language model, so I guess it's about 3 months. Now I'm working for a language model. So I'm working on "
rank 0 sample 3: Hello, I'm a language model, and here's what I found:
The term "language" does not have as long a meaning as "evern
step 10350, loss: 3.351171, norm:0.3420, lr:1.9809e-04 dt: 48525.06ms, tok/sec:10804.48
step 10351, loss: 3.268859, norm:0.2805, lr:1.9804e-04 dt: 3332.52ms, tok/sec:157324.93
step 10352, loss: 3.349643, norm:0.2939, lr:1.9799e-04 dt: 3332.07ms, tok/sec:157346.02
step 10353, loss: 3.281281, norm:0.2866, lr:1.9794e-04 dt: 3332.09ms, tok/sec:157345.18
step 10354, loss: 3.237901, norm:0.2640, lr:1.9789e-04 dt: 3332.29ms, tok/sec:157335.59
step 10355, loss: 3.332381, norm:0.2983, lr:1.9784e-04 dt: 3332.01ms, tok/sec:157348.72
step 10356, loss: 3.268820, norm:0.2605, lr:1.9779e-04 dt: 3332.01ms, tok/sec:157348.91
step 10357, loss: 3.298878, norm:0.2752, lr:1.9774e-04 dt: 3332.50ms, tok/sec:157325.61
step 10358, loss: 3.342345, norm:0.2983, lr:1.9769e-04 dt: 3332.33ms, tok/sec:157333.92
step 10359, loss: 3.321088, norm:0.2595, lr:1.9763e-04 dt: 3332.58ms, tok/sec:157321.98
step 10360, loss: 3.282806, norm:0.2820, lr:1.9758e-04 dt: 3332.10ms, tok/sec:157344.84
step 10361, loss: 3.245226, norm:0.2942, lr:1.9753e-04 dt: 3332.13ms, tok/sec:157343.00
step 10362, loss: 3.275778, norm:0.2709, lr:1.9748e-04 dt: 3332.30ms, tok/sec:157335.04
step 10363, loss: 3.272269, norm:0.2738, lr:1.9743e-04 dt: 3332.09ms, tok/sec:157345.30
step 10364, loss: 3.377644, norm:0.3027, lr:1.9738e-04 dt: 3331.87ms, tok/sec:157355.50
step 10365, loss: 3.288465, norm:0.2692, lr:1.9733e-04 dt: 3332.31ms, tok/sec:157334.70
step 10366, loss: 3.271662, norm:0.3030, lr:1.9728e-04 dt: 3332.69ms, tok/sec:157316.81
step 10367, loss: 3.246818, norm:0.2669, lr:1.9723e-04 dt: 3332.00ms, tok/sec:157349.31
step 10368, loss: 3.304969, norm:0.2632, lr:1.9718e-04 dt: 3332.09ms, tok/sec:157345.23
step 10369, loss: 3.216343, norm:0.2787, lr:1.9713e-04 dt: 3332.03ms, tok/sec:157347.74
step 10370, loss: 3.232021, norm:0.2666, lr:1.9708e-04 dt: 3332.27ms, tok/sec:157336.41
step 10371, loss: 3.218043, norm:0.2665, lr:1.9703e-04 dt: 3332.21ms, tok/sec:157339.30
step 10372, loss: 3.191852, norm:0.2515, lr:1.9697e-04 dt: 3332.05ms, tok/sec:157347.13
step 10373, loss: 3.282207, norm:0.2431, lr:1.9692e-04 dt: 3332.19ms, tok/sec:157340.28
step 10374, loss: 3.239945, norm:0.2955, lr:1.9687e-04 dt: 3332.07ms, tok/sec:157346.24
step 10375, loss: 3.209911, norm:0.2448, lr:1.9682e-04 dt: 3332.42ms, tok/sec:157329.54
step 10376, loss: 3.241295, norm:0.2531, lr:1.9677e-04 dt: 3332.11ms, tok/sec:157344.06
step 10377, loss: 3.192911, norm:0.2753, lr:1.9672e-04 dt: 3332.21ms, tok/sec:157339.21
step 10378, loss: 3.212868, norm:0.2652, lr:1.9667e-04 dt: 3332.16ms, tok/sec:157341.81
step 10379, loss: 3.253764, norm:0.2846, lr:1.9662e-04 dt: 3332.34ms, tok/sec:157333.32
step 10380, loss: 3.323996, norm:0.2623, lr:1.9657e-04 dt: 3332.17ms, tok/sec:157341.51
step 10381, loss: 3.289395, norm:0.2797, lr:1.9652e-04 dt: 3332.24ms, tok/sec:157337.92
step 10382, loss: 3.260876, norm:0.2856, lr:1.9647e-04 dt: 3332.48ms, tok/sec:157326.90
step 10383, loss: 3.257110, norm:0.2721, lr:1.9642e-04 dt: 3332.00ms, tok/sec:157349.54
step 10384, loss: 3.269190, norm:0.2688, lr:1.9637e-04 dt: 3332.16ms, tok/sec:157341.69
step 10385, loss: 3.251169, norm:0.2632, lr:1.9632e-04 dt: 3332.19ms, tok/sec:157340.14
step 10386, loss: 3.274248, norm:0.2671, lr:1.9626e-04 dt: 3332.01ms, tok/sec:157349.08
step 10387, loss: 3.283907, norm:0.2705, lr:1.9621e-04 dt: 3332.24ms, tok/sec:157337.84
step 10388, loss: 3.287068, norm:0.2688, lr:1.9616e-04 dt: 3332.23ms, tok/sec:157338.32
step 10389, loss: 3.249852, norm:0.2730, lr:1.9611e-04 dt: 3332.02ms, tok/sec:157348.61
step 10390, loss: 3.243842, norm:0.2630, lr:1.9606e-04 dt: 3332.56ms, tok/sec:157322.92
step 10391, loss: 3.258078, norm:0.2867, lr:1.9601e-04 dt: 3332.31ms, tok/sec:157334.66
step 10392, loss: 3.318381, norm:0.2657, lr:1.9596e-04 dt: 3332.29ms, tok/sec:157335.67
step 10393, loss: 3.283464, norm:0.2601, lr:1.9591e-04 dt: 3332.13ms, tok/sec:157343.07
step 10394, loss: 3.259764, norm:0.2601, lr:1.9586e-04 dt: 3332.46ms, tok/sec:157327.61
step 10395, loss: 3.309719, norm:0.2514, lr:1.9581e-04 dt: 3332.29ms, tok/sec:157335.56
step 10396, loss: 3.296572, norm:0.2572, lr:1.9576e-04 dt: 3332.23ms, tok/sec:157338.27
step 10397, loss: 3.287441, norm:0.2511, lr:1.9571e-04 dt: 3332.83ms, tok/sec:157309.93
step 10398, loss: 3.288726, norm:0.2686, lr:1.9566e-04 dt: 3332.32ms, tok/sec:157334.23
step 10399, loss: 3.274357, norm:0.2454, lr:1.9561e-04 dt: 3332.22ms, tok/sec:157338.95
validation loss: 3.2611
Model and optimizer state saved.
HellaSwag accuracy:-6898261524778349487/-2=3449130762389174784.0000
rank 1 sample 0: Hello, I'm a language model, and my writing is in a language my mother. It's interesting to me that I can also read and write in a
rank 1 sample 1: Hello, I'm a language model, which means I think we're going to move on to understanding the real world with something meaningful and intuitive, and I'm
rank 1 sample 2: Hello, I'm a language model, but since the way I'm writing this is a bit complicated by my language, I'm just looking for the perfect layout
rank 1 sample 3: Hello, I'm a language model, and I'm using the first one (English) you said I like because I'm more sophisticated in my syntax. I
rank 0 sample 0: Hello, I'm a language model, and I think I can be very effective -
I'd love to find some examples and tutorials.<|endoftext|>I have had
rank 0 sample 1: Hello, I'm a language model, so there's a lot of work a language engineer spends most of his time. I have the ability to model many things
rank 0 sample 2: Hello, I'm a language model, so I didn't know any vocabulary or grammar before I started using that. I have to admit that I don't have
rank 0 sample 3: Hello, I'm a language model, and when I teach a language, I think: "I can tell you that my grammar is correct: I am.
step 10400, loss: 3.335468, norm:0.2614, lr:1.9556e-04 dt: 56188.94ms, tok/sec:9330.80
step 10401, loss: 3.314997, norm:0.2802, lr:1.9551e-04 dt: 3332.08ms, tok/sec:157345.63
step 10402, loss: 3.279048, norm:0.2652, lr:1.9545e-04 dt: 3332.20ms, tok/sec:157340.04
step 10403, loss: 3.163636, norm:0.2497, lr:1.9540e-04 dt: 3332.19ms, tok/sec:157340.46
step 10404, loss: 3.212002, norm:0.2686, lr:1.9535e-04 dt: 3332.38ms, tok/sec:157331.42
step 10405, loss: 3.187286, norm:0.2426, lr:1.9530e-04 dt: 3332.19ms, tok/sec:157340.58
step 10406, loss: 3.237891, norm:0.2527, lr:1.9525e-04 dt: 3332.00ms, tok/sec:157349.38
step 10407, loss: 3.211249, norm:0.2392, lr:1.9520e-04 dt: 3332.10ms, tok/sec:157344.63
step 10408, loss: 3.225846, norm:0.2643, lr:1.9515e-04 dt: 3332.29ms, tok/sec:157335.56
step 10409, loss: 3.245252, norm:0.2608, lr:1.9510e-04 dt: 3332.04ms, tok/sec:157347.32
step 10410, loss: 3.209806, norm:0.2468, lr:1.9505e-04 dt: 3332.10ms, tok/sec:157344.46
step 10411, loss: 3.224805, norm:0.2567, lr:1.9500e-04 dt: 3332.34ms, tok/sec:157333.23
step 10412, loss: 3.262150, norm:0.2543, lr:1.9495e-04 dt: 3332.22ms, tok/sec:157338.73
step 10413, loss: 3.278810, norm:0.2610, lr:1.9490e-04 dt: 3332.52ms, tok/sec:157324.89
step 10414, loss: 3.250203, norm:0.2874, lr:1.9485e-04 dt: 3331.94ms, tok/sec:157352.12
step 10415, loss: 3.291330, norm:0.2978, lr:1.9480e-04 dt: 3332.09ms, tok/sec:157344.90
step 10416, loss: 3.348180, norm:0.2826, lr:1.9475e-04 dt: 3332.16ms, tok/sec:157341.78
step 10417, loss: 3.250702, norm:0.2801, lr:1.9470e-04 dt: 3332.38ms, tok/sec:157331.59
step 10418, loss: 3.239795, norm:0.2894, lr:1.9465e-04 dt: 3332.13ms, tok/sec:157343.23
step 10419, loss: 3.250144, norm:0.2644, lr:1.9460e-04 dt: 3332.16ms, tok/sec:157341.56
step 10420, loss: 3.241959, norm:0.2826, lr:1.9455e-04 dt: 3332.50ms, tok/sec:157325.72
step 10421, loss: 3.280144, norm:0.2732, lr:1.9450e-04 dt: 3332.48ms, tok/sec:157326.57
step 10422, loss: 3.282791, norm:0.2767, lr:1.9444e-04 dt: 3332.16ms, tok/sec:157341.59
step 10423, loss: 3.247915, norm:0.2673, lr:1.9439e-04 dt: 3332.12ms, tok/sec:157343.78
step 10424, loss: 3.279816, norm:0.3027, lr:1.9434e-04 dt: 3332.07ms, tok/sec:157345.83
step 10425, loss: 3.313329, norm:0.2720, lr:1.9429e-04 dt: 3332.21ms, tok/sec:157339.63
step 10426, loss: 3.330674, norm:0.2815, lr:1.9424e-04 dt: 3332.16ms, tok/sec:157341.96
step 10427, loss: 3.285754, norm:0.2875, lr:1.9419e-04 dt: 3332.29ms, tok/sec:157335.80
step 10428, loss: 3.273252, norm:0.2525, lr:1.9414e-04 dt: 3332.09ms, tok/sec:157344.86
step 10429, loss: 3.284220, norm:0.2654, lr:1.9409e-04 dt: 3332.33ms, tok/sec:157333.66
step 10430, loss: 3.272808, norm:0.2624, lr:1.9404e-04 dt: 3332.30ms, tok/sec:157335.20
step 10431, loss: 3.258549, norm:0.2743, lr:1.9399e-04 dt: 3332.01ms, tok/sec:157348.98
step 10432, loss: 3.308470, norm:0.2518, lr:1.9394e-04 dt: 3332.23ms, tok/sec:157338.59
step 10433, loss: 3.296868, norm:0.2676, lr:1.9389e-04 dt: 3332.17ms, tok/sec:157341.35
step 10434, loss: 3.336380, norm:0.3043, lr:1.9384e-04 dt: 3332.08ms, tok/sec:157345.58
step 10435, loss: 3.336459, norm:0.2770, lr:1.9379e-04 dt: 3332.19ms, tok/sec:157340.35
step 10436, loss: 3.232854, norm:0.2439, lr:1.9374e-04 dt: 3332.29ms, tok/sec:157335.45
step 10437, loss: 3.201256, norm:0.2768, lr:1.9369e-04 dt: 3332.00ms, tok/sec:157349.28
step 10438, loss: 3.248748, norm:0.2554, lr:1.9364e-04 dt: 3332.33ms, tok/sec:157333.98
step 10439, loss: 3.257133, norm:0.2482, lr:1.9359e-04 dt: 3332.00ms, tok/sec:157349.46
step 10440, loss: 3.299130, norm:0.2647, lr:1.9354e-04 dt: 3332.03ms, tok/sec:157348.13
step 10441, loss: 3.269644, norm:0.2506, lr:1.9349e-04 dt: 3332.22ms, tok/sec:157339.05
step 10442, loss: 3.260759, norm:0.2550, lr:1.9344e-04 dt: 3331.89ms, tok/sec:157354.48
step 10443, loss: 3.234233, norm:0.2592, lr:1.9339e-04 dt: 3332.08ms, tok/sec:157345.34
step 10444, loss: 3.237060, norm:0.2533, lr:1.9334e-04 dt: 3332.09ms, tok/sec:157344.89
step 10445, loss: 3.208608, norm:0.2793, lr:1.9329e-04 dt: 3332.30ms, tok/sec:157334.97
step 10446, loss: 3.215546, norm:0.2388, lr:1.9324e-04 dt: 3332.51ms, tok/sec:157325.12
step 10447, loss: 3.265077, norm:0.3028, lr:1.9319e-04 dt: 3331.82ms, tok/sec:157358.03
step 10448, loss: 3.260847, norm:0.2720, lr:1.9314e-04 dt: 3331.94ms, tok/sec:157352.40
step 10449, loss: 3.287371, norm:0.2927, lr:1.9309e-04 dt: 3332.40ms, tok/sec:157330.56
HellaSwag accuracy:-2286593098537005999/-2=1143296549268503040.0000
rank 1 sample 0: Hello, I'm a language model, and a number of projects I've drawn from those, mostly that are just generalizations about what the language means.

rank 1 sample 1: Hello, I'm a language model, which means I've been learning the language well. I need to figure out if you have the 'I' in the
rank 1 sample 2: Hello, I'm a language model, but where did this come from?
I'm a teacher (I know I'm a student!) I've worked in
rank 1 sample 3: Hello, I'm a language model, and I'm using a web server as (a) terminal for input into Microsoft's Open System Office.
As I
rank 0 sample 0: Hello, I'm a language model, and I'll be talking about this from birth onwards.
"How far has the English language got from being a native
rank 0 sample 1: Hello, I'm a language model, so i used to talk to you there. So maybe not quite so you have a language model. Well, I have
rank 0 sample 2: Hello, I'm a language model, so I guess it's important to learn how people use the system in a way that is understandable to them. I used
rank 0 sample 3: Hello, I'm a language model, and now I have a way to work on such a thing.” (http://www.khev1
step 10450, loss: 3.240656, norm:0.2626, lr:1.9304e-04 dt: 48517.98ms, tok/sec:10806.06
step 10451, loss: 3.235632, norm:0.2615, lr:1.9298e-04 dt: 3332.17ms, tok/sec:157341.23
step 10452, loss: 3.293284, norm:0.2644, lr:1.9293e-04 dt: 3332.24ms, tok/sec:157337.84
step 10453, loss: 3.337587, norm:0.2986, lr:1.9288e-04 dt: 3332.29ms, tok/sec:157335.76
step 10454, loss: 3.253762, norm:0.2548, lr:1.9283e-04 dt: 3332.41ms, tok/sec:157330.16
step 10455, loss: 3.258887, norm:0.2680, lr:1.9278e-04 dt: 3332.15ms, tok/sec:157342.02
step 10456, loss: 3.239736, norm:0.2565, lr:1.9273e-04 dt: 3332.39ms, tok/sec:157330.98
step 10457, loss: 3.238127, norm:0.2649, lr:1.9268e-04 dt: 3332.38ms, tok/sec:157331.59
step 10458, loss: 3.287452, norm:0.2683, lr:1.9263e-04 dt: 3332.26ms, tok/sec:157337.20
step 10459, loss: 3.252077, norm:0.2509, lr:1.9258e-04 dt: 3332.21ms, tok/sec:157339.32
step 10460, loss: 3.352092, norm:0.2929, lr:1.9253e-04 dt: 3332.39ms, tok/sec:157331.03
step 10461, loss: 3.220183, norm:0.2819, lr:1.9248e-04 dt: 3332.14ms, tok/sec:157342.54
step 10462, loss: 3.311514, norm:0.2662, lr:1.9243e-04 dt: 3332.59ms, tok/sec:157321.46
step 10463, loss: 3.270502, norm:0.2708, lr:1.9238e-04 dt: 3332.31ms, tok/sec:157334.78
step 10464, loss: 3.284095, norm:0.2651, lr:1.9233e-04 dt: 3331.99ms, tok/sec:157349.60
step 10465, loss: 3.255985, norm:0.2500, lr:1.9228e-04 dt: 3332.16ms, tok/sec:157341.64
step 10466, loss: 3.260941, norm:0.2591, lr:1.9223e-04 dt: 3332.01ms, tok/sec:157348.91
step 10467, loss: 3.224543, norm:0.2527, lr:1.9218e-04 dt: 3332.33ms, tok/sec:157333.93
step 10468, loss: 3.275585, norm:0.2772, lr:1.9213e-04 dt: 3332.22ms, tok/sec:157338.85
step 10469, loss: 3.297826, norm:0.2692, lr:1.9208e-04 dt: 3332.41ms, tok/sec:157330.00
step 10470, loss: 3.267318, norm:0.2473, lr:1.9203e-04 dt: 3332.19ms, tok/sec:157340.17
step 10471, loss: 3.239276, norm:0.2542, lr:1.9198e-04 dt: 3332.02ms, tok/sec:157348.18
step 10472, loss: 3.260638, norm:0.2437, lr:1.9193e-04 dt: 3332.21ms, tok/sec:157339.61
step 10473, loss: 3.196134, norm:0.2606, lr:1.9188e-04 dt: 3332.03ms, tok/sec:157347.79
step 10474, loss: 3.274536, norm:0.2470, lr:1.9183e-04 dt: 3332.05ms, tok/sec:157347.05
step 10475, loss: 3.240778, norm:0.2451, lr:1.9178e-04 dt: 3332.12ms, tok/sec:157343.72
step 10476, loss: 3.219372, norm:0.2376, lr:1.9173e-04 dt: 3332.53ms, tok/sec:157324.22
step 10477, loss: 3.268324, norm:0.2451, lr:1.9168e-04 dt: 3334.43ms, tok/sec:157234.67
step 10478, loss: 3.284298, norm:0.2530, lr:1.9163e-04 dt: 3332.44ms, tok/sec:157328.40
step 10479, loss: 3.210942, norm:0.2300, lr:1.9158e-04 dt: 3332.19ms, tok/sec:157340.34
step 10480, loss: 3.199900, norm:0.2596, lr:1.9153e-04 dt: 3331.95ms, tok/sec:157351.47
step 10481, loss: 3.187081, norm:0.2359, lr:1.9148e-04 dt: 3332.02ms, tok/sec:157348.42
step 10482, loss: 3.212002, norm:0.2506, lr:1.9143e-04 dt: 3332.06ms, tok/sec:157346.52
step 10483, loss: 3.195209, norm:0.2991, lr:1.9138e-04 dt: 3332.06ms, tok/sec:157346.69
step 10484, loss: 3.225002, norm:0.2490, lr:1.9133e-04 dt: 3332.25ms, tok/sec:157337.65
step 10485, loss: 3.210112, norm:0.2721, lr:1.9128e-04 dt: 3332.27ms, tok/sec:157336.36
step 10486, loss: 3.198406, norm:0.2965, lr:1.9123e-04 dt: 3332.11ms, tok/sec:157343.97
step 10487, loss: 3.229551, norm:0.2668, lr:1.9118e-04 dt: 3332.14ms, tok/sec:157342.81
step 10488, loss: 3.285445, norm:0.3004, lr:1.9113e-04 dt: 3332.19ms, tok/sec:157340.44
step 10489, loss: 3.175823, norm:0.2603, lr:1.9108e-04 dt: 3332.39ms, tok/sec:157331.05
step 10490, loss: 3.222811, norm:0.2712, lr:1.9103e-04 dt: 3332.05ms, tok/sec:157347.10
step 10491, loss: 3.266241, norm:0.2757, lr:1.9098e-04 dt: 3332.06ms, tok/sec:157346.43
step 10492, loss: 3.271510, norm:0.2625, lr:1.9093e-04 dt: 3332.22ms, tok/sec:157338.93
step 10493, loss: 3.206650, norm:0.2607, lr:1.9088e-04 dt: 3331.96ms, tok/sec:157351.42
step 10494, loss: 3.370027, norm:0.3210, lr:1.9083e-04 dt: 3332.03ms, tok/sec:157347.70
step 10495, loss: 3.314880, norm:0.2851, lr:1.9078e-04 dt: 3331.96ms, tok/sec:157351.38
step 10496, loss: 3.290632, norm:0.2745, lr:1.9073e-04 dt: 3332.13ms, tok/sec:157343.01
step 10497, loss: 3.316777, norm:0.2676, lr:1.9068e-04 dt: 3332.13ms, tok/sec:157343.43
step 10498, loss: 3.293089, norm:0.2958, lr:1.9063e-04 dt: 3332.24ms, tok/sec:157337.91
step 10499, loss: 3.288611, norm:0.2630, lr:1.9058e-04 dt: 3332.02ms, tok/sec:157348.26
validation loss: 3.2599
Model and optimizer state saved.
HellaSwag accuracy:-2286452361048650671/-2=1143226180524325376.0000
rank 1 sample 0: Hello, I'm a language model, and you're going to use it whenever it wants you to
- to describe the
- (a) to describe
rank 1 sample 1: Hello, I'm a language model, I want to go back to the 'learning' topic so I can be better prepared then I can be a language model
rank 1 sample 2: Hello, I'm a language model, but why would we want to use it?
1: My name is Maria. I have never used this myself,
rank 1 sample 3: Hello, I'm a language model, and I'm working with C++. Before I got more of a technical education, someone made me a few years ago
rank 0 sample 0: Hello, I'm a language model, and I love it because it gives me control over my culture and gives me access to language because you have to be there
rank 0 sample 1: Hello, I'm a language model, so please visit my site for my in-person coaching program, "the web". I was born.
I've
rank 0 sample 2: Hello, I'm a language model, but I didn't always read one in the middle. I don't think I was taught that. I'm not trying
rank 0 sample 3: Hello, I'm a language model, so a lot of people use the same words instead of the same, because that actually creates a language like English that seems
step 10500, loss: 3.324957, norm:0.2807, lr:1.9053e-04 dt: 56174.96ms, tok/sec:9333.13
step 10501, loss: 3.328763, norm:0.3164, lr:1.9048e-04 dt: 3332.13ms, tok/sec:157343.02
step 10502, loss: 3.310575, norm:0.2749, lr:1.9043e-04 dt: 3332.52ms, tok/sec:157324.75
step 10503, loss: 3.352436, norm:0.2870, lr:1.9038e-04 dt: 3332.35ms, tok/sec:157332.79
step 10504, loss: 3.292056, norm:0.2607, lr:1.9033e-04 dt: 3332.05ms, tok/sec:157346.97
step 10505, loss: 3.289936, norm:0.2654, lr:1.9028e-04 dt: 3332.12ms, tok/sec:157343.88
step 10506, loss: 3.194446, norm:0.2435, lr:1.9023e-04 dt: 3332.17ms, tok/sec:157341.27
step 10507, loss: 3.240480, norm:0.2518, lr:1.9018e-04 dt: 3332.16ms, tok/sec:157341.69
step 10508, loss: 3.230502, norm:0.2334, lr:1.9013e-04 dt: 3332.03ms, tok/sec:157347.90
step 10509, loss: 3.213862, norm:0.2472, lr:1.9008e-04 dt: 3332.19ms, tok/sec:157340.21
step 10510, loss: 3.194541, norm:0.2554, lr:1.9003e-04 dt: 3332.05ms, tok/sec:157347.20
step 10511, loss: 3.197577, norm:0.2526, lr:1.8998e-04 dt: 3332.53ms, tok/sec:157324.29
step 10512, loss: 3.203611, norm:0.2647, lr:1.8993e-04 dt: 3332.00ms, tok/sec:157349.27
step 10513, loss: 3.150771, norm:0.2516, lr:1.8988e-04 dt: 3332.08ms, tok/sec:157345.39
step 10514, loss: 3.217702, norm:0.2398, lr:1.8983e-04 dt: 3332.09ms, tok/sec:157344.97
step 10515, loss: 3.220188, norm:0.2312, lr:1.8978e-04 dt: 3332.09ms, tok/sec:157345.26
step 10516, loss: 3.241620, norm:0.2478, lr:1.8973e-04 dt: 3332.17ms, tok/sec:157341.22
step 10517, loss: 3.213511, norm:0.2457, lr:1.8968e-04 dt: 3331.91ms, tok/sec:157353.73
step 10518, loss: 3.264324, norm:0.2799, lr:1.8963e-04 dt: 3332.00ms, tok/sec:157349.16
step 10519, loss: 3.163452, norm:0.2527, lr:1.8958e-04 dt: 3332.16ms, tok/sec:157341.77
step 10520, loss: 3.212182, norm:0.2541, lr:1.8953e-04 dt: 3332.42ms, tok/sec:157329.44
step 10521, loss: 3.271313, norm:0.2615, lr:1.8948e-04 dt: 3332.08ms, tok/sec:157345.78
step 10522, loss: 3.248742, norm:0.2550, lr:1.8943e-04 dt: 3332.08ms, tok/sec:157345.36
step 10523, loss: 3.257777, norm:0.2708, lr:1.8938e-04 dt: 3332.12ms, tok/sec:157343.60
step 10524, loss: 3.344784, norm:0.2649, lr:1.8933e-04 dt: 3332.18ms, tok/sec:157340.82
step 10525, loss: 3.236125, norm:0.2713, lr:1.8928e-04 dt: 3332.12ms, tok/sec:157343.60
step 10526, loss: 3.351856, norm:0.2892, lr:1.8923e-04 dt: 3332.18ms, tok/sec:157340.96
step 10527, loss: 3.283003, norm:0.2794, lr:1.8918e-04 dt: 3332.29ms, tok/sec:157335.53
step 10528, loss: 3.359034, norm:0.2912, lr:1.8913e-04 dt: 3332.05ms, tok/sec:157346.75
step 10529, loss: 3.273716, norm:0.2794, lr:1.8908e-04 dt: 3332.64ms, tok/sec:157319.33
step 10530, loss: 3.303665, norm:0.2974, lr:1.8903e-04 dt: 3332.24ms, tok/sec:157337.95
step 10531, loss: 3.335349, norm:0.2837, lr:1.8899e-04 dt: 3332.08ms, tok/sec:157345.54
step 10532, loss: 3.306118, norm:0.2645, lr:1.8894e-04 dt: 3332.19ms, tok/sec:157340.29
step 10533, loss: 3.263985, norm:0.2934, lr:1.8889e-04 dt: 3332.14ms, tok/sec:157342.52
step 10534, loss: 3.333270, norm:0.2757, lr:1.8884e-04 dt: 3332.11ms, tok/sec:157343.96
step 10535, loss: 3.295399, norm:0.2483, lr:1.8879e-04 dt: 3332.32ms, tok/sec:157334.04
step 10536, loss: 3.316893, norm:0.2642, lr:1.8874e-04 dt: 3332.20ms, tok/sec:157339.69
step 10537, loss: 3.313206, norm:0.2548, lr:1.8869e-04 dt: 3332.02ms, tok/sec:157348.58
step 10538, loss: 3.303744, norm:0.2515, lr:1.8864e-04 dt: 3332.13ms, tok/sec:157343.21
step 10539, loss: 3.261232, norm:0.2601, lr:1.8859e-04 dt: 3332.59ms, tok/sec:157321.65
step 10540, loss: 3.297055, norm:0.2729, lr:1.8854e-04 dt: 3332.01ms, tok/sec:157348.80
step 10541, loss: 3.248798, norm:0.2621, lr:1.8849e-04 dt: 3332.26ms, tok/sec:157337.10
step 10542, loss: 3.234317, norm:0.2595, lr:1.8844e-04 dt: 3332.30ms, tok/sec:157335.17
step 10543, loss: 3.219350, norm:0.2763, lr:1.8839e-04 dt: 3332.13ms, tok/sec:157342.99
step 10544, loss: 3.265409, norm:0.2861, lr:1.8834e-04 dt: 3332.14ms, tok/sec:157342.79
step 10545, loss: 3.239610, norm:0.2627, lr:1.8829e-04 dt: 3332.06ms, tok/sec:157346.58
step 10546, loss: 3.196793, norm:0.2795, lr:1.8824e-04 dt: 3332.31ms, tok/sec:157334.49
step 10547, loss: 3.247632, norm:0.2476, lr:1.8819e-04 dt: 3332.06ms, tok/sec:157346.61
step 10548, loss: 3.192621, norm:0.2674, lr:1.8814e-04 dt: 3332.47ms, tok/sec:157327.24
step 10549, loss: 3.259915, norm:0.2644, lr:1.8809e-04 dt: 3331.88ms, tok/sec:157355.24
HellaSwag accuracy:2325075327704073297/-2=-1162537663852036608.0000
rank 1 sample 0: Hello, I'm a language model, and you're going to find something we don't like at the moment. Now, it's my first language model,
rank 1 sample 1: Hello, I'm a language model, I can't express my thoughts and feel comfortable with that question. As a human being though, however, I feel that
rank 1 sample 2: Hello, I'm a language model, but sometimes, I'm just a language model.
If one were to run a program like "C" for a
rank 1 sample 3: Hello, I'm a language model, and I'm really excited to share how good it is–an analysis tool built on deep learning.
First, I
rank 0 sample 0: Hello, I'm a language model, and I love the way my words have worked!"
Towards the end of each term the children created a short
rank 0 sample 1: Hello, I'm a language model, so now I'm going to learn from the way,
- the class name, and a class number.
-
rank 0 sample 2: Hello, I'm a language model, so I won't bother doing it again. My reason is so to have a nice set of rules.
- 1
rank 0 sample 3: Hello, I'm a language model, which just comes from the fact that when I tell a friend I'm doing the verb form with a certain verb. As
step 10550, loss: 3.228519, norm:0.2533, lr:1.8804e-04 dt: 48521.14ms, tok/sec:10805.35
step 10551, loss: 3.309586, norm:0.2551, lr:1.8799e-04 dt: 3332.53ms, tok/sec:157324.52
step 10552, loss: 3.245304, norm:0.2936, lr:1.8794e-04 dt: 3332.01ms, tok/sec:157349.00
step 10553, loss: 3.265294, norm:0.2637, lr:1.8789e-04 dt: 3331.98ms, tok/sec:157350.51
step 10554, loss: 3.240240, norm:0.2634, lr:1.8784e-04 dt: 3332.09ms, tok/sec:157345.14
step 10555, loss: 3.240547, norm:0.2611, lr:1.8779e-04 dt: 3331.91ms, tok/sec:157353.46
step 10556, loss: 3.252577, norm:0.2521, lr:1.8774e-04 dt: 3332.31ms, tok/sec:157334.91
step 10557, loss: 3.213829, norm:0.2491, lr:1.8769e-04 dt: 3332.37ms, tok/sec:157331.86
step 10558, loss: 3.243018, norm:0.2692, lr:1.8764e-04 dt: 3332.11ms, tok/sec:157344.23
step 10559, loss: 3.267772, norm:0.2659, lr:1.8760e-04 dt: 3332.00ms, tok/sec:157349.20
step 10560, loss: 3.254424, norm:0.2612, lr:1.8755e-04 dt: 3332.32ms, tok/sec:157334.43
step 10561, loss: 3.260956, norm:0.2979, lr:1.8750e-04 dt: 3332.02ms, tok/sec:157348.52
step 10562, loss: 3.229240, norm:0.3238, lr:1.8745e-04 dt: 3332.05ms, tok/sec:157346.85
step 10563, loss: 3.171426, norm:0.2723, lr:1.8740e-04 dt: 3331.95ms, tok/sec:157351.65
step 10564, loss: 3.311466, norm:0.2977, lr:1.8735e-04 dt: 3332.30ms, tok/sec:157335.06
step 10565, loss: 3.328144, norm:0.2927, lr:1.8730e-04 dt: 3331.97ms, tok/sec:157350.94
step 10566, loss: 3.264728, norm:0.2827, lr:1.8725e-04 dt: 3332.08ms, tok/sec:157345.44
step 10567, loss: 3.323299, norm:0.2777, lr:1.8720e-04 dt: 3332.11ms, tok/sec:157344.14
step 10568, loss: 3.308764, norm:0.2887, lr:1.8715e-04 dt: 3332.12ms, tok/sec:157343.61
step 10569, loss: 3.271975, norm:0.2648, lr:1.8710e-04 dt: 3332.33ms, tok/sec:157333.62
step 10570, loss: 3.325446, norm:0.2682, lr:1.8705e-04 dt: 3332.23ms, tok/sec:157338.34
step 10571, loss: 3.245023, norm:0.2515, lr:1.8700e-04 dt: 3331.71ms, tok/sec:157362.83
step 10572, loss: 3.288049, norm:0.2539, lr:1.8695e-04 dt: 3331.99ms, tok/sec:157349.97
step 10573, loss: 3.268995, norm:0.2672, lr:1.8690e-04 dt: 3332.17ms, tok/sec:157341.10
step 10574, loss: 3.272235, norm:0.2643, lr:1.8685e-04 dt: 3332.38ms, tok/sec:157331.55
step 10575, loss: 3.323943, norm:0.3096, lr:1.8680e-04 dt: 3332.07ms, tok/sec:157345.95
step 10576, loss: 3.293365, norm:0.2776, lr:1.8675e-04 dt: 3332.20ms, tok/sec:157339.67
step 10577, loss: 3.238960, norm:0.2613, lr:1.8670e-04 dt: 3332.04ms, tok/sec:157347.59
step 10578, loss: 3.276303, norm:0.2513, lr:1.8665e-04 dt: 3332.62ms, tok/sec:157320.03
step 10579, loss: 3.219946, norm:0.2750, lr:1.8661e-04 dt: 3332.20ms, tok/sec:157339.74
step 10580, loss: 3.198462, norm:0.2750, lr:1.8656e-04 dt: 3331.98ms, tok/sec:157350.39
step 10581, loss: 3.224885, norm:0.2643, lr:1.8651e-04 dt: 3332.14ms, tok/sec:157342.62
step 10582, loss: 3.303845, norm:0.2746, lr:1.8646e-04 dt: 3332.17ms, tok/sec:157341.11
step 10583, loss: 3.269658, norm:0.2762, lr:1.8641e-04 dt: 3332.12ms, tok/sec:157343.51
step 10584, loss: 3.201826, norm:0.2747, lr:1.8636e-04 dt: 3332.16ms, tok/sec:157341.77
step 10585, loss: 3.239593, norm:0.2746, lr:1.8631e-04 dt: 3332.11ms, tok/sec:157343.98
step 10586, loss: 3.236931, norm:0.2569, lr:1.8626e-04 dt: 3332.14ms, tok/sec:157342.93
step 10587, loss: 3.242378, norm:0.2631, lr:1.8621e-04 dt: 3332.25ms, tok/sec:157337.43
step 10588, loss: 3.222879, norm:0.2754, lr:1.8616e-04 dt: 3332.34ms, tok/sec:157333.33
step 10589, loss: 3.229989, norm:0.2850, lr:1.8611e-04 dt: 3332.15ms, tok/sec:157342.45
step 10590, loss: 3.263875, norm:0.2830, lr:1.8606e-04 dt: 3332.33ms, tok/sec:157333.95
step 10591, loss: 3.197423, norm:0.2700, lr:1.8601e-04 dt: 3331.92ms, tok/sec:157353.08
step 10592, loss: 3.269713, norm:0.2773, lr:1.8596e-04 dt: 3332.10ms, tok/sec:157344.84
step 10593, loss: 3.216632, norm:0.2608, lr:1.8591e-04 dt: 3332.07ms, tok/sec:157346.14
step 10594, loss: 3.278722, norm:0.2611, lr:1.8586e-04 dt: 3332.10ms, tok/sec:157344.71
step 10595, loss: 3.243516, norm:0.2685, lr:1.8582e-04 dt: 3332.10ms, tok/sec:157344.84
step 10596, loss: 3.189005, norm:0.2467, lr:1.8577e-04 dt: 3332.12ms, tok/sec:157343.86
step 10597, loss: 3.235821, norm:0.2620, lr:1.8572e-04 dt: 3332.41ms, tok/sec:157329.98
step 10598, loss: 3.243223, norm:0.2733, lr:1.8567e-04 dt: 3332.24ms, tok/sec:157337.95
step 10599, loss: 3.257571, norm:0.2466, lr:1.8562e-04 dt: 3332.08ms, tok/sec:157345.62
validation loss: 3.2553
Model and optimizer state saved.
HellaSwag accuracy:6936902083617983569/-2=-3468451041808991744.0000
rank 1 sample 0: Hello, I'm a language model, and so don't feel like there've never been anything like this before. :)
Ok, this is an example of
rank 1 sample 1: Hello, I'm a language model, a computer language. I'm using Java for this. We'll build a class that calls the Arduino IDE.
I
rank 1 sample 2: Hello, I'm a language model, but only the code that I need to write my code for each object in my model. So that means that we can
rank 1 sample 3: Hello, I'm a language model, and I'm pretty good at it. Any help would be great. The "I/O" is a pretty good
rank 0 sample 0: Hello, I'm a language model, and I'll be talking about my future," Liesel said.
Towards the end of this lesson, she
rank 0 sample 1: Hello, I'm a language model, so don't worry if you don't know a "gugler" is a language model, then the two languages
rank 0 sample 2: Hello, I'm a language model, but I guess it's fun to teach. (Okay, let me see what a language model is.)
So I
rank 0 sample 3: Hello, I'm a language model, and can't tell you how much money you borrowed.
[email protected] Hi, thank you.
Posted February
step 10600, loss: 3.270772, norm:0.2741, lr:1.8557e-04 dt: 56205.54ms, tok/sec:9328.05
step 10601, loss: 3.267917, norm:0.2765, lr:1.8552e-04 dt: 3331.96ms, tok/sec:157351.23
step 10602, loss: 3.308400, norm:0.2561, lr:1.8547e-04 dt: 3332.41ms, tok/sec:157329.85
step 10603, loss: 3.322967, norm:0.2675, lr:1.8542e-04 dt: 3332.10ms, tok/sec:157344.64
step 10604, loss: 3.292089, norm:0.2685, lr:1.8537e-04 dt: 3331.98ms, tok/sec:157350.34
step 10605, loss: 3.352272, norm:0.2744, lr:1.8532e-04 dt: 3332.21ms, tok/sec:157339.53
step 10606, loss: 3.258536, norm:0.2581, lr:1.8527e-04 dt: 3332.22ms, tok/sec:157338.81
step 10607, loss: 3.321728, norm:0.2644, lr:1.8522e-04 dt: 3331.96ms, tok/sec:157351.43
step 10608, loss: 3.344868, norm:0.3022, lr:1.8518e-04 dt: 3331.95ms, tok/sec:157351.65
step 10609, loss: 3.246200, norm:0.2723, lr:1.8513e-04 dt: 3332.39ms, tok/sec:157330.98
step 10610, loss: 3.327672, norm:0.2804, lr:1.8508e-04 dt: 3331.90ms, tok/sec:157353.97
step 10611, loss: 3.274339, norm:0.2879, lr:1.8503e-04 dt: 3332.46ms, tok/sec:157327.52
step 10612, loss: 3.296126, norm:0.2769, lr:1.8498e-04 dt: 3331.95ms, tok/sec:157351.85
step 10613, loss: 3.218380, norm:0.2784, lr:1.8493e-04 dt: 3332.07ms, tok/sec:157346.14
step 10614, loss: 3.252882, norm:0.2828, lr:1.8488e-04 dt: 3332.05ms, tok/sec:157347.06
step 10615, loss: 3.242776, norm:0.2608, lr:1.8483e-04 dt: 3332.18ms, tok/sec:157341.03
step 10616, loss: 3.230304, norm:0.2781, lr:1.8478e-04 dt: 3332.05ms, tok/sec:157346.90
step 10617, loss: 3.245088, norm:0.2666, lr:1.8473e-04 dt: 3332.07ms, tok/sec:157346.16
step 10618, loss: 3.188106, norm:0.2682, lr:1.8468e-04 dt: 3332.11ms, tok/sec:157344.23
step 10619, loss: 3.253758, norm:0.2598, lr:1.8463e-04 dt: 3332.20ms, tok/sec:157340.01
step 10620, loss: 3.238546, norm:0.2598, lr:1.8458e-04 dt: 3332.22ms, tok/sec:157338.81
step 10621, loss: 3.197293, norm:0.2628, lr:1.8454e-04 dt: 3332.02ms, tok/sec:157348.49
step 10622, loss: 3.205036, norm:0.2424, lr:1.8449e-04 dt: 3331.96ms, tok/sec:157351.29
step 10623, loss: 3.267614, norm:0.2795, lr:1.8444e-04 dt: 3332.09ms, tok/sec:157345.17
step 10624, loss: 3.239667, norm:0.2718, lr:1.8439e-04 dt: 3332.19ms, tok/sec:157340.24
step 10625, loss: 3.230856, norm:0.2647, lr:1.8434e-04 dt: 3332.25ms, tok/sec:157337.62
step 10626, loss: 3.244134, norm:0.2864, lr:1.8429e-04 dt: 3331.97ms, tok/sec:157350.54
step 10627, loss: 3.314202, norm:0.2728, lr:1.8424e-04 dt: 3332.02ms, tok/sec:157348.57
step 10628, loss: 3.233886, norm:0.2775, lr:1.8419e-04 dt: 3332.33ms, tok/sec:157333.85
step 10629, loss: 3.196322, norm:0.2695, lr:1.8414e-04 dt: 3332.07ms, tok/sec:157345.91
step 10630, loss: 3.308875, norm:0.2885, lr:1.8409e-04 dt: 3332.26ms, tok/sec:157337.05
step 10631, loss: 3.266912, norm:0.2725, lr:1.8404e-04 dt: 3332.09ms, tok/sec:157345.15
step 10632, loss: 3.287962, norm:0.2748, lr:1.8400e-04 dt: 3332.20ms, tok/sec:157339.87
step 10633, loss: 3.214986, norm:0.2561, lr:1.8395e-04 dt: 3332.06ms, tok/sec:157346.50
step 10634, loss: 3.279575, norm:0.2605, lr:1.8390e-04 dt: 3332.11ms, tok/sec:157344.30
step 10635, loss: 3.355393, norm:0.2967, lr:1.8385e-04 dt: 3332.20ms, tok/sec:157339.79
step 10636, loss: 3.347880, norm:0.2659, lr:1.8380e-04 dt: 3332.14ms, tok/sec:157342.54
step 10637, loss: 3.328164, norm:0.2925, lr:1.8375e-04 dt: 3332.02ms, tok/sec:157348.22
step 10638, loss: 3.291838, norm:0.2679, lr:1.8370e-04 dt: 3332.15ms, tok/sec:157342.36
step 10639, loss: 3.342677, norm:0.2807, lr:1.8365e-04 dt: 3332.16ms, tok/sec:157341.99
step 10640, loss: 3.322280, norm:0.2731, lr:1.8360e-04 dt: 3332.39ms, tok/sec:157330.82
step 10641, loss: 3.330455, norm:0.2904, lr:1.8355e-04 dt: 3332.03ms, tok/sec:157347.99
step 10642, loss: 3.242518, norm:0.2601, lr:1.8351e-04 dt: 3331.94ms, tok/sec:157352.37
step 10643, loss: 3.252186, norm:0.2803, lr:1.8346e-04 dt: 3331.98ms, tok/sec:157350.31
step 10644, loss: 3.284790, norm:0.2722, lr:1.8341e-04 dt: 3331.94ms, tok/sec:157352.37
step 10645, loss: 3.261486, norm:0.2708, lr:1.8336e-04 dt: 3332.18ms, tok/sec:157340.66
step 10646, loss: 3.255427, norm:0.2735, lr:1.8331e-04 dt: 3331.87ms, tok/sec:157355.59
step 10647, loss: 3.245942, norm:0.2606, lr:1.8326e-04 dt: 3332.13ms, tok/sec:157343.28
step 10648, loss: 3.228272, norm:0.2708, lr:1.8321e-04 dt: 3331.96ms, tok/sec:157351.03
step 10649, loss: 3.231786, norm:0.2886, lr:1.8316e-04 dt: 3332.36ms, tok/sec:157332.22
HellaSwag accuracy:-6898279116964656047/-2=3449139558482328064.0000
rank 1 sample 0: Hello, I'm a language model, and so can't make a computer and it does. When you're done, you just open and open an I/
rank 1 sample 1: Hello, I'm a language model, a model of knowledge. I want to have a lot as a scientist, as an English teacher, and as a teacher
rank 1 sample 2: Hello, I'm a language model, but because the concept of language is not a one-way equation for a language, it seems very different from how it
rank 1 sample 3: Hello, I'm a language model, and I'm using the grammar to talk much more about coding and getting data oriented. "That's a good point,
rank 0 sample 0: Hello, I'm a language model, and I love to learn to program the machine much better.
To build my own robot, first we have to define
rank 0 sample 1: Hello, I'm a language model, so there's a lot of language I can do over the course of my career, so I've worked on a lot
rank 0 sample 2: Hello, I'm a language model, so I decided to leave school and continue my job at the end. I'm very happy to be able to work in
rank 0 sample 3: Hello, I'm a language model, and what I like to do with that? Are you a language model that I think has all of the same things all
step 10650, loss: 3.210953, norm:0.2607, lr:1.8311e-04 dt: 48519.28ms, tok/sec:10805.77
step 10651, loss: 3.201280, norm:0.2833, lr:1.8306e-04 dt: 3332.39ms, tok/sec:157330.78
step 10652, loss: 3.181765, norm:0.2758, lr:1.8302e-04 dt: 3332.03ms, tok/sec:157348.00
step 10653, loss: 3.215719, norm:0.2607, lr:1.8297e-04 dt: 3332.07ms, tok/sec:157345.91
step 10654, loss: 3.214088, norm:0.2624, lr:1.8292e-04 dt: 3332.08ms, tok/sec:157345.67
step 10655, loss: 3.262342, norm:0.2572, lr:1.8287e-04 dt: 3332.22ms, tok/sec:157339.06
step 10656, loss: 3.226089, norm:0.2462, lr:1.8282e-04 dt: 3331.93ms, tok/sec:157352.47
step 10657, loss: 3.293528, norm:0.2511, lr:1.8277e-04 dt: 3331.94ms, tok/sec:157352.29
step 10658, loss: 3.211720, norm:0.2745, lr:1.8272e-04 dt: 3332.28ms, tok/sec:157336.26
step 10659, loss: 3.302120, norm:0.3016, lr:1.8267e-04 dt: 3332.12ms, tok/sec:157343.53
step 10660, loss: 3.226754, norm:0.2501, lr:1.8262e-04 dt: 3332.64ms, tok/sec:157319.28
step 10661, loss: 3.242497, norm:0.2643, lr:1.8258e-04 dt: 3332.05ms, tok/sec:157347.01
step 10662, loss: 3.280372, norm:0.2597, lr:1.8253e-04 dt: 3331.93ms, tok/sec:157352.53
step 10663, loss: 3.343168, norm:0.3134, lr:1.8248e-04 dt: 3332.04ms, tok/sec:157347.54
step 10664, loss: 3.246096, norm:0.2653, lr:1.8243e-04 dt: 3332.29ms, tok/sec:157335.63
step 10665, loss: 3.259541, norm:0.2655, lr:1.8238e-04 dt: 3332.07ms, tok/sec:157346.15
step 10666, loss: 3.315875, norm:0.3260, lr:1.8233e-04 dt: 3331.84ms, tok/sec:157356.94
step 10667, loss: 3.254072, norm:0.2741, lr:1.8228e-04 dt: 3334.37ms, tok/sec:157237.62
step 10668, loss: 3.235100, norm:0.2763, lr:1.8223e-04 dt: 3332.54ms, tok/sec:157324.01
step 10669, loss: 3.259286, norm:0.2595, lr:1.8219e-04 dt: 3331.99ms, tok/sec:157349.79
step 10670, loss: 3.308388, norm:0.3238, lr:1.8214e-04 dt: 3332.11ms, tok/sec:157344.24
step 10671, loss: 3.256233, norm:0.2760, lr:1.8209e-04 dt: 3332.01ms, tok/sec:157348.94
step 10672, loss: 3.301942, norm:0.2612, lr:1.8204e-04 dt: 3332.11ms, tok/sec:157344.26
step 10673, loss: 3.292890, norm:0.2731, lr:1.8199e-04 dt: 3332.42ms, tok/sec:157329.30
step 10674, loss: 3.315407, norm:0.2683, lr:1.8194e-04 dt: 3332.41ms, tok/sec:157330.07
step 10675, loss: 3.243105, norm:0.2651, lr:1.8189e-04 dt: 3332.18ms, tok/sec:157340.93
step 10676, loss: 3.309976, norm:0.2760, lr:1.8184e-04 dt: 3332.16ms, tok/sec:157341.98
step 10677, loss: 3.289890, norm:0.2802, lr:1.8180e-04 dt: 3332.04ms, tok/sec:157347.36
step 10678, loss: 3.314588, norm:0.2624, lr:1.8175e-04 dt: 3332.13ms, tok/sec:157343.12
step 10679, loss: 3.252326, norm:0.2680, lr:1.8170e-04 dt: 3331.98ms, tok/sec:157350.14
step 10680, loss: 3.375604, norm:0.2759, lr:1.8165e-04 dt: 3332.53ms, tok/sec:157324.52
step 10681, loss: 3.228696, norm:0.2689, lr:1.8160e-04 dt: 3332.09ms, tok/sec:157345.32
step 10682, loss: 3.207279, norm:0.2704, lr:1.8155e-04 dt: 3332.13ms, tok/sec:157343.42
step 10683, loss: 3.236854, norm:0.2649, lr:1.8150e-04 dt: 3332.24ms, tok/sec:157338.14
step 10684, loss: 3.186149, norm:0.2875, lr:1.8145e-04 dt: 3332.23ms, tok/sec:157338.58
step 10685, loss: 3.178466, norm:0.2732, lr:1.8141e-04 dt: 3332.09ms, tok/sec:157345.16
step 10686, loss: 3.266759, norm:0.3024, lr:1.8136e-04 dt: 3332.17ms, tok/sec:157341.46
step 10687, loss: 3.244879, norm:0.2870, lr:1.8131e-04 dt: 3332.00ms, tok/sec:157349.13
step 10688, loss: 3.264299, norm:0.2890, lr:1.8126e-04 dt: 3332.31ms, tok/sec:157334.83
step 10689, loss: 3.251369, norm:0.2819, lr:1.8121e-04 dt: 3332.23ms, tok/sec:157338.29
step 10690, loss: 3.205675, norm:0.2843, lr:1.8116e-04 dt: 3332.07ms, tok/sec:157346.11
step 10691, loss: 3.212417, norm:0.2755, lr:1.8111e-04 dt: 3332.20ms, tok/sec:157339.95
step 10692, loss: 3.218491, norm:0.2571, lr:1.8106e-04 dt: 3332.22ms, tok/sec:157339.02
step 10693, loss: 3.225120, norm:0.2838, lr:1.8102e-04 dt: 3332.07ms, tok/sec:157346.21
step 10694, loss: 3.298836, norm:0.2712, lr:1.8097e-04 dt: 3332.08ms, tok/sec:157345.41
step 10695, loss: 3.277199, norm:0.2570, lr:1.8092e-04 dt: 3332.23ms, tok/sec:157338.33
step 10696, loss: 3.239362, norm:0.2806, lr:1.8087e-04 dt: 3332.56ms, tok/sec:157322.84
step 10697, loss: 3.340816, norm:0.2986, lr:1.8082e-04 dt: 3331.99ms, tok/sec:157349.94
step 10698, loss: 3.310224, norm:0.2615, lr:1.8077e-04 dt: 3332.10ms, tok/sec:157344.82
step 10699, loss: 3.229813, norm:0.2708, lr:1.8072e-04 dt: 3332.17ms, tok/sec:157341.45
validation loss: 3.2542
Model and optimizer state saved.
HellaSwag accuracy:6945772943432715345/-2=-3472886471716357632.0000
rank 1 sample 0: Hello, I'm a language model, i'll speak to you as a bit. This isn't too hard, but it makes a nice little mess.

rank 1 sample 1: Hello, I'm a language model, which I think really helps me. If you want to change the form of this module at any point, you can do
rank 1 sample 2: Hello, I'm a language model, but then the first thing I want to do is to take me through some of the basics and try to get you started
rank 1 sample 3: Hello, I'm a language model, and I'm using the Pangloss library. I decided to give an assessment of grammar but not a particular level of
rank 0 sample 0: Hello, I'm a language model, and I love the idea of making your website usable.
So when I see a web page I can use it and
rank 0 sample 1: Hello, I'm a language model, so my thoughts are in the right range. But one should not just speak it, because language is always evolving.

rank 0 sample 2: Hello, I'm a language model, so I understand how humans speak. That's pretty cool. The reason I'm going to use it is because it takes
rank 0 sample 3: Hello, I'm a language model, and can't help but help you understand how what you're doing are.<|endoftext|>(1830-1869). C
step 10700, loss: 3.335697, norm:0.3571, lr:1.8068e-04 dt: 56207.02ms, tok/sec:9327.80
step 10701, loss: 3.274330, norm:0.2845, lr:1.8063e-04 dt: 3332.10ms, tok/sec:157344.69
step 10702, loss: 3.263357, norm:0.2839, lr:1.8058e-04 dt: 3332.22ms, tok/sec:157339.08
step 10703, loss: 3.293494, norm:0.2952, lr:1.8053e-04 dt: 3332.31ms, tok/sec:157334.79
step 10704, loss: 3.325656, norm:0.2779, lr:1.8048e-04 dt: 3332.04ms, tok/sec:157347.57
step 10705, loss: 3.256539, norm:0.2862, lr:1.8043e-04 dt: 3331.98ms, tok/sec:157350.19
step 10706, loss: 3.282294, norm:0.3071, lr:1.8038e-04 dt: 3332.37ms, tok/sec:157331.92
step 10707, loss: 3.279127, norm:0.2700, lr:1.8034e-04 dt: 3332.20ms, tok/sec:157339.84
step 10708, loss: 3.292996, norm:0.2840, lr:1.8029e-04 dt: 3332.50ms, tok/sec:157325.84
step 10709, loss: 3.307789, norm:0.2810, lr:1.8024e-04 dt: 3331.90ms, tok/sec:157354.12
step 10710, loss: 3.288722, norm:0.2847, lr:1.8019e-04 dt: 3332.17ms, tok/sec:157341.24
step 10711, loss: 3.327102, norm:0.2835, lr:1.8014e-04 dt: 3332.31ms, tok/sec:157334.47
step 10712, loss: 3.257546, norm:0.2836, lr:1.8009e-04 dt: 3332.01ms, tok/sec:157348.82
step 10713, loss: 3.289465, norm:0.2658, lr:1.8004e-04 dt: 3332.05ms, tok/sec:157347.13
step 10714, loss: 3.270738, norm:0.2633, lr:1.8000e-04 dt: 3332.05ms, tok/sec:157347.02
step 10715, loss: 3.300912, norm:0.2683, lr:1.7995e-04 dt: 3332.68ms, tok/sec:157317.27
step 10716, loss: 3.270863, norm:0.2610, lr:1.7990e-04 dt: 3332.10ms, tok/sec:157344.82
step 10717, loss: 3.241518, norm:0.2859, lr:1.7985e-04 dt: 3331.99ms, tok/sec:157349.98
step 10718, loss: 3.242731, norm:0.2786, lr:1.7980e-04 dt: 3332.06ms, tok/sec:157346.48
step 10719, loss: 3.198790, norm:0.2716, lr:1.7975e-04 dt: 3332.02ms, tok/sec:157348.40
step 10720, loss: 3.230627, norm:0.2815, lr:1.7971e-04 dt: 3331.88ms, tok/sec:157354.81
step 10721, loss: 3.184247, norm:0.2783, lr:1.7966e-04 dt: 3332.15ms, tok/sec:157342.41
step 10722, loss: 3.230930, norm:0.2799, lr:1.7961e-04 dt: 3332.14ms, tok/sec:157342.69
step 10723, loss: 3.240561, norm:0.2676, lr:1.7956e-04 dt: 3332.29ms, tok/sec:157335.86
step 10724, loss: 3.218189, norm:0.2680, lr:1.7951e-04 dt: 3332.41ms, tok/sec:157330.21
step 10725, loss: 3.210737, norm:0.2606, lr:1.7946e-04 dt: 3331.97ms, tok/sec:157350.66
step 10726, loss: 3.202641, norm:0.2628, lr:1.7941e-04 dt: 3332.15ms, tok/sec:157342.18
step 10727, loss: 3.255696, norm:0.3192, lr:1.7937e-04 dt: 3332.01ms, tok/sec:157348.69
step 10728, loss: 3.206478, norm:0.2588, lr:1.7932e-04 dt: 3332.05ms, tok/sec:157347.03
step 10729, loss: 3.300446, norm:0.3162, lr:1.7927e-04 dt: 3332.19ms, tok/sec:157340.50
step 10730, loss: 3.247172, norm:0.2871, lr:1.7922e-04 dt: 3332.15ms, tok/sec:157342.10
step 10731, loss: 3.241091, norm:0.2615, lr:1.7917e-04 dt: 3332.44ms, tok/sec:157328.69
step 10732, loss: 3.265212, norm:0.3043, lr:1.7912e-04 dt: 3332.07ms, tok/sec:157346.10
step 10733, loss: 3.256392, norm:0.2889, lr:1.7908e-04 dt: 3331.81ms, tok/sec:157358.49
step 10734, loss: 3.278865, norm:0.2870, lr:1.7903e-04 dt: 3332.22ms, tok/sec:157339.15
step 10735, loss: 3.216348, norm:0.2584, lr:1.7898e-04 dt: 3332.03ms, tok/sec:157347.91
step 10736, loss: 3.247338, norm:0.2700, lr:1.7893e-04 dt: 3332.10ms, tok/sec:157344.62
step 10737, loss: 3.307440, norm:0.2676, lr:1.7888e-04 dt: 3332.20ms, tok/sec:157339.87
step 10738, loss: 3.259645, norm:0.2708, lr:1.7883e-04 dt: 3332.11ms, tok/sec:157344.30
step 10739, loss: 3.218247, norm:0.2635, lr:1.7879e-04 dt: 3332.27ms, tok/sec:157336.70
step 10740, loss: 3.259549, norm:0.2743, lr:1.7874e-04 dt: 3332.38ms, tok/sec:157331.24
step 10741, loss: 3.279233, norm:0.2685, lr:1.7869e-04 dt: 3331.88ms, tok/sec:157355.04
step 10742, loss: 3.218182, norm:0.2527, lr:1.7864e-04 dt: 3332.16ms, tok/sec:157341.62
step 10743, loss: 3.270671, norm:0.2634, lr:1.7859e-04 dt: 3332.16ms, tok/sec:157341.80
step 10744, loss: 3.294890, norm:0.2619, lr:1.7854e-04 dt: 3331.88ms, tok/sec:157355.23
step 10745, loss: 3.234188, norm:0.2595, lr:1.7850e-04 dt: 3332.12ms, tok/sec:157343.57
step 10746, loss: 3.243635, norm:0.2517, lr:1.7845e-04 dt: 3332.32ms, tok/sec:157334.11
step 10747, loss: 3.285501, norm:0.3103, lr:1.7840e-04 dt: 3332.32ms, tok/sec:157334.06
step 10748, loss: 3.264425, norm:0.2651, lr:1.7835e-04 dt: 3332.11ms, tok/sec:157344.31
step 10749, loss: 3.242477, norm:0.2908, lr:1.7830e-04 dt: 3332.42ms, tok/sec:157329.65
HellaSwag accuracy:-2286452361048650671/-2=1143226180524325376.0000
rank 1 sample 0: Hello, I'm a language model, and you can't have to deal with it."
Lantern, N. C. A. (1894
rank 1 sample 1: Hello, I'm a language model, not an artist, but a human being. I can not understand or not grasp certain facts about music. I can't
rank 1 sample 2: Hello, I'm a language model, but some things don't work for me. I'm also using an object-oriented language to model and understand what students
rank 1 sample 3: Hello, I'm a language model, and I'm really excited to learn new resources and ways to interact with different online communities including people in the home, in
rank 0 sample 0: Hello, I'm a language model, and I need to learn English and how to think about language learning if I were to do it for a career. The
rank 0 sample 1: Hello, I'm a language model, so here's a link to that section.<|endoftext|>Losing Weight is Now Being Called "No-Wast"

rank 0 sample 2: Hello, I'm a language model, so I should say if that's how I teach it, the students would be learning to use it. I think it
rank 0 sample 3: Hello, I'm a language model, and how do I get started. In my programming language, I'd like a nice place to start?
And we
step 10750, loss: 3.211839, norm:0.2710, lr:1.7825e-04 dt: 48526.86ms, tok/sec:10804.08
step 10751, loss: 3.238456, norm:0.2602, lr:1.7821e-04 dt: 3332.10ms, tok/sec:157344.78
step 10752, loss: 3.329109, norm:0.3035, lr:1.7816e-04 dt: 3332.21ms, tok/sec:157339.60
step 10753, loss: 3.186772, norm:0.2527, lr:1.7811e-04 dt: 3331.96ms, tok/sec:157350.99
step 10754, loss: 3.200559, norm:0.2652, lr:1.7806e-04 dt: 3331.96ms, tok/sec:157351.38
step 10755, loss: 3.193251, norm:0.2508, lr:1.7801e-04 dt: 3332.23ms, tok/sec:157338.32
step 10756, loss: 3.216224, norm:0.2605, lr:1.7797e-04 dt: 3332.19ms, tok/sec:157340.52
step 10757, loss: 3.211512, norm:0.2569, lr:1.7792e-04 dt: 3332.56ms, tok/sec:157322.95
step 10758, loss: 3.184687, norm:0.2639, lr:1.7787e-04 dt: 3332.16ms, tok/sec:157341.75
step 10759, loss: 3.230887, norm:0.2499, lr:1.7782e-04 dt: 3331.95ms, tok/sec:157351.92
step 10760, loss: 3.197619, norm:0.2538, lr:1.7777e-04 dt: 3331.88ms, tok/sec:157354.78
step 10761, loss: 3.190004, norm:0.2436, lr:1.7772e-04 dt: 3332.23ms, tok/sec:157338.34
step 10762, loss: 3.205672, norm:0.2672, lr:1.7768e-04 dt: 3332.30ms, tok/sec:157335.24
step 10763, loss: 3.192635, norm:0.2629, lr:1.7763e-04 dt: 3331.90ms, tok/sec:157353.97
step 10764, loss: 3.204243, norm:0.2458, lr:1.7758e-04 dt: 3332.40ms, tok/sec:157330.41
step 10765, loss: 3.320306, norm:0.3054, lr:1.7753e-04 dt: 3332.37ms, tok/sec:157331.75
step 10766, loss: 3.251239, norm:0.2712, lr:1.7748e-04 dt: 3332.09ms, tok/sec:157345.32
step 10767, loss: 3.264453, norm:0.2948, lr:1.7744e-04 dt: 3332.00ms, tok/sec:157349.22
step 10768, loss: 3.204200, norm:0.2812, lr:1.7739e-04 dt: 3332.17ms, tok/sec:157341.31
step 10769, loss: 3.268274, norm:0.2814, lr:1.7734e-04 dt: 3332.30ms, tok/sec:157335.22
step 10770, loss: 3.305325, norm:0.2553, lr:1.7729e-04 dt: 3332.24ms, tok/sec:157337.86
step 10771, loss: 3.266458, norm:0.2842, lr:1.7724e-04 dt: 3332.33ms, tok/sec:157333.77
step 10772, loss: 3.223773, norm:0.2725, lr:1.7720e-04 dt: 3332.19ms, tok/sec:157340.47
step 10773, loss: 3.285297, norm:0.2538, lr:1.7715e-04 dt: 3332.59ms, tok/sec:157321.46
step 10774, loss: 3.261130, norm:0.2790, lr:1.7710e-04 dt: 3332.25ms, tok/sec:157337.65
step 10775, loss: 3.251304, norm:0.2890, lr:1.7705e-04 dt: 3332.02ms, tok/sec:157348.36
step 10776, loss: 3.242968, norm:0.2616, lr:1.7700e-04 dt: 3332.16ms, tok/sec:157341.68
step 10777, loss: 3.231211, norm:0.2741, lr:1.7695e-04 dt: 3332.28ms, tok/sec:157336.16
step 10778, loss: 3.261809, norm:0.2587, lr:1.7691e-04 dt: 3332.12ms, tok/sec:157343.47
step 10779, loss: 3.329866, norm:0.2757, lr:1.7686e-04 dt: 3332.08ms, tok/sec:157345.66
step 10780, loss: 3.264377, norm:0.2637, lr:1.7681e-04 dt: 3332.09ms, tok/sec:157345.04
step 10781, loss: 3.233953, norm:0.2854, lr:1.7676e-04 dt: 3332.17ms, tok/sec:157341.35
step 10782, loss: 3.211682, norm:0.2741, lr:1.7671e-04 dt: 3332.57ms, tok/sec:157322.20
step 10783, loss: 3.217193, norm:0.2685, lr:1.7667e-04 dt: 3332.06ms, tok/sec:157346.61
step 10784, loss: 3.223058, norm:0.2718, lr:1.7662e-04 dt: 3332.10ms, tok/sec:157344.62
step 10785, loss: 3.251988, norm:0.2674, lr:1.7657e-04 dt: 3332.16ms, tok/sec:157341.78
step 10786, loss: 3.244365, norm:0.2571, lr:1.7652e-04 dt: 3332.02ms, tok/sec:157348.33
step 10787, loss: 3.219692, norm:0.2586, lr:1.7647e-04 dt: 3332.19ms, tok/sec:157340.57
step 10788, loss: 3.286634, norm:0.2880, lr:1.7643e-04 dt: 3332.05ms, tok/sec:157346.85
step 10789, loss: 3.312377, norm:0.3233, lr:1.7638e-04 dt: 3332.26ms, tok/sec:157337.07
step 10790, loss: 3.188849, norm:0.2705, lr:1.7633e-04 dt: 3332.42ms, tok/sec:157329.32
step 10791, loss: 3.207477, norm:0.2647, lr:1.7628e-04 dt: 3331.81ms, tok/sec:157358.30
step 10792, loss: 3.246294, norm:0.2599, lr:1.7624e-04 dt: 3332.12ms, tok/sec:157343.55
step 10793, loss: 3.239542, norm:0.2552, lr:1.7619e-04 dt: 3332.17ms, tok/sec:157341.44
step 10794, loss: 3.207711, norm:0.2584, lr:1.7614e-04 dt: 3332.05ms, tok/sec:157347.01
step 10795, loss: 3.307620, norm:0.3187, lr:1.7609e-04 dt: 3332.11ms, tok/sec:157343.91
step 10796, loss: 3.209896, norm:0.2750, lr:1.7604e-04 dt: 3332.08ms, tok/sec:157345.63
step 10797, loss: 3.240573, norm:0.2790, lr:1.7600e-04 dt: 3331.86ms, tok/sec:157355.99
step 10798, loss: 3.241436, norm:0.2636, lr:1.7595e-04 dt: 3332.12ms, tok/sec:157343.52
step 10799, loss: 3.348273, norm:0.3318, lr:1.7590e-04 dt: 3332.38ms, tok/sec:157331.23
validation loss: 3.2544
Model and optimizer state saved.
HellaSwag accuracy:-2286469953234697135/-2=1143234976617348608.0000
rank 1 sample 0: Hello, I'm a language model, and my current book is the original about that language and some parts of it.
"Language modeling is not a new
rank 1 sample 1: Hello, I'm a language model, a teacher. The question is, How do you learn, and in what terms, exactly, really, really?

rank 1 sample 2: Hello, I'm a language model, but people like to say that it's a language. So here is an example of a language model that's called I
rank 0 sample 0: Hello, I'm a language model, and I'll be writing that book here from home.
The next challenge will be a different model, so I can
rank 1 sample 3: Hello, I'm a language model, and I'm looking for examples to describe words that use english for an entire essay. Not easy, but I am.
rank 0 sample 1: Hello, I'm a language model, so there's a lot of work out there to take back. It comes out in two versions. That's the version
rank 0 sample 2: Hello, I'm a language model, but I understand it like so many of us here at the library that it's really like a language model. I hope
rank 0 sample 3: Hello, I'm a language model, so, a language model is the process that builds a language model from scratch and performs code to create other languages that share
step 10800, loss: 3.234728, norm:0.2699, lr:1.7585e-04 dt: 56460.08ms, tok/sec:9286.00
step 10801, loss: 3.200683, norm:0.2630, lr:1.7580e-04 dt: 3332.34ms, tok/sec:157333.36
step 10802, loss: 3.304723, norm:0.2956, lr:1.7576e-04 dt: 3332.00ms, tok/sec:157349.52
step 10803, loss: 3.305436, norm:0.2602, lr:1.7571e-04 dt: 3332.36ms, tok/sec:157332.34
step 10804, loss: 3.284010, norm:0.2740, lr:1.7566e-04 dt: 3332.08ms, tok/sec:157345.66
step 10805, loss: 3.277121, norm:0.2629, lr:1.7561e-04 dt: 3332.38ms, tok/sec:157331.35
step 10806, loss: 3.243863, norm:0.2598, lr:1.7556e-04 dt: 3332.70ms, tok/sec:157316.41
step 10807, loss: 3.316955, norm:0.2674, lr:1.7552e-04 dt: 3332.02ms, tok/sec:157348.57
step 10808, loss: 3.271046, norm:0.2543, lr:1.7547e-04 dt: 3332.15ms, tok/sec:157342.09
step 10809, loss: 3.271301, norm:0.2583, lr:1.7542e-04 dt: 3332.39ms, tok/sec:157330.87
step 10810, loss: 3.264920, norm:0.2683, lr:1.7537e-04 dt: 3332.02ms, tok/sec:157348.46
step 10811, loss: 3.327487, norm:0.2474, lr:1.7533e-04 dt: 3332.65ms, tok/sec:157318.83
step 10812, loss: 3.363469, norm:0.2927, lr:1.7528e-04 dt: 3332.39ms, tok/sec:157331.01
step 10813, loss: 3.287582, norm:0.2686, lr:1.7523e-04 dt: 3332.17ms, tok/sec:157341.13
step 10814, loss: 3.284298, norm:0.2704, lr:1.7518e-04 dt: 3332.08ms, tok/sec:157345.65
step 10815, loss: 3.302715, norm:0.2744, lr:1.7513e-04 dt: 3332.21ms, tok/sec:157339.29
step 10816, loss: 3.306726, norm:0.3189, lr:1.7509e-04 dt: 3332.37ms, tok/sec:157332.09
step 10817, loss: 3.297266, norm:0.2851, lr:1.7504e-04 dt: 3332.20ms, tok/sec:157340.04
step 10818, loss: 3.251158, norm:0.2909, lr:1.7499e-04 dt: 3332.64ms, tok/sec:157318.98
step 10819, loss: 3.302294, norm:0.2864, lr:1.7494e-04 dt: 3332.07ms, tok/sec:157345.84
step 10820, loss: 3.279268, norm:0.3466, lr:1.7490e-04 dt: 3331.91ms, tok/sec:157353.44
step 10821, loss: 3.216624, norm:0.2709, lr:1.7485e-04 dt: 3332.13ms, tok/sec:157343.43
step 10822, loss: 3.268062, norm:0.2733, lr:1.7480e-04 dt: 3332.16ms, tok/sec:157342.00
step 10823, loss: 3.308461, norm:0.3292, lr:1.7475e-04 dt: 3331.98ms, tok/sec:157350.18
step 10824, loss: 3.230212, norm:0.2776, lr:1.7470e-04 dt: 3332.12ms, tok/sec:157343.71
step 10825, loss: 3.229474, norm:0.2732, lr:1.7466e-04 dt: 3332.24ms, tok/sec:157338.23
step 10826, loss: 3.243551, norm:0.2664, lr:1.7461e-04 dt: 3332.18ms, tok/sec:157340.79
step 10827, loss: 3.247976, norm:0.2713, lr:1.7456e-04 dt: 3332.58ms, tok/sec:157321.76
step 10828, loss: 3.243256, norm:0.3129, lr:1.7451e-04 dt: 3331.93ms, tok/sec:157352.83
step 10829, loss: 3.222227, norm:0.2652, lr:1.7447e-04 dt: 3332.10ms, tok/sec:157344.62
step 10830, loss: 3.228368, norm:0.2445, lr:1.7442e-04 dt: 3332.05ms, tok/sec:157346.84
step 10831, loss: 3.197793, norm:0.2601, lr:1.7437e-04 dt: 3331.84ms, tok/sec:157356.94
step 10832, loss: 3.198377, norm:0.2523, lr:1.7432e-04 dt: 3332.16ms, tok/sec:157341.85
step 10833, loss: 3.202024, norm:0.2596, lr:1.7428e-04 dt: 3332.29ms, tok/sec:157335.72
step 10834, loss: 3.223164, norm:0.2771, lr:1.7423e-04 dt: 3332.20ms, tok/sec:157339.87
step 10835, loss: 3.193916, norm:0.2653, lr:1.7418e-04 dt: 3332.08ms, tok/sec:157345.47
step 10836, loss: 3.257096, norm:0.2749, lr:1.7413e-04 dt: 3332.21ms, tok/sec:157339.47
step 10837, loss: 3.223418, norm:0.2576, lr:1.7409e-04 dt: 3332.16ms, tok/sec:157341.84
step 10838, loss: 3.257837, norm:0.2634, lr:1.7404e-04 dt: 3332.21ms, tok/sec:157339.30
step 10839, loss: 3.285982, norm:0.2771, lr:1.7399e-04 dt: 3332.08ms, tok/sec:157345.69
step 10840, loss: 3.267340, norm:0.2631, lr:1.7394e-04 dt: 3332.40ms, tok/sec:157330.61
step 10841, loss: 3.274325, norm:0.2642, lr:1.7389e-04 dt: 3332.27ms, tok/sec:157336.37
step 10842, loss: 3.281442, norm:0.2643, lr:1.7385e-04 dt: 3331.99ms, tok/sec:157349.86
step 10843, loss: 3.234625, norm:0.2584, lr:1.7380e-04 dt: 3332.19ms, tok/sec:157340.57
step 10844, loss: 3.259456, norm:0.2759, lr:1.7375e-04 dt: 3332.22ms, tok/sec:157338.86
step 10845, loss: 3.287917, norm:0.2564, lr:1.7370e-04 dt: 3332.02ms, tok/sec:157348.56
step 10846, loss: 3.227998, norm:0.2711, lr:1.7366e-04 dt: 3331.97ms, tok/sec:157350.59
step 10847, loss: 3.281802, norm:0.2667, lr:1.7361e-04 dt: 3332.42ms, tok/sec:157329.28
step 10848, loss: 3.207101, norm:0.2614, lr:1.7356e-04 dt: 3331.98ms, tok/sec:157350.40
step 10849, loss: 3.245012, norm:0.2559, lr:1.7351e-04 dt: 3332.55ms, tok/sec:157323.15
HellaSwag accuracy:8089823588226925649/-2=-4044911794113462784.0000
rank 1 sample 0: Hello, I'm a language model, this is the first of the three domains that define languages. All of the domains are determined by an I.D.
rank 1 sample 1: Hello, I'm a language model, which means I think I'm trying to apply the basic building blocks of the language. Then I talk to the people who
rank 1 sample 2: Hello, I'm a language model, but...
A. The first thing to do is to take an object from the class, copy its contents (to
rank 1 sample 3: Hello, I'm a language model, and I'm pretty good at it. After all, to keep running code using the new C++ compiler, I'm
rank 0 sample 0: Hello, I'm a language model, and I'll be writing to this, or it'll be my personal English as a first language for a very long time
rank 0 sample 1: Hello, I'm a language model, so how do I do that? When I teach computer science I do this in the language model, you use the language
rank 0 sample 2: Hello, I'm a language model, so I hope I found a way to do so.
My model is a way of trying to break down language types
rank 0 sample 3: Hello, I'm a language model, and want to use it. I will need, first, to model and define something called language. You can't think
step 10850, loss: 3.266919, norm:0.2615, lr:1.7347e-04 dt: 48498.81ms, tok/sec:10810.33
step 10851, loss: 3.284596, norm:0.2562, lr:1.7342e-04 dt: 3332.11ms, tok/sec:157343.95
step 10852, loss: 3.289148, norm:0.2546, lr:1.7337e-04 dt: 3332.25ms, tok/sec:157337.68
step 10853, loss: 3.293499, norm:0.2639, lr:1.7332e-04 dt: 3332.05ms, tok/sec:157346.88
step 10854, loss: 3.291181, norm:0.2759, lr:1.7328e-04 dt: 3331.95ms, tok/sec:157351.53
step 10855, loss: 3.246848, norm:0.2883, lr:1.7323e-04 dt: 3332.21ms, tok/sec:157339.32
step 10856, loss: 3.221380, norm:0.2499, lr:1.7318e-04 dt: 3332.18ms, tok/sec:157340.68
step 10857, loss: 3.304227, norm:0.2707, lr:1.7313e-04 dt: 3332.44ms, tok/sec:157328.65
step 10858, loss: 3.253555, norm:0.2602, lr:1.7309e-04 dt: 3334.54ms, tok/sec:157229.49
step 10859, loss: 3.260146, norm:0.2579, lr:1.7304e-04 dt: 3332.00ms, tok/sec:157349.50
step 10860, loss: 3.272057, norm:0.2609, lr:1.7299e-04 dt: 3332.06ms, tok/sec:157346.39
step 10861, loss: 3.258567, norm:0.2547, lr:1.7294e-04 dt: 3332.01ms, tok/sec:157349.10
step 10862, loss: 3.209884, norm:0.2483, lr:1.7290e-04 dt: 3332.04ms, tok/sec:157347.23
step 10863, loss: 3.177753, norm:0.2875, lr:1.7285e-04 dt: 3332.13ms, tok/sec:157343.29
step 10864, loss: 3.250874, norm:0.2638, lr:1.7280e-04 dt: 3332.13ms, tok/sec:157343.39
step 10865, loss: 3.249777, norm:0.2540, lr:1.7275e-04 dt: 3332.16ms, tok/sec:157341.95
step 10866, loss: 3.183306, norm:0.2893, lr:1.7271e-04 dt: 3331.91ms, tok/sec:157353.56
step 10867, loss: 3.220224, norm:0.2668, lr:1.7266e-04 dt: 3331.98ms, tok/sec:157350.37
step 10868, loss: 3.229007, norm:0.2490, lr:1.7261e-04 dt: 3332.17ms, tok/sec:157341.19
step 10869, loss: 3.175436, norm:0.2461, lr:1.7257e-04 dt: 3331.80ms, tok/sec:157358.99
step 10870, loss: 3.227356, norm:0.2507, lr:1.7252e-04 dt: 3332.40ms, tok/sec:157330.26
step 10871, loss: 3.198890, norm:0.2522, lr:1.7247e-04 dt: 3332.03ms, tok/sec:157348.00
step 10872, loss: 3.259055, norm:0.2810, lr:1.7242e-04 dt: 3332.12ms, tok/sec:157343.83
step 10873, loss: 3.299490, norm:0.2803, lr:1.7238e-04 dt: 3332.01ms, tok/sec:157348.66
step 10874, loss: 3.248564, norm:0.2700, lr:1.7233e-04 dt: 3331.96ms, tok/sec:157351.43
step 10875, loss: 3.235324, norm:0.2921, lr:1.7228e-04 dt: 3331.84ms, tok/sec:157356.98
step 10876, loss: 3.219046, norm:0.3105, lr:1.7223e-04 dt: 3331.91ms, tok/sec:157353.61
step 10877, loss: 3.279043, norm:0.2739, lr:1.7219e-04 dt: 3332.19ms, tok/sec:157340.38
step 10878, loss: 3.302027, norm:0.2972, lr:1.7214e-04 dt: 3331.97ms, tok/sec:157350.84
step 10879, loss: 3.249244, norm:0.2631, lr:1.7209e-04 dt: 3332.62ms, tok/sec:157320.30
step 10880, loss: 3.357293, norm:0.2598, lr:1.7204e-04 dt: 3332.26ms, tok/sec:157336.88
step 10881, loss: 3.270910, norm:0.2952, lr:1.7200e-04 dt: 3331.92ms, tok/sec:157352.89
step 10882, loss: 3.243124, norm:0.2759, lr:1.7195e-04 dt: 3331.89ms, tok/sec:157354.75
step 10883, loss: 3.256986, norm:0.2694, lr:1.7190e-04 dt: 3331.91ms, tok/sec:157353.58
step 10884, loss: 3.261869, norm:0.2773, lr:1.7186e-04 dt: 3332.12ms, tok/sec:157343.52
step 10885, loss: 3.256332, norm:0.2760, lr:1.7181e-04 dt: 3331.74ms, tok/sec:157361.63
step 10886, loss: 3.228323, norm:0.2784, lr:1.7176e-04 dt: 3332.18ms, tok/sec:157341.01
step 10887, loss: 3.271959, norm:0.2795, lr:1.7171e-04 dt: 3332.32ms, tok/sec:157334.35
step 10888, loss: 3.219954, norm:0.2725, lr:1.7167e-04 dt: 3332.08ms, tok/sec:157345.52
step 10889, loss: 3.241212, norm:0.2702, lr:1.7162e-04 dt: 3332.08ms, tok/sec:157345.63
step 10890, loss: 3.220979, norm:0.2699, lr:1.7157e-04 dt: 3332.46ms, tok/sec:157327.59
step 10891, loss: 3.279681, norm:0.2608, lr:1.7152e-04 dt: 3331.99ms, tok/sec:157349.68
step 10892, loss: 3.248244, norm:0.2650, lr:1.7148e-04 dt: 3332.25ms, tok/sec:157337.36
step 10893, loss: 3.228621, norm:0.2567, lr:1.7143e-04 dt: 3332.02ms, tok/sec:157348.36
step 10894, loss: 3.271092, norm:0.2552, lr:1.7138e-04 dt: 3331.95ms, tok/sec:157351.86
step 10895, loss: 3.276217, norm:0.2523, lr:1.7134e-04 dt: 3331.97ms, tok/sec:157350.85
step 10896, loss: 3.220756, norm:0.2955, lr:1.7129e-04 dt: 3332.32ms, tok/sec:157334.17
step 10897, loss: 3.211442, norm:0.2897, lr:1.7124e-04 dt: 3332.11ms, tok/sec:157343.91
step 10898, loss: 3.222556, norm:0.2824, lr:1.7119e-04 dt: 3331.97ms, tok/sec:157350.63
step 10899, loss: 3.205278, norm:0.2682, lr:1.7115e-04 dt: 3332.10ms, tok/sec:157344.44
validation loss: 3.2508
Model and optimizer state saved.
HellaSwag accuracy:6929020784272180305/-2=-3464510392136090112.0000
rank 1 sample 0: Hello, I'm a language model, just so you can define the language environment and its properties. As you can see, in my examples, there are two
rank 1 sample 1: Hello, I'm a language model, a teacher. The reason I've done this is that i am one of the strongest cultures in the world. I am
rank 1 sample 2: Hello, I'm a language model, but then, as I said, I'm not a programmer: it just turns out that there is something about you.
rank 1 sample 3: Hello, I'm a language model, and I'm using the most powerful machine learning techniques to increase understanding of our human language – not just the linguistic.

rank 0 sample 0: Hello, I'm a language model, and I'll be writing some scripts myself here with that. I need to understand the way those language types work, which
rank 0 sample 1: Hello, I'm a language model, so there's a lot of work required to understand . There's an . There are some more . Some languages are easier
rank 0 sample 2: Hello, I'm a language model, so I’m in a room with me. I was able to talk to the other students and I’
rank 0 sample 3: Hello, I'm a language model, and one of the things that you would want to do is to find an environment to get all of the language related elements
step 10900, loss: 3.200148, norm:0.2516, lr:1.7110e-04 dt: 56413.58ms, tok/sec:9293.65
step 10901, loss: 3.272013, norm:0.2747, lr:1.7105e-04 dt: 3331.94ms, tok/sec:157352.33
step 10902, loss: 3.171829, norm:0.2562, lr:1.7101e-04 dt: 3332.40ms, tok/sec:157330.26
step 10903, loss: 3.225085, norm:0.2590, lr:1.7096e-04 dt: 3332.08ms, tok/sec:157345.38
step 10904, loss: 3.223577, norm:0.2481, lr:1.7091e-04 dt: 3331.85ms, tok/sec:157356.52
step 10905, loss: 3.165342, norm:0.2486, lr:1.7086e-04 dt: 3332.00ms, tok/sec:157349.58
step 10906, loss: 3.216897, norm:0.2646, lr:1.7082e-04 dt: 3332.31ms, tok/sec:157334.75
step 10907, loss: 3.234827, norm:0.2437, lr:1.7077e-04 dt: 3332.01ms, tok/sec:157348.70
step 10908, loss: 3.250044, norm:0.2703, lr:1.7072e-04 dt: 3332.03ms, tok/sec:157347.80
step 10909, loss: 3.262622, norm:0.2616, lr:1.7068e-04 dt: 3332.13ms, tok/sec:157343.37
step 10910, loss: 3.236752, norm:0.2853, lr:1.7063e-04 dt: 3332.08ms, tok/sec:157345.54
step 10911, loss: 3.349862, norm:0.2919, lr:1.7058e-04 dt: 3332.65ms, tok/sec:157318.59
step 10912, loss: 3.249536, norm:0.2659, lr:1.7053e-04 dt: 3331.83ms, tok/sec:157357.15
step 10913, loss: 3.274649, norm:0.2904, lr:1.7049e-04 dt: 3332.00ms, tok/sec:157349.33
step 10914, loss: 3.247642, norm:0.2989, lr:1.7044e-04 dt: 3331.96ms, tok/sec:157351.12
step 10915, loss: 3.230765, norm:0.2718, lr:1.7039e-04 dt: 3332.06ms, tok/sec:157346.55
step 10916, loss: 3.263725, norm:0.2650, lr:1.7035e-04 dt: 3332.35ms, tok/sec:157332.95
step 10917, loss: 3.260707, norm:0.2758, lr:1.7030e-04 dt: 3332.04ms, tok/sec:157347.40
step 10918, loss: 3.355818, norm:0.2630, lr:1.7025e-04 dt: 3331.99ms, tok/sec:157349.91
step 10919, loss: 3.311215, norm:0.2766, lr:1.7021e-04 dt: 3332.32ms, tok/sec:157334.14
step 10920, loss: 3.247659, norm:0.2663, lr:1.7016e-04 dt: 3331.91ms, tok/sec:157353.38
step 10921, loss: 3.165143, norm:0.2595, lr:1.7011e-04 dt: 3332.42ms, tok/sec:157329.35
step 10922, loss: 3.225433, norm:0.2626, lr:1.7006e-04 dt: 3332.26ms, tok/sec:157337.17
step 10923, loss: 3.274030, norm:0.2495, lr:1.7002e-04 dt: 3332.00ms, tok/sec:157349.56
step 10924, loss: 3.246223, norm:0.2618, lr:1.6997e-04 dt: 3332.20ms, tok/sec:157339.98
step 10925, loss: 3.250259, norm:0.2521, lr:1.6992e-04 dt: 3332.04ms, tok/sec:157347.42
step 10926, loss: 3.244395, norm:0.2579, lr:1.6988e-04 dt: 3332.05ms, tok/sec:157347.20
step 10927, loss: 3.253523, norm:0.2489, lr:1.6983e-04 dt: 3332.29ms, tok/sec:157335.78
step 10928, loss: 3.226669, norm:0.2572, lr:1.6978e-04 dt: 3332.05ms, tok/sec:157347.03
step 10929, loss: 3.311076, norm:0.2649, lr:1.6974e-04 dt: 3332.06ms, tok/sec:157346.51
step 10930, loss: 3.252736, norm:0.2603, lr:1.6969e-04 dt: 3332.50ms, tok/sec:157325.78
step 10931, loss: 3.219030, norm:0.3579, lr:1.6964e-04 dt: 3331.93ms, tok/sec:157352.73
step 10932, loss: 3.267996, norm:0.2755, lr:1.6959e-04 dt: 3331.97ms, tok/sec:157350.64
step 10933, loss: 3.182642, norm:0.2903, lr:1.6955e-04 dt: 3331.98ms, tok/sec:157350.19
step 10934, loss: 3.200121, norm:0.2545, lr:1.6950e-04 dt: 3332.01ms, tok/sec:157349.08
step 10935, loss: 3.205059, norm:0.2730, lr:1.6945e-04 dt: 3332.14ms, tok/sec:157342.80
step 10936, loss: 3.210916, norm:0.2684, lr:1.6941e-04 dt: 3332.18ms, tok/sec:157340.69
step 10937, loss: 3.240028, norm:0.2598, lr:1.6936e-04 dt: 3331.90ms, tok/sec:157354.21
step 10938, loss: 3.250359, norm:0.2688, lr:1.6931e-04 dt: 3331.98ms, tok/sec:157350.37
step 10939, loss: 3.206580, norm:0.2668, lr:1.6927e-04 dt: 3332.28ms, tok/sec:157336.12
step 10940, loss: 3.235642, norm:0.2677, lr:1.6922e-04 dt: 3332.08ms, tok/sec:157345.44
step 10941, loss: 3.251431, norm:0.2622, lr:1.6917e-04 dt: 3331.92ms, tok/sec:157353.07
step 10942, loss: 3.247214, norm:0.2729, lr:1.6913e-04 dt: 3331.93ms, tok/sec:157352.66
step 10943, loss: 3.226218, norm:0.2772, lr:1.6908e-04 dt: 3332.16ms, tok/sec:157341.66
step 10944, loss: 3.208437, norm:0.2912, lr:1.6903e-04 dt: 3331.95ms, tok/sec:157351.65
step 10945, loss: 3.222496, norm:0.2654, lr:1.6899e-04 dt: 3331.99ms, tok/sec:157349.74
step 10946, loss: 3.278538, norm:0.2878, lr:1.6894e-04 dt: 3332.02ms, tok/sec:157348.49
step 10947, loss: 3.153099, norm:0.2932, lr:1.6889e-04 dt: 3332.32ms, tok/sec:157334.06
step 10948, loss: 3.200998, norm:0.2874, lr:1.6885e-04 dt: 3332.06ms, tok/sec:157346.70
step 10949, loss: 3.236176, norm:0.2564, lr:1.6880e-04 dt: 3332.06ms, tok/sec:157346.70
HellaSwag accuracy:3478119977613493329/-2=-1739059988806746624.0000
rank 0 sample 0: Hello, I'm a language model, and I just want you to see some nice words on your site right now and see them if you like.
If
rank 0 sample 1: Hello, I'm a language model, but when I'm a language model:
The difference between the two would be the differences in syntax or syntax. In
rank 0 sample 2: Hello, I'm a language model, but I live in a time when you're all sitting in on one. I'm not a language model, I just
rank 0 sample 3: Hello, I'm a language model, and when I talk about it I talk about making a language model as if you already have that language model. So as
rank 1 sample 0: Hello, I'm a language model, as you can see the examples above being a function,
s, and in the last two chapters, the one that
rank 1 sample 1: Hello, I'm a language model, which I use regularly. I can also look at the grammar of our language as well when I talk about it.

rank 1 sample 2: Hello, I'm a language model, but some languages can be improved.
I'm happy about a new project at the University of Oxford to improve a basic
rank 1 sample 3: Hello, I'm a language model, and I'm using the new-line data structure. Is this? The model is only 4-5% true,
step 10950, loss: 3.221203, norm:0.2737, lr:1.6875e-04 dt: 48472.49ms, tok/sec:10816.20
step 10951, loss: 3.261533, norm:0.3031, lr:1.6870e-04 dt: 3332.26ms, tok/sec:157337.14
step 10952, loss: 3.216968, norm:0.2544, lr:1.6866e-04 dt: 3332.27ms, tok/sec:157336.74
step 10953, loss: 3.206860, norm:0.2530, lr:1.6861e-04 dt: 3332.19ms, tok/sec:157340.39
step 10954, loss: 3.182400, norm:0.2587, lr:1.6856e-04 dt: 3331.98ms, tok/sec:157350.32
step 10955, loss: 3.240932, norm:0.2742, lr:1.6852e-04 dt: 3331.94ms, tok/sec:157352.21
step 10956, loss: 3.284501, norm:0.2558, lr:1.6847e-04 dt: 3331.94ms, tok/sec:157352.38
step 10957, loss: 3.247734, norm:0.2667, lr:1.6842e-04 dt: 3331.98ms, tok/sec:157350.44
step 10958, loss: 3.237257, norm:0.2689, lr:1.6838e-04 dt: 3332.04ms, tok/sec:157347.49
step 10959, loss: 3.207212, norm:0.2501, lr:1.6833e-04 dt: 3331.87ms, tok/sec:157355.27
step 10960, loss: 3.259299, norm:0.3698, lr:1.6828e-04 dt: 3331.76ms, tok/sec:157360.85
step 10961, loss: 3.262214, norm:0.2703, lr:1.6824e-04 dt: 3332.22ms, tok/sec:157339.02
step 10962, loss: 3.225696, norm:0.2675, lr:1.6819e-04 dt: 3332.11ms, tok/sec:157344.00
step 10963, loss: 3.269303, norm:0.2879, lr:1.6814e-04 dt: 3331.73ms, tok/sec:157362.02
step 10964, loss: 3.282009, norm:0.2740, lr:1.6810e-04 dt: 3332.01ms, tok/sec:157348.65
step 10965, loss: 3.273232, norm:0.2532, lr:1.6805e-04 dt: 3332.07ms, tok/sec:157346.01
step 10966, loss: 3.275175, norm:0.2858, lr:1.6800e-04 dt: 3331.96ms, tok/sec:157351.17
step 10967, loss: 3.215080, norm:0.2842, lr:1.6796e-04 dt: 3332.16ms, tok/sec:157341.94
step 10968, loss: 3.187405, norm:0.2650, lr:1.6791e-04 dt: 3331.93ms, tok/sec:157352.50
step 10969, loss: 3.240934, norm:0.2760, lr:1.6786e-04 dt: 3332.03ms, tok/sec:157347.76
step 10970, loss: 3.233992, norm:0.2766, lr:1.6782e-04 dt: 3332.09ms, tok/sec:157344.93
step 10971, loss: 3.191688, norm:0.2567, lr:1.6777e-04 dt: 3332.35ms, tok/sec:157332.99
step 10972, loss: 3.220806, norm:0.2471, lr:1.6772e-04 dt: 3331.93ms, tok/sec:157352.45
step 10973, loss: 3.209687, norm:0.2524, lr:1.6768e-04 dt: 3332.11ms, tok/sec:157344.26
step 10974, loss: 3.183498, norm:0.2540, lr:1.6763e-04 dt: 3332.10ms, tok/sec:157344.73
step 10975, loss: 3.199893, norm:0.2466, lr:1.6758e-04 dt: 3332.02ms, tok/sec:157348.19
step 10976, loss: 3.160394, norm:0.2548, lr:1.6754e-04 dt: 3331.96ms, tok/sec:157351.24
step 10977, loss: 3.255030, norm:0.2514, lr:1.6749e-04 dt: 3331.86ms, tok/sec:157356.00
step 10978, loss: 3.173818, norm:0.2395, lr:1.6744e-04 dt: 3332.09ms, tok/sec:157344.86
step 10979, loss: 3.257539, norm:0.2672, lr:1.6740e-04 dt: 3331.91ms, tok/sec:157353.47
step 10980, loss: 3.220512, norm:0.2635, lr:1.6735e-04 dt: 3332.07ms, tok/sec:157345.83
step 10981, loss: 3.231145, norm:0.2919, lr:1.6731e-04 dt: 3332.02ms, tok/sec:157348.54
step 10982, loss: 3.230483, norm:0.2743, lr:1.6726e-04 dt: 3332.01ms, tok/sec:157348.71
step 10983, loss: 3.306592, norm:0.3067, lr:1.6721e-04 dt: 3331.85ms, tok/sec:157356.64
step 10984, loss: 3.247310, norm:0.2925, lr:1.6717e-04 dt: 3332.16ms, tok/sec:157341.64
step 10985, loss: 3.239740, norm:0.3037, lr:1.6712e-04 dt: 3331.86ms, tok/sec:157355.72
step 10986, loss: 3.259264, norm:0.2890, lr:1.6707e-04 dt: 3332.14ms, tok/sec:157342.93
step 10987, loss: 3.262744, norm:0.2820, lr:1.6703e-04 dt: 3331.93ms, tok/sec:157352.69
step 10988, loss: 3.231951, norm:0.2806, lr:1.6698e-04 dt: 3331.95ms, tok/sec:157351.84
step 10989, loss: 3.230990, norm:0.2707, lr:1.6693e-04 dt: 3332.31ms, tok/sec:157334.60
step 10990, loss: 3.205486, norm:0.2747, lr:1.6689e-04 dt: 3332.12ms, tok/sec:157343.88
step 10991, loss: 3.277411, norm:0.2731, lr:1.6684e-04 dt: 3331.87ms, tok/sec:157355.68
step 10992, loss: 3.316576, norm:0.2682, lr:1.6679e-04 dt: 3331.98ms, tok/sec:157350.27
step 10993, loss: 3.263417, norm:0.2775, lr:1.6675e-04 dt: 3331.98ms, tok/sec:157350.07
step 10994, loss: 3.266046, norm:0.2563, lr:1.6670e-04 dt: 3332.05ms, tok/sec:157346.83
step 10995, loss: 3.266138, norm:0.2607, lr:1.6665e-04 dt: 3332.01ms, tok/sec:157348.89
step 10996, loss: 3.194483, norm:0.2665, lr:1.6661e-04 dt: 3332.02ms, tok/sec:157348.51
step 10997, loss: 3.296211, norm:0.2642, lr:1.6656e-04 dt: 3331.95ms, tok/sec:157351.91
step 10998, loss: 3.278852, norm:0.2574, lr:1.6651e-04 dt: 3332.18ms, tok/sec:157340.91
step 10999, loss: 3.230614, norm:0.2425, lr:1.6647e-04 dt: 3331.99ms, tok/sec:157349.98
validation loss: 3.2467
Model and optimizer state saved.
HellaSwag accuracy:2316050536263550033/-2=-1158025268131774976.0000
rank 0 sample 0: Hello, I'm a language model, and I'll be able to understand them from time to time, just like my friends in their daydreams. But
rank 0 sample 1: Hello, I'm a language model, so here's a little bit about natively written interfaces you'll want to keep in the context. Here's a bit
rank 0 sample 2: Hello, I'm a language model, but I never heard of C++ in the classroom. I didn't understand it, and I'm not sure how it
rank 0 sample 3: Hello, I'm a language model, so yeah, you know, you can do two things. First, you need to write out a model that will actually
rank 1 sample 0: Hello, I'm a language model, we need to understand what it means right here - it's got to be our language models and our language models.

rank 1 sample 1: Hello, I'm a language model, which means I don't have to worry about the language aspect of the language or how "under the influence of the language
rank 1 sample 2: Hello, I'm a language model, I actually think you're going to have to do some work! So you really need to work out which languages. Because
rank 1 sample 3: Hello, I'm a language model, and I'm using a grammar program like OOP, though this could be good for everyone from my own back, but
step 11000, loss: 3.274688, norm:0.2533, lr:1.6642e-04 dt: 56134.87ms, tok/sec:9339.79
step 11001, loss: 3.206700, norm:0.2587, lr:1.6638e-04 dt: 3332.23ms, tok/sec:157338.56
step 11002, loss: 3.204508, norm:0.2574, lr:1.6633e-04 dt: 3331.84ms, tok/sec:157357.06
step 11003, loss: 3.209159, norm:0.2716, lr:1.6628e-04 dt: 3331.78ms, tok/sec:157359.69
step 11004, loss: 3.208291, norm:0.2743, lr:1.6624e-04 dt: 3332.10ms, tok/sec:157344.39
step 11005, loss: 3.268262, norm:0.2789, lr:1.6619e-04 dt: 3332.35ms, tok/sec:157332.86
step 11006, loss: 3.243970, norm:0.2709, lr:1.6614e-04 dt: 3331.64ms, tok/sec:157366.41
step 11007, loss: 3.215812, norm:0.2679, lr:1.6610e-04 dt: 3331.95ms, tok/sec:157351.73
step 11008, loss: 3.239885, norm:0.2587, lr:1.6605e-04 dt: 3331.98ms, tok/sec:157350.27
step 11009, loss: 3.213544, norm:0.2656, lr:1.6600e-04 dt: 3331.85ms, tok/sec:157356.42
step 11010, loss: 3.216975, norm:0.2567, lr:1.6596e-04 dt: 3332.00ms, tok/sec:157349.32
step 11011, loss: 3.205203, norm:0.2851, lr:1.6591e-04 dt: 3332.02ms, tok/sec:157348.37
step 11012, loss: 3.168364, norm:0.2530, lr:1.6587e-04 dt: 3332.01ms, tok/sec:157348.76
step 11013, loss: 3.193631, norm:0.2730, lr:1.6582e-04 dt: 3331.93ms, tok/sec:157352.78
step 11014, loss: 3.177943, norm:0.2827, lr:1.6577e-04 dt: 3332.06ms, tok/sec:157346.51
step 11015, loss: 3.274991, norm:0.2753, lr:1.6573e-04 dt: 3332.24ms, tok/sec:157337.90
step 11016, loss: 3.230532, norm:0.2761, lr:1.6568e-04 dt: 3332.23ms, tok/sec:157338.62
step 11017, loss: 3.216262, norm:0.2681, lr:1.6563e-04 dt: 3332.27ms, tok/sec:157336.74
step 11018, loss: 3.243236, norm:0.2605, lr:1.6559e-04 dt: 3332.75ms, tok/sec:157314.15
step 11019, loss: 3.247767, norm:0.2740, lr:1.6554e-04 dt: 3332.17ms, tok/sec:157341.30
step 11020, loss: 3.230960, norm:0.2619, lr:1.6550e-04 dt: 3332.11ms, tok/sec:157344.04
step 11021, loss: 3.269934, norm:0.2956, lr:1.6545e-04 dt: 3332.16ms, tok/sec:157341.95
step 11022, loss: 3.233682, norm:0.2644, lr:1.6540e-04 dt: 3332.06ms, tok/sec:157346.43
step 11023, loss: 3.203683, norm:0.3073, lr:1.6536e-04 dt: 3332.07ms, tok/sec:157345.84
step 11024, loss: 3.234061, norm:0.2970, lr:1.6531e-04 dt: 3331.96ms, tok/sec:157351.15
step 11025, loss: 3.248122, norm:0.2697, lr:1.6526e-04 dt: 3332.22ms, tok/sec:157339.07
step 11026, loss: 3.334779, norm:0.2878, lr:1.6522e-04 dt: 3331.88ms, tok/sec:157355.14
step 11027, loss: 3.292143, norm:0.2676, lr:1.6517e-04 dt: 3332.02ms, tok/sec:157348.48
step 11028, loss: 3.253742, norm:0.2819, lr:1.6513e-04 dt: 3332.41ms, tok/sec:157330.16
step 11029, loss: 3.276238, norm:0.2699, lr:1.6508e-04 dt: 3332.23ms, tok/sec:157338.59
step 11030, loss: 3.264138, norm:0.2749, lr:1.6503e-04 dt: 3331.87ms, tok/sec:157355.63
step 11031, loss: 3.267870, norm:0.2697, lr:1.6499e-04 dt: 3331.94ms, tok/sec:157352.16
step 11032, loss: 3.250500, norm:0.2654, lr:1.6494e-04 dt: 3332.04ms, tok/sec:157347.49
step 11033, loss: 3.264066, norm:0.2715, lr:1.6490e-04 dt: 3332.06ms, tok/sec:157346.70
step 11034, loss: 3.249401, norm:0.2744, lr:1.6485e-04 dt: 3332.04ms, tok/sec:157347.24
step 11035, loss: 3.221479, norm:0.2802, lr:1.6480e-04 dt: 3331.88ms, tok/sec:157355.05
step 11036, loss: 3.216901, norm:0.2815, lr:1.6476e-04 dt: 3331.97ms, tok/sec:157350.75
step 11037, loss: 3.240387, norm:0.3043, lr:1.6471e-04 dt: 3332.02ms, tok/sec:157348.20
step 11038, loss: 3.213098, norm:0.2679, lr:1.6466e-04 dt: 3331.96ms, tok/sec:157351.26
step 11039, loss: 3.234884, norm:0.2600, lr:1.6462e-04 dt: 3331.87ms, tok/sec:157355.39
step 11040, loss: 3.105301, norm:0.2641, lr:1.6457e-04 dt: 3331.84ms, tok/sec:157356.86
step 11041, loss: 3.228711, norm:0.2583, lr:1.6453e-04 dt: 3332.08ms, tok/sec:157345.67
step 11042, loss: 3.182302, norm:0.2606, lr:1.6448e-04 dt: 3331.86ms, tok/sec:157356.15
step 11043, loss: 3.227933, norm:0.2750, lr:1.6443e-04 dt: 3331.83ms, tok/sec:157357.48
step 11044, loss: 3.257877, norm:0.2583, lr:1.6439e-04 dt: 3332.01ms, tok/sec:157348.83
step 11045, loss: 3.220890, norm:0.2628, lr:1.6434e-04 dt: 3332.05ms, tok/sec:157346.99
step 11046, loss: 3.169355, norm:0.2622, lr:1.6430e-04 dt: 3332.23ms, tok/sec:157338.45
step 11047, loss: 3.233263, norm:0.2517, lr:1.6425e-04 dt: 3332.24ms, tok/sec:157338.13
step 11048, loss: 3.180122, norm:0.2575, lr:1.6420e-04 dt: 3333.87ms, tok/sec:157260.99
step 11049, loss: 3.303351, norm:0.2855, lr:1.6416e-04 dt: 3332.12ms, tok/sec:157343.84
HellaSwag accuracy:2325092919890379857/-2=-1162546459945189888.0000
rank 0 sample 0: Hello, I'm a language model, and I love it because it allows me to teach it to a student outside of the traditional classroom and in the real world
rank 0 sample 1: Hello, I'm a language model, but for your own purposes, let me explain: . Languages have different degrees of independence, that's right. .

rank 0 sample 2: Hello, I'm a language model, so I should take no trouble now with this!
I am an educator, and if you want to learn a new
rank 0 sample 3: Hello, I'm a language model, and what I can do is use it for another language.
As far as this one does not involve the creation,
rank 1 sample 0: Hello, I'm a language model, and a master of language learning.I am looking to introduce many new things to my kids about how to be a language
rank 1 sample 1: Hello, I'm a language model, which means I don't have to worry about the things or the concepts. You wouldn't have to be a language model
rank 1 sample 2: Hello, I'm a language model, but then, in the future, I'll be able to talk with you about the language that makes me happy in my
rank 1 sample 3: Hello, I'm a language model, and I'm sure I didn't even even know what makes them go down
- He thinks we should use 'if
step 11050, loss: 3.239971, norm:0.2779, lr:1.6411e-04 dt: 48457.44ms, tok/sec:10819.56
step 11051, loss: 3.238299, norm:0.2539, lr:1.6407e-04 dt: 3332.22ms, tok/sec:157338.80
step 11052, loss: 3.198747, norm:0.2742, lr:1.6402e-04 dt: 3332.33ms, tok/sec:157333.56
step 11053, loss: 3.200170, norm:0.2651, lr:1.6397e-04 dt: 3332.04ms, tok/sec:157347.22
step 11054, loss: 3.240346, norm:0.2553, lr:1.6393e-04 dt: 3331.92ms, tok/sec:157352.91
step 11055, loss: 3.267973, norm:0.2559, lr:1.6388e-04 dt: 3332.06ms, tok/sec:157346.65
step 11056, loss: 3.259950, norm:0.2702, lr:1.6384e-04 dt: 3332.19ms, tok/sec:157340.40
step 11057, loss: 3.211366, norm:0.2810, lr:1.6379e-04 dt: 3331.85ms, tok/sec:157356.33
step 11058, loss: 3.250118, norm:0.2692, lr:1.6374e-04 dt: 3332.20ms, tok/sec:157339.81
step 11059, loss: 3.214713, norm:0.2903, lr:1.6370e-04 dt: 3332.28ms, tok/sec:157335.95
step 11060, loss: 3.230692, norm:0.2654, lr:1.6365e-04 dt: 3332.05ms, tok/sec:157346.91
step 11061, loss: 3.233777, norm:0.2613, lr:1.6361e-04 dt: 3331.93ms, tok/sec:157352.42
step 11062, loss: 3.312214, norm:0.2858, lr:1.6356e-04 dt: 3331.83ms, tok/sec:157357.47
step 11063, loss: 3.274606, norm:0.2678, lr:1.6351e-04 dt: 3332.25ms, tok/sec:157337.64
step 11064, loss: 3.254733, norm:0.2691, lr:1.6347e-04 dt: 3332.22ms, tok/sec:157339.04
step 11065, loss: 3.227666, norm:0.3128, lr:1.6342e-04 dt: 3331.98ms, tok/sec:157350.37
step 11066, loss: 3.216552, norm:0.2651, lr:1.6338e-04 dt: 3332.00ms, tok/sec:157349.47
step 11067, loss: 3.287258, norm:0.2718, lr:1.6333e-04 dt: 3332.18ms, tok/sec:157341.01
step 11068, loss: 3.222654, norm:0.2624, lr:1.6328e-04 dt: 3332.06ms, tok/sec:157346.47
step 11069, loss: 3.303817, norm:0.2608, lr:1.6324e-04 dt: 3332.05ms, tok/sec:157347.04
step 11070, loss: 3.265289, norm:0.2714, lr:1.6319e-04 dt: 3331.93ms, tok/sec:157352.51
step 11071, loss: 3.243131, norm:0.2596, lr:1.6315e-04 dt: 3331.94ms, tok/sec:157352.22
step 11072, loss: 3.208634, norm:0.2628, lr:1.6310e-04 dt: 3332.14ms, tok/sec:157342.52
step 11073, loss: 3.191078, norm:0.2687, lr:1.6306e-04 dt: 3331.92ms, tok/sec:157353.18
step 11074, loss: 3.202519, norm:0.2530, lr:1.6301e-04 dt: 3332.08ms, tok/sec:157345.71
step 11075, loss: 3.212975, norm:0.2953, lr:1.6296e-04 dt: 3332.10ms, tok/sec:157344.62
step 11076, loss: 3.229164, norm:0.2521, lr:1.6292e-04 dt: 3332.41ms, tok/sec:157329.92
step 11077, loss: 3.159737, norm:0.2534, lr:1.6287e-04 dt: 3331.92ms, tok/sec:157352.92
step 11078, loss: 3.224749, norm:0.2520, lr:1.6283e-04 dt: 3332.05ms, tok/sec:157346.86
step 11079, loss: 3.149306, norm:0.2590, lr:1.6278e-04 dt: 3332.07ms, tok/sec:157346.23
step 11080, loss: 3.205213, norm:0.2555, lr:1.6273e-04 dt: 3332.06ms, tok/sec:157346.41
step 11081, loss: 3.264392, norm:0.2517, lr:1.6269e-04 dt: 3331.87ms, tok/sec:157355.34
step 11082, loss: 3.293673, norm:0.2623, lr:1.6264e-04 dt: 3331.98ms, tok/sec:157350.31
step 11083, loss: 3.223258, norm:0.2617, lr:1.6260e-04 dt: 3332.18ms, tok/sec:157340.81
step 11084, loss: 3.219044, norm:0.2757, lr:1.6255e-04 dt: 3332.15ms, tok/sec:157342.16
step 11085, loss: 3.264138, norm:0.2926, lr:1.6251e-04 dt: 3331.84ms, tok/sec:157357.09
step 11086, loss: 3.280846, norm:0.2681, lr:1.6246e-04 dt: 3332.01ms, tok/sec:157348.69
step 11087, loss: 3.237287, norm:0.2659, lr:1.6241e-04 dt: 3332.19ms, tok/sec:157340.35
step 11088, loss: 3.256565, norm:0.2877, lr:1.6237e-04 dt: 3332.16ms, tok/sec:157341.99
step 11089, loss: 3.251490, norm:0.2759, lr:1.6232e-04 dt: 3331.82ms, tok/sec:157358.01
step 11090, loss: 3.266123, norm:0.2919, lr:1.6228e-04 dt: 3331.86ms, tok/sec:157356.16
step 11091, loss: 3.220357, norm:0.2950, lr:1.6223e-04 dt: 3332.11ms, tok/sec:157344.06
step 11092, loss: 3.253609, norm:0.3132, lr:1.6219e-04 dt: 3332.26ms, tok/sec:157337.24
step 11093, loss: 3.255997, norm:0.2703, lr:1.6214e-04 dt: 3331.99ms, tok/sec:157349.91
step 11094, loss: 3.259333, norm:0.2801, lr:1.6209e-04 dt: 3332.09ms, tok/sec:157344.97
step 11095, loss: 3.214302, norm:0.3231, lr:1.6205e-04 dt: 3331.78ms, tok/sec:157359.92
step 11096, loss: 3.246886, norm:0.2999, lr:1.6200e-04 dt: 3332.05ms, tok/sec:157347.11
step 11097, loss: 3.257785, norm:0.3130, lr:1.6196e-04 dt: 3331.99ms, tok/sec:157349.73
step 11098, loss: 3.277203, norm:0.2765, lr:1.6191e-04 dt: 3331.94ms, tok/sec:157352.26
step 11099, loss: 3.258351, norm:0.2702, lr:1.6187e-04 dt: 3332.33ms, tok/sec:157333.76
validation loss: 3.2442
Model and optimizer state saved.
HellaSwag accuracy:-1142713977557253103/-2=571356988778626560.0000
rank 0 sample 0: Hello, I'm a language model, and I'll be writing some scripts here too.
OK, OK, but I can't wait for your next lesson
rank 0 sample 1: Hello, I'm a language model, but when I'm doing a word as a language model, it's also my model, not the sentence model. So
rank 0 sample 2: Hello, I'm a language model, but I understand it anyway in my native language?
I am an english teacher, and I am a language teacher,
rank 0 sample 3: Hello, I'm a language model, which should be used to make a system-supported language model. A language model isn't like the computer language. What
rank 1 sample 0: Hello, I'm a language model, and i've been thinking about teaching stuff. How does something help me? ...
Sylie, a third-
rank 1 sample 1: Hello, I'm a language model, which means I've been learning the fundamentals of programming languages a lot. I want to bring some kind of a "learning
rank 1 sample 2: Hello, I'm a language model, but some languages aren't. I'm not a language scientist either; the general public is pretty comfortable in a formal writing
rank 1 sample 3: Hello, I'm a language model, and I'm using a free platform such as GitHub to express my thoughts online before I give your email.
Here's
step 11100, loss: 3.229480, norm:0.2718, lr:1.6182e-04 dt: 56143.33ms, tok/sec:9338.38
step 11101, loss: 3.245047, norm:0.2867, lr:1.6178e-04 dt: 3331.97ms, tok/sec:157350.87
step 11102, loss: 3.284711, norm:0.2552, lr:1.6173e-04 dt: 3332.16ms, tok/sec:157341.82
step 11103, loss: 3.302474, norm:0.2992, lr:1.6168e-04 dt: 3332.30ms, tok/sec:157335.17
step 11104, loss: 3.302963, norm:0.2567, lr:1.6164e-04 dt: 3331.97ms, tok/sec:157350.72
step 11105, loss: 3.272752, norm:0.2748, lr:1.6159e-04 dt: 3331.78ms, tok/sec:157359.73
step 11106, loss: 3.225305, norm:0.2696, lr:1.6155e-04 dt: 3332.17ms, tok/sec:157341.50
step 11107, loss: 3.289841, norm:0.2745, lr:1.6150e-04 dt: 3332.15ms, tok/sec:157342.07
step 11108, loss: 3.265246, norm:0.2782, lr:1.6146e-04 dt: 3331.76ms, tok/sec:157360.53
step 11109, loss: 3.255872, norm:0.2818, lr:1.6141e-04 dt: 3331.83ms, tok/sec:157357.16
step 11110, loss: 3.165908, norm:0.2740, lr:1.6136e-04 dt: 3332.13ms, tok/sec:157343.27
step 11111, loss: 3.215895, norm:0.2580, lr:1.6132e-04 dt: 3332.08ms, tok/sec:157345.72
step 11112, loss: 3.182906, norm:0.2633, lr:1.6127e-04 dt: 3332.00ms, tok/sec:157349.28
step 11113, loss: 3.202722, norm:0.2499, lr:1.6123e-04 dt: 3332.01ms, tok/sec:157348.76
step 11114, loss: 3.159063, norm:0.2541, lr:1.6118e-04 dt: 3332.02ms, tok/sec:157348.37
step 11115, loss: 3.170086, norm:0.2525, lr:1.6114e-04 dt: 3332.04ms, tok/sec:157347.40
step 11116, loss: 3.168901, norm:0.2435, lr:1.6109e-04 dt: 3331.81ms, tok/sec:157358.34
step 11117, loss: 3.212302, norm:0.2717, lr:1.6105e-04 dt: 3331.74ms, tok/sec:157361.79
step 11118, loss: 3.218136, norm:0.2484, lr:1.6100e-04 dt: 3331.90ms, tok/sec:157354.05
step 11119, loss: 3.197393, norm:0.2591, lr:1.6096e-04 dt: 3332.37ms, tok/sec:157331.64
step 11120, loss: 3.296808, norm:0.2628, lr:1.6091e-04 dt: 3331.95ms, tok/sec:157351.75
step 11121, loss: 3.229519, norm:0.2670, lr:1.6086e-04 dt: 3331.80ms, tok/sec:157358.90
step 11122, loss: 3.244814, norm:0.2547, lr:1.6082e-04 dt: 3331.90ms, tok/sec:157353.90
step 11123, loss: 3.227901, norm:0.2487, lr:1.6077e-04 dt: 3332.13ms, tok/sec:157343.06
step 11124, loss: 3.244094, norm:0.2624, lr:1.6073e-04 dt: 3332.14ms, tok/sec:157342.81
step 11125, loss: 3.217422, norm:0.2528, lr:1.6068e-04 dt: 3332.00ms, tok/sec:157349.14
step 11126, loss: 3.234209, norm:0.2462, lr:1.6064e-04 dt: 3332.02ms, tok/sec:157348.19
step 11127, loss: 3.283978, norm:0.2692, lr:1.6059e-04 dt: 3332.40ms, tok/sec:157330.44
step 11128, loss: 3.284018, norm:0.2557, lr:1.6055e-04 dt: 3332.02ms, tok/sec:157348.40
step 11129, loss: 3.314744, norm:0.2686, lr:1.6050e-04 dt: 3331.92ms, tok/sec:157352.90
step 11130, loss: 3.216182, norm:0.2600, lr:1.6046e-04 dt: 3331.85ms, tok/sec:157356.20
step 11131, loss: 3.270371, norm:0.2834, lr:1.6041e-04 dt: 3332.02ms, tok/sec:157348.19
step 11132, loss: 3.250932, norm:0.2832, lr:1.6036e-04 dt: 3331.92ms, tok/sec:157353.30
step 11133, loss: 3.235583, norm:0.2794, lr:1.6032e-04 dt: 3331.79ms, tok/sec:157359.06
step 11134, loss: 3.266465, norm:0.3040, lr:1.6027e-04 dt: 3331.68ms, tok/sec:157364.64
step 11135, loss: 3.359849, norm:0.2695, lr:1.6023e-04 dt: 3332.48ms, tok/sec:157326.69
step 11136, loss: 3.229777, norm:0.2876, lr:1.6018e-04 dt: 3331.84ms, tok/sec:157356.73
step 11137, loss: 3.271418, norm:0.2511, lr:1.6014e-04 dt: 3331.89ms, tok/sec:157354.61
step 11138, loss: 3.252775, norm:0.2634, lr:1.6009e-04 dt: 3332.17ms, tok/sec:157341.51
step 11139, loss: 3.292296, norm:0.2765, lr:1.6005e-04 dt: 3332.20ms, tok/sec:157339.83
step 11140, loss: 3.297546, norm:0.2516, lr:1.6000e-04 dt: 3331.86ms, tok/sec:157355.90
step 11141, loss: 3.263242, norm:0.2616, lr:1.5996e-04 dt: 3332.00ms, tok/sec:157349.49
step 11142, loss: 3.263544, norm:0.2597, lr:1.5991e-04 dt: 3331.84ms, tok/sec:157357.09
step 11143, loss: 3.181695, norm:0.2797, lr:1.5987e-04 dt: 3332.25ms, tok/sec:157337.44
step 11144, loss: 3.258259, norm:0.2564, lr:1.5982e-04 dt: 3332.00ms, tok/sec:157349.15
step 11145, loss: 3.174916, norm:0.2751, lr:1.5978e-04 dt: 3332.00ms, tok/sec:157349.56
step 11146, loss: 3.235942, norm:0.2905, lr:1.5973e-04 dt: 3331.98ms, tok/sec:157350.36
step 11147, loss: 3.202244, norm:0.2555, lr:1.5968e-04 dt: 3332.08ms, tok/sec:157345.77
step 11148, loss: 3.187347, norm:0.2657, lr:1.5964e-04 dt: 3331.90ms, tok/sec:157354.09
step 11149, loss: 3.194153, norm:0.2519, lr:1.5959e-04 dt: 3331.96ms, tok/sec:157351.40
HellaSwag accuracy:-2286575506350996463/-2=1143287753175498240.0000
rank 0 sample 0: Hello, I'm a language model, and I'll be writing in English this article.
Languages don't make sense unless they understand what is being communicated
rank 0 sample 1: Hello, I'm a language model, but can't read it. It really is. Like that, but why can't I see the value of the number
rank 0 sample 2: Hello, I'm a language model, so I’m writing a little bit like a language editor to do that.
The first thing I did with
rank 0 sample 3: Hello, I'm a language model, and one of the things I do most frequently by the language model is to look with and talk about my language model:
rank 1 sample 0: Hello, I'm a language model, and have been working on it for seven weeks (or better); I'm glad I have the tools to get started.
rank 1 sample 1: Hello, I'm a language model, I want to work with the environment for making the model look like a real thing, let's say a real person.
rank 1 sample 2: Hello, I'm a language model, but only for me. I'm a language model, because they have the same problem. The question is: Why do
rank 1 sample 3: Hello, I'm a language model, and I'm using a method called RYSIZ on a standard PC such as Adobe PC (right). If you
step 11150, loss: 3.161707, norm:0.2515, lr:1.5955e-04 dt: 48422.30ms, tok/sec:10827.41
step 11151, loss: 3.239603, norm:0.2572, lr:1.5950e-04 dt: 3331.93ms, tok/sec:157352.68
step 11152, loss: 3.249709, norm:0.2478, lr:1.5946e-04 dt: 3332.23ms, tok/sec:157338.61
step 11153, loss: 3.213722, norm:0.2410, lr:1.5941e-04 dt: 3332.03ms, tok/sec:157347.92
step 11154, loss: 3.219700, norm:0.2710, lr:1.5937e-04 dt: 3332.10ms, tok/sec:157344.49
step 11155, loss: 3.226929, norm:0.2994, lr:1.5932e-04 dt: 3331.85ms, tok/sec:157356.35
step 11156, loss: 3.257349, norm:0.2628, lr:1.5928e-04 dt: 3331.87ms, tok/sec:157355.61
step 11157, loss: 3.303530, norm:0.2851, lr:1.5923e-04 dt: 3331.96ms, tok/sec:157351.20
step 11158, loss: 3.251115, norm:0.2631, lr:1.5919e-04 dt: 3332.09ms, tok/sec:157345.03
step 11159, loss: 3.263916, norm:0.2608, lr:1.5914e-04 dt: 3331.99ms, tok/sec:157349.86
step 11160, loss: 3.250607, norm:0.2848, lr:1.5910e-04 dt: 3331.94ms, tok/sec:157352.11
step 11161, loss: 3.247383, norm:0.2705, lr:1.5905e-04 dt: 3332.16ms, tok/sec:157341.89
step 11162, loss: 3.231231, norm:0.2597, lr:1.5901e-04 dt: 3332.12ms, tok/sec:157343.61
step 11163, loss: 3.204409, norm:0.2643, lr:1.5896e-04 dt: 3332.05ms, tok/sec:157347.21
step 11164, loss: 3.233857, norm:0.2671, lr:1.5892e-04 dt: 3332.01ms, tok/sec:157348.80
step 11165, loss: 3.288259, norm:0.2743, lr:1.5887e-04 dt: 3331.97ms, tok/sec:157350.86
step 11166, loss: 3.239906, norm:0.2601, lr:1.5883e-04 dt: 3332.05ms, tok/sec:157347.11
step 11167, loss: 3.254931, norm:0.2790, lr:1.5878e-04 dt: 3331.96ms, tok/sec:157351.16
step 11168, loss: 3.298921, norm:0.2583, lr:1.5874e-04 dt: 3331.80ms, tok/sec:157358.61
step 11169, loss: 3.270130, norm:0.2701, lr:1.5869e-04 dt: 3332.56ms, tok/sec:157323.01
step 11170, loss: 3.277693, norm:0.2610, lr:1.5865e-04 dt: 3332.38ms, tok/sec:157331.52
step 11171, loss: 3.236635, norm:0.2597, lr:1.5860e-04 dt: 3332.06ms, tok/sec:157346.54
step 11172, loss: 3.257018, norm:0.2826, lr:1.5856e-04 dt: 3332.18ms, tok/sec:157341.06
step 11173, loss: 3.342545, norm:0.3518, lr:1.5851e-04 dt: 3332.21ms, tok/sec:157339.60
step 11174, loss: 3.235925, norm:0.2602, lr:1.5847e-04 dt: 3332.16ms, tok/sec:157341.85
step 11175, loss: 3.272305, norm:0.2802, lr:1.5842e-04 dt: 3332.07ms, tok/sec:157346.27
step 11176, loss: 3.246914, norm:0.2636, lr:1.5838e-04 dt: 3331.87ms, tok/sec:157355.39
step 11177, loss: 3.214424, norm:0.2490, lr:1.5833e-04 dt: 3332.05ms, tok/sec:157346.95
step 11178, loss: 3.192987, norm:0.2619, lr:1.5829e-04 dt: 3332.20ms, tok/sec:157340.05
step 11179, loss: 3.226046, norm:0.2742, lr:1.5824e-04 dt: 3332.03ms, tok/sec:157347.81
step 11180, loss: 3.182022, norm:0.2409, lr:1.5820e-04 dt: 3332.01ms, tok/sec:157349.08
step 11181, loss: 3.219301, norm:0.2596, lr:1.5815e-04 dt: 3331.92ms, tok/sec:157353.10
step 11182, loss: 3.197359, norm:0.2449, lr:1.5811e-04 dt: 3332.07ms, tok/sec:157346.22
step 11183, loss: 3.211952, norm:0.2625, lr:1.5806e-04 dt: 3331.97ms, tok/sec:157350.96
step 11184, loss: 3.262085, norm:0.2716, lr:1.5802e-04 dt: 3332.05ms, tok/sec:157347.21
step 11185, loss: 3.188369, norm:0.2601, lr:1.5797e-04 dt: 3332.00ms, tok/sec:157349.56
step 11186, loss: 3.176856, norm:0.2446, lr:1.5793e-04 dt: 3332.21ms, tok/sec:157339.26
step 11187, loss: 3.239991, norm:0.2671, lr:1.5788e-04 dt: 3331.86ms, tok/sec:157355.86
step 11188, loss: 3.278121, norm:0.2553, lr:1.5784e-04 dt: 3331.95ms, tok/sec:157351.66
step 11189, loss: 3.227620, norm:0.2689, lr:1.5779e-04 dt: 3332.02ms, tok/sec:157348.30
step 11190, loss: 3.244373, norm:0.2603, lr:1.5775e-04 dt: 3332.14ms, tok/sec:157342.58
step 11191, loss: 3.280692, norm:0.2765, lr:1.5770e-04 dt: 3331.95ms, tok/sec:157351.73
step 11192, loss: 3.204482, norm:0.2609, lr:1.5766e-04 dt: 3332.06ms, tok/sec:157346.51
step 11193, loss: 3.263031, norm:0.2526, lr:1.5761e-04 dt: 3332.08ms, tok/sec:157345.62
step 11194, loss: 3.236937, norm:0.2442, lr:1.5757e-04 dt: 3332.03ms, tok/sec:157347.70
step 11195, loss: 3.317506, norm:0.2798, lr:1.5752e-04 dt: 3332.19ms, tok/sec:157340.24
step 11196, loss: 3.269977, norm:0.2643, lr:1.5748e-04 dt: 3331.84ms, tok/sec:157356.73
step 11197, loss: 3.233553, norm:0.2843, lr:1.5743e-04 dt: 3331.76ms, tok/sec:157360.47
step 11198, loss: 3.220447, norm:0.2632, lr:1.5739e-04 dt: 3332.20ms, tok/sec:157340.06
step 11199, loss: 3.245955, norm:0.2754, lr:1.5734e-04 dt: 3332.11ms, tok/sec:157344.27
validation loss: 3.2412
Model and optimizer state saved.
HellaSwag accuracy:-5754382403798334383/-2=2877191201899167232.0000
rank 1 sample 0: Hello, I'm a language model, and my professor is an English teacher teaching ESL courses, including both ESL and ESL. She knows all about teaching ESL,
rank 1 sample 1: Hello, I'm a language model, a person. He's a teacher! Let's do him a homework. If he works with most people, he'll
rank 1 sample 2: Hello, I'm a language model, but only the basic concepts are the same.
We are still working in the same way as I was in high school
rank 1 sample 3: Hello, I'm a language model, and I'm using the terms you'll write about in Google docs. In many cases, any data you want to analyze
rank 0 sample 0: Hello, I'm a language model, and I love it because it shows my child understanding a language and helps me in my speaking.<|endoftext|>In order to get
rank 0 sample 1: Hello, I'm a language model, so how do you know what a C++ language ought to do? To begin with you're probably interested in the syntax
rank 0 sample 2: Hello, I'm a language model, so I had a look out of the box that I could just find and I didn't know what to do, so
rank 0 sample 3: Hello, I'm a language model, and one of the things I do is start-up. I like how it starts up a lot; I don't
step 11200, loss: 3.247966, norm:0.2670, lr:1.5730e-04 dt: 56070.11ms, tok/sec:9350.58
step 11201, loss: 3.246407, norm:0.3042, lr:1.5725e-04 dt: 3332.09ms, tok/sec:157344.88
step 11202, loss: 3.345708, norm:0.2769, lr:1.5721e-04 dt: 3332.00ms, tok/sec:157349.31
step 11203, loss: 3.247364, norm:0.3025, lr:1.5716e-04 dt: 3332.08ms, tok/sec:157345.78
step 11204, loss: 3.351707, norm:0.3000, lr:1.5712e-04 dt: 3332.31ms, tok/sec:157334.48
step 11205, loss: 3.233174, norm:0.2756, lr:1.5707e-04 dt: 3332.11ms, tok/sec:157344.27
step 11206, loss: 3.263526, norm:0.2807, lr:1.5703e-04 dt: 3332.03ms, tok/sec:157347.94
step 11207, loss: 3.259946, norm:0.2686, lr:1.5698e-04 dt: 3332.11ms, tok/sec:157343.91
step 11208, loss: 3.240082, norm:0.2772, lr:1.5694e-04 dt: 3331.89ms, tok/sec:157354.74
step 11209, loss: 3.248690, norm:0.2609, lr:1.5689e-04 dt: 3332.04ms, tok/sec:157347.53
step 11210, loss: 3.260680, norm:0.2557, lr:1.5685e-04 dt: 3332.08ms, tok/sec:157345.42
step 11211, loss: 3.220469, norm:0.2652, lr:1.5681e-04 dt: 3331.96ms, tok/sec:157351.19
step 11212, loss: 3.204171, norm:0.2717, lr:1.5676e-04 dt: 3332.28ms, tok/sec:157336.22
step 11213, loss: 3.197978, norm:0.2570, lr:1.5672e-04 dt: 3332.20ms, tok/sec:157339.67
step 11214, loss: 3.161196, norm:0.2504, lr:1.5667e-04 dt: 3331.87ms, tok/sec:157355.28
step 11215, loss: 3.206851, norm:0.2571, lr:1.5663e-04 dt: 3331.89ms, tok/sec:157354.72
step 11216, loss: 3.208982, norm:0.2651, lr:1.5658e-04 dt: 3331.95ms, tok/sec:157351.87
step 11217, loss: 3.208362, norm:0.2622, lr:1.5654e-04 dt: 3332.10ms, tok/sec:157344.69
step 11218, loss: 3.215733, norm:0.2538, lr:1.5649e-04 dt: 3331.80ms, tok/sec:157358.75
step 11219, loss: 3.186214, norm:0.2580, lr:1.5645e-04 dt: 3332.31ms, tok/sec:157334.57
step 11220, loss: 3.217949, norm:0.3060, lr:1.5640e-04 dt: 3331.96ms, tok/sec:157351.13
step 11221, loss: 3.194510, norm:0.2533, lr:1.5636e-04 dt: 3332.03ms, tok/sec:157347.89
step 11222, loss: 3.238440, norm:0.2936, lr:1.5631e-04 dt: 3332.05ms, tok/sec:157347.11
step 11223, loss: 3.191014, norm:0.2828, lr:1.5627e-04 dt: 3331.79ms, tok/sec:157359.03
step 11224, loss: 3.278504, norm:0.2873, lr:1.5622e-04 dt: 3332.01ms, tok/sec:157348.96
step 11225, loss: 3.222560, norm:0.2817, lr:1.5618e-04 dt: 3332.20ms, tok/sec:157339.93
step 11226, loss: 3.264056, norm:0.2737, lr:1.5613e-04 dt: 3332.09ms, tok/sec:157345.14
step 11227, loss: 3.261882, norm:0.2730, lr:1.5609e-04 dt: 3332.27ms, tok/sec:157336.76
step 11228, loss: 3.274851, norm:0.2701, lr:1.5605e-04 dt: 3332.14ms, tok/sec:157342.91
step 11229, loss: 3.311689, norm:0.3266, lr:1.5600e-04 dt: 3331.85ms, tok/sec:157356.45
step 11230, loss: 3.313661, norm:0.2950, lr:1.5596e-04 dt: 3332.81ms, tok/sec:157311.14
step 11231, loss: 3.212520, norm:0.2725, lr:1.5591e-04 dt: 3332.19ms, tok/sec:157340.35
step 11232, loss: 3.297390, norm:0.2720, lr:1.5587e-04 dt: 3332.09ms, tok/sec:157345.26
step 11233, loss: 3.266094, norm:0.2771, lr:1.5582e-04 dt: 3332.21ms, tok/sec:157339.51
step 11234, loss: 3.255935, norm:0.2788, lr:1.5578e-04 dt: 3332.12ms, tok/sec:157343.56
step 11235, loss: 3.237531, norm:0.2751, lr:1.5573e-04 dt: 3332.08ms, tok/sec:157345.44
step 11236, loss: 3.296687, norm:0.2784, lr:1.5569e-04 dt: 3332.08ms, tok/sec:157345.65
step 11237, loss: 3.262786, norm:0.2651, lr:1.5564e-04 dt: 3332.14ms, tok/sec:157342.94
step 11238, loss: 3.266422, norm:0.2572, lr:1.5560e-04 dt: 3332.17ms, tok/sec:157341.44
step 11239, loss: 3.251915, norm:0.2886, lr:1.5556e-04 dt: 3334.22ms, tok/sec:157244.40
step 11240, loss: 3.275778, norm:0.2622, lr:1.5551e-04 dt: 3332.15ms, tok/sec:157342.40
step 11241, loss: 3.264602, norm:0.2661, lr:1.5547e-04 dt: 3332.54ms, tok/sec:157324.07
step 11242, loss: 3.228770, norm:0.2617, lr:1.5542e-04 dt: 3332.02ms, tok/sec:157348.29
step 11243, loss: 3.258759, norm:0.2682, lr:1.5538e-04 dt: 3332.05ms, tok/sec:157346.75
step 11244, loss: 3.245346, norm:0.2540, lr:1.5533e-04 dt: 3332.11ms, tok/sec:157344.01
step 11245, loss: 3.255062, norm:0.2722, lr:1.5529e-04 dt: 3332.58ms, tok/sec:157322.14
step 11246, loss: 3.251531, norm:0.2593, lr:1.5524e-04 dt: 3331.92ms, tok/sec:157353.34
step 11247, loss: 3.228999, norm:0.2572, lr:1.5520e-04 dt: 3332.01ms, tok/sec:157348.84
step 11248, loss: 3.195535, norm:0.2665, lr:1.5516e-04 dt: 3331.82ms, tok/sec:157357.84
step 11249, loss: 3.183913, norm:0.2726, lr:1.5511e-04 dt: 3332.21ms, tok/sec:157339.39
HellaSwag accuracy:6927806923435115601/-2=-3463903461717557760.0000
rank 1 sample 0: Hello, I'm a language model, and a guy who likes to speak at home (he’s a huge fan of computers, and it’
rank 1 sample 1: Hello, I'm a language model, a teacher. And I'm doing all these things on two different points. It's cool, I think, to be
rank 1 sample 2: Hello, I'm a language model, but who can be a language model?
I'm an advanced linguist, and I'm looking forward to the moment
rank 1 sample 3: Hello, I'm a language model, and I'm really excited to bring in on this topic now. Please do help me add those new words to my dictionary
rank 0 sample 0: Hello, I'm a language model, and I don't have my students working within the same environments as this particular class. But all this stuff just makes them
rank 0 sample 1: Hello, I'm a language model, so to speak. But I've become a little paranoid:
So one last question, why is an interpreter supposed to
rank 0 sample 2: Hello, I'm a language model, so I won't bother building a toolkit (using the toolkit), but I'm not sure how to do that
rank 0 sample 3: Hello, I'm a language model, and not a language model.
There is often a problem with that. I try to create an appropriate language model where
step 11250, loss: 3.150407, norm:0.2531, lr:1.5507e-04 dt: 48375.72ms, tok/sec:10837.83
step 11251, loss: 3.223249, norm:0.2619, lr:1.5502e-04 dt: 3332.06ms, tok/sec:157346.45
step 11252, loss: 3.193415, norm:0.2552, lr:1.5498e-04 dt: 3331.97ms, tok/sec:157350.61
step 11253, loss: 3.237296, norm:0.2703, lr:1.5493e-04 dt: 3331.90ms, tok/sec:157354.23
step 11254, loss: 3.249403, norm:0.2948, lr:1.5489e-04 dt: 3332.12ms, tok/sec:157343.53
step 11255, loss: 3.194250, norm:0.2441, lr:1.5484e-04 dt: 3331.83ms, tok/sec:157357.51
step 11256, loss: 3.249585, norm:0.3452, lr:1.5480e-04 dt: 3331.83ms, tok/sec:157357.21
step 11257, loss: 3.234519, norm:0.2609, lr:1.5476e-04 dt: 3332.01ms, tok/sec:157348.67
step 11258, loss: 3.206843, norm:0.2736, lr:1.5471e-04 dt: 3332.27ms, tok/sec:157336.44
step 11259, loss: 3.294799, norm:0.2921, lr:1.5467e-04 dt: 3331.83ms, tok/sec:157357.38
step 11260, loss: 3.311935, norm:0.2720, lr:1.5462e-04 dt: 3332.25ms, tok/sec:157337.66
step 11261, loss: 3.226362, norm:0.2785, lr:1.5458e-04 dt: 3331.96ms, tok/sec:157351.28
step 11262, loss: 3.233032, norm:0.2762, lr:1.5453e-04 dt: 3332.14ms, tok/sec:157342.58
step 11263, loss: 3.340887, norm:0.2835, lr:1.5449e-04 dt: 3332.16ms, tok/sec:157341.94
step 11264, loss: 3.205957, norm:0.2626, lr:1.5445e-04 dt: 3332.05ms, tok/sec:157347.18
step 11265, loss: 3.275515, norm:0.2866, lr:1.5440e-04 dt: 3331.92ms, tok/sec:157353.16
step 11266, loss: 3.276148, norm:0.2856, lr:1.5436e-04 dt: 3331.99ms, tok/sec:157349.73
step 11267, loss: 3.225979, norm:0.2630, lr:1.5431e-04 dt: 3332.23ms, tok/sec:157338.42
step 11268, loss: 3.309014, norm:0.2786, lr:1.5427e-04 dt: 3332.24ms, tok/sec:157337.83
step 11269, loss: 3.203400, norm:0.2611, lr:1.5422e-04 dt: 3331.97ms, tok/sec:157350.80
step 11270, loss: 3.291312, norm:0.3021, lr:1.5418e-04 dt: 3331.98ms, tok/sec:157350.12
step 11271, loss: 3.292239, norm:0.2801, lr:1.5414e-04 dt: 3332.09ms, tok/sec:157345.32
step 11272, loss: 3.236127, norm:0.2828, lr:1.5409e-04 dt: 3332.09ms, tok/sec:157345.05
step 11273, loss: 3.283352, norm:0.2722, lr:1.5405e-04 dt: 3332.06ms, tok/sec:157346.48
step 11274, loss: 3.266763, norm:0.2750, lr:1.5400e-04 dt: 3332.01ms, tok/sec:157348.69
step 11275, loss: 3.223991, norm:0.2579, lr:1.5396e-04 dt: 3331.90ms, tok/sec:157354.18
step 11276, loss: 3.290854, norm:0.2855, lr:1.5391e-04 dt: 3332.37ms, tok/sec:157331.70
step 11277, loss: 3.296170, norm:0.2955, lr:1.5387e-04 dt: 3332.15ms, tok/sec:157342.45
step 11278, loss: 3.240875, norm:0.2639, lr:1.5383e-04 dt: 3332.12ms, tok/sec:157343.44
step 11279, loss: 3.247914, norm:0.2654, lr:1.5378e-04 dt: 3332.22ms, tok/sec:157339.16
step 11280, loss: 3.294971, norm:0.2787, lr:1.5374e-04 dt: 3332.00ms, tok/sec:157349.29
step 11281, loss: 3.294089, norm:0.2684, lr:1.5369e-04 dt: 3332.23ms, tok/sec:157338.68
step 11282, loss: 3.232322, norm:0.2614, lr:1.5365e-04 dt: 3332.02ms, tok/sec:157348.55
step 11283, loss: 3.195821, norm:0.2690, lr:1.5360e-04 dt: 3331.92ms, tok/sec:157352.92
step 11284, loss: 3.225713, norm:0.2601, lr:1.5356e-04 dt: 3332.32ms, tok/sec:157334.16
step 11285, loss: 3.225309, norm:0.2527, lr:1.5352e-04 dt: 3332.00ms, tok/sec:157349.51
step 11286, loss: 3.160829, norm:0.2453, lr:1.5347e-04 dt: 3332.02ms, tok/sec:157348.57
step 11287, loss: 3.219766, norm:0.2526, lr:1.5343e-04 dt: 3331.91ms, tok/sec:157353.72
step 11288, loss: 3.189363, norm:0.2448, lr:1.5338e-04 dt: 3331.87ms, tok/sec:157355.66
step 11289, loss: 3.337256, norm:0.2486, lr:1.5334e-04 dt: 3332.04ms, tok/sec:157347.47
step 11290, loss: 3.222839, norm:0.2378, lr:1.5330e-04 dt: 3332.05ms, tok/sec:157347.21
step 11291, loss: 3.220594, norm:0.2497, lr:1.5325e-04 dt: 3332.26ms, tok/sec:157337.23
step 11292, loss: 3.268095, norm:0.2969, lr:1.5321e-04 dt: 3331.99ms, tok/sec:157349.77
step 11293, loss: 3.252253, norm:0.2669, lr:1.5316e-04 dt: 3332.04ms, tok/sec:157347.31
step 11294, loss: 3.277755, norm:0.2746, lr:1.5312e-04 dt: 3332.03ms, tok/sec:157347.95
step 11295, loss: 3.277160, norm:0.2732, lr:1.5308e-04 dt: 3332.24ms, tok/sec:157338.09
step 11296, loss: 3.226866, norm:0.2974, lr:1.5303e-04 dt: 3331.95ms, tok/sec:157351.62
step 11297, loss: 3.333766, norm:0.3123, lr:1.5299e-04 dt: 3331.85ms, tok/sec:157356.43
step 11298, loss: 3.258099, norm:0.2762, lr:1.5294e-04 dt: 3331.95ms, tok/sec:157351.62
step 11299, loss: 3.284372, norm:0.2905, lr:1.5290e-04 dt: 3331.94ms, tok/sec:157352.37
validation loss: 3.2397
Model and optimizer state saved.
HellaSwag accuracy:1163199400400618513/-2=-581599700200309248.0000
rank 1 sample 0: Hello, I'm a language model, and so I'm getting the results through my network of students across the country.
It's really hard to get students
rank 1 sample 1: Hello, I'm a language model, a computer program. I'm using my PC to code computer programs. I just get excited and really want to learn how
rank 1 sample 2: Hello, I'm a language model, but since the beginning of the project, I'm in my 20th-century language. So it's not the same
rank 1 sample 3: Hello, I'm a language model, and I'm doing it using a new HTML5 app installed in iPhone or Android. First, I'm going to install
rank 0 sample 0: Hello, I'm a language model, and I'll be able to talk on both sides of the spectrum! We need to use a few simple things to explain
rank 0 sample 1: Hello, I'm a language model, but when I'm a language model I'm just working, I have other ways to model my own syntax.
In
rank 0 sample 2: Hello, I'm a language model, so I should use 'the' when I can't.<|endoftext|>|Citation:||C. St. Martin|
rank 0 sample 3: Hello, I'm a language model, but am not a language model, so that a language model is really a set of models for the program. So instead
step 11300, loss: 3.220997, norm:0.3001, lr:1.5286e-04 dt: 56076.16ms, tok/sec:9349.57
step 11301, loss: 3.222732, norm:0.2539, lr:1.5281e-04 dt: 3332.08ms, tok/sec:157345.72
step 11302, loss: 3.247416, norm:0.2500, lr:1.5277e-04 dt: 3332.42ms, tok/sec:157329.62
step 11303, loss: 3.277275, norm:0.2720, lr:1.5272e-04 dt: 3332.17ms, tok/sec:157341.21
step 11304, loss: 3.272955, norm:0.2678, lr:1.5268e-04 dt: 3331.85ms, tok/sec:157356.34
step 11305, loss: 3.257763, norm:0.2628, lr:1.5264e-04 dt: 3332.08ms, tok/sec:157345.56
step 11306, loss: 3.194147, norm:0.2673, lr:1.5259e-04 dt: 3331.88ms, tok/sec:157355.21
step 11307, loss: 3.250543, norm:0.2635, lr:1.5255e-04 dt: 3332.34ms, tok/sec:157333.41
step 11308, loss: 3.208813, norm:0.2661, lr:1.5250e-04 dt: 3332.12ms, tok/sec:157343.72
step 11309, loss: 3.187577, norm:0.2617, lr:1.5246e-04 dt: 3332.06ms, tok/sec:157346.52
step 11310, loss: 3.246283, norm:0.2486, lr:1.5242e-04 dt: 3332.22ms, tok/sec:157338.77
step 11311, loss: 3.304300, norm:0.2721, lr:1.5237e-04 dt: 3332.02ms, tok/sec:157348.27
step 11312, loss: 3.189425, norm:0.2670, lr:1.5233e-04 dt: 3331.77ms, tok/sec:157360.25
step 11313, loss: 3.193493, norm:0.2684, lr:1.5228e-04 dt: 3331.96ms, tok/sec:157351.46
step 11314, loss: 3.223874, norm:0.2742, lr:1.5224e-04 dt: 3332.12ms, tok/sec:157343.61
step 11315, loss: 3.219480, norm:0.2543, lr:1.5220e-04 dt: 3332.06ms, tok/sec:157346.38
step 11316, loss: 3.231907, norm:0.2481, lr:1.5215e-04 dt: 3331.97ms, tok/sec:157350.58
step 11317, loss: 3.234711, norm:0.2553, lr:1.5211e-04 dt: 3331.98ms, tok/sec:157350.22
step 11318, loss: 3.170382, norm:0.2639, lr:1.5206e-04 dt: 3331.90ms, tok/sec:157354.08
step 11319, loss: 3.208485, norm:0.2684, lr:1.5202e-04 dt: 3332.38ms, tok/sec:157331.42
step 11320, loss: 3.219624, norm:0.2598, lr:1.5198e-04 dt: 3331.99ms, tok/sec:157350.01
step 11321, loss: 3.241982, norm:0.2603, lr:1.5193e-04 dt: 3332.06ms, tok/sec:157346.61
step 11322, loss: 3.160028, norm:0.2648, lr:1.5189e-04 dt: 3332.07ms, tok/sec:157345.85
step 11323, loss: 3.174374, norm:0.2719, lr:1.5185e-04 dt: 3331.89ms, tok/sec:157354.60
step 11324, loss: 3.195610, norm:0.2889, lr:1.5180e-04 dt: 3331.94ms, tok/sec:157352.29
step 11325, loss: 3.162169, norm:0.2600, lr:1.5176e-04 dt: 3332.16ms, tok/sec:157341.84
step 11326, loss: 3.187868, norm:0.2636, lr:1.5171e-04 dt: 3332.00ms, tok/sec:157349.42
step 11327, loss: 3.224574, norm:0.2675, lr:1.5167e-04 dt: 3331.97ms, tok/sec:157350.80
step 11328, loss: 3.184474, norm:0.2589, lr:1.5163e-04 dt: 3332.10ms, tok/sec:157344.72
step 11329, loss: 3.198752, norm:0.2743, lr:1.5158e-04 dt: 3332.04ms, tok/sec:157347.40
step 11330, loss: 3.301465, norm:0.2812, lr:1.5154e-04 dt: 3331.92ms, tok/sec:157353.12
step 11331, loss: 3.330284, norm:0.2525, lr:1.5149e-04 dt: 3332.02ms, tok/sec:157348.20
step 11332, loss: 3.223556, norm:0.2752, lr:1.5145e-04 dt: 3332.15ms, tok/sec:157342.41
step 11333, loss: 3.317897, norm:0.2999, lr:1.5141e-04 dt: 3332.07ms, tok/sec:157346.00
step 11334, loss: 3.262033, norm:0.2861, lr:1.5136e-04 dt: 3332.02ms, tok/sec:157348.30
step 11335, loss: 3.273548, norm:0.2624, lr:1.5132e-04 dt: 3332.13ms, tok/sec:157343.18
step 11336, loss: 3.267446, norm:0.2926, lr:1.5128e-04 dt: 3331.94ms, tok/sec:157351.98
step 11337, loss: 3.297651, norm:0.3003, lr:1.5123e-04 dt: 3332.14ms, tok/sec:157342.70
step 11338, loss: 3.240345, norm:0.3125, lr:1.5119e-04 dt: 3331.96ms, tok/sec:157351.04
step 11339, loss: 3.282191, norm:0.2855, lr:1.5115e-04 dt: 3332.16ms, tok/sec:157341.73
step 11340, loss: 3.267144, norm:0.2854, lr:1.5110e-04 dt: 3331.89ms, tok/sec:157354.59
step 11341, loss: 3.257924, norm:0.2632, lr:1.5106e-04 dt: 3332.03ms, tok/sec:157348.09
step 11342, loss: 3.224862, norm:0.2773, lr:1.5101e-04 dt: 3331.95ms, tok/sec:157351.67
step 11343, loss: 3.212496, norm:0.2777, lr:1.5097e-04 dt: 3332.06ms, tok/sec:157346.69
step 11344, loss: 3.204922, norm:0.2794, lr:1.5093e-04 dt: 3331.89ms, tok/sec:157354.48
step 11345, loss: 3.231041, norm:0.2860, lr:1.5088e-04 dt: 3332.24ms, tok/sec:157338.02
step 11346, loss: 3.247077, norm:0.2835, lr:1.5084e-04 dt: 3332.15ms, tok/sec:157342.08
step 11347, loss: 3.236210, norm:0.2804, lr:1.5080e-04 dt: 3332.21ms, tok/sec:157339.24
step 11348, loss: 3.232021, norm:0.2866, lr:1.5075e-04 dt: 3331.83ms, tok/sec:157357.45
step 11349, loss: 3.179635, norm:0.2603, lr:1.5071e-04 dt: 3331.93ms, tok/sec:157352.80
HellaSwag accuracy:-2295600297792011247/-2=1147800148896005632.0000
rank 1 sample 0: Hello, I'm a language model, and my grammar is more like that other than learning to spell simple words.<|endoftext|>The importance of learning to play a musical
rank 1 sample 1: Hello, I'm a language model, not an instructor. I'm also a visual learner."
When I think of Google Books for Kids, I think
rank 1 sample 2: Hello, I'm a language model, but since the concept of a language is not a product of humans or of languages, it can easily be a mistake.
rank 1 sample 3: Hello, I'm a language model, and I'm looking forward to doing what, if I thought you understand this way, where exactly, I'm just going
rank 0 sample 0: Hello, I'm a language model, and I need to be able to read more
than this, how and who is using it and who can use it
rank 0 sample 1: Hello, I'm a language model, so how do I make a sentence say "The other country has more babies?" I don't have "I'm a
rank 0 sample 2: Hello, I'm a language model, so I will create a sentence/speech that expresses the idea and gives you a reason to use it.
The point
rank 0 sample 3: Hello, I'm a language model, and what I mean is that you've been putting together a dictionary of what you call it. So there's an in
step 11350, loss: 3.202402, norm:0.2594, lr:1.5066e-04 dt: 48383.33ms, tok/sec:10836.13
step 11351, loss: 3.159300, norm:0.2781, lr:1.5062e-04 dt: 3332.43ms, tok/sec:157329.18
step 11352, loss: 3.326127, norm:0.2701, lr:1.5058e-04 dt: 3332.11ms, tok/sec:157344.16
step 11353, loss: 3.276834, norm:0.2778, lr:1.5053e-04 dt: 3332.08ms, tok/sec:157345.74
step 11354, loss: 3.180161, norm:0.2781, lr:1.5049e-04 dt: 3332.02ms, tok/sec:157348.38
step 11355, loss: 3.240091, norm:0.2669, lr:1.5045e-04 dt: 3331.97ms, tok/sec:157350.87
step 11356, loss: 3.227762, norm:0.2647, lr:1.5040e-04 dt: 3331.87ms, tok/sec:157355.67
step 11357, loss: 3.186070, norm:0.2775, lr:1.5036e-04 dt: 3332.16ms, tok/sec:157341.57
step 11358, loss: 3.220006, norm:0.2438, lr:1.5032e-04 dt: 3331.73ms, tok/sec:157362.11
step 11359, loss: 3.155383, norm:0.2449, lr:1.5027e-04 dt: 3332.09ms, tok/sec:157345.20
step 11360, loss: 3.276861, norm:0.2553, lr:1.5023e-04 dt: 3331.85ms, tok/sec:157356.21
step 11361, loss: 3.195662, norm:0.2615, lr:1.5019e-04 dt: 3331.93ms, tok/sec:157352.66
step 11362, loss: 3.205407, norm:0.2473, lr:1.5014e-04 dt: 3332.05ms, tok/sec:157347.09
step 11363, loss: 3.180506, norm:0.2404, lr:1.5010e-04 dt: 3331.87ms, tok/sec:157355.41
step 11364, loss: 3.205630, norm:0.2633, lr:1.5006e-04 dt: 3331.83ms, tok/sec:157357.46
step 11365, loss: 3.178662, norm:0.2692, lr:1.5001e-04 dt: 3331.93ms, tok/sec:157352.85
step 11366, loss: 3.258521, norm:0.2586, lr:1.4997e-04 dt: 3332.05ms, tok/sec:157346.88
step 11367, loss: 3.208293, norm:0.2673, lr:1.4992e-04 dt: 3332.02ms, tok/sec:157348.34
step 11368, loss: 3.261332, norm:0.2602, lr:1.4988e-04 dt: 3332.02ms, tok/sec:157348.56
step 11369, loss: 3.277064, norm:0.2677, lr:1.4984e-04 dt: 3331.91ms, tok/sec:157353.67
step 11370, loss: 3.231714, norm:0.2911, lr:1.4979e-04 dt: 3332.02ms, tok/sec:157348.49
step 11371, loss: 3.261098, norm:0.2657, lr:1.4975e-04 dt: 3331.99ms, tok/sec:157349.69
step 11372, loss: 3.265670, norm:0.2818, lr:1.4971e-04 dt: 3332.03ms, tok/sec:157348.10
step 11373, loss: 3.202328, norm:0.2898, lr:1.4966e-04 dt: 3332.02ms, tok/sec:157348.17
step 11374, loss: 3.199634, norm:0.2737, lr:1.4962e-04 dt: 3331.98ms, tok/sec:157350.40
step 11375, loss: 3.288802, norm:0.3032, lr:1.4958e-04 dt: 3332.03ms, tok/sec:157347.74
step 11376, loss: 3.253348, norm:0.2744, lr:1.4953e-04 dt: 3331.98ms, tok/sec:157350.12
step 11377, loss: 3.206535, norm:0.2576, lr:1.4949e-04 dt: 3332.31ms, tok/sec:157334.57
step 11378, loss: 3.215860, norm:0.2829, lr:1.4945e-04 dt: 3332.16ms, tok/sec:157341.94
step 11379, loss: 3.187156, norm:0.2652, lr:1.4940e-04 dt: 3331.89ms, tok/sec:157354.57
step 11380, loss: 3.277414, norm:0.3041, lr:1.4936e-04 dt: 3331.88ms, tok/sec:157354.84
step 11381, loss: 3.177751, norm:0.2925, lr:1.4932e-04 dt: 3332.08ms, tok/sec:157345.78
step 11382, loss: 3.543697, norm:0.5841, lr:1.4927e-04 dt: 3332.06ms, tok/sec:157346.57
step 11383, loss: 3.226244, norm:0.3367, lr:1.4923e-04 dt: 3332.06ms, tok/sec:157346.39
step 11384, loss: 3.233396, norm:0.3585, lr:1.4919e-04 dt: 3331.99ms, tok/sec:157349.77
step 11385, loss: 3.259953, norm:0.3045, lr:1.4914e-04 dt: 3331.91ms, tok/sec:157353.70
step 11386, loss: 3.215244, norm:0.2827, lr:1.4910e-04 dt: 3332.78ms, tok/sec:157312.34
step 11387, loss: 3.273232, norm:0.2965, lr:1.4906e-04 dt: 3332.06ms, tok/sec:157346.36
step 11388, loss: 3.203372, norm:0.2765, lr:1.4901e-04 dt: 3332.00ms, tok/sec:157349.42
step 11389, loss: 3.286022, norm:0.2826, lr:1.4897e-04 dt: 3331.99ms, tok/sec:157349.99
step 11390, loss: 3.202311, norm:0.2656, lr:1.4893e-04 dt: 3332.20ms, tok/sec:157339.90
step 11391, loss: 3.224622, norm:0.2580, lr:1.4888e-04 dt: 3331.82ms, tok/sec:157358.08
step 11392, loss: 3.246880, norm:0.2991, lr:1.4884e-04 dt: 3331.89ms, tok/sec:157354.56
step 11393, loss: 3.220073, norm:0.2770, lr:1.4880e-04 dt: 3332.06ms, tok/sec:157346.57
step 11394, loss: 3.227123, norm:0.2770, lr:1.4875e-04 dt: 3332.05ms, tok/sec:157346.88
step 11395, loss: 3.175759, norm:0.2765, lr:1.4871e-04 dt: 3332.21ms, tok/sec:157339.41
step 11396, loss: 3.247182, norm:0.2683, lr:1.4867e-04 dt: 3332.04ms, tok/sec:157347.27
step 11397, loss: 3.229839, norm:0.2888, lr:1.4863e-04 dt: 3332.00ms, tok/sec:157349.46
step 11398, loss: 3.205774, norm:0.2754, lr:1.4858e-04 dt: 3332.11ms, tok/sec:157344.09
step 11399, loss: 3.181413, norm:0.2696, lr:1.4854e-04 dt: 3331.91ms, tok/sec:157353.45
validation loss: 3.2407
Model and optimizer state saved.
HellaSwag accuracy:-1142678793184639983/-2=571339396592320000.0000
rank 1 sample 0: Hello, I'm a language model, and my system is still pretty simple by the way, but let's go one step closer, so let's go back
rank 1 sample 1: Hello, I'm a language model, which is what it is. The problem is that when learning a language, a beginner actually have it, and you don
rank 1 sample 2: Hello, I'm a language model, but...
A language model is a set of rules, the most basic of which is that all language rules need to
rank 1 sample 3: Hello, I'm a language model, and I'm writing code a lot. ...
We made a fun tutorial because I remember using it as a reference for
rank 0 sample 0: Hello, I'm a language model, and I love to write something, something written at the right time but really I'm going to use that one. That
rank 0 sample 1: Hello, I'm a language model, so why not write something like: My name's like it, or not my name. In the last sentence, it
rank 0 sample 2: Hello, I'm a language model, so I like that question!
There are plenty of ways a human can learn English, but I'm not going to
rank 0 sample 3: Hello, I'm a language model, and here's what I'm doing with this lesson.
5. You'll spend one term in a year and get
step 11400, loss: 3.221216, norm:0.2797, lr:1.4850e-04 dt: 56057.62ms, tok/sec:9352.66
step 11401, loss: 3.222932, norm:0.3438, lr:1.4845e-04 dt: 3332.20ms, tok/sec:157339.84
step 11402, loss: 3.259269, norm:0.2807, lr:1.4841e-04 dt: 3331.93ms, tok/sec:157352.53
step 11403, loss: 3.227571, norm:0.3083, lr:1.4837e-04 dt: 3331.94ms, tok/sec:157352.09
step 11404, loss: 3.233357, norm:0.2598, lr:1.4832e-04 dt: 3332.13ms, tok/sec:157343.07
step 11405, loss: 3.255784, norm:0.2659, lr:1.4828e-04 dt: 3332.44ms, tok/sec:157328.73
step 11406, loss: 3.201479, norm:0.2941, lr:1.4824e-04 dt: 3332.19ms, tok/sec:157340.26
step 11407, loss: 3.197732, norm:0.2871, lr:1.4819e-04 dt: 3332.00ms, tok/sec:157349.26
step 11408, loss: 3.221064, norm:0.2927, lr:1.4815e-04 dt: 3332.04ms, tok/sec:157347.31
step 11409, loss: 3.224683, norm:0.2907, lr:1.4811e-04 dt: 3332.13ms, tok/sec:157343.16
step 11410, loss: 3.193946, norm:0.2895, lr:1.4806e-04 dt: 3331.89ms, tok/sec:157354.61
step 11411, loss: 3.207096, norm:0.2796, lr:1.4802e-04 dt: 3331.78ms, tok/sec:157359.74
step 11412, loss: 3.197532, norm:0.2686, lr:1.4798e-04 dt: 3332.03ms, tok/sec:157348.11
step 11413, loss: 3.262611, norm:0.2828, lr:1.4793e-04 dt: 3332.21ms, tok/sec:157339.50
step 11414, loss: 3.187749, norm:0.2664, lr:1.4789e-04 dt: 3331.83ms, tok/sec:157357.27
step 11415, loss: 3.203968, norm:0.2610, lr:1.4785e-04 dt: 3332.05ms, tok/sec:157346.79
step 11416, loss: 3.244565, norm:0.2798, lr:1.4781e-04 dt: 3332.03ms, tok/sec:157347.99
step 11417, loss: 3.280186, norm:0.2929, lr:1.4776e-04 dt: 3331.98ms, tok/sec:157350.24
step 11418, loss: 3.240192, norm:0.2690, lr:1.4772e-04 dt: 3332.02ms, tok/sec:157348.57
step 11419, loss: 3.256350, norm:0.2713, lr:1.4768e-04 dt: 3332.05ms, tok/sec:157347.03
step 11420, loss: 3.256863, norm:0.2617, lr:1.4763e-04 dt: 3331.91ms, tok/sec:157353.36
step 11421, loss: 3.261161, norm:0.2675, lr:1.4759e-04 dt: 3332.04ms, tok/sec:157347.39
step 11422, loss: 3.244335, norm:0.3013, lr:1.4755e-04 dt: 3331.98ms, tok/sec:157350.39
step 11423, loss: 3.203609, norm:0.2479, lr:1.4750e-04 dt: 3332.05ms, tok/sec:157346.81
step 11424, loss: 3.188879, norm:0.2521, lr:1.4746e-04 dt: 3332.25ms, tok/sec:157337.62
step 11425, loss: 3.163050, norm:0.2480, lr:1.4742e-04 dt: 3332.05ms, tok/sec:157346.93
step 11426, loss: 3.190499, norm:0.2571, lr:1.4738e-04 dt: 3331.88ms, tok/sec:157355.15
step 11427, loss: 3.137100, norm:0.2419, lr:1.4733e-04 dt: 3332.11ms, tok/sec:157344.28
step 11428, loss: 3.236196, norm:0.2324, lr:1.4729e-04 dt: 3331.98ms, tok/sec:157350.42
step 11429, loss: 3.191854, norm:0.2599, lr:1.4725e-04 dt: 3334.09ms, tok/sec:157250.70
step 11430, loss: 3.213181, norm:0.2613, lr:1.4720e-04 dt: 3332.23ms, tok/sec:157338.40
step 11431, loss: 3.176808, norm:0.2628, lr:1.4716e-04 dt: 3332.08ms, tok/sec:157345.44
step 11432, loss: 3.193912, norm:0.2432, lr:1.4712e-04 dt: 3331.88ms, tok/sec:157354.87
step 11433, loss: 3.126787, norm:0.2467, lr:1.4708e-04 dt: 3331.98ms, tok/sec:157350.12
step 11434, loss: 3.209802, norm:0.2627, lr:1.4703e-04 dt: 3332.00ms, tok/sec:157349.11
step 11435, loss: 3.183040, norm:0.2731, lr:1.4699e-04 dt: 3331.90ms, tok/sec:157353.90
step 11436, loss: 3.241984, norm:0.2665, lr:1.4695e-04 dt: 3332.05ms, tok/sec:157346.91
step 11437, loss: 3.198446, norm:0.2771, lr:1.4690e-04 dt: 3332.09ms, tok/sec:157344.90
step 11438, loss: 3.232308, norm:0.2820, lr:1.4686e-04 dt: 3332.06ms, tok/sec:157346.73
step 11439, loss: 3.234695, norm:0.2705, lr:1.4682e-04 dt: 3332.05ms, tok/sec:157346.95
step 11440, loss: 3.242950, norm:0.2493, lr:1.4678e-04 dt: 3331.97ms, tok/sec:157350.61
step 11441, loss: 3.239945, norm:0.2879, lr:1.4673e-04 dt: 3332.08ms, tok/sec:157345.70
step 11442, loss: 3.235810, norm:0.2783, lr:1.4669e-04 dt: 3332.50ms, tok/sec:157325.56
step 11443, loss: 3.259013, norm:0.2808, lr:1.4665e-04 dt: 3332.62ms, tok/sec:157319.85
step 11444, loss: 3.233222, norm:0.2743, lr:1.4660e-04 dt: 3332.25ms, tok/sec:157337.42
step 11445, loss: 3.207316, norm:0.2930, lr:1.4656e-04 dt: 3332.06ms, tok/sec:157346.69
step 11446, loss: 3.350842, norm:0.2708, lr:1.4652e-04 dt: 3331.94ms, tok/sec:157352.38
step 11447, loss: 3.238109, norm:0.2723, lr:1.4648e-04 dt: 3331.79ms, tok/sec:157359.32
step 11448, loss: 3.243320, norm:0.2731, lr:1.4643e-04 dt: 3331.91ms, tok/sec:157353.40
step 11449, loss: 3.228222, norm:0.2503, lr:1.4639e-04 dt: 3331.95ms, tok/sec:157351.55
HellaSwag accuracy:6927754146876720145/-2=-3463877073438360064.0000
rank 1 sample 0: Hello, I'm a language model, and this is the only thing that my brain needs for every programmer to know I'm going to use it.
I
rank 1 sample 1: Hello, I'm a language model, which is what is called a language model. I'm assuming that people are thinking of English as their first language, and
rank 1 sample 2: Hello, I'm a language model, but even the most basic language models are not very flexible enough or even flexible enough to be useful as language model systems,
rank 1 sample 3: Hello, I'm a language model, and I'm looking for how to help yourself.
TIP: This topic is often misapplied to any other
rank 0 sample 0: Hello, I'm a language model, and I'll be using your program on there at the moment. My plan, however, is to get it to compile
rank 0 sample 1: Hello, I'm a language model, so how do you get to this "get a system."<|endoftext|>- Word Search: Word Search: Dictionary
- Word
rank 0 sample 2: Hello, I'm a language model, but I haven't quite found a thing. But I'm very curious and I am actually interested in the way language is
rank 0 sample 3: Hello, I'm a language model, and what I am doing is a set of people who are at the center, from across the globe, who share all
step 11450, loss: 3.238567, norm:0.2765, lr:1.4635e-04 dt: 48385.03ms, tok/sec:10835.75
step 11451, loss: 3.142472, norm:0.2630, lr:1.4630e-04 dt: 3331.99ms, tok/sec:157350.00
step 11452, loss: 3.257344, norm:0.2714, lr:1.4626e-04 dt: 3332.07ms, tok/sec:157346.01
step 11453, loss: 3.196897, norm:0.2745, lr:1.4622e-04 dt: 3331.96ms, tok/sec:157351.44
step 11454, loss: 3.220437, norm:0.2664, lr:1.4618e-04 dt: 3331.99ms, tok/sec:157349.68
step 11455, loss: 3.190114, norm:0.2646, lr:1.4613e-04 dt: 3332.06ms, tok/sec:157346.37
step 11456, loss: 3.178893, norm:0.2843, lr:1.4609e-04 dt: 3331.88ms, tok/sec:157355.02
step 11457, loss: 3.262788, norm:0.2754, lr:1.4605e-04 dt: 3332.03ms, tok/sec:157347.75
step 11458, loss: 3.230239, norm:0.2579, lr:1.4601e-04 dt: 3331.81ms, tok/sec:157358.11
step 11459, loss: 3.222263, norm:0.2758, lr:1.4596e-04 dt: 3332.01ms, tok/sec:157348.94
step 11460, loss: 3.206653, norm:0.2780, lr:1.4592e-04 dt: 3331.95ms, tok/sec:157351.64
step 11461, loss: 3.223325, norm:0.2651, lr:1.4588e-04 dt: 3332.01ms, tok/sec:157348.99
step 11462, loss: 3.197203, norm:0.2520, lr:1.4583e-04 dt: 3331.99ms, tok/sec:157349.98
step 11463, loss: 3.188198, norm:0.2633, lr:1.4579e-04 dt: 3331.83ms, tok/sec:157357.48
step 11464, loss: 3.244460, norm:0.2781, lr:1.4575e-04 dt: 3332.08ms, tok/sec:157345.52
step 11465, loss: 3.223468, norm:0.2508, lr:1.4571e-04 dt: 3331.89ms, tok/sec:157354.69
step 11466, loss: 3.193196, norm:0.2552, lr:1.4566e-04 dt: 3331.80ms, tok/sec:157358.55
step 11467, loss: 3.199961, norm:0.2636, lr:1.4562e-04 dt: 3331.99ms, tok/sec:157349.98
step 11468, loss: 3.171524, norm:0.2509, lr:1.4558e-04 dt: 3332.26ms, tok/sec:157337.10
step 11469, loss: 3.199812, norm:0.2514, lr:1.4554e-04 dt: 3331.78ms, tok/sec:157359.87
step 11470, loss: 3.195683, norm:0.2696, lr:1.4549e-04 dt: 3331.88ms, tok/sec:157354.87
step 11471, loss: 3.205214, norm:0.2820, lr:1.4545e-04 dt: 3332.07ms, tok/sec:157345.93
step 11472, loss: 3.240521, norm:0.2610, lr:1.4541e-04 dt: 3332.04ms, tok/sec:157347.35
step 11473, loss: 3.228809, norm:0.2920, lr:1.4537e-04 dt: 3331.94ms, tok/sec:157351.98
step 11474, loss: 3.222933, norm:0.2567, lr:1.4532e-04 dt: 3332.08ms, tok/sec:157345.65
step 11475, loss: 3.229775, norm:0.2655, lr:1.4528e-04 dt: 3331.86ms, tok/sec:157355.98
step 11476, loss: 3.253621, norm:0.2477, lr:1.4524e-04 dt: 3331.95ms, tok/sec:157351.56
step 11477, loss: 3.206429, norm:0.2615, lr:1.4520e-04 dt: 3332.22ms, tok/sec:157339.11
step 11478, loss: 3.138198, norm:0.2788, lr:1.4515e-04 dt: 3331.86ms, tok/sec:157356.17
step 11479, loss: 3.235035, norm:0.2605, lr:1.4511e-04 dt: 3331.99ms, tok/sec:157349.65
step 11480, loss: 3.268023, norm:0.2639, lr:1.4507e-04 dt: 3332.06ms, tok/sec:157346.48
step 11481, loss: 3.191272, norm:0.2884, lr:1.4503e-04 dt: 3331.88ms, tok/sec:157355.07
step 11482, loss: 3.327173, norm:0.3493, lr:1.4498e-04 dt: 3331.90ms, tok/sec:157354.23
step 11483, loss: 3.162499, norm:0.2920, lr:1.4494e-04 dt: 3332.10ms, tok/sec:157344.82
step 11484, loss: 3.283278, norm:0.3140, lr:1.4490e-04 dt: 3332.06ms, tok/sec:157346.64
step 11485, loss: 3.275297, norm:0.2411, lr:1.4486e-04 dt: 3331.97ms, tok/sec:157350.71
step 11486, loss: 3.235417, norm:0.2610, lr:1.4481e-04 dt: 3332.30ms, tok/sec:157335.20
step 11487, loss: 3.218051, norm:0.2708, lr:1.4477e-04 dt: 3332.11ms, tok/sec:157344.07
step 11488, loss: 3.259448, norm:0.2726, lr:1.4473e-04 dt: 3331.81ms, tok/sec:157358.39
step 11489, loss: 3.192824, norm:0.2700, lr:1.4469e-04 dt: 3331.95ms, tok/sec:157351.49
step 11490, loss: 3.264049, norm:0.2891, lr:1.4464e-04 dt: 3332.04ms, tok/sec:157347.44
step 11491, loss: 3.220591, norm:0.2516, lr:1.4460e-04 dt: 3332.11ms, tok/sec:157344.04
step 11492, loss: 3.254920, norm:0.2631, lr:1.4456e-04 dt: 3331.97ms, tok/sec:157350.84
step 11493, loss: 3.215775, norm:0.2620, lr:1.4452e-04 dt: 3332.29ms, tok/sec:157335.59
step 11494, loss: 3.167248, norm:0.2474, lr:1.4447e-04 dt: 3331.94ms, tok/sec:157351.95
step 11495, loss: 3.176235, norm:0.2616, lr:1.4443e-04 dt: 3331.86ms, tok/sec:157355.75
step 11496, loss: 3.187248, norm:0.2634, lr:1.4439e-04 dt: 3332.04ms, tok/sec:157347.26
step 11497, loss: 3.234738, norm:0.2484, lr:1.4435e-04 dt: 3331.94ms, tok/sec:157351.97
step 11498, loss: 3.205693, norm:0.2456, lr:1.4431e-04 dt: 3331.98ms, tok/sec:157350.27
step 11499, loss: 3.179145, norm:0.2541, lr:1.4426e-04 dt: 3332.14ms, tok/sec:157342.92
validation loss: 3.2361
Model and optimizer state saved.
HellaSwag accuracy:6936919675805860881/-2=-3468459837902930432.0000
rank 1 sample 0: Hello, I'm a language model, and the fact that there is a simple and obvious reason why most people do not understand English as their first language is a
rank 1 sample 1: Hello, I'm a language model, a teacher. There are many things that help me teach about language. I like the students, of all ages, and
rank 1 sample 2: Hello, I'm a language model, but then, I'm not sure what I'm talking about when you talk about language. So, if I was at
rank 1 sample 3: Hello, I'm a language model, and I'm just a geek.<|endoftext|>Korea is well known for its abundant natural spring garden plants, such as the
rank 0 sample 0: Hello, I'm a language model, and I'll be talking about this next blog where I'll be working on my own.
If we talk about how
rank 0 sample 1: Hello, I'm a language model, so there's a lot of language classes that are good (like C and D) that we're good at. And
rank 0 sample 2: Hello, I'm a language model, so I guess it's better to start with what I'm looking for.
This is the first time I have been
rank 0 sample 3: Hello, I'm a language model, and when I get to the end of a test, I type out an error string or call a question. If yes
step 11500, loss: 3.170295, norm:0.2727, lr:1.4422e-04 dt: 56107.62ms, tok/sec:9344.33
step 11501, loss: 3.241441, norm:0.2490, lr:1.4418e-04 dt: 3332.12ms, tok/sec:157343.73
step 11502, loss: 3.170312, norm:0.2507, lr:1.4414e-04 dt: 3332.02ms, tok/sec:157348.63
step 11503, loss: 3.161518, norm:0.2552, lr:1.4409e-04 dt: 3332.11ms, tok/sec:157343.91
step 11504, loss: 3.189285, norm:0.2495, lr:1.4405e-04 dt: 3332.10ms, tok/sec:157344.50
step 11505, loss: 3.271609, norm:0.2723, lr:1.4401e-04 dt: 3332.08ms, tok/sec:157345.63
step 11506, loss: 3.265839, norm:0.2688, lr:1.4397e-04 dt: 3332.01ms, tok/sec:157348.82
step 11507, loss: 3.233464, norm:0.2615, lr:1.4392e-04 dt: 3332.01ms, tok/sec:157348.96
step 11508, loss: 3.176685, norm:0.2654, lr:1.4388e-04 dt: 3331.97ms, tok/sec:157350.67
step 11509, loss: 3.340317, norm:0.2774, lr:1.4384e-04 dt: 3332.17ms, tok/sec:157341.42
step 11510, loss: 3.235751, norm:0.2624, lr:1.4380e-04 dt: 3332.13ms, tok/sec:157343.18
step 11511, loss: 3.221348, norm:0.3045, lr:1.4376e-04 dt: 3331.91ms, tok/sec:157353.74
step 11512, loss: 3.185923, norm:0.2592, lr:1.4371e-04 dt: 3332.07ms, tok/sec:157345.80
step 11513, loss: 3.326713, norm:0.2760, lr:1.4367e-04 dt: 3332.25ms, tok/sec:157337.47
step 11514, loss: 3.277584, norm:0.2847, lr:1.4363e-04 dt: 3331.92ms, tok/sec:157353.14
step 11515, loss: 3.241759, norm:0.2665, lr:1.4359e-04 dt: 3332.23ms, tok/sec:157338.41
step 11516, loss: 3.255702, norm:0.2750, lr:1.4354e-04 dt: 3331.97ms, tok/sec:157350.94
step 11517, loss: 3.220045, norm:0.2685, lr:1.4350e-04 dt: 3332.00ms, tok/sec:157349.16
step 11518, loss: 3.239210, norm:0.2622, lr:1.4346e-04 dt: 3331.99ms, tok/sec:157349.87
step 11519, loss: 3.269178, norm:0.2732, lr:1.4342e-04 dt: 3332.09ms, tok/sec:157345.31
step 11520, loss: 3.221992, norm:0.2696, lr:1.4338e-04 dt: 3331.97ms, tok/sec:157350.90
step 11521, loss: 3.220661, norm:0.2586, lr:1.4333e-04 dt: 3332.28ms, tok/sec:157335.99
step 11522, loss: 3.211257, norm:0.2601, lr:1.4329e-04 dt: 3331.85ms, tok/sec:157356.20
step 11523, loss: 3.248784, norm:0.2651, lr:1.4325e-04 dt: 3332.21ms, tok/sec:157339.20
step 11524, loss: 3.254562, norm:0.2655, lr:1.4321e-04 dt: 3332.10ms, tok/sec:157344.84
step 11525, loss: 3.202397, norm:0.2531, lr:1.4317e-04 dt: 3332.03ms, tok/sec:157347.88
step 11526, loss: 3.253010, norm:0.2783, lr:1.4312e-04 dt: 3331.97ms, tok/sec:157350.60
step 11527, loss: 3.253754, norm:0.2529, lr:1.4308e-04 dt: 3331.92ms, tok/sec:157352.90
step 11528, loss: 3.204864, norm:0.2686, lr:1.4304e-04 dt: 3331.88ms, tok/sec:157355.14
step 11529, loss: 3.356001, norm:0.3030, lr:1.4300e-04 dt: 3332.03ms, tok/sec:157347.84
step 11530, loss: 3.201522, norm:0.2767, lr:1.4295e-04 dt: 3332.31ms, tok/sec:157334.62
step 11531, loss: 3.202206, norm:0.2956, lr:1.4291e-04 dt: 3332.07ms, tok/sec:157346.19
step 11532, loss: 3.155549, norm:0.2729, lr:1.4287e-04 dt: 3331.86ms, tok/sec:157356.15
step 11533, loss: 3.201248, norm:0.2602, lr:1.4283e-04 dt: 3331.80ms, tok/sec:157358.80
step 11534, loss: 3.264102, norm:0.2759, lr:1.4279e-04 dt: 3332.03ms, tok/sec:157347.80
step 11535, loss: 3.219484, norm:0.2775, lr:1.4274e-04 dt: 3331.96ms, tok/sec:157351.40
step 11536, loss: 3.173422, norm:0.2794, lr:1.4270e-04 dt: 3331.86ms, tok/sec:157356.15
step 11537, loss: 3.154562, norm:0.2866, lr:1.4266e-04 dt: 3332.32ms, tok/sec:157334.19
step 11538, loss: 3.203850, norm:0.2546, lr:1.4262e-04 dt: 3332.13ms, tok/sec:157343.36
step 11539, loss: 3.179961, norm:0.2713, lr:1.4258e-04 dt: 3332.10ms, tok/sec:157344.44
step 11540, loss: 3.133863, norm:0.2684, lr:1.4253e-04 dt: 3331.87ms, tok/sec:157355.49
step 11541, loss: 3.245931, norm:0.2950, lr:1.4249e-04 dt: 3332.17ms, tok/sec:157341.23
step 11542, loss: 3.275564, norm:0.2732, lr:1.4245e-04 dt: 3332.17ms, tok/sec:157341.26
step 11543, loss: 3.207670, norm:0.2704, lr:1.4241e-04 dt: 3331.92ms, tok/sec:157353.02
step 11544, loss: 3.278322, norm:0.2789, lr:1.4237e-04 dt: 3331.93ms, tok/sec:157352.71
step 11545, loss: 3.270651, norm:0.2550, lr:1.4232e-04 dt: 3331.93ms, tok/sec:157352.77
step 11546, loss: 3.221866, norm:0.2877, lr:1.4228e-04 dt: 3332.16ms, tok/sec:157341.86
step 11547, loss: 3.263838, norm:0.2653, lr:1.4224e-04 dt: 3332.28ms, tok/sec:157335.94
step 11548, loss: 3.200447, norm:0.2685, lr:1.4220e-04 dt: 3331.83ms, tok/sec:157357.59
step 11549, loss: 3.262798, norm:0.2692, lr:1.4216e-04 dt: 3332.08ms, tok/sec:157345.48
HellaSwag accuracy:-2286593098537270255/-2=1143296549268635136.0000
rank 1 sample 0: Hello, I'm a language model, and you're going to want to call the new machine which comes out of the box in the language model.
The
rank 1 sample 1: Hello, I'm a language model, a person that reads and writes to my browser. I would like to explain my reasoning, and at least I can explain
rank 1 sample 2: Hello, I'm a language model, but who cares?
He's a language model, a theory that I just don't have an understanding of because he
rank 1 sample 3: Hello, I'm a language model, and I'm interested to develop a method framework for that -- my theory was as follows -- my model was a framework for
rank 0 sample 0: Hello, I'm a language model, and I'll be talking about this with every sentence and what it might look like. But I think I don't really
rank 0 sample 1: Hello, I'm a language model, but we're not a language model?
Well how the grammar in Chinese, for the most part, is translated into
rank 0 sample 2: Hello, I'm a language model, so I thought this was good. The first version of the document has two parts: the first part is a description,
rank 0 sample 3: Hello, I'm a language model, and what I mean is that you're talking about a language model, so that we're saying, like, we say
step 11550, loss: 3.261010, norm:0.3494, lr:1.4212e-04 dt: 48385.11ms, tok/sec:10835.73
step 11551, loss: 3.240510, norm:0.2729, lr:1.4207e-04 dt: 3332.17ms, tok/sec:157341.36
step 11552, loss: 3.223298, norm:0.3182, lr:1.4203e-04 dt: 3331.95ms, tok/sec:157351.68
step 11553, loss: 3.264351, norm:0.2781, lr:1.4199e-04 dt: 3331.93ms, tok/sec:157352.59
step 11554, loss: 3.268159, norm:0.2606, lr:1.4195e-04 dt: 3332.04ms, tok/sec:157347.66
step 11555, loss: 3.216954, norm:0.2609, lr:1.4191e-04 dt: 3332.27ms, tok/sec:157336.44
step 11556, loss: 3.238615, norm:0.2784, lr:1.4186e-04 dt: 3331.84ms, tok/sec:157356.94
step 11557, loss: 3.203321, norm:0.2608, lr:1.4182e-04 dt: 3331.94ms, tok/sec:157352.27
step 11558, loss: 3.246043, norm:0.2729, lr:1.4178e-04 dt: 3332.19ms, tok/sec:157340.55
step 11559, loss: 3.239979, norm:0.2957, lr:1.4174e-04 dt: 3332.01ms, tok/sec:157348.65
step 11560, loss: 3.249807, norm:0.2564, lr:1.4170e-04 dt: 3331.86ms, tok/sec:157355.95
step 11561, loss: 3.223547, norm:0.2624, lr:1.4166e-04 dt: 3332.04ms, tok/sec:157347.23
step 11562, loss: 3.313384, norm:0.2661, lr:1.4161e-04 dt: 3332.36ms, tok/sec:157332.41
step 11563, loss: 3.187874, norm:0.2743, lr:1.4157e-04 dt: 3331.93ms, tok/sec:157352.75
step 11564, loss: 3.234122, norm:0.2564, lr:1.4153e-04 dt: 3331.94ms, tok/sec:157352.36
step 11565, loss: 3.259434, norm:0.2811, lr:1.4149e-04 dt: 3331.79ms, tok/sec:157359.09
step 11566, loss: 3.271493, norm:0.3554, lr:1.4145e-04 dt: 3332.04ms, tok/sec:157347.44
step 11567, loss: 3.214522, norm:0.2461, lr:1.4140e-04 dt: 3332.04ms, tok/sec:157347.57
step 11568, loss: 3.199632, norm:0.2754, lr:1.4136e-04 dt: 3331.90ms, tok/sec:157354.15
step 11569, loss: 3.203286, norm:0.2596, lr:1.4132e-04 dt: 3332.01ms, tok/sec:157348.89
step 11570, loss: 3.182628, norm:0.2728, lr:1.4128e-04 dt: 3332.26ms, tok/sec:157337.03
step 11571, loss: 3.167789, norm:0.2487, lr:1.4124e-04 dt: 3332.02ms, tok/sec:157348.42
step 11572, loss: 3.196413, norm:0.3090, lr:1.4120e-04 dt: 3331.99ms, tok/sec:157349.70
step 11573, loss: 3.190578, norm:0.2668, lr:1.4115e-04 dt: 3331.93ms, tok/sec:157352.82
step 11574, loss: 3.174319, norm:0.2576, lr:1.4111e-04 dt: 3332.09ms, tok/sec:157345.31
step 11575, loss: 3.258114, norm:0.2714, lr:1.4107e-04 dt: 3332.12ms, tok/sec:157343.63
step 11576, loss: 3.226226, norm:0.2798, lr:1.4103e-04 dt: 3332.02ms, tok/sec:157348.52
step 11577, loss: 3.262778, norm:0.2650, lr:1.4099e-04 dt: 3332.12ms, tok/sec:157343.50
step 11578, loss: 3.296704, norm:0.2812, lr:1.4095e-04 dt: 3332.16ms, tok/sec:157341.64
step 11579, loss: 3.207795, norm:0.2654, lr:1.4090e-04 dt: 3332.09ms, tok/sec:157345.05
step 11580, loss: 3.154393, norm:0.2565, lr:1.4086e-04 dt: 3331.88ms, tok/sec:157354.88
step 11581, loss: 3.236475, norm:0.2915, lr:1.4082e-04 dt: 3331.90ms, tok/sec:157354.14
step 11582, loss: 3.260707, norm:0.2720, lr:1.4078e-04 dt: 3332.26ms, tok/sec:157336.93
step 11583, loss: 3.250295, norm:0.2584, lr:1.4074e-04 dt: 3332.04ms, tok/sec:157347.32
step 11584, loss: 3.286116, norm:0.2698, lr:1.4070e-04 dt: 3332.10ms, tok/sec:157344.76
step 11585, loss: 3.283867, norm:0.2757, lr:1.4065e-04 dt: 3332.10ms, tok/sec:157344.81
step 11586, loss: 3.375582, norm:0.4639, lr:1.4061e-04 dt: 3331.99ms, tok/sec:157349.91
step 11587, loss: 3.183969, norm:0.2662, lr:1.4057e-04 dt: 3332.14ms, tok/sec:157342.87
step 11588, loss: 3.187681, norm:0.2606, lr:1.4053e-04 dt: 3331.88ms, tok/sec:157354.95
step 11589, loss: 3.259411, norm:0.2848, lr:1.4049e-04 dt: 3331.82ms, tok/sec:157358.00
step 11590, loss: 3.238206, norm:0.2645, lr:1.4045e-04 dt: 3332.06ms, tok/sec:157346.72
step 11591, loss: 3.262195, norm:0.2663, lr:1.4041e-04 dt: 3331.96ms, tok/sec:157350.99
step 11592, loss: 3.240339, norm:0.2685, lr:1.4036e-04 dt: 3331.99ms, tok/sec:157349.97
step 11593, loss: 3.230053, norm:0.2741, lr:1.4032e-04 dt: 3332.19ms, tok/sec:157340.41
step 11594, loss: 3.238813, norm:0.2643, lr:1.4028e-04 dt: 3331.92ms, tok/sec:157352.99
step 11595, loss: 3.326613, norm:0.2613, lr:1.4024e-04 dt: 3331.94ms, tok/sec:157352.14
step 11596, loss: 3.224202, norm:0.2664, lr:1.4020e-04 dt: 3332.14ms, tok/sec:157342.94
step 11597, loss: 3.252906, norm:0.2638, lr:1.4016e-04 dt: 3331.92ms, tok/sec:157353.00
step 11598, loss: 3.243727, norm:0.2723, lr:1.4011e-04 dt: 3332.07ms, tok/sec:157346.23
step 11599, loss: 3.237788, norm:0.2561, lr:1.4007e-04 dt: 3332.10ms, tok/sec:157344.44
validation loss: 3.2325
Model and optimizer state saved.
HellaSwag accuracy:-2295600297792009199/-2=1147800148896004608.0000
rank 1 sample 0: Hello, I'm a language model, and this is the time when I hope you had something more or less useful to say, I would like to answer you
rank 1 sample 1: Hello, I'm a language model, a person, and a machine. You'd think that because you do that, like programming languages, that you can do
rank 1 sample 2: Hello, I'm a language model, but then it's not at all clear what I'm trying to teach, and I'm trying to get a sense for
rank 1 sample 3: Hello, I'm a language model, and I'm writing this with the language understanding. I did my very best writing this out myself, and I know I
rank 0 sample 0: Hello, I'm a language model, and I'll be talking about this on how they're not the perfect book on how the books go together, but what
rank 0 sample 1: Hello, I'm a language model, I was recently asked if I should name a word when I'm not yet using the dictionary.
Well, I don
rank 0 sample 2: Hello, I'm a language model, but I wanted to explain everything I'd like to know about my mother tongue.
As you can see, the term
rank 0 sample 3: Hello, I'm a language model, and want to make sure that you don't introduce too many ambiguities, because there's no time to panic right
step 11600, loss: 3.167738, norm:0.2800, lr:1.4003e-04 dt: 56039.22ms, tok/sec:9355.73
step 11601, loss: 3.159961, norm:0.2469, lr:1.3999e-04 dt: 3332.15ms, tok/sec:157342.30
step 11602, loss: 3.213212, norm:0.2544, lr:1.3995e-04 dt: 3331.78ms, tok/sec:157359.63
step 11603, loss: 3.183482, norm:0.2716, lr:1.3991e-04 dt: 3331.91ms, tok/sec:157353.69
step 11604, loss: 3.219269, norm:0.2462, lr:1.3987e-04 dt: 3331.85ms, tok/sec:157356.25
step 11605, loss: 3.195709, norm:0.2459, lr:1.3982e-04 dt: 3332.01ms, tok/sec:157348.73
step 11606, loss: 3.171809, norm:0.2554, lr:1.3978e-04 dt: 3331.94ms, tok/sec:157352.16
step 11607, loss: 3.247959, norm:0.2527, lr:1.3974e-04 dt: 3331.88ms, tok/sec:157354.84
step 11608, loss: 3.210000, norm:0.2518, lr:1.3970e-04 dt: 3331.86ms, tok/sec:157355.85
step 11609, loss: 3.174204, norm:0.2624, lr:1.3966e-04 dt: 3331.97ms, tok/sec:157350.66
step 11610, loss: 3.158867, norm:0.2582, lr:1.3962e-04 dt: 3332.02ms, tok/sec:157348.31
step 11611, loss: 3.180890, norm:0.2641, lr:1.3958e-04 dt: 3331.93ms, tok/sec:157352.71
step 11612, loss: 3.256467, norm:0.2774, lr:1.3954e-04 dt: 3332.07ms, tok/sec:157346.27
step 11613, loss: 3.239328, norm:0.2704, lr:1.3949e-04 dt: 3332.18ms, tok/sec:157340.86
step 11614, loss: 3.180096, norm:0.2522, lr:1.3945e-04 dt: 3332.00ms, tok/sec:157349.52
step 11615, loss: 3.266369, norm:0.2916, lr:1.3941e-04 dt: 3332.07ms, tok/sec:157346.24
step 11616, loss: 3.233654, norm:0.2760, lr:1.3937e-04 dt: 3332.06ms, tok/sec:157346.46
step 11617, loss: 3.251092, norm:0.3044, lr:1.3933e-04 dt: 3332.01ms, tok/sec:157349.03
step 11618, loss: 3.154191, norm:0.2828, lr:1.3929e-04 dt: 3331.83ms, tok/sec:157357.29
step 11619, loss: 3.170775, norm:0.2874, lr:1.3925e-04 dt: 3331.93ms, tok/sec:157352.42
step 11620, loss: 3.209737, norm:0.2591, lr:1.3920e-04 dt: 3334.34ms, tok/sec:157238.72
step 11621, loss: 3.239578, norm:0.2640, lr:1.3916e-04 dt: 3332.47ms, tok/sec:157327.25
step 11622, loss: 3.171548, norm:0.2719, lr:1.3912e-04 dt: 3332.30ms, tok/sec:157335.08
step 11623, loss: 3.220344, norm:0.2765, lr:1.3908e-04 dt: 3331.89ms, tok/sec:157354.34
step 11624, loss: 3.247591, norm:0.2718, lr:1.3904e-04 dt: 3332.11ms, tok/sec:157344.20
step 11625, loss: 3.240514, norm:0.2723, lr:1.3900e-04 dt: 3332.03ms, tok/sec:157347.77
step 11626, loss: 3.252200, norm:0.2766, lr:1.3896e-04 dt: 3332.02ms, tok/sec:157348.18
step 11627, loss: 3.247899, norm:0.2853, lr:1.3892e-04 dt: 3331.87ms, tok/sec:157355.42
step 11628, loss: 3.261951, norm:0.3536, lr:1.3888e-04 dt: 3331.97ms, tok/sec:157350.79
step 11629, loss: 3.205621, norm:0.2693, lr:1.3883e-04 dt: 3332.01ms, tok/sec:157348.85
step 11630, loss: 3.221630, norm:0.2967, lr:1.3879e-04 dt: 3331.91ms, tok/sec:157353.65
step 11631, loss: 3.264283, norm:0.2836, lr:1.3875e-04 dt: 3331.90ms, tok/sec:157354.24
step 11632, loss: 3.244840, norm:0.2577, lr:1.3871e-04 dt: 3331.94ms, tok/sec:157351.97
step 11633, loss: 3.247911, norm:0.2728, lr:1.3867e-04 dt: 3331.89ms, tok/sec:157354.60
step 11634, loss: 3.180725, norm:0.2947, lr:1.3863e-04 dt: 3332.02ms, tok/sec:157348.47
step 11635, loss: 3.168552, norm:0.2691, lr:1.3859e-04 dt: 3331.98ms, tok/sec:157350.22
step 11636, loss: 3.253954, norm:0.2737, lr:1.3855e-04 dt: 3332.12ms, tok/sec:157343.60
step 11637, loss: 3.219344, norm:0.2563, lr:1.3850e-04 dt: 3332.23ms, tok/sec:157338.67
step 11638, loss: 3.218253, norm:0.2701, lr:1.3846e-04 dt: 3331.97ms, tok/sec:157350.70
step 11639, loss: 3.195343, norm:0.2583, lr:1.3842e-04 dt: 3331.91ms, tok/sec:157353.40
step 11640, loss: 3.189551, norm:0.2798, lr:1.3838e-04 dt: 3331.90ms, tok/sec:157354.18
step 11641, loss: 3.192626, norm:0.2531, lr:1.3834e-04 dt: 3331.92ms, tok/sec:157353.11
step 11642, loss: 3.153940, norm:0.2538, lr:1.3830e-04 dt: 3331.73ms, tok/sec:157362.25
step 11643, loss: 3.229446, norm:0.2705, lr:1.3826e-04 dt: 3332.23ms, tok/sec:157338.41
step 11644, loss: 3.230845, norm:0.2667, lr:1.3822e-04 dt: 3332.19ms, tok/sec:157340.53
step 11645, loss: 3.194279, norm:0.2630, lr:1.3818e-04 dt: 3331.92ms, tok/sec:157353.02
step 11646, loss: 3.278203, norm:0.2760, lr:1.3814e-04 dt: 3332.03ms, tok/sec:157347.85
step 11647, loss: 3.243343, norm:0.2828, lr:1.3809e-04 dt: 3332.05ms, tok/sec:157346.85
step 11648, loss: 3.233559, norm:0.2788, lr:1.3805e-04 dt: 3332.09ms, tok/sec:157345.08
step 11649, loss: 3.241759, norm:0.2905, lr:1.3801e-04 dt: 3332.22ms, tok/sec:157339.00
HellaSwag accuracy:6927912476551119889/-2=-3463956238275559936.0000
rank 1 sample 0: Hello, I'm a language model, and my thoughts are always about how i should get into C (and not so much) in a language course.

rank 1 sample 1: Hello, I'm a language model, a teacher.<|endoftext|>The first and third books of the Qur’an
Our knowledge of the first and third books
rank 1 sample 2: Hello, I'm a language model, but since the term "language" is a bit misleading to make things more confusing, I think you've probably had the
rank 1 sample 3: Hello, I'm a language model, and I'm really happy with that. After all, so many projects we'd like to improve and improve the system.
rank 0 sample 0: Hello, I'm a language model, and I'll be talking about this on several occasion.
- For someone like me who never used a foreign language and
rank 0 sample 1: Hello, I'm a language model, but how do you get to this first step of our research project? There are two steps that I needed to take,
rank 0 sample 2: Hello, I'm a language model, so I will take things for granted here. However, I've had my own language model and I'm not going to
rank 0 sample 3: Hello, I'm a language model, and like you, I'm a beginner, why not use that?
The syntax I use to communicate with a group
step 11650, loss: 3.227035, norm:0.2855, lr:1.3797e-04 dt: 48381.59ms, tok/sec:10836.52
step 11651, loss: 3.271701, norm:0.2707, lr:1.3793e-04 dt: 3331.98ms, tok/sec:157350.18
step 11652, loss: 3.341792, norm:0.3046, lr:1.3789e-04 dt: 3332.15ms, tok/sec:157342.47
step 11653, loss: 3.203526, norm:0.3042, lr:1.3785e-04 dt: 3331.77ms, tok/sec:157360.18
step 11654, loss: 3.190881, norm:0.2704, lr:1.3781e-04 dt: 3332.30ms, tok/sec:157335.21
step 11655, loss: 3.241764, norm:0.2875, lr:1.3777e-04 dt: 3332.00ms, tok/sec:157349.56
step 11656, loss: 3.253657, norm:0.2851, lr:1.3773e-04 dt: 3332.07ms, tok/sec:157346.24
step 11657, loss: 3.303734, norm:0.2851, lr:1.3768e-04 dt: 3331.80ms, tok/sec:157358.74
step 11658, loss: 3.222524, norm:0.2810, lr:1.3764e-04 dt: 3332.03ms, tok/sec:157347.72
step 11659, loss: 3.204198, norm:0.2830, lr:1.3760e-04 dt: 3331.90ms, tok/sec:157354.05
step 11660, loss: 3.285684, norm:0.2979, lr:1.3756e-04 dt: 3332.06ms, tok/sec:157346.31
step 11661, loss: 3.243021, norm:0.2592, lr:1.3752e-04 dt: 3331.97ms, tok/sec:157350.58
step 11662, loss: 3.225935, norm:0.2499, lr:1.3748e-04 dt: 3332.20ms, tok/sec:157339.80
step 11663, loss: 3.248604, norm:0.2697, lr:1.3744e-04 dt: 3331.89ms, tok/sec:157354.73
step 11664, loss: 3.195458, norm:0.2440, lr:1.3740e-04 dt: 3332.00ms, tok/sec:157349.36
step 11665, loss: 3.178955, norm:0.2622, lr:1.3736e-04 dt: 3332.13ms, tok/sec:157342.98
step 11666, loss: 3.378130, norm:0.3395, lr:1.3732e-04 dt: 3332.21ms, tok/sec:157339.23
step 11667, loss: 3.204912, norm:0.2823, lr:1.3728e-04 dt: 3331.99ms, tok/sec:157349.64
step 11668, loss: 3.180360, norm:0.2906, lr:1.3723e-04 dt: 3332.21ms, tok/sec:157339.49
step 11669, loss: 3.263481, norm:0.2648, lr:1.3719e-04 dt: 3331.81ms, tok/sec:157358.16
step 11670, loss: 3.276557, norm:0.2705, lr:1.3715e-04 dt: 3331.99ms, tok/sec:157349.62
step 11671, loss: 3.181343, norm:0.2855, lr:1.3711e-04 dt: 3332.06ms, tok/sec:157346.68
step 11672, loss: 3.247883, norm:0.2615, lr:1.3707e-04 dt: 3331.94ms, tok/sec:157352.07
step 11673, loss: 3.188962, norm:0.2717, lr:1.3703e-04 dt: 3331.95ms, tok/sec:157351.84
step 11674, loss: 3.186748, norm:0.2615, lr:1.3699e-04 dt: 3331.97ms, tok/sec:157350.85
step 11675, loss: 3.237245, norm:0.2491, lr:1.3695e-04 dt: 3332.12ms, tok/sec:157343.59
step 11676, loss: 3.170943, norm:0.2525, lr:1.3691e-04 dt: 3331.91ms, tok/sec:157353.73
step 11677, loss: 3.230591, norm:0.2640, lr:1.3687e-04 dt: 3331.88ms, tok/sec:157354.86
step 11678, loss: 3.248749, norm:0.2826, lr:1.3683e-04 dt: 3331.90ms, tok/sec:157354.28
step 11679, loss: 3.185792, norm:0.2460, lr:1.3679e-04 dt: 3332.16ms, tok/sec:157341.64
step 11680, loss: 3.215504, norm:0.2706, lr:1.3675e-04 dt: 3331.96ms, tok/sec:157351.12
step 11681, loss: 3.158679, norm:0.2611, lr:1.3670e-04 dt: 3331.94ms, tok/sec:157352.10
step 11682, loss: 3.179957, norm:0.2753, lr:1.3666e-04 dt: 3332.37ms, tok/sec:157331.81
step 11683, loss: 3.271093, norm:0.2771, lr:1.3662e-04 dt: 3331.98ms, tok/sec:157350.10
step 11684, loss: 3.223756, norm:0.2755, lr:1.3658e-04 dt: 3332.07ms, tok/sec:157346.03
step 11685, loss: 3.245344, norm:0.2734, lr:1.3654e-04 dt: 3332.02ms, tok/sec:157348.29
step 11686, loss: 3.198694, norm:0.2685, lr:1.3650e-04 dt: 3331.90ms, tok/sec:157353.89
step 11687, loss: 3.359547, norm:0.2960, lr:1.3646e-04 dt: 3332.08ms, tok/sec:157345.48
step 11688, loss: 3.247205, norm:0.2810, lr:1.3642e-04 dt: 3332.02ms, tok/sec:157348.21
step 11689, loss: 3.227658, norm:0.2644, lr:1.3638e-04 dt: 3331.97ms, tok/sec:157350.76
step 11690, loss: 3.225327, norm:0.2633, lr:1.3634e-04 dt: 3332.26ms, tok/sec:157336.92
step 11691, loss: 3.231385, norm:0.2912, lr:1.3630e-04 dt: 3332.16ms, tok/sec:157341.83
step 11692, loss: 3.247429, norm:0.2569, lr:1.3626e-04 dt: 3332.20ms, tok/sec:157339.97
step 11693, loss: 3.245567, norm:0.2637, lr:1.3622e-04 dt: 3331.95ms, tok/sec:157351.80
step 11694, loss: 3.253860, norm:0.2580, lr:1.3618e-04 dt: 3331.83ms, tok/sec:157357.37
step 11695, loss: 3.273938, norm:0.2565, lr:1.3614e-04 dt: 3332.00ms, tok/sec:157349.24
step 11696, loss: 3.225360, norm:0.2655, lr:1.3609e-04 dt: 3332.19ms, tok/sec:157340.59
step 11697, loss: 3.224475, norm:0.2718, lr:1.3605e-04 dt: 3331.92ms, tok/sec:157353.34
step 11698, loss: 3.256506, norm:0.2755, lr:1.3601e-04 dt: 3332.06ms, tok/sec:157346.31
step 11699, loss: 3.202657, norm:0.2719, lr:1.3597e-04 dt: 3332.29ms, tok/sec:157335.65
validation loss: 3.2303
Model and optimizer state saved.
HellaSwag accuracy:6936778938317507601/-2=-3468389469158753792.0000
rank 1 sample 0: Hello, I'm a language model, and my knowledge of how to implement Ruby in Windows and applications as well as an understanding of how to use a Ruby language
rank 1 sample 1: Hello, I'm a language model, which means I think I'm learning more about the language more than I know the native ones. All of the language models
rank 1 sample 2: Hello, I'm a language model, but could be just as engaging as I am in the process...I think it's a really exciting and interesting experience for
rank 1 sample 3: Hello, I'm a language model, and I'm looking at one of the two languages that, being bilingual, and being educated in one of the two languages
rank 0 sample 0: Hello, I'm a language model, and I'll be talking about this topic to try to start the journey here!
So, the next step is to
rank 0 sample 1: Hello, I'm a language model, but as a language model, the underlying language of education does not work without its underlying model. The other reason why it
rank 0 sample 2: Hello, I'm a language model, so I like that what kind of information I'd like to give them?
A: I'm a language model.
rank 0 sample 3: Hello, I'm a language model, and here's what I need:
I find it difficult to define, because our first two columns say "hello world
step 11700, loss: 3.213884, norm:0.2868, lr:1.3593e-04 dt: 56103.01ms, tok/sec:9345.10
step 11701, loss: 3.182788, norm:0.2568, lr:1.3589e-04 dt: 3332.10ms, tok/sec:157344.73
step 11702, loss: 3.208116, norm:0.2851, lr:1.3585e-04 dt: 3332.06ms, tok/sec:157346.40
step 11703, loss: 3.218757, norm:0.3183, lr:1.3581e-04 dt: 3332.21ms, tok/sec:157339.45
step 11704, loss: 3.308951, norm:0.2650, lr:1.3577e-04 dt: 3331.89ms, tok/sec:157354.42
step 11705, loss: 3.193476, norm:0.2753, lr:1.3573e-04 dt: 3331.78ms, tok/sec:157359.64
step 11706, loss: 3.194217, norm:0.2835, lr:1.3569e-04 dt: 3332.01ms, tok/sec:157348.90
step 11707, loss: 3.204874, norm:0.2755, lr:1.3565e-04 dt: 3331.93ms, tok/sec:157352.46
step 11708, loss: 3.200991, norm:0.2630, lr:1.3561e-04 dt: 3331.72ms, tok/sec:157362.79
step 11709, loss: 3.159705, norm:0.2527, lr:1.3557e-04 dt: 3331.99ms, tok/sec:157349.86
step 11710, loss: 3.227220, norm:0.2653, lr:1.3553e-04 dt: 3331.88ms, tok/sec:157355.07
step 11711, loss: 3.172123, norm:0.2791, lr:1.3549e-04 dt: 3332.04ms, tok/sec:157347.38
step 11712, loss: 3.215476, norm:0.2558, lr:1.3545e-04 dt: 3332.09ms, tok/sec:157345.27
step 11713, loss: 3.212864, norm:0.2736, lr:1.3541e-04 dt: 3331.97ms, tok/sec:157350.87
step 11714, loss: 3.155972, norm:0.2480, lr:1.3537e-04 dt: 3331.99ms, tok/sec:157349.62
step 11715, loss: 3.208996, norm:0.2588, lr:1.3533e-04 dt: 3332.21ms, tok/sec:157339.47
step 11716, loss: 3.189446, norm:0.3066, lr:1.3528e-04 dt: 3331.87ms, tok/sec:157355.64
step 11717, loss: 3.251645, norm:0.2770, lr:1.3524e-04 dt: 3331.89ms, tok/sec:157354.62
step 11718, loss: 3.262784, norm:0.2828, lr:1.3520e-04 dt: 3331.80ms, tok/sec:157358.67
step 11719, loss: 3.300215, norm:0.2707, lr:1.3516e-04 dt: 3332.11ms, tok/sec:157344.34
step 11720, loss: 3.209716, norm:0.2746, lr:1.3512e-04 dt: 3331.94ms, tok/sec:157352.01
step 11721, loss: 3.290389, norm:0.2676, lr:1.3508e-04 dt: 3332.21ms, tok/sec:157339.33
step 11722, loss: 3.246746, norm:0.2832, lr:1.3504e-04 dt: 3331.98ms, tok/sec:157350.16
step 11723, loss: 3.226468, norm:0.2692, lr:1.3500e-04 dt: 3331.92ms, tok/sec:157352.89
step 11724, loss: 3.273743, norm:0.2640, lr:1.3496e-04 dt: 3332.07ms, tok/sec:157346.27
step 11725, loss: 3.268429, norm:0.2619, lr:1.3492e-04 dt: 3332.07ms, tok/sec:157346.20
step 11726, loss: 3.251661, norm:0.2562, lr:1.3488e-04 dt: 3331.90ms, tok/sec:157354.08
step 11727, loss: 3.248400, norm:0.2715, lr:1.3484e-04 dt: 3331.94ms, tok/sec:157352.26
step 11728, loss: 3.222886, norm:0.2750, lr:1.3480e-04 dt: 3331.89ms, tok/sec:157354.45
step 11729, loss: 3.239119, norm:0.2701, lr:1.3476e-04 dt: 3331.87ms, tok/sec:157355.33
step 11730, loss: 3.276649, norm:0.2749, lr:1.3472e-04 dt: 3332.00ms, tok/sec:157349.29
step 11731, loss: 3.256840, norm:0.2612, lr:1.3468e-04 dt: 3332.21ms, tok/sec:157339.62
step 11732, loss: 3.198988, norm:0.2572, lr:1.3464e-04 dt: 3332.09ms, tok/sec:157345.17
step 11733, loss: 3.198953, norm:0.2769, lr:1.3460e-04 dt: 3332.01ms, tok/sec:157348.87
step 11734, loss: 3.221134, norm:0.2583, lr:1.3456e-04 dt: 3331.80ms, tok/sec:157358.67
step 11735, loss: 3.247397, norm:0.2455, lr:1.3452e-04 dt: 3332.23ms, tok/sec:157338.67
step 11736, loss: 3.303704, norm:0.2600, lr:1.3448e-04 dt: 3331.96ms, tok/sec:157351.07
step 11737, loss: 3.220542, norm:0.2494, lr:1.3444e-04 dt: 3331.79ms, tok/sec:157359.29
step 11738, loss: 3.257644, norm:0.2804, lr:1.3440e-04 dt: 3332.23ms, tok/sec:157338.69
step 11739, loss: 3.277739, norm:0.2671, lr:1.3436e-04 dt: 3331.96ms, tok/sec:157351.16
step 11740, loss: 3.197898, norm:0.2734, lr:1.3432e-04 dt: 3332.05ms, tok/sec:157346.76
step 11741, loss: 3.204251, norm:0.2552, lr:1.3428e-04 dt: 3332.17ms, tok/sec:157341.47
step 11742, loss: 3.209720, norm:0.2742, lr:1.3424e-04 dt: 3332.14ms, tok/sec:157342.64
step 11743, loss: 3.135326, norm:0.2666, lr:1.3420e-04 dt: 3332.12ms, tok/sec:157343.55
step 11744, loss: 3.260652, norm:0.2986, lr:1.3416e-04 dt: 3331.94ms, tok/sec:157351.98
step 11745, loss: 3.169958, norm:0.2549, lr:1.3412e-04 dt: 3332.10ms, tok/sec:157344.39
step 11746, loss: 3.207126, norm:0.2669, lr:1.3408e-04 dt: 3332.12ms, tok/sec:157343.51
step 11747, loss: 3.227262, norm:0.2626, lr:1.3404e-04 dt: 3332.09ms, tok/sec:157345.26
step 11748, loss: 3.165905, norm:0.2504, lr:1.3400e-04 dt: 3332.15ms, tok/sec:157342.43
step 11749, loss: 3.164676, norm:0.2482, lr:1.3396e-04 dt: 3332.36ms, tok/sec:157332.41
HellaSwag accuracy:-2295600297792011247/-2=1147800148896005632.0000
rank 1 sample 0: Hello, I'm a language model,
What's the difference between a set
(A) sets (B) sets (C) sets (D)
rank 1 sample 1: Hello, I'm a language model, which means I'd like to use your toolkit to implement your Python code. I haven't noticed anything like this before
rank 1 sample 2: Hello, I'm a language model, but at this point I'm not sure what to do about those problems that I have. So the best way would be
rank 1 sample 3: Hello, I'm a language model, and I'm looking at other languages with words like "what?" and "hello." Can you say what's on the
rank 0 sample 0: Hello, I'm a language model, and I don't have much idea if i do anything at all now. It's the opposite of a simple language,
rank 0 sample 1: Hello, I'm a language model, but let's face it, it ain't quite finished-and that's where we'll end up on this in the
rank 0 sample 2: Hello, I'm a language model, so I wrote it because language models for the web are a bunch of words that look very similar to each other, and
rank 0 sample 3: Hello, I'm a language model, you say, we're going to get into word-processing. This is where our main ideas are getting in, to
step 11750, loss: 3.221579, norm:0.2517, lr:1.3392e-04 dt: 48380.33ms, tok/sec:10836.80
step 11751, loss: 3.206844, norm:0.2737, lr:1.3388e-04 dt: 3332.11ms, tok/sec:157344.08
step 11752, loss: 3.219695, norm:0.2845, lr:1.3384e-04 dt: 3331.97ms, tok/sec:157350.60
step 11753, loss: 3.272675, norm:0.2695, lr:1.3380e-04 dt: 3332.15ms, tok/sec:157342.05
step 11754, loss: 3.229071, norm:0.2850, lr:1.3376e-04 dt: 3332.01ms, tok/sec:157348.81
step 11755, loss: 3.267110, norm:0.2983, lr:1.3372e-04 dt: 3332.08ms, tok/sec:157345.77
step 11756, loss: 3.242100, norm:0.2655, lr:1.3368e-04 dt: 3331.95ms, tok/sec:157351.59
step 11757, loss: 3.268379, norm:0.2903, lr:1.3364e-04 dt: 3331.93ms, tok/sec:157352.76
step 11758, loss: 3.196155, norm:0.2574, lr:1.3360e-04 dt: 3332.03ms, tok/sec:157347.81
step 11759, loss: 3.205136, norm:0.3040, lr:1.3356e-04 dt: 3331.73ms, tok/sec:157361.89
step 11760, loss: 3.299519, norm:0.2702, lr:1.3352e-04 dt: 3332.36ms, tok/sec:157332.55
step 11761, loss: 3.307150, norm:0.2747, lr:1.3348e-04 dt: 3332.00ms, tok/sec:157349.44
step 11762, loss: 3.207842, norm:0.2685, lr:1.3344e-04 dt: 3332.05ms, tok/sec:157346.87
step 11763, loss: 3.291222, norm:0.2665, lr:1.3340e-04 dt: 3331.84ms, tok/sec:157357.06
step 11764, loss: 3.221515, norm:0.2780, lr:1.3336e-04 dt: 3332.15ms, tok/sec:157342.31
step 11765, loss: 3.212818, norm:0.2546, lr:1.3332e-04 dt: 3331.95ms, tok/sec:157351.50
step 11766, loss: 3.254238, norm:0.3108, lr:1.3328e-04 dt: 3331.80ms, tok/sec:157358.61
step 11767, loss: 3.182068, norm:0.2649, lr:1.3324e-04 dt: 3331.85ms, tok/sec:157356.66
step 11768, loss: 3.222758, norm:0.3034, lr:1.3320e-04 dt: 3332.26ms, tok/sec:157337.24
step 11769, loss: 3.201750, norm:0.2765, lr:1.3316e-04 dt: 3331.92ms, tok/sec:157353.34
step 11770, loss: 3.193436, norm:0.2693, lr:1.3312e-04 dt: 3331.98ms, tok/sec:157350.51
step 11771, loss: 3.185168, norm:0.2864, lr:1.3308e-04 dt: 3331.91ms, tok/sec:157353.52
step 11772, loss: 3.265032, norm:0.2680, lr:1.3304e-04 dt: 3331.94ms, tok/sec:157351.95
step 11773, loss: 3.235783, norm:0.2761, lr:1.3300e-04 dt: 3332.10ms, tok/sec:157344.84
step 11774, loss: 3.211856, norm:0.2643, lr:1.3296e-04 dt: 3332.18ms, tok/sec:157340.71
step 11775, loss: 3.242111, norm:0.2607, lr:1.3292e-04 dt: 3331.93ms, tok/sec:157352.45
step 11776, loss: 3.215800, norm:0.2638, lr:1.3288e-04 dt: 3331.92ms, tok/sec:157353.03
step 11777, loss: 3.194324, norm:0.2669, lr:1.3284e-04 dt: 3332.21ms, tok/sec:157339.41
step 11778, loss: 3.186584, norm:0.2568, lr:1.3280e-04 dt: 3331.89ms, tok/sec:157354.33
step 11779, loss: 3.196473, norm:0.2786, lr:1.3276e-04 dt: 3331.95ms, tok/sec:157351.83
step 11780, loss: 3.302678, norm:0.2962, lr:1.3272e-04 dt: 3331.96ms, tok/sec:157351.25
step 11781, loss: 3.232970, norm:0.2748, lr:1.3268e-04 dt: 3331.96ms, tok/sec:157351.07
step 11782, loss: 3.175059, norm:0.3187, lr:1.3264e-04 dt: 3331.94ms, tok/sec:157352.19
step 11783, loss: 3.205222, norm:0.2647, lr:1.3260e-04 dt: 3331.94ms, tok/sec:157352.22
step 11784, loss: 3.197367, norm:0.2657, lr:1.3256e-04 dt: 3331.97ms, tok/sec:157350.80
step 11785, loss: 3.215490, norm:0.2568, lr:1.3252e-04 dt: 3331.94ms, tok/sec:157352.22
step 11786, loss: 3.150190, norm:0.2547, lr:1.3248e-04 dt: 3332.04ms, tok/sec:157347.58
step 11787, loss: 3.195623, norm:0.2903, lr:1.3244e-04 dt: 3331.93ms, tok/sec:157352.42
step 11788, loss: 3.183466, norm:0.2913, lr:1.3240e-04 dt: 3331.94ms, tok/sec:157352.22
step 11789, loss: 3.230421, norm:0.2794, lr:1.3236e-04 dt: 3332.12ms, tok/sec:157343.45
step 11790, loss: 3.243547, norm:0.2894, lr:1.3232e-04 dt: 3332.05ms, tok/sec:157347.03
step 11791, loss: 3.140958, norm:0.2717, lr:1.3228e-04 dt: 3331.73ms, tok/sec:157362.13
step 11792, loss: 3.307808, norm:0.2984, lr:1.3224e-04 dt: 3332.35ms, tok/sec:157332.89
step 11793, loss: 3.209154, norm:0.3032, lr:1.3220e-04 dt: 3332.01ms, tok/sec:157349.08
step 11794, loss: 3.271537, norm:0.2813, lr:1.3216e-04 dt: 3332.34ms, tok/sec:157333.17
step 11795, loss: 3.379982, norm:0.3393, lr:1.3212e-04 dt: 3331.90ms, tok/sec:157354.07
step 11796, loss: 3.234324, norm:0.2664, lr:1.3208e-04 dt: 3332.14ms, tok/sec:157342.53
step 11797, loss: 3.253184, norm:0.2640, lr:1.3204e-04 dt: 3332.00ms, tok/sec:157349.23
step 11798, loss: 3.201134, norm:0.2842, lr:1.3200e-04 dt: 3332.14ms, tok/sec:157342.72
step 11799, loss: 3.223723, norm:0.2699, lr:1.3196e-04 dt: 3332.26ms, tok/sec:157337.11
validation loss: 3.2286
Model and optimizer state saved.
HellaSwag accuracy:6927806923166417937/-2=-3463903461583208960.0000
rank 1 sample 0: Hello, I'm a language model, and so I'm using the following function. First I write the function:
Now it's my first question: what
rank 1 sample 1: Hello, I'm a language model, which means I've been learning and doing research for a month now. I want to build a really simple model to help
rank 1 sample 2: Hello, I'm a language model, but then there's a lot of things that I couldn't think of, and I'm a native speaker of it,
rank 1 sample 3: Hello, I'm a language model, and I'm working with C++ with multiple languages. With all of the syntax, even as long as I keep the
rank 0 sample 0: Hello, I'm a language model, and I don't have many of this info going on.
Here's how to make a game of these things,
rank 0 sample 1: Hello, I'm a language model, but don't know where to use code. It probably can be improved... or you'll want to be aware of this
rank 0 sample 2: Hello, I'm a language model, so I really don't believe you've ever come across a question or comment about how you're going to use it,
rank 0 sample 3: Hello, I'm a language model, and when I get to the point, it needs to be clear from the language level how they are represented. So first
step 11800, loss: 3.235252, norm:0.2893, lr:1.3192e-04 dt: 56051.30ms, tok/sec:9353.72
step 11801, loss: 3.201553, norm:0.2588, lr:1.3188e-04 dt: 3332.20ms, tok/sec:157340.01
step 11802, loss: 3.247802, norm:0.2714, lr:1.3184e-04 dt: 3332.30ms, tok/sec:157335.06
step 11803, loss: 3.231978, norm:0.2557, lr:1.3180e-04 dt: 3332.32ms, tok/sec:157334.01
step 11804, loss: 3.224426, norm:0.2604, lr:1.3176e-04 dt: 3332.28ms, tok/sec:157336.04
step 11805, loss: 3.251778, norm:0.2696, lr:1.3173e-04 dt: 3332.07ms, tok/sec:157346.15
step 11806, loss: 3.224952, norm:0.2602, lr:1.3169e-04 dt: 3332.11ms, tok/sec:157343.99
step 11807, loss: 3.237344, norm:0.2509, lr:1.3165e-04 dt: 3332.10ms, tok/sec:157344.44
step 11808, loss: 3.201864, norm:0.2642, lr:1.3161e-04 dt: 3332.03ms, tok/sec:157348.00
step 11809, loss: 3.187492, norm:0.2748, lr:1.3157e-04 dt: 3332.04ms, tok/sec:157347.23
step 11810, loss: 3.207590, norm:0.2535, lr:1.3153e-04 dt: 3334.03ms, tok/sec:157253.76
step 11811, loss: 3.167137, norm:0.2527, lr:1.3149e-04 dt: 3332.37ms, tok/sec:157331.65
step 11812, loss: 3.192181, norm:0.2826, lr:1.3145e-04 dt: 3332.16ms, tok/sec:157341.83
step 11813, loss: 3.191507, norm:0.2539, lr:1.3141e-04 dt: 3331.93ms, tok/sec:157352.77
step 11814, loss: 3.184157, norm:0.2598, lr:1.3137e-04 dt: 3331.79ms, tok/sec:157359.21
step 11815, loss: 3.193251, norm:0.2490, lr:1.3133e-04 dt: 3332.05ms, tok/sec:157347.06
step 11816, loss: 3.165200, norm:0.2498, lr:1.3129e-04 dt: 3332.11ms, tok/sec:157344.00
step 11817, loss: 3.185896, norm:0.2473, lr:1.3125e-04 dt: 3332.04ms, tok/sec:157347.62
step 11818, loss: 3.226662, norm:0.2476, lr:1.3121e-04 dt: 3331.90ms, tok/sec:157354.16
step 11819, loss: 3.219895, norm:0.2534, lr:1.3117e-04 dt: 3331.81ms, tok/sec:157358.27
step 11820, loss: 3.161556, norm:0.2565, lr:1.3113e-04 dt: 3331.99ms, tok/sec:157349.81
step 11821, loss: 3.165714, norm:0.2941, lr:1.3109e-04 dt: 3332.10ms, tok/sec:157344.82
step 11822, loss: 3.179418, norm:0.2570, lr:1.3105e-04 dt: 3332.15ms, tok/sec:157342.22
step 11823, loss: 3.282928, norm:0.2748, lr:1.3101e-04 dt: 3331.98ms, tok/sec:157350.10
step 11824, loss: 3.328285, norm:0.2827, lr:1.3097e-04 dt: 3331.98ms, tok/sec:157350.41
step 11825, loss: 3.180669, norm:0.2668, lr:1.3094e-04 dt: 3332.13ms, tok/sec:157343.06
step 11826, loss: 3.216466, norm:0.2802, lr:1.3090e-04 dt: 3331.93ms, tok/sec:157352.51
step 11827, loss: 3.193751, norm:0.2641, lr:1.3086e-04 dt: 3332.08ms, tok/sec:157345.43
step 11828, loss: 3.245510, norm:0.2721, lr:1.3082e-04 dt: 3332.04ms, tok/sec:157347.33
step 11829, loss: 3.285834, norm:0.2847, lr:1.3078e-04 dt: 3332.14ms, tok/sec:157342.89
step 11830, loss: 3.255363, norm:0.2760, lr:1.3074e-04 dt: 3332.29ms, tok/sec:157335.72
step 11831, loss: 3.205174, norm:0.2698, lr:1.3070e-04 dt: 3331.96ms, tok/sec:157351.44
step 11832, loss: 3.188946, norm:0.2615, lr:1.3066e-04 dt: 3332.01ms, tok/sec:157349.09
step 11833, loss: 3.229495, norm:0.2757, lr:1.3062e-04 dt: 3332.08ms, tok/sec:157345.48
step 11834, loss: 3.218755, norm:0.3022, lr:1.3058e-04 dt: 3332.08ms, tok/sec:157345.77
step 11835, loss: 3.213325, norm:0.2730, lr:1.3054e-04 dt: 3332.02ms, tok/sec:157348.60
step 11836, loss: 3.239158, norm:0.2693, lr:1.3050e-04 dt: 3332.18ms, tok/sec:157340.93
step 11837, loss: 3.254720, norm:0.2652, lr:1.3046e-04 dt: 3332.25ms, tok/sec:157337.37
step 11838, loss: 3.253164, norm:0.2589, lr:1.3042e-04 dt: 3331.95ms, tok/sec:157351.89
step 11839, loss: 3.270766, norm:0.2712, lr:1.3038e-04 dt: 3332.17ms, tok/sec:157341.37
step 11840, loss: 3.248228, norm:0.2625, lr:1.3035e-04 dt: 3332.03ms, tok/sec:157347.73
step 11841, loss: 3.227538, norm:0.2738, lr:1.3031e-04 dt: 3332.15ms, tok/sec:157342.46
step 11842, loss: 3.279680, norm:0.2633, lr:1.3027e-04 dt: 3332.00ms, tok/sec:157349.35
step 11843, loss: 3.210103, norm:0.2558, lr:1.3023e-04 dt: 3332.04ms, tok/sec:157347.63
step 11844, loss: 3.261815, norm:0.2969, lr:1.3019e-04 dt: 3331.86ms, tok/sec:157356.06
step 11845, loss: 3.184971, norm:0.2651, lr:1.3015e-04 dt: 3332.01ms, tok/sec:157349.06
step 11846, loss: 3.349533, norm:0.2731, lr:1.3011e-04 dt: 3332.24ms, tok/sec:157338.19
step 11847, loss: 3.183414, norm:0.2538, lr:1.3007e-04 dt: 3331.92ms, tok/sec:157353.30
step 11848, loss: 3.138094, norm:0.2570, lr:1.3003e-04 dt: 3332.11ms, tok/sec:157344.30
step 11849, loss: 3.177684, norm:0.2720, lr:1.2999e-04 dt: 3331.79ms, tok/sec:157359.20
HellaSwag accuracy:-2295600297792009199/-2=1147800148896004608.0000
rank 1 sample 0: Hello, I'm a language model, i was introduced to them, which worked out quite well. When I was born, it was not uncommon for me to
rank 1 sample 1: Hello, I'm a language model, which is why I'm a language model. I'm in the program, the other code behind my program. I'm
rank 1 sample 2: Hello, I'm a language model, but don't you know what I'm talking about? It could have something to do with your knowledge and understanding but it
rank 1 sample 3: Hello, I'm a language model, and I'm looking for how to learn basic programming languages while building small computers I'm only having fun with being able to
rank 0 sample 0: Hello, I'm a language model, and I love to use models that show an open-ended and user-centred environment. This makes for a good
rank 0 sample 1: Hello, I'm a language model, so when I hear the word ‘language,’ it seems like an odd word. I want to find the
rank 0 sample 2: Hello, I'm a language model, so I should always put myself in front of that statement. The other person is just a language model.
So a
rank 0 sample 3: Hello, I'm a language model, so please stay tuned.
I would love to hear your thoughts.
Thanks in advance
for checking out my comments
step 11850, loss: 3.203096, norm:0.2526, lr:1.2995e-04 dt: 48384.73ms, tok/sec:10835.82
step 11851, loss: 3.263637, norm:0.2610, lr:1.2991e-04 dt: 3332.10ms, tok/sec:157344.77
step 11852, loss: 3.228536, norm:0.2659, lr:1.2987e-04 dt: 3332.06ms, tok/sec:157346.68
step 11853, loss: 3.161800, norm:0.2594, lr:1.2984e-04 dt: 3332.57ms, tok/sec:157322.51
step 11854, loss: 3.151209, norm:0.2554, lr:1.2980e-04 dt: 3332.35ms, tok/sec:157332.96
step 11855, loss: 3.134045, norm:0.2554, lr:1.2976e-04 dt: 3332.19ms, tok/sec:157340.30
step 11856, loss: 3.200909, norm:0.2550, lr:1.2972e-04 dt: 3332.36ms, tok/sec:157332.16
step 11857, loss: 3.263989, norm:0.3362, lr:1.2968e-04 dt: 3331.96ms, tok/sec:157351.12
step 11858, loss: 3.242446, norm:0.2798, lr:1.2964e-04 dt: 3331.97ms, tok/sec:157350.77
step 11859, loss: 3.210722, norm:0.2850, lr:1.2960e-04 dt: 3332.01ms, tok/sec:157349.10
step 11860, loss: 3.197343, norm:0.2622, lr:1.2956e-04 dt: 3332.23ms, tok/sec:157338.44
step 11861, loss: 3.249352, norm:0.2830, lr:1.2952e-04 dt: 3331.76ms, tok/sec:157360.66
step 11862, loss: 3.292294, norm:0.2826, lr:1.2948e-04 dt: 3331.96ms, tok/sec:157351.04
step 11863, loss: 3.202962, norm:0.2808, lr:1.2944e-04 dt: 3331.81ms, tok/sec:157358.12
step 11864, loss: 3.221009, norm:0.2870, lr:1.2941e-04 dt: 3332.45ms, tok/sec:157327.93
step 11865, loss: 3.183338, norm:0.3261, lr:1.2937e-04 dt: 3332.02ms, tok/sec:157348.22
step 11866, loss: 3.155560, norm:0.2648, lr:1.2933e-04 dt: 3332.17ms, tok/sec:157341.45
step 11867, loss: 3.252812, norm:0.2852, lr:1.2929e-04 dt: 3332.06ms, tok/sec:157346.47
step 11868, loss: 3.223069, norm:0.2800, lr:1.2925e-04 dt: 3332.10ms, tok/sec:157344.42
step 11869, loss: 3.197669, norm:0.2678, lr:1.2921e-04 dt: 3331.90ms, tok/sec:157354.16
step 11870, loss: 3.202441, norm:0.2551, lr:1.2917e-04 dt: 3331.87ms, tok/sec:157355.71
step 11871, loss: 3.208588, norm:0.2546, lr:1.2913e-04 dt: 3332.29ms, tok/sec:157335.42
step 11872, loss: 3.251916, norm:0.2650, lr:1.2909e-04 dt: 3332.10ms, tok/sec:157344.50
step 11873, loss: 3.195532, norm:0.2530, lr:1.2905e-04 dt: 3332.02ms, tok/sec:157348.36
step 11874, loss: 3.275713, norm:0.2620, lr:1.2902e-04 dt: 3331.95ms, tok/sec:157351.91
step 11875, loss: 3.221062, norm:0.2816, lr:1.2898e-04 dt: 3332.01ms, tok/sec:157348.69
step 11876, loss: 3.194587, norm:0.2616, lr:1.2894e-04 dt: 3332.05ms, tok/sec:157346.76
step 11877, loss: 3.302646, norm:0.2729, lr:1.2890e-04 dt: 3331.81ms, tok/sec:157358.34
step 11878, loss: 3.289954, norm:0.3014, lr:1.2886e-04 dt: 3331.99ms, tok/sec:157349.88
step 11879, loss: 3.242644, norm:0.2640, lr:1.2882e-04 dt: 3332.08ms, tok/sec:157345.60
step 11880, loss: 3.186475, norm:0.2528, lr:1.2878e-04 dt: 3332.43ms, tok/sec:157328.96
step 11881, loss: 3.242903, norm:0.2837, lr:1.2874e-04 dt: 3332.00ms, tok/sec:157349.41
step 11882, loss: 3.234836, norm:0.2674, lr:1.2870e-04 dt: 3332.51ms, tok/sec:157325.39
step 11883, loss: 3.196318, norm:0.2551, lr:1.2867e-04 dt: 3332.10ms, tok/sec:157344.77
step 11884, loss: 3.200726, norm:0.2795, lr:1.2863e-04 dt: 3332.07ms, tok/sec:157346.05
step 11885, loss: 3.178274, norm:0.2562, lr:1.2859e-04 dt: 3332.12ms, tok/sec:157343.73
step 11886, loss: 3.187977, norm:0.2643, lr:1.2855e-04 dt: 3331.95ms, tok/sec:157351.76
step 11887, loss: 3.125198, norm:0.2598, lr:1.2851e-04 dt: 3332.07ms, tok/sec:157345.87
step 11888, loss: 3.186125, norm:0.2392, lr:1.2847e-04 dt: 3332.29ms, tok/sec:157335.78
step 11889, loss: 3.236130, norm:0.2803, lr:1.2843e-04 dt: 3331.94ms, tok/sec:157352.30
step 11890, loss: 3.212206, norm:0.2653, lr:1.2839e-04 dt: 3331.82ms, tok/sec:157358.02
step 11891, loss: 3.212926, norm:0.2826, lr:1.2835e-04 dt: 3332.27ms, tok/sec:157336.44
step 11892, loss: 3.249915, norm:0.2684, lr:1.2832e-04 dt: 3332.04ms, tok/sec:157347.24
step 11893, loss: 3.224357, norm:0.2557, lr:1.2828e-04 dt: 3332.02ms, tok/sec:157348.38
step 11894, loss: 3.210515, norm:0.2883, lr:1.2824e-04 dt: 3332.06ms, tok/sec:157346.66
step 11895, loss: 3.194329, norm:0.2955, lr:1.2820e-04 dt: 3331.88ms, tok/sec:157354.86
step 11896, loss: 3.277707, norm:0.2928, lr:1.2816e-04 dt: 3332.35ms, tok/sec:157332.78
step 11897, loss: 3.223716, norm:0.2946, lr:1.2812e-04 dt: 3332.10ms, tok/sec:157344.85
step 11898, loss: 3.204234, norm:0.2699, lr:1.2808e-04 dt: 3332.04ms, tok/sec:157347.64
step 11899, loss: 3.301017, norm:0.3483, lr:1.2804e-04 dt: 3331.95ms, tok/sec:157351.70
validation loss: 3.2262
Model and optimizer state saved.
HellaSwag accuracy:6936919675806123025/-2=-3468459837903061504.0000
rank 1 sample 0: Hello, I'm a language model,
It's a big question. Here is another
concept called "the Internet of Everything". In the last
word
rank 1 sample 1: Hello, I'm a language model, a computer scientist, and a scientist and inventor. I use the tools of computers and communications technologies.
I'm a
rank 1 sample 2: Hello, I'm a language model, but its language is not a language. I'm a language researcher and it's not a language model.
If I
rank 1 sample 3: Hello, I'm a language model, and I'm looking forward to teaching. They're doing fine for both language tasks. At the end of the day,
rank 0 sample 0: Hello, I'm a language model, and I am a computer, a robot, and a computer. A "human" and "robot" are words
rank 0 sample 1: Hello, I'm a language model, so when I'm doing a grammar with a language with many languages, I've got the correct grammar I want to use
rank 0 sample 2: Hello, I'm a language model, but I wanted to keep your hands and feet at ease. The other day, I noticed that my nose was slightly crooked
rank 0 sample 3: Hello, I'm a language model, so just add and subtract the first letter from here.
There's some kind of magic I'm missing, and for
step 11900, loss: 3.227752, norm:0.2830, lr:1.2801e-04 dt: 56078.87ms, tok/sec:9349.12
step 11901, loss: 3.234061, norm:0.2979, lr:1.2797e-04 dt: 3331.95ms, tok/sec:157351.50
step 11902, loss: 3.264243, norm:0.2909, lr:1.2793e-04 dt: 3332.07ms, tok/sec:157345.93
step 11903, loss: 3.218456, norm:0.2871, lr:1.2789e-04 dt: 3332.04ms, tok/sec:157347.41
step 11904, loss: 3.249465, norm:0.2702, lr:1.2785e-04 dt: 3331.98ms, tok/sec:157350.33
step 11905, loss: 3.223095, norm:0.2842, lr:1.2781e-04 dt: 3331.98ms, tok/sec:157350.35
step 11906, loss: 3.255037, norm:0.2778, lr:1.2777e-04 dt: 3332.11ms, tok/sec:157344.01
step 11907, loss: 3.245899, norm:0.2503, lr:1.2774e-04 dt: 3332.24ms, tok/sec:157338.13
step 11908, loss: 3.264526, norm:0.2629, lr:1.2770e-04 dt: 3332.17ms, tok/sec:157341.47
step 11909, loss: 3.261297, norm:0.2821, lr:1.2766e-04 dt: 3331.93ms, tok/sec:157352.67
step 11910, loss: 3.231628, norm:0.2593, lr:1.2762e-04 dt: 3332.12ms, tok/sec:157343.46
step 11911, loss: 3.223148, norm:0.2562, lr:1.2758e-04 dt: 3331.98ms, tok/sec:157350.32
step 11912, loss: 3.202363, norm:0.2748, lr:1.2754e-04 dt: 3332.06ms, tok/sec:157346.49
step 11913, loss: 3.227032, norm:0.2603, lr:1.2750e-04 dt: 3331.92ms, tok/sec:157353.29
step 11914, loss: 3.217330, norm:0.2711, lr:1.2747e-04 dt: 3332.08ms, tok/sec:157345.63
step 11915, loss: 3.146093, norm:0.2610, lr:1.2743e-04 dt: 3331.95ms, tok/sec:157351.77
step 11916, loss: 3.204891, norm:0.2520, lr:1.2739e-04 dt: 3331.95ms, tok/sec:157351.77
step 11917, loss: 3.201720, norm:0.2690, lr:1.2735e-04 dt: 3332.43ms, tok/sec:157329.21
step 11918, loss: 3.161976, norm:0.2654, lr:1.2731e-04 dt: 3332.03ms, tok/sec:157347.88
step 11919, loss: 3.234308, norm:0.2722, lr:1.2727e-04 dt: 3332.17ms, tok/sec:157341.41
step 11920, loss: 3.162098, norm:0.2516, lr:1.2723e-04 dt: 3332.17ms, tok/sec:157341.21
step 11921, loss: 3.197611, norm:0.2427, lr:1.2720e-04 dt: 3331.95ms, tok/sec:157351.78
step 11922, loss: 3.223236, norm:0.2579, lr:1.2716e-04 dt: 3331.89ms, tok/sec:157354.32
step 11923, loss: 3.178814, norm:0.2501, lr:1.2712e-04 dt: 3332.16ms, tok/sec:157341.82
step 11924, loss: 3.164920, norm:0.2500, lr:1.2708e-04 dt: 3331.86ms, tok/sec:157355.93
step 11925, loss: 3.190613, norm:0.2669, lr:1.2704e-04 dt: 3332.14ms, tok/sec:157342.58
step 11926, loss: 3.250511, norm:0.2815, lr:1.2700e-04 dt: 3331.85ms, tok/sec:157356.66
step 11927, loss: 3.253676, norm:0.2673, lr:1.2696e-04 dt: 3332.19ms, tok/sec:157340.51
step 11928, loss: 3.204415, norm:0.2700, lr:1.2693e-04 dt: 3332.08ms, tok/sec:157345.67
step 11929, loss: 3.239842, norm:0.2769, lr:1.2689e-04 dt: 3332.15ms, tok/sec:157342.46
step 11930, loss: 3.204782, norm:0.2731, lr:1.2685e-04 dt: 3331.93ms, tok/sec:157352.50
step 11931, loss: 3.205973, norm:0.2997, lr:1.2681e-04 dt: 3332.09ms, tok/sec:157345.11
step 11932, loss: 3.193058, norm:0.2672, lr:1.2677e-04 dt: 3331.85ms, tok/sec:157356.31
step 11933, loss: 3.265344, norm:0.2747, lr:1.2673e-04 dt: 3332.16ms, tok/sec:157341.84
step 11934, loss: 3.224172, norm:0.2730, lr:1.2670e-04 dt: 3332.17ms, tok/sec:157341.32
step 11935, loss: 3.232028, norm:0.2682, lr:1.2666e-04 dt: 3331.93ms, tok/sec:157352.55
step 11936, loss: 3.229100, norm:0.2570, lr:1.2662e-04 dt: 3331.98ms, tok/sec:157350.51
step 11937, loss: 3.180752, norm:0.2626, lr:1.2658e-04 dt: 3332.20ms, tok/sec:157339.79
step 11938, loss: 3.295778, norm:0.3040, lr:1.2654e-04 dt: 3331.89ms, tok/sec:157354.32
step 11939, loss: 3.219012, norm:0.2592, lr:1.2650e-04 dt: 3332.16ms, tok/sec:157341.96
step 11940, loss: 3.229263, norm:0.2653, lr:1.2647e-04 dt: 3331.93ms, tok/sec:157352.85
step 11941, loss: 3.164684, norm:0.2639, lr:1.2643e-04 dt: 3332.18ms, tok/sec:157341.01
step 11942, loss: 3.249441, norm:0.2598, lr:1.2639e-04 dt: 3332.14ms, tok/sec:157342.56
step 11943, loss: 3.224066, norm:0.2722, lr:1.2635e-04 dt: 3331.91ms, tok/sec:157353.64
step 11944, loss: 3.240742, norm:0.2624, lr:1.2631e-04 dt: 3331.88ms, tok/sec:157355.23
step 11945, loss: 3.183899, norm:0.2715, lr:1.2627e-04 dt: 3331.88ms, tok/sec:157354.80
step 11946, loss: 3.185200, norm:0.2852, lr:1.2624e-04 dt: 3331.87ms, tok/sec:157355.63
step 11947, loss: 3.134211, norm:0.3625, lr:1.2620e-04 dt: 3332.14ms, tok/sec:157342.74
step 11948, loss: 3.165091, norm:0.2720, lr:1.2616e-04 dt: 3331.96ms, tok/sec:157351.42
step 11949, loss: 3.178699, norm:0.2660, lr:1.2612e-04 dt: 3332.13ms, tok/sec:157343.41
HellaSwag accuracy:6936796530501454929/-2=-3468398265250727424.0000
rank 1 sample 0: Hello, I'm a language model, with many users. I can't answer questions but, rather, I can find answers and ask a lot of questions.
rank 1 sample 1: Hello, I'm a language model, a teacher.<|endoftext|>The first two books that I will present to you are A Comprehensive Reader and Diverse Books.

rank 1 sample 2: Hello, I'm a language model, but sometimes the way I speak is not the same as it seemed at that time.
"You are not just playing
rank 1 sample 3: Hello, I'm a language model, and I'm looking at that now. Are you able to solve your word problem? Just by doing it. The problem
rank 0 sample 0: Hello, I'm a language model, and I'll be talking about this on her Facebook page.
As my mother, she just wants a name for the
rank 0 sample 1: Hello, I'm a language model, but am not a native speaker of other languages." They told me. Their responses are very different from others who have not
rank 0 sample 2: Hello, I'm a language model, so I will take things for granted, but as a language consultant you have to get the right tools to do it myself
rank 0 sample 3: Hello, I'm a language model, and what I am doing is trying to learn or teach. I do it, every day, I see a bunch of
step 11950, loss: 3.202697, norm:0.2574, lr:1.2608e-04 dt: 48385.82ms, tok/sec:10835.57
step 11951, loss: 3.237256, norm:0.2527, lr:1.2604e-04 dt: 3332.23ms, tok/sec:157338.43
step 11952, loss: 3.165269, norm:0.2692, lr:1.2601e-04 dt: 3331.99ms, tok/sec:157349.67
step 11953, loss: 3.179099, norm:0.2652, lr:1.2597e-04 dt: 3331.86ms, tok/sec:157356.02
step 11954, loss: 3.150249, norm:0.2499, lr:1.2593e-04 dt: 3332.00ms, tok/sec:157349.15
step 11955, loss: 3.205983, norm:0.2886, lr:1.2589e-04 dt: 3331.76ms, tok/sec:157360.72
step 11956, loss: 3.180035, norm:0.2663, lr:1.2585e-04 dt: 3332.07ms, tok/sec:157345.95
step 11957, loss: 3.205585, norm:0.2565, lr:1.2582e-04 dt: 3332.08ms, tok/sec:157345.45
step 11958, loss: 3.144258, norm:0.2470, lr:1.2578e-04 dt: 3332.16ms, tok/sec:157341.64
step 11959, loss: 3.203010, norm:0.2487, lr:1.2574e-04 dt: 3331.92ms, tok/sec:157353.25
step 11960, loss: 3.179757, norm:0.2589, lr:1.2570e-04 dt: 3332.04ms, tok/sec:157347.42
step 11961, loss: 3.259440, norm:0.2800, lr:1.2566e-04 dt: 3332.01ms, tok/sec:157348.75
step 11962, loss: 3.231803, norm:0.2593, lr:1.2563e-04 dt: 3331.96ms, tok/sec:157351.19
step 11963, loss: 3.266580, norm:0.2672, lr:1.2559e-04 dt: 3332.08ms, tok/sec:157345.75
step 11964, loss: 3.266549, norm:0.2744, lr:1.2555e-04 dt: 3331.92ms, tok/sec:157352.92
step 11965, loss: 3.228678, norm:0.2755, lr:1.2551e-04 dt: 3331.96ms, tok/sec:157351.14
step 11966, loss: 3.194144, norm:0.2523, lr:1.2547e-04 dt: 3331.98ms, tok/sec:157350.35
step 11967, loss: 3.215068, norm:0.2896, lr:1.2543e-04 dt: 3332.07ms, tok/sec:157345.93
step 11968, loss: 3.211994, norm:0.2544, lr:1.2540e-04 dt: 3332.18ms, tok/sec:157340.81
step 11969, loss: 3.269195, norm:0.2591, lr:1.2536e-04 dt: 3332.24ms, tok/sec:157337.91
step 11970, loss: 3.355959, norm:0.2844, lr:1.2532e-04 dt: 3332.16ms, tok/sec:157341.72
step 11971, loss: 3.280172, norm:0.2586, lr:1.2528e-04 dt: 3332.14ms, tok/sec:157342.51
step 11972, loss: 3.220736, norm:0.2838, lr:1.2524e-04 dt: 3331.85ms, tok/sec:157356.23
step 11973, loss: 3.228071, norm:0.2813, lr:1.2521e-04 dt: 3331.99ms, tok/sec:157349.61
step 11974, loss: 3.220599, norm:0.2648, lr:1.2517e-04 dt: 3332.03ms, tok/sec:157347.76
step 11975, loss: 3.173855, norm:0.2581, lr:1.2513e-04 dt: 3331.93ms, tok/sec:157352.60
step 11976, loss: 3.189952, norm:0.2712, lr:1.2509e-04 dt: 3332.15ms, tok/sec:157342.11
step 11977, loss: 3.193646, norm:0.2510, lr:1.2505e-04 dt: 3332.11ms, tok/sec:157344.22
step 11978, loss: 3.176994, norm:0.3303, lr:1.2502e-04 dt: 3332.05ms, tok/sec:157346.87
step 11979, loss: 3.197737, norm:0.2654, lr:1.2498e-04 dt: 3331.97ms, tok/sec:157350.66
step 11980, loss: 3.182866, norm:0.2615, lr:1.2494e-04 dt: 3331.93ms, tok/sec:157352.46
step 11981, loss: 3.199115, norm:0.2762, lr:1.2490e-04 dt: 3332.07ms, tok/sec:157346.27
step 11982, loss: 3.236810, norm:0.2686, lr:1.2486e-04 dt: 3332.12ms, tok/sec:157343.84
step 11983, loss: 3.192757, norm:0.2638, lr:1.2483e-04 dt: 3332.12ms, tok/sec:157343.48
step 11984, loss: 3.172053, norm:0.2642, lr:1.2479e-04 dt: 3332.07ms, tok/sec:157345.91
step 11985, loss: 3.192467, norm:0.2721, lr:1.2475e-04 dt: 3331.94ms, tok/sec:157352.28
step 11986, loss: 3.147416, norm:0.2506, lr:1.2471e-04 dt: 3331.94ms, tok/sec:157352.37
step 11987, loss: 3.143312, norm:0.2634, lr:1.2468e-04 dt: 3331.92ms, tok/sec:157352.99
step 11988, loss: 3.213257, norm:0.2637, lr:1.2464e-04 dt: 3331.96ms, tok/sec:157351.05
step 11989, loss: 3.155323, norm:0.2788, lr:1.2460e-04 dt: 3331.87ms, tok/sec:157355.54
step 11990, loss: 3.149829, norm:0.2603, lr:1.2456e-04 dt: 3332.01ms, tok/sec:157348.98
step 11991, loss: 3.185606, norm:0.2654, lr:1.2452e-04 dt: 3332.32ms, tok/sec:157334.19
step 11992, loss: 3.158349, norm:0.2675, lr:1.2449e-04 dt: 3332.02ms, tok/sec:157348.29
step 11993, loss: 3.193671, norm:0.2679, lr:1.2445e-04 dt: 3331.87ms, tok/sec:157355.34
step 11994, loss: 3.206653, norm:0.2494, lr:1.2441e-04 dt: 3332.03ms, tok/sec:157347.85
step 11995, loss: 3.211780, norm:0.2849, lr:1.2437e-04 dt: 3332.01ms, tok/sec:157348.93
step 11996, loss: 3.241189, norm:0.2896, lr:1.2433e-04 dt: 3332.15ms, tok/sec:157342.12
step 11997, loss: 3.237500, norm:0.2720, lr:1.2430e-04 dt: 3332.05ms, tok/sec:157346.85
step 11998, loss: 3.229012, norm:0.2704, lr:1.2426e-04 dt: 3332.29ms, tok/sec:157335.46
step 11999, loss: 3.291336, norm:0.2716, lr:1.2422e-04 dt: 3331.99ms, tok/sec:157349.99
validation loss: 3.2243
Model and optimizer state saved.
HellaSwag accuracy:-2286443564955892655/-2=1143221782477946368.0000
rank 1 sample 0: Hello, I'm a language model, and my writing is in English. Of the languages we wrote as a student, I think the one that was most interesting
rank 1 sample 1: Hello, I'm a language model, a computer program, a computer, a word processor, one of the two computers in our complex (or at least,
rank 1 sample 2: Hello, I'm a language model, but some languages like Python are not. I'm a native programmer. So I'm a native Python interpreter, a great
rank 1 sample 3: Hello, I'm a language model, and I'm looking forward to teaching myself even more. After my 10 years in the UK, I'm a language model
rank 0 sample 0: Hello, I'm a language model, and I think it is. But this really gets my attention. For any kind of information, let me show you that
rank 0 sample 1: Hello, I'm a language model, so when I'm doing a programming course, you go at a programming event to see how a program responds to a particular
rank 0 sample 2: Hello, I'm a language model, but I really need to figure out which language a dog is in to help me understand what's going on. I mean
rank 0 sample 3: Hello, I'm a language model, a bit of a puzzle. I love it where I'm just doing this stuff--but then, once I've taken
step 12000, loss: 3.257018, norm:0.2745, lr:1.2418e-04 dt: 56064.59ms, tok/sec:9351.50
step 12001, loss: 3.236747, norm:0.2850, lr:1.2415e-04 dt: 3334.41ms, tok/sec:157235.53
step 12002, loss: 3.173023, norm:0.2576, lr:1.2411e-04 dt: 3331.87ms, tok/sec:157355.58
step 12003, loss: 3.235778, norm:0.2680, lr:1.2407e-04 dt: 3332.12ms, tok/sec:157343.77
step 12004, loss: 3.234111, norm:0.2583, lr:1.2403e-04 dt: 3332.08ms, tok/sec:157345.78
step 12005, loss: 3.138421, norm:0.2827, lr:1.2400e-04 dt: 3331.98ms, tok/sec:157350.33
step 12006, loss: 3.187787, norm:0.2865, lr:1.2396e-04 dt: 3331.99ms, tok/sec:157349.89
step 12007, loss: 3.214752, norm:0.3648, lr:1.2392e-04 dt: 3331.76ms, tok/sec:157360.68
step 12008, loss: 3.215715, norm:0.2644, lr:1.2388e-04 dt: 3332.00ms, tok/sec:157349.13
step 12009, loss: 3.225435, norm:0.2742, lr:1.2384e-04 dt: 3332.05ms, tok/sec:157347.05
step 12010, loss: 3.150259, norm:0.2644, lr:1.2381e-04 dt: 3331.92ms, tok/sec:157353.00
step 12011, loss: 3.209365, norm:0.2786, lr:1.2377e-04 dt: 3332.19ms, tok/sec:157340.46
step 12012, loss: 3.134418, norm:0.2638, lr:1.2373e-04 dt: 3331.97ms, tok/sec:157350.87
step 12013, loss: 3.161844, norm:0.2875, lr:1.2369e-04 dt: 3331.91ms, tok/sec:157353.62
step 12014, loss: 3.250457, norm:0.2514, lr:1.2366e-04 dt: 3332.29ms, tok/sec:157335.49
step 12015, loss: 3.218755, norm:0.2863, lr:1.2362e-04 dt: 3332.00ms, tok/sec:157349.34
step 12016, loss: 3.145532, norm:0.2643, lr:1.2358e-04 dt: 3331.97ms, tok/sec:157350.68
step 12017, loss: 3.241358, norm:0.2638, lr:1.2354e-04 dt: 3331.99ms, tok/sec:157349.88
step 12018, loss: 3.214935, norm:0.2517, lr:1.2351e-04 dt: 3331.76ms, tok/sec:157360.50
step 12019, loss: 3.192133, norm:0.2458, lr:1.2347e-04 dt: 3331.85ms, tok/sec:157356.22
step 12020, loss: 3.216287, norm:0.2566, lr:1.2343e-04 dt: 3332.24ms, tok/sec:157338.11
step 12021, loss: 3.180212, norm:0.2470, lr:1.2339e-04 dt: 3332.09ms, tok/sec:157345.08
step 12022, loss: 3.157865, norm:0.2450, lr:1.2336e-04 dt: 3332.01ms, tok/sec:157348.87
step 12023, loss: 3.237367, norm:0.2923, lr:1.2332e-04 dt: 3332.18ms, tok/sec:157340.95
step 12024, loss: 3.180315, norm:0.2856, lr:1.2328e-04 dt: 3332.07ms, tok/sec:157345.97
step 12025, loss: 3.130901, norm:0.2613, lr:1.2324e-04 dt: 3332.01ms, tok/sec:157349.09
step 12026, loss: 3.131506, norm:0.2594, lr:1.2321e-04 dt: 3331.78ms, tok/sec:157359.75
step 12027, loss: 3.192729, norm:0.2876, lr:1.2317e-04 dt: 3332.04ms, tok/sec:157347.44
step 12028, loss: 3.174274, norm:0.2654, lr:1.2313e-04 dt: 3331.99ms, tok/sec:157349.89
step 12029, loss: 3.268433, norm:0.2650, lr:1.2309e-04 dt: 3332.06ms, tok/sec:157346.28
step 12030, loss: 3.195588, norm:0.2851, lr:1.2306e-04 dt: 3331.87ms, tok/sec:157355.39
step 12031, loss: 3.250350, norm:0.2827, lr:1.2302e-04 dt: 3332.27ms, tok/sec:157336.76
step 12032, loss: 3.251392, norm:0.2728, lr:1.2298e-04 dt: 3332.05ms, tok/sec:157346.88
step 12033, loss: 3.195961, norm:0.2756, lr:1.2294e-04 dt: 3332.08ms, tok/sec:157345.66
step 12034, loss: 3.250913, norm:0.2765, lr:1.2291e-04 dt: 3332.01ms, tok/sec:157348.98
step 12035, loss: 3.204233, norm:0.2657, lr:1.2287e-04 dt: 3332.14ms, tok/sec:157342.70
step 12036, loss: 3.204182, norm:0.2688, lr:1.2283e-04 dt: 3332.07ms, tok/sec:157346.07
step 12037, loss: 3.249868, norm:0.2948, lr:1.2279e-04 dt: 3332.00ms, tok/sec:157349.31
step 12038, loss: 3.178432, norm:0.2569, lr:1.2276e-04 dt: 3332.04ms, tok/sec:157347.39
step 12039, loss: 3.283000, norm:0.2769, lr:1.2272e-04 dt: 3331.99ms, tok/sec:157349.77
step 12040, loss: 3.190275, norm:0.2666, lr:1.2268e-04 dt: 3332.20ms, tok/sec:157339.98
step 12041, loss: 3.202988, norm:0.2646, lr:1.2264e-04 dt: 3332.18ms, tok/sec:157340.74
step 12042, loss: 3.207716, norm:0.2543, lr:1.2261e-04 dt: 3331.99ms, tok/sec:157350.00
step 12043, loss: 3.199556, norm:0.2519, lr:1.2257e-04 dt: 3331.99ms, tok/sec:157349.70
step 12044, loss: 3.206737, norm:0.2628, lr:1.2253e-04 dt: 3331.93ms, tok/sec:157352.85
step 12045, loss: 3.158096, norm:0.2633, lr:1.2249e-04 dt: 3332.15ms, tok/sec:157342.27
step 12046, loss: 3.188556, norm:0.2456, lr:1.2246e-04 dt: 3331.93ms, tok/sec:157352.51
step 12047, loss: 3.170776, norm:0.2502, lr:1.2242e-04 dt: 3332.02ms, tok/sec:157348.56
step 12048, loss: 3.192926, norm:0.2601, lr:1.2238e-04 dt: 3332.04ms, tok/sec:157347.63
step 12049, loss: 3.226938, norm:0.2587, lr:1.2235e-04 dt: 3332.10ms, tok/sec:157344.68
HellaSwag accuracy:-2295582705605966767/-2=1147791352802983424.0000
rank 1 sample 0: Hello, I'm a language model, and my theory is that you can "talk" about many complex things with an open mind. But I can't say
rank 1 sample 1: Hello, I'm a language model, a teacher. The idea is to use students' ideas that are outside the school, because it becomes the basis of the
rank 0 sample 0: Hello, I'm a language model, and I'll be talking about this later ... a language model that would come into play if I'm a person. It
rank 1 sample 2: Hello, I'm a language model, but even I think it's not the same as a computer and the way it works is it does it a few things
rank 0 sample 1: Hello, I'm a language model, I believe that every human being should really be learning any language, even before learning a foreign language, if you're teaching
rank 1 sample 3: Hello, I'm a language model, and I'm sure I never get that big, little chunk of vocabulary you will find online like a word, so I
rank 0 sample 2: Hello, I'm a language model, but I understand it the moment I talk to an engineer. But what does that mean?
Well, I think that
rank 0 sample 3: Hello, I'm a language model, and when I was a kid I wanted to live in a world where there are three language models. This is what led
step 12050, loss: 3.210108, norm:0.2559, lr:1.2231e-04 dt: 48384.45ms, tok/sec:10835.88
step 12051, loss: 3.232493, norm:0.2856, lr:1.2227e-04 dt: 3332.02ms, tok/sec:157348.28
step 12052, loss: 3.162253, norm:0.2751, lr:1.2223e-04 dt: 3332.35ms, tok/sec:157332.90
step 12053, loss: 3.184149, norm:0.2563, lr:1.2220e-04 dt: 3331.92ms, tok/sec:157353.16
step 12054, loss: 3.248446, norm:0.2633, lr:1.2216e-04 dt: 3331.85ms, tok/sec:157356.38
step 12055, loss: 3.197529, norm:0.2522, lr:1.2212e-04 dt: 3332.13ms, tok/sec:157343.06
step 12056, loss: 3.223916, norm:0.2457, lr:1.2208e-04 dt: 3332.08ms, tok/sec:157345.42
step 12057, loss: 3.184597, norm:0.2569, lr:1.2205e-04 dt: 3332.02ms, tok/sec:157348.57
step 12058, loss: 3.195658, norm:0.2781, lr:1.2201e-04 dt: 3331.84ms, tok/sec:157357.11
step 12059, loss: 3.198102, norm:0.2603, lr:1.2197e-04 dt: 3331.71ms, tok/sec:157362.84
step 12060, loss: 3.172694, norm:0.2612, lr:1.2194e-04 dt: 3331.99ms, tok/sec:157349.65
step 12061, loss: 3.162455, norm:0.2568, lr:1.2190e-04 dt: 3332.19ms, tok/sec:157340.15
step 12062, loss: 3.192279, norm:0.2624, lr:1.2186e-04 dt: 3332.03ms, tok/sec:157347.70
step 12063, loss: 3.232961, norm:0.2798, lr:1.2182e-04 dt: 3331.94ms, tok/sec:157352.05
step 12064, loss: 3.238294, norm:0.2587, lr:1.2179e-04 dt: 3332.20ms, tok/sec:157339.69
step 12065, loss: 3.241362, norm:0.2680, lr:1.2175e-04 dt: 3332.26ms, tok/sec:157337.17
step 12066, loss: 3.287349, norm:0.2746, lr:1.2171e-04 dt: 3332.59ms, tok/sec:157321.69
step 12067, loss: 3.294032, norm:0.2727, lr:1.2168e-04 dt: 3332.04ms, tok/sec:157347.24
step 12068, loss: 3.237208, norm:0.2632, lr:1.2164e-04 dt: 3332.22ms, tok/sec:157338.95
step 12069, loss: 3.298760, norm:0.2792, lr:1.2160e-04 dt: 3332.02ms, tok/sec:157348.36
step 12070, loss: 3.202062, norm:0.2818, lr:1.2156e-04 dt: 3332.33ms, tok/sec:157333.75
step 12071, loss: 3.190779, norm:0.2642, lr:1.2153e-04 dt: 3331.89ms, tok/sec:157354.51
step 12072, loss: 3.277567, norm:0.3668, lr:1.2149e-04 dt: 3331.98ms, tok/sec:157350.33
step 12073, loss: 3.260777, norm:0.2845, lr:1.2145e-04 dt: 3331.95ms, tok/sec:157351.85
step 12074, loss: 3.200730, norm:0.3012, lr:1.2142e-04 dt: 3331.92ms, tok/sec:157352.96
step 12075, loss: 3.201288, norm:0.2659, lr:1.2138e-04 dt: 3332.10ms, tok/sec:157344.77
step 12076, loss: 3.192211, norm:0.2653, lr:1.2134e-04 dt: 3332.04ms, tok/sec:157347.30
step 12077, loss: 3.156928, norm:0.2896, lr:1.2131e-04 dt: 3331.75ms, tok/sec:157361.17
step 12078, loss: 3.218916, norm:0.2504, lr:1.2127e-04 dt: 3331.94ms, tok/sec:157352.00
step 12079, loss: 3.199645, norm:0.2617, lr:1.2123e-04 dt: 3332.30ms, tok/sec:157335.09
step 12080, loss: 3.206113, norm:0.2518, lr:1.2119e-04 dt: 3331.95ms, tok/sec:157351.74
step 12081, loss: 3.244491, norm:0.2592, lr:1.2116e-04 dt: 3331.91ms, tok/sec:157353.49
step 12082, loss: 3.197030, norm:0.2552, lr:1.2112e-04 dt: 3331.92ms, tok/sec:157352.89
step 12083, loss: 3.173841, norm:0.2551, lr:1.2108e-04 dt: 3332.14ms, tok/sec:157342.69
step 12084, loss: 3.223168, norm:0.2639, lr:1.2105e-04 dt: 3332.07ms, tok/sec:157345.96
step 12085, loss: 3.235211, norm:0.3088, lr:1.2101e-04 dt: 3331.85ms, tok/sec:157356.58
step 12086, loss: 3.174424, norm:0.2707, lr:1.2097e-04 dt: 3331.75ms, tok/sec:157361.05
step 12087, loss: 3.202246, norm:0.2562, lr:1.2094e-04 dt: 3331.97ms, tok/sec:157350.92
step 12088, loss: 3.241731, norm:0.2672, lr:1.2090e-04 dt: 3332.37ms, tok/sec:157331.93
step 12089, loss: 3.137614, norm:0.2708, lr:1.2086e-04 dt: 3331.90ms, tok/sec:157354.00
step 12090, loss: 3.165424, norm:0.2516, lr:1.2083e-04 dt: 3331.89ms, tok/sec:157354.77
step 12091, loss: 3.177890, norm:0.2452, lr:1.2079e-04 dt: 3331.95ms, tok/sec:157351.67
step 12092, loss: 3.194630, norm:0.2447, lr:1.2075e-04 dt: 3332.06ms, tok/sec:157346.41
step 12093, loss: 3.183917, norm:0.2630, lr:1.2071e-04 dt: 3331.96ms, tok/sec:157351.43
step 12094, loss: 3.167702, norm:0.2386, lr:1.2068e-04 dt: 3332.09ms, tok/sec:157344.96
step 12095, loss: 3.154732, norm:0.2501, lr:1.2064e-04 dt: 3332.04ms, tok/sec:157347.65
step 12096, loss: 3.160312, norm:0.2661, lr:1.2060e-04 dt: 3332.14ms, tok/sec:157342.87
step 12097, loss: 3.229098, norm:0.2936, lr:1.2057e-04 dt: 3332.18ms, tok/sec:157340.92
step 12098, loss: 3.246535, norm:0.2771, lr:1.2053e-04 dt: 3331.87ms, tok/sec:157355.70
step 12099, loss: 3.203181, norm:0.2634, lr:1.2049e-04 dt: 3332.57ms, tok/sec:157322.41
validation loss: 3.2232
Model and optimizer state saved.
HellaSwag accuracy:6927806923435115601/-2=-3463903461717557760.0000
rank 1 sample 0: Hello, I'm a language model, and you can't see how things progress, how you interact, or how you interact in real-world contexts.

rank 1 sample 1: Hello, I'm a language model, I am an associate professor of philosophy at Massachusetts Institute of Technology. We are here for those who ask questions about the nature
rank 1 sample 2: Hello, I'm a language model, but since you have to learn it, you can't use other people’s language for language training purposes because it
rank 1 sample 3: Hello, I'm a language model, and I'm looking for other languages to contribute to the growth of knowledge base people. While researching and writing about learning languages
rank 0 sample 0: Hello, I'm a language model, and I'll be writing to this page next week.
This course contains some of the most advanced languages available. The
rank 0 sample 1: Hello, I'm a language model, but in truth, the programming is far more than in writing. So yes, you know how to be a language model
rank 0 sample 2: Hello, I'm a language model, so I like it :)
1) I know the language should not be a problem, but I'm a good translator
rank 0 sample 3: Hello, I'm a language model, but do you know what's going on here?
I'm gonna look for information from different sources -- I think ...
step 12100, loss: 3.223553, norm:0.2713, lr:1.2046e-04 dt: 56068.84ms, tok/sec:9350.79
step 12101, loss: 3.188088, norm:0.2594, lr:1.2042e-04 dt: 3332.16ms, tok/sec:157341.64
step 12102, loss: 3.186673, norm:0.2608, lr:1.2038e-04 dt: 3332.12ms, tok/sec:157343.81
step 12103, loss: 3.213208, norm:0.2598, lr:1.2035e-04 dt: 3332.38ms, tok/sec:157331.26
step 12104, loss: 3.234462, norm:0.2614, lr:1.2031e-04 dt: 3331.79ms, tok/sec:157359.29
step 12105, loss: 3.209161, norm:0.2709, lr:1.2027e-04 dt: 3332.07ms, tok/sec:157345.92
step 12106, loss: 3.279164, norm:0.2616, lr:1.2024e-04 dt: 3331.80ms, tok/sec:157358.80
step 12107, loss: 3.200641, norm:0.2614, lr:1.2020e-04 dt: 3332.05ms, tok/sec:157346.77
step 12108, loss: 3.173749, norm:0.2637, lr:1.2016e-04 dt: 3332.05ms, tok/sec:157346.78
step 12109, loss: 3.198588, norm:0.2786, lr:1.2013e-04 dt: 3331.93ms, tok/sec:157352.75
step 12110, loss: 3.208425, norm:0.2556, lr:1.2009e-04 dt: 3331.91ms, tok/sec:157353.50
step 12111, loss: 3.221179, norm:0.2666, lr:1.2005e-04 dt: 3332.20ms, tok/sec:157339.72
step 12112, loss: 3.188937, norm:0.2552, lr:1.2002e-04 dt: 3332.18ms, tok/sec:157340.66
step 12113, loss: 3.184087, norm:0.2627, lr:1.1998e-04 dt: 3331.99ms, tok/sec:157349.85
step 12114, loss: 3.210185, norm:0.2590, lr:1.1994e-04 dt: 3332.30ms, tok/sec:157335.13
step 12115, loss: 3.184578, norm:0.2661, lr:1.1991e-04 dt: 3332.03ms, tok/sec:157347.81
step 12116, loss: 3.186524, norm:0.2814, lr:1.1987e-04 dt: 3332.08ms, tok/sec:157345.36
step 12117, loss: 3.235069, norm:0.2920, lr:1.1983e-04 dt: 3332.10ms, tok/sec:157344.45
step 12118, loss: 3.168864, norm:0.2500, lr:1.1980e-04 dt: 3332.08ms, tok/sec:157345.63
step 12119, loss: 3.189991, norm:0.2886, lr:1.1976e-04 dt: 3332.05ms, tok/sec:157347.18
step 12120, loss: 3.256796, norm:0.2794, lr:1.1972e-04 dt: 3331.78ms, tok/sec:157359.96
step 12121, loss: 3.203228, norm:0.2559, lr:1.1969e-04 dt: 3332.00ms, tok/sec:157349.56
step 12122, loss: 3.136265, norm:0.3064, lr:1.1965e-04 dt: 3331.98ms, tok/sec:157350.39
step 12123, loss: 3.186062, norm:0.2654, lr:1.1961e-04 dt: 3332.03ms, tok/sec:157348.07
step 12124, loss: 3.325238, norm:0.3190, lr:1.1958e-04 dt: 3331.79ms, tok/sec:157359.41
step 12125, loss: 3.180703, norm:0.2667, lr:1.1954e-04 dt: 3331.90ms, tok/sec:157354.28
step 12126, loss: 3.185033, norm:0.2803, lr:1.1950e-04 dt: 3332.04ms, tok/sec:157347.50
step 12127, loss: 3.155877, norm:0.2570, lr:1.1947e-04 dt: 3332.11ms, tok/sec:157343.95
step 12128, loss: 3.136548, norm:0.2627, lr:1.1943e-04 dt: 3331.79ms, tok/sec:157359.18
step 12129, loss: 3.139683, norm:0.2598, lr:1.1939e-04 dt: 3331.94ms, tok/sec:157352.35
step 12130, loss: 3.180308, norm:0.2607, lr:1.1936e-04 dt: 3332.20ms, tok/sec:157340.02
step 12131, loss: 3.207268, norm:0.2590, lr:1.1932e-04 dt: 3331.94ms, tok/sec:157352.10
step 12132, loss: 3.224622, norm:0.2699, lr:1.1929e-04 dt: 3331.98ms, tok/sec:157350.37
step 12133, loss: 3.232098, norm:0.2872, lr:1.1925e-04 dt: 3331.97ms, tok/sec:157350.78
step 12134, loss: 3.197963, norm:0.2768, lr:1.1921e-04 dt: 3332.24ms, tok/sec:157337.91
step 12135, loss: 3.341622, norm:0.3189, lr:1.1918e-04 dt: 3332.16ms, tok/sec:157341.77
step 12136, loss: 3.240960, norm:0.2865, lr:1.1914e-04 dt: 3331.82ms, tok/sec:157357.69
step 12137, loss: 3.261362, norm:0.3338, lr:1.1910e-04 dt: 3332.03ms, tok/sec:157347.71
step 12138, loss: 3.196750, norm:0.2701, lr:1.1907e-04 dt: 3332.21ms, tok/sec:157339.41
step 12139, loss: 3.250471, norm:0.2682, lr:1.1903e-04 dt: 3332.30ms, tok/sec:157335.05
step 12140, loss: 3.244270, norm:0.2818, lr:1.1899e-04 dt: 3332.02ms, tok/sec:157348.33
step 12141, loss: 3.236733, norm:0.2885, lr:1.1896e-04 dt: 3331.98ms, tok/sec:157350.08
step 12142, loss: 3.258640, norm:0.2949, lr:1.1892e-04 dt: 3331.91ms, tok/sec:157353.41
step 12143, loss: 3.222577, norm:0.2725, lr:1.1888e-04 dt: 3332.18ms, tok/sec:157340.91
step 12144, loss: 3.201351, norm:0.2775, lr:1.1885e-04 dt: 3331.90ms, tok/sec:157354.25
step 12145, loss: 3.272096, norm:0.2661, lr:1.1881e-04 dt: 3332.07ms, tok/sec:157346.01
step 12146, loss: 3.198965, norm:0.2716, lr:1.1878e-04 dt: 3332.07ms, tok/sec:157345.98
step 12147, loss: 3.220614, norm:0.2688, lr:1.1874e-04 dt: 3331.97ms, tok/sec:157350.55
step 12148, loss: 3.196846, norm:0.2519, lr:1.1870e-04 dt: 3332.33ms, tok/sec:157333.95
step 12149, loss: 3.227290, norm:0.2596, lr:1.1867e-04 dt: 3332.17ms, tok/sec:157341.36
HellaSwag accuracy:6927806923434853457/-2=-3463903461717426688.0000
rank 1 sample 0: Hello, I'm a language model, and this is the best place to develop the next generation of social media.
I would be grateful if you could help
rank 1 sample 1: Hello, I'm a language model, which is an incredibly powerful tool in language learning. I wanted to build a simple, practical, tool for language learning.
rank 1 sample 2: Hello, I'm a language model, but learning the basics of programming is a great way to build something more powerful.
I am also very excited at discovering
rank 1 sample 3: Hello, I'm a language model, and I'm really excited to share how interesting it is by my readers. With the increase of connectivity, it's becoming
rank 0 sample 0: Hello, I'm a language model, and I don't have my head full as there are over 200 years who've been taught them the basics. I know
rank 0 sample 1: Hello, I'm a language model, but how do I get my head around this. To create an application called language.lang (or just like, just
rank 0 sample 2: Hello, I'm a language model, so I do the research."
Purdue graduate student and faculty adviser, Dr. George F. Martin, says he
rank 0 sample 3: Hello, I'm a language model, and when I was a kid I loved to invent. I did all that, that was really cool. I never gave
step 12150, loss: 3.204543, norm:0.2611, lr:1.1863e-04 dt: 48381.42ms, tok/sec:10836.56
step 12151, loss: 3.281898, norm:0.3108, lr:1.1859e-04 dt: 3331.97ms, tok/sec:157350.92
step 12152, loss: 3.182572, norm:0.2674, lr:1.1856e-04 dt: 3331.85ms, tok/sec:157356.43
step 12153, loss: 3.171066, norm:0.2599, lr:1.1852e-04 dt: 3332.12ms, tok/sec:157343.44
step 12154, loss: 3.243689, norm:0.2661, lr:1.1849e-04 dt: 3332.03ms, tok/sec:157347.97
step 12155, loss: 3.154014, norm:0.3025, lr:1.1845e-04 dt: 3331.92ms, tok/sec:157353.18
step 12156, loss: 3.163121, norm:0.2584, lr:1.1841e-04 dt: 3331.98ms, tok/sec:157350.43
step 12157, loss: 3.188737, norm:0.2809, lr:1.1838e-04 dt: 3332.23ms, tok/sec:157338.42
step 12158, loss: 3.127892, norm:0.2775, lr:1.1834e-04 dt: 3332.21ms, tok/sec:157339.48
step 12159, loss: 3.183825, norm:0.2719, lr:1.1830e-04 dt: 3331.81ms, tok/sec:157358.47
step 12160, loss: 3.191861, norm:0.2818, lr:1.1827e-04 dt: 3331.91ms, tok/sec:157353.71
step 12161, loss: 3.266427, norm:0.2843, lr:1.1823e-04 dt: 3332.30ms, tok/sec:157335.40
step 12162, loss: 3.160296, norm:0.2619, lr:1.1820e-04 dt: 3332.25ms, tok/sec:157337.36
step 12163, loss: 3.231765, norm:0.2990, lr:1.1816e-04 dt: 3332.01ms, tok/sec:157348.97
step 12164, loss: 3.167038, norm:0.2902, lr:1.1812e-04 dt: 3331.94ms, tok/sec:157351.94
step 12165, loss: 3.203722, norm:0.2593, lr:1.1809e-04 dt: 3332.00ms, tok/sec:157349.42
step 12166, loss: 3.255957, norm:0.2856, lr:1.1805e-04 dt: 3332.28ms, tok/sec:157336.35
step 12167, loss: 3.235475, norm:0.2726, lr:1.1801e-04 dt: 3332.18ms, tok/sec:157340.74
step 12168, loss: 3.233111, norm:0.2853, lr:1.1798e-04 dt: 3331.95ms, tok/sec:157351.52
step 12169, loss: 3.251388, norm:0.2720, lr:1.1794e-04 dt: 3332.00ms, tok/sec:157349.45
step 12170, loss: 3.244648, norm:0.2767, lr:1.1791e-04 dt: 3331.98ms, tok/sec:157350.42
step 12171, loss: 3.155201, norm:0.2955, lr:1.1787e-04 dt: 3331.87ms, tok/sec:157355.48
step 12172, loss: 3.217039, norm:0.2991, lr:1.1783e-04 dt: 3331.96ms, tok/sec:157351.07
step 12173, loss: 3.204052, norm:0.2714, lr:1.1780e-04 dt: 3332.22ms, tok/sec:157339.07
step 12174, loss: 3.225750, norm:0.2854, lr:1.1776e-04 dt: 3332.23ms, tok/sec:157338.56
step 12175, loss: 3.257814, norm:0.2654, lr:1.1773e-04 dt: 3331.93ms, tok/sec:157352.50
step 12176, loss: 3.160409, norm:0.2921, lr:1.1769e-04 dt: 3331.97ms, tok/sec:157350.85
step 12177, loss: 3.193994, norm:0.2940, lr:1.1765e-04 dt: 3331.97ms, tok/sec:157350.94
step 12178, loss: 3.146930, norm:0.2587, lr:1.1762e-04 dt: 3332.01ms, tok/sec:157349.09
step 12179, loss: 3.194663, norm:0.2584, lr:1.1758e-04 dt: 3332.01ms, tok/sec:157348.75
step 12180, loss: 3.166790, norm:0.2655, lr:1.1755e-04 dt: 3332.27ms, tok/sec:157336.74
step 12181, loss: 3.187426, norm:0.2500, lr:1.1751e-04 dt: 3332.17ms, tok/sec:157341.10
step 12182, loss: 3.269577, norm:0.2615, lr:1.1747e-04 dt: 3331.95ms, tok/sec:157351.92
step 12183, loss: 3.206016, norm:0.2652, lr:1.1744e-04 dt: 3332.11ms, tok/sec:157344.26
step 12184, loss: 3.230890, norm:0.2585, lr:1.1740e-04 dt: 3332.03ms, tok/sec:157348.00
step 12185, loss: 3.233122, norm:0.2603, lr:1.1737e-04 dt: 3331.84ms, tok/sec:157356.97
step 12186, loss: 3.259540, norm:0.2819, lr:1.1733e-04 dt: 3331.91ms, tok/sec:157353.82
step 12187, loss: 3.217090, norm:0.2610, lr:1.1729e-04 dt: 3331.95ms, tok/sec:157351.52
step 12188, loss: 3.231774, norm:0.2595, lr:1.1726e-04 dt: 3332.28ms, tok/sec:157336.19
step 12189, loss: 3.209171, norm:0.2702, lr:1.1722e-04 dt: 3331.88ms, tok/sec:157354.78
step 12190, loss: 3.190870, norm:0.3043, lr:1.1719e-04 dt: 3331.87ms, tok/sec:157355.45
step 12191, loss: 3.166308, norm:0.2659, lr:1.1715e-04 dt: 3333.85ms, tok/sec:157261.87
step 12192, loss: 3.173220, norm:0.2592, lr:1.1712e-04 dt: 3332.13ms, tok/sec:157342.98
step 12193, loss: 3.189842, norm:0.2840, lr:1.1708e-04 dt: 3332.05ms, tok/sec:157347.14
step 12194, loss: 3.159670, norm:0.2762, lr:1.1704e-04 dt: 3332.06ms, tok/sec:157346.42
step 12195, loss: 3.194458, norm:0.2596, lr:1.1701e-04 dt: 3331.90ms, tok/sec:157353.87
step 12196, loss: 3.211629, norm:0.2681, lr:1.1697e-04 dt: 3331.87ms, tok/sec:157355.44
step 12197, loss: 3.234242, norm:0.2553, lr:1.1694e-04 dt: 3332.11ms, tok/sec:157344.31
step 12198, loss: 3.170877, norm:0.2571, lr:1.1690e-04 dt: 3332.11ms, tok/sec:157343.99
step 12199, loss: 3.227992, norm:0.2770, lr:1.1686e-04 dt: 3332.11ms, tok/sec:157343.93
validation loss: 3.2228
Model and optimizer state saved.
HellaSwag accuracy:6927806923435115601/-2=-3463903461717557760.0000
rank 1 sample 0: Hello, I'm a language model, and my programming language is Python. By now, I need a little help with Python programming. The first thing I want
rank 1 sample 1: Hello, I'm a language model, a teacher. One of the reasons I find it challenging with my grammar is that it tends to involve the use of a
rank 1 sample 2: Hello, I'm a language model, but like all languages, I'm not a language master.<|endoftext|>In a move that has made its way to our cities
rank 1 sample 3: Hello, I'm a language model, and I'm sure I got the impression my first language to have seemed pretty foreign to me even then.
It's
rank 0 sample 0: Hello, I'm a language model, and I am a model to my model- it's gonna be your class model. You really, you really, I
rank 0 sample 1: Hello, I'm a language model, so why not write something like: Language model. Languages don't work properly, but it's a matter of setting up
rank 0 sample 2: Hello, I'm a language model, so I will take over from here.
It's a way I have to understand how the language works. I could
rank 0 sample 3: Hello, I'm a language model, you see, there's a language model, even if you don't know what they're in, then you know this
step 12200, loss: 3.176050, norm:0.2762, lr:1.1683e-04 dt: 56078.86ms, tok/sec:9349.12
step 12201, loss: 3.216260, norm:0.2721, lr:1.1679e-04 dt: 3332.05ms, tok/sec:157346.97
step 12202, loss: 3.206081, norm:0.2756, lr:1.1676e-04 dt: 3332.02ms, tok/sec:157348.42
step 12203, loss: 3.246915, norm:0.2893, lr:1.1672e-04 dt: 3332.17ms, tok/sec:157341.26
step 12204, loss: 3.232720, norm:0.2995, lr:1.1669e-04 dt: 3332.18ms, tok/sec:157340.76
step 12205, loss: 3.227079, norm:0.2974, lr:1.1665e-04 dt: 3331.98ms, tok/sec:157350.17
step 12206, loss: 3.212102, norm:0.2792, lr:1.1661e-04 dt: 3331.89ms, tok/sec:157354.47
step 12207, loss: 3.194036, norm:0.2822, lr:1.1658e-04 dt: 3331.74ms, tok/sec:157361.51
step 12208, loss: 3.227363, norm:0.2654, lr:1.1654e-04 dt: 3332.20ms, tok/sec:157340.04
step 12209, loss: 3.176373, norm:0.2601, lr:1.1651e-04 dt: 3332.04ms, tok/sec:157347.36
step 12210, loss: 3.228343, norm:0.2779, lr:1.1647e-04 dt: 3332.01ms, tok/sec:157348.67
step 12211, loss: 3.191869, norm:0.2587, lr:1.1644e-04 dt: 3331.99ms, tok/sec:157349.60
step 12212, loss: 3.204128, norm:0.2714, lr:1.1640e-04 dt: 3332.22ms, tok/sec:157338.93
step 12213, loss: 3.203733, norm:0.2691, lr:1.1636e-04 dt: 3332.10ms, tok/sec:157344.43
step 12214, loss: 3.209032, norm:0.2630, lr:1.1633e-04 dt: 3331.96ms, tok/sec:157351.35
step 12215, loss: 3.239010, norm:0.3023, lr:1.1629e-04 dt: 3332.07ms, tok/sec:157345.98
step 12216, loss: 3.226184, norm:0.2613, lr:1.1626e-04 dt: 3332.09ms, tok/sec:157345.25
step 12217, loss: 3.198402, norm:0.2549, lr:1.1622e-04 dt: 3332.17ms, tok/sec:157341.54
step 12218, loss: 3.189770, norm:0.2516, lr:1.1619e-04 dt: 3331.90ms, tok/sec:157354.09
step 12219, loss: 3.170360, norm:0.2445, lr:1.1615e-04 dt: 3331.97ms, tok/sec:157350.83
step 12220, loss: 3.179690, norm:0.2470, lr:1.1611e-04 dt: 3331.97ms, tok/sec:157350.66
step 12221, loss: 3.182476, norm:0.2578, lr:1.1608e-04 dt: 3332.36ms, tok/sec:157332.44
step 12222, loss: 3.206995, norm:0.2543, lr:1.1604e-04 dt: 3331.81ms, tok/sec:157358.17
step 12223, loss: 3.153381, norm:0.2608, lr:1.1601e-04 dt: 3331.90ms, tok/sec:157353.87
step 12224, loss: 3.159644, norm:0.2496, lr:1.1597e-04 dt: 3332.07ms, tok/sec:157345.87
step 12225, loss: 3.165038, norm:0.2542, lr:1.1594e-04 dt: 3332.14ms, tok/sec:157342.92
step 12226, loss: 3.193751, norm:0.2646, lr:1.1590e-04 dt: 3331.89ms, tok/sec:157354.54
step 12227, loss: 3.193927, norm:0.2651, lr:1.1587e-04 dt: 3331.89ms, tok/sec:157354.70
step 12228, loss: 3.184962, norm:0.2503, lr:1.1583e-04 dt: 3331.94ms, tok/sec:157351.96
step 12229, loss: 3.254685, norm:0.2661, lr:1.1579e-04 dt: 3332.12ms, tok/sec:157343.60
step 12230, loss: 3.182371, norm:0.2700, lr:1.1576e-04 dt: 3332.14ms, tok/sec:157342.78
step 12231, loss: 3.215585, norm:0.2770, lr:1.1572e-04 dt: 3331.99ms, tok/sec:157349.83
step 12232, loss: 3.192121, norm:0.2473, lr:1.1569e-04 dt: 3332.23ms, tok/sec:157338.50
step 12233, loss: 3.187538, norm:0.2613, lr:1.1565e-04 dt: 3331.91ms, tok/sec:157353.75
step 12234, loss: 3.230070, norm:0.2523, lr:1.1562e-04 dt: 3332.06ms, tok/sec:157346.70
step 12235, loss: 3.202288, norm:0.2807, lr:1.1558e-04 dt: 3332.08ms, tok/sec:157345.56
step 12236, loss: 3.224161, norm:0.3078, lr:1.1555e-04 dt: 3332.08ms, tok/sec:157345.38
step 12237, loss: 3.257602, norm:0.2732, lr:1.1551e-04 dt: 3332.44ms, tok/sec:157328.63
step 12238, loss: 3.194197, norm:0.2717, lr:1.1548e-04 dt: 3332.01ms, tok/sec:157348.66
step 12239, loss: 3.264731, norm:0.2895, lr:1.1544e-04 dt: 3332.22ms, tok/sec:157338.81
step 12240, loss: 3.201150, norm:0.2928, lr:1.1540e-04 dt: 3332.04ms, tok/sec:157347.63
step 12241, loss: 3.215203, norm:0.2815, lr:1.1537e-04 dt: 3332.14ms, tok/sec:157342.54
step 12242, loss: 3.234645, norm:0.2685, lr:1.1533e-04 dt: 3332.00ms, tok/sec:157349.28
step 12243, loss: 3.269680, norm:0.2782, lr:1.1530e-04 dt: 3331.87ms, tok/sec:157355.32
step 12244, loss: 3.282159, norm:0.2588, lr:1.1526e-04 dt: 3331.88ms, tok/sec:157355.23
step 12245, loss: 3.229815, norm:0.2773, lr:1.1523e-04 dt: 3332.21ms, tok/sec:157339.27
step 12246, loss: 3.203941, norm:0.2696, lr:1.1519e-04 dt: 3331.96ms, tok/sec:157351.02
step 12247, loss: 3.192093, norm:0.2649, lr:1.1516e-04 dt: 3332.03ms, tok/sec:157348.16
step 12248, loss: 3.161372, norm:0.2588, lr:1.1512e-04 dt: 3332.03ms, tok/sec:157348.11
step 12249, loss: 3.202339, norm:0.2634, lr:1.1509e-04 dt: 3332.39ms, tok/sec:157331.08
HellaSwag accuracy:-6907303908405443503/-2=3453651954202721792.0000
rank 1 sample 0: Hello, I'm a language model, and that's what I want to come back to."
After a week of training and getting into the world of programming
rank 1 sample 1: Hello, I'm a language model, not an academic...
I don't understand the word word, right? That is actually the translation.
I'm
rank 1 sample 2: Hello, I'm a language model, but its the best way to learn a language.
"So if we just don't understand something in a very big
rank 1 sample 3: Hello, I'm a language model, and I'm sure I understand you're writing a few games in. And yet, every programming language is different. So
rank 0 sample 0: Hello, I'm a language model, and I'll be talking about this on paper or in the book here.<|endoftext|>The U.S. has a lot
rank 0 sample 1: Hello, I'm a language model, so why do I want to do something? Well at least that's what the world of English has known for centuries.
rank 0 sample 2: Hello, I'm a language model, so I would be proud of how you're trying to teach. And you're also helping me with my language skills,
rank 0 sample 3: Hello, I'm a language model, I hope you have a few ideas to do what I am doing at my own expense. My initial plans for a test
step 12250, loss: 3.168024, norm:0.2650, lr:1.1505e-04 dt: 48378.48ms, tok/sec:10837.22
step 12251, loss: 3.231983, norm:0.2647, lr:1.1502e-04 dt: 3332.21ms, tok/sec:157339.44
step 12252, loss: 3.174763, norm:0.2608, lr:1.1498e-04 dt: 3332.01ms, tok/sec:157348.66
step 12253, loss: 3.199195, norm:0.2566, lr:1.1495e-04 dt: 3332.07ms, tok/sec:157345.92
step 12254, loss: 3.174401, norm:0.2671, lr:1.1491e-04 dt: 3332.00ms, tok/sec:157349.32
step 12255, loss: 3.280551, norm:0.3610, lr:1.1487e-04 dt: 3331.97ms, tok/sec:157350.90
step 12256, loss: 3.219736, norm:0.2655, lr:1.1484e-04 dt: 3332.05ms, tok/sec:157346.93
step 12257, loss: 3.247056, norm:0.2677, lr:1.1480e-04 dt: 3332.10ms, tok/sec:157344.50
step 12258, loss: 3.196122, norm:0.2828, lr:1.1477e-04 dt: 3331.90ms, tok/sec:157353.94
step 12259, loss: 3.199726, norm:0.2673, lr:1.1473e-04 dt: 3331.95ms, tok/sec:157351.48
step 12260, loss: 3.224906, norm:0.2660, lr:1.1470e-04 dt: 3332.28ms, tok/sec:157336.31
step 12261, loss: 3.274889, norm:0.2719, lr:1.1466e-04 dt: 3332.05ms, tok/sec:157347.03
step 12262, loss: 3.164458, norm:0.2567, lr:1.1463e-04 dt: 3331.77ms, tok/sec:157359.98
step 12263, loss: 3.197978, norm:0.2662, lr:1.1459e-04 dt: 3332.03ms, tok/sec:157348.13
step 12264, loss: 3.219257, norm:0.2866, lr:1.1456e-04 dt: 3332.30ms, tok/sec:157335.23
step 12265, loss: 3.201328, norm:0.2867, lr:1.1452e-04 dt: 3331.98ms, tok/sec:157350.31
step 12266, loss: 3.208680, norm:0.2924, lr:1.1449e-04 dt: 3331.89ms, tok/sec:157354.36
step 12267, loss: 3.170552, norm:0.2695, lr:1.1445e-04 dt: 3332.01ms, tok/sec:157349.03
step 12268, loss: 3.205727, norm:0.2937, lr:1.1442e-04 dt: 3332.06ms, tok/sec:157346.51
step 12269, loss: 3.200380, norm:0.2598, lr:1.1438e-04 dt: 3332.16ms, tok/sec:157341.59
step 12270, loss: 3.189843, norm:0.2869, lr:1.1435e-04 dt: 3331.94ms, tok/sec:157352.31
step 12271, loss: 3.283691, norm:0.3076, lr:1.1431e-04 dt: 3331.91ms, tok/sec:157353.54
step 12272, loss: 3.257339, norm:0.2798, lr:1.1428e-04 dt: 3331.93ms, tok/sec:157352.84
step 12273, loss: 3.226628, norm:0.3049, lr:1.1424e-04 dt: 3331.99ms, tok/sec:157349.87
step 12274, loss: 3.189258, norm:0.2960, lr:1.1421e-04 dt: 3331.92ms, tok/sec:157353.30
step 12275, loss: 3.257591, norm:0.2876, lr:1.1417e-04 dt: 3332.05ms, tok/sec:157347.04
step 12276, loss: 3.230522, norm:0.2588, lr:1.1414e-04 dt: 3332.06ms, tok/sec:157346.55
step 12277, loss: 3.185576, norm:0.2884, lr:1.1410e-04 dt: 3332.06ms, tok/sec:157346.57
step 12278, loss: 3.204046, norm:0.2733, lr:1.1407e-04 dt: 3333.09ms, tok/sec:157298.03
step 12279, loss: 3.241345, norm:0.2973, lr:1.1403e-04 dt: 3332.19ms, tok/sec:157340.39
step 12280, loss: 3.220187, norm:0.3005, lr:1.1400e-04 dt: 3332.15ms, tok/sec:157342.47
step 12281, loss: 3.222088, norm:0.2786, lr:1.1396e-04 dt: 3331.96ms, tok/sec:157351.14
step 12282, loss: 3.272081, norm:0.5957, lr:1.1393e-04 dt: 3332.08ms, tok/sec:157345.75
step 12283, loss: 3.263708, norm:0.2871, lr:1.1389e-04 dt: 3332.15ms, tok/sec:157342.36
step 12284, loss: 3.154514, norm:0.2788, lr:1.1386e-04 dt: 3331.96ms, tok/sec:157351.08
step 12285, loss: 3.175424, norm:0.3175, lr:1.1382e-04 dt: 3331.94ms, tok/sec:157352.37
step 12286, loss: 3.225670, norm:0.2555, lr:1.1379e-04 dt: 3332.13ms, tok/sec:157343.37
step 12287, loss: 3.231134, norm:0.2826, lr:1.1375e-04 dt: 3332.02ms, tok/sec:157348.60
step 12288, loss: 3.181124, norm:0.2624, lr:1.1372e-04 dt: 3332.01ms, tok/sec:157348.91
step 12289, loss: 3.193340, norm:0.2908, lr:1.1368e-04 dt: 3332.10ms, tok/sec:157344.69
step 12290, loss: 3.204530, norm:0.2672, lr:1.1365e-04 dt: 3331.90ms, tok/sec:157354.08
step 12291, loss: 3.170937, norm:0.2684, lr:1.1361e-04 dt: 3332.07ms, tok/sec:157346.05
step 12292, loss: 3.211037, norm:0.2531, lr:1.1358e-04 dt: 3332.25ms, tok/sec:157337.47
step 12293, loss: 3.161769, norm:0.2564, lr:1.1354e-04 dt: 3331.95ms, tok/sec:157351.64
step 12294, loss: 3.201068, norm:0.2801, lr:1.1351e-04 dt: 3331.92ms, tok/sec:157353.16
step 12295, loss: 3.208909, norm:0.2520, lr:1.1347e-04 dt: 3331.96ms, tok/sec:157351.23
step 12296, loss: 3.194612, norm:0.3903, lr:1.1344e-04 dt: 3332.06ms, tok/sec:157346.59
step 12297, loss: 3.225429, norm:0.2544, lr:1.1340e-04 dt: 3332.10ms, tok/sec:157344.72
step 12298, loss: 3.199495, norm:0.2630, lr:1.1337e-04 dt: 3332.04ms, tok/sec:157347.49
step 12299, loss: 3.188932, norm:0.2647, lr:1.1333e-04 dt: 3332.16ms, tok/sec:157341.63
validation loss: 3.2205
Model and optimizer state saved.
HellaSwag accuracy:6927806923435115601/-2=-3463903461717557760.0000
rank 1 sample 0: Hello, I'm a language model, and a reader. I want to check if something works! Here's my current project and the best way to do it
rank 1 sample 1: Hello, I'm a language model, a computer language trainer, and a writer. I hope everything in this article and what my articles reveal about the language model
rank 1 sample 2: Hello, I'm a language model, but are there any other differences?
I'm a teacher that has taught languages for over 30 years, but am I
rank 1 sample 3: Hello, I'm a language model, and I'm learning how to design my native language. While I'm doing exercises, people tell me that I'm a
rank 0 sample 0: Hello, I'm a language model, and I don't have my word friends."
I made up stories from this book that seemed a lot more like an
rank 0 sample 1: Hello, I'm a language model, I love and hate language.
Can you help me find out the code I'm working on for some languages?

rank 0 sample 2: Hello, I'm a language model, so I do,
1. Can you do something that has no value?
2. Can you do a substitution
rank 0 sample 3: Hello, I'm a language model, I hope you will be able to tell the person who wrote this message in English - who, who, who. For
step 12300, loss: 3.193832, norm:0.2703, lr:1.1330e-04 dt: 56054.08ms, tok/sec:9353.25
step 12301, loss: 3.209602, norm:0.2605, lr:1.1326e-04 dt: 3331.86ms, tok/sec:157355.81
step 12302, loss: 3.156056, norm:0.2530, lr:1.1323e-04 dt: 3332.17ms, tok/sec:157341.53
step 12303, loss: 3.221031, norm:0.2769, lr:1.1319e-04 dt: 3331.92ms, tok/sec:157353.19
step 12304, loss: 3.222266, norm:0.2491, lr:1.1316e-04 dt: 3331.87ms, tok/sec:157355.31
step 12305, loss: 3.245634, norm:0.2679, lr:1.1313e-04 dt: 3332.01ms, tok/sec:157348.78
step 12306, loss: 3.177035, norm:0.2669, lr:1.1309e-04 dt: 3331.92ms, tok/sec:157353.17
step 12307, loss: 3.183652, norm:0.2498, lr:1.1306e-04 dt: 3332.25ms, tok/sec:157337.59
step 12308, loss: 3.265741, norm:0.2698, lr:1.1302e-04 dt: 3332.13ms, tok/sec:157343.32
step 12309, loss: 3.173560, norm:0.2598, lr:1.1299e-04 dt: 3332.02ms, tok/sec:157348.44
step 12310, loss: 3.237576, norm:0.2620, lr:1.1295e-04 dt: 3331.85ms, tok/sec:157356.40
step 12311, loss: 3.199097, norm:0.2866, lr:1.1292e-04 dt: 3332.33ms, tok/sec:157333.68
step 12312, loss: 3.220155, norm:0.2875, lr:1.1288e-04 dt: 3332.07ms, tok/sec:157345.87
step 12313, loss: 3.184719, norm:0.2623, lr:1.1285e-04 dt: 3331.92ms, tok/sec:157352.98
step 12314, loss: 3.167137, norm:0.2536, lr:1.1281e-04 dt: 3332.02ms, tok/sec:157348.38
step 12315, loss: 3.215745, norm:0.2719, lr:1.1278e-04 dt: 3332.20ms, tok/sec:157339.68
step 12316, loss: 3.174389, norm:0.2823, lr:1.1274e-04 dt: 3332.05ms, tok/sec:157346.79
step 12317, loss: 3.242014, norm:0.2639, lr:1.1271e-04 dt: 3332.00ms, tok/sec:157349.53
step 12318, loss: 3.243754, norm:0.2756, lr:1.1267e-04 dt: 3331.92ms, tok/sec:157352.99
step 12319, loss: 3.204497, norm:0.2501, lr:1.1264e-04 dt: 3332.12ms, tok/sec:157343.78
step 12320, loss: 3.193408, norm:0.2516, lr:1.1261e-04 dt: 3332.31ms, tok/sec:157334.76
step 12321, loss: 3.206716, norm:0.2452, lr:1.1257e-04 dt: 3331.79ms, tok/sec:157359.26
step 12322, loss: 3.211221, norm:0.2604, lr:1.1254e-04 dt: 3331.74ms, tok/sec:157361.43
step 12323, loss: 3.208879, norm:0.2467, lr:1.1250e-04 dt: 3332.30ms, tok/sec:157335.11
step 12324, loss: 3.190434, norm:0.2906, lr:1.1247e-04 dt: 3331.87ms, tok/sec:157355.26
step 12325, loss: 3.244690, norm:0.2802, lr:1.1243e-04 dt: 3332.06ms, tok/sec:157346.57
step 12326, loss: 3.393228, norm:0.2973, lr:1.1240e-04 dt: 3331.93ms, tok/sec:157352.54
step 12327, loss: 3.172685, norm:0.2821, lr:1.1236e-04 dt: 3331.91ms, tok/sec:157353.53
step 12328, loss: 3.181112, norm:0.2579, lr:1.1233e-04 dt: 3332.08ms, tok/sec:157345.79
step 12329, loss: 3.140239, norm:0.2705, lr:1.1229e-04 dt: 3332.14ms, tok/sec:157342.90
step 12330, loss: 3.170390, norm:0.2643, lr:1.1226e-04 dt: 3332.00ms, tok/sec:157349.54
step 12331, loss: 3.144844, norm:0.2620, lr:1.1223e-04 dt: 3332.02ms, tok/sec:157348.37
step 12332, loss: 3.192421, norm:0.2653, lr:1.1219e-04 dt: 3331.77ms, tok/sec:157360.37
step 12333, loss: 3.147228, norm:0.2645, lr:1.1216e-04 dt: 3332.00ms, tok/sec:157349.52
step 12334, loss: 3.161600, norm:0.2666, lr:1.1212e-04 dt: 3331.87ms, tok/sec:157355.60
step 12335, loss: 3.183701, norm:0.2549, lr:1.1209e-04 dt: 3331.97ms, tok/sec:157350.92
step 12336, loss: 3.170608, norm:0.2639, lr:1.1205e-04 dt: 3331.88ms, tok/sec:157354.90
step 12337, loss: 3.232625, norm:0.2729, lr:1.1202e-04 dt: 3332.13ms, tok/sec:157342.97
step 12338, loss: 3.220258, norm:0.3223, lr:1.1198e-04 dt: 3332.04ms, tok/sec:157347.56
step 12339, loss: 3.189722, norm:0.2916, lr:1.1195e-04 dt: 3332.19ms, tok/sec:157340.37
step 12340, loss: 3.266110, norm:0.2883, lr:1.1192e-04 dt: 3331.90ms, tok/sec:157353.96
step 12341, loss: 3.318553, norm:0.3687, lr:1.1188e-04 dt: 3332.00ms, tok/sec:157349.15
step 12342, loss: 3.205013, norm:0.2689, lr:1.1185e-04 dt: 3332.04ms, tok/sec:157347.35
step 12343, loss: 3.175650, norm:0.2908, lr:1.1181e-04 dt: 3332.05ms, tok/sec:157346.91
step 12344, loss: 3.215363, norm:0.3340, lr:1.1178e-04 dt: 3331.92ms, tok/sec:157353.09
step 12345, loss: 3.207254, norm:0.2740, lr:1.1174e-04 dt: 3332.21ms, tok/sec:157339.62
step 12346, loss: 3.214533, norm:0.2697, lr:1.1171e-04 dt: 3332.24ms, tok/sec:157338.23
step 12347, loss: 3.238584, norm:0.3082, lr:1.1168e-04 dt: 3332.12ms, tok/sec:157343.47
step 12348, loss: 3.246402, norm:0.2758, lr:1.1164e-04 dt: 3331.91ms, tok/sec:157353.63
step 12349, loss: 3.208511, norm:0.2916, lr:1.1161e-04 dt: 3331.91ms, tok/sec:157353.49
HellaSwag accuracy:-2286434768862608367/-2=1143217384431304192.0000
rank 1 sample 0: Hello, I'm a language model, and this is the question that I's trying to address from the beginning. Now, in a new paper I'm going
rank 1 sample 1: Hello, I'm a language model, which means I know how to program the web. In any case, I've taken more time knowing the basics of programming
rank 1 sample 2: Hello, I'm a language model, but who is my language model?
I'm a computer expert and the majority of the language model people are used for
rank 1 sample 3: Hello, I'm a language model, and I'm the language editor for the BSD. Every two years I compile and manage. It's a really nice
rank 0 sample 0: Hello, I'm a language model, and I'll be talking about this from within to the next few decades. The language modeling project started out with a couple
rank 0 sample 1: Hello, I'm a language model, but we're not a language model unless we know every person who knows his native language and can understand any one of our
rank 0 sample 2: Hello, I'm a language model, so I really don't even know it. This is a bit strange and I want to make sure I'm getting back
rank 0 sample 3: Hello, I'm a language model, and now I have a question: "Is Microsoft a language model?"
- No, I don't know what word
step 12350, loss: 3.208027, norm:0.2770, lr:1.1157e-04 dt: 48383.89ms, tok/sec:10836.00
step 12351, loss: 3.212178, norm:0.2722, lr:1.1154e-04 dt: 3332.07ms, tok/sec:157346.25
step 12352, loss: 3.224657, norm:0.2767, lr:1.1150e-04 dt: 3331.99ms, tok/sec:157349.64
step 12353, loss: 3.267337, norm:0.2742, lr:1.1147e-04 dt: 3332.06ms, tok/sec:157346.60
step 12354, loss: 3.151752, norm:0.2984, lr:1.1144e-04 dt: 3332.02ms, tok/sec:157348.61
step 12355, loss: 3.292448, norm:0.2830, lr:1.1140e-04 dt: 3332.16ms, tok/sec:157341.81
step 12356, loss: 3.167108, norm:0.2629, lr:1.1137e-04 dt: 3332.09ms, tok/sec:157345.05
step 12357, loss: 3.241014, norm:0.2555, lr:1.1133e-04 dt: 3331.94ms, tok/sec:157352.40
step 12358, loss: 3.153763, norm:0.2485, lr:1.1130e-04 dt: 3331.87ms, tok/sec:157355.66
step 12359, loss: 3.199555, norm:0.2576, lr:1.1126e-04 dt: 3332.07ms, tok/sec:157346.21
step 12360, loss: 3.165819, norm:0.2496, lr:1.1123e-04 dt: 3332.11ms, tok/sec:157344.32
step 12361, loss: 3.243894, norm:0.2581, lr:1.1120e-04 dt: 3332.15ms, tok/sec:157342.46
step 12362, loss: 3.216510, norm:0.2552, lr:1.1116e-04 dt: 3332.05ms, tok/sec:157347.06
step 12363, loss: 3.209663, norm:0.2422, lr:1.1113e-04 dt: 3331.93ms, tok/sec:157352.48
step 12364, loss: 3.170503, norm:0.2677, lr:1.1109e-04 dt: 3332.04ms, tok/sec:157347.29
step 12365, loss: 3.248861, norm:0.2670, lr:1.1106e-04 dt: 3332.11ms, tok/sec:157343.91
step 12366, loss: 3.163218, norm:0.2561, lr:1.1103e-04 dt: 3332.22ms, tok/sec:157339.18
step 12367, loss: 3.225671, norm:0.2672, lr:1.1099e-04 dt: 3332.00ms, tok/sec:157349.19
step 12368, loss: 3.197116, norm:0.2547, lr:1.1096e-04 dt: 3332.00ms, tok/sec:157349.55
step 12369, loss: 3.150475, norm:0.2721, lr:1.1092e-04 dt: 3331.98ms, tok/sec:157350.50
step 12370, loss: 3.221942, norm:0.2606, lr:1.1089e-04 dt: 3331.94ms, tok/sec:157352.09
step 12371, loss: 3.261187, norm:0.2676, lr:1.1085e-04 dt: 3332.17ms, tok/sec:157341.42
step 12372, loss: 3.245277, norm:0.2573, lr:1.1082e-04 dt: 3332.14ms, tok/sec:157342.76
step 12373, loss: 3.201066, norm:0.2684, lr:1.1079e-04 dt: 3332.03ms, tok/sec:157347.92
step 12374, loss: 3.209713, norm:0.2728, lr:1.1075e-04 dt: 3332.15ms, tok/sec:157342.04
step 12375, loss: 3.240804, norm:0.2735, lr:1.1072e-04 dt: 3331.94ms, tok/sec:157352.05
step 12376, loss: 3.194544, norm:0.2632, lr:1.1068e-04 dt: 3331.92ms, tok/sec:157353.18
step 12377, loss: 3.189139, norm:0.2744, lr:1.1065e-04 dt: 3332.20ms, tok/sec:157339.75
step 12378, loss: 3.185012, norm:0.2555, lr:1.1062e-04 dt: 3332.40ms, tok/sec:157330.25
step 12379, loss: 3.331448, norm:0.3255, lr:1.1058e-04 dt: 3332.33ms, tok/sec:157333.78
step 12380, loss: 3.253428, norm:0.3805, lr:1.1055e-04 dt: 3332.02ms, tok/sec:157348.17
step 12381, loss: 3.248576, norm:0.3026, lr:1.1051e-04 dt: 3331.86ms, tok/sec:157355.91
step 12382, loss: 3.195838, norm:0.2714, lr:1.1048e-04 dt: 3334.66ms, tok/sec:157223.95
step 12383, loss: 3.215388, norm:0.2910, lr:1.1045e-04 dt: 3332.11ms, tok/sec:157343.97
step 12384, loss: 3.250694, norm:0.2891, lr:1.1041e-04 dt: 3332.06ms, tok/sec:157346.41
step 12385, loss: 3.242556, norm:0.2768, lr:1.1038e-04 dt: 3331.98ms, tok/sec:157350.45
step 12386, loss: 3.222898, norm:0.3007, lr:1.1034e-04 dt: 3332.08ms, tok/sec:157345.36
step 12387, loss: 3.211172, norm:0.2597, lr:1.1031e-04 dt: 3332.13ms, tok/sec:157343.41
step 12388, loss: 3.177333, norm:0.2582, lr:1.1028e-04 dt: 3332.23ms, tok/sec:157338.50
step 12389, loss: 3.219164, norm:0.2663, lr:1.1024e-04 dt: 3331.90ms, tok/sec:157354.18
step 12390, loss: 3.178115, norm:0.2714, lr:1.1021e-04 dt: 3332.03ms, tok/sec:157348.01
step 12391, loss: 3.226347, norm:0.2811, lr:1.1018e-04 dt: 3331.91ms, tok/sec:157353.52
step 12392, loss: 3.198227, norm:0.2757, lr:1.1014e-04 dt: 3332.13ms, tok/sec:157343.15
step 12393, loss: 3.182326, norm:0.2614, lr:1.1011e-04 dt: 3331.95ms, tok/sec:157351.80
step 12394, loss: 3.261801, norm:0.2772, lr:1.1007e-04 dt: 3332.09ms, tok/sec:157345.25
step 12395, loss: 3.191875, norm:0.2742, lr:1.1004e-04 dt: 3332.14ms, tok/sec:157342.81
step 12396, loss: 3.190781, norm:0.2644, lr:1.1001e-04 dt: 3331.98ms, tok/sec:157350.15
step 12397, loss: 3.181216, norm:0.2673, lr:1.0997e-04 dt: 3332.17ms, tok/sec:157341.20
step 12398, loss: 3.226675, norm:0.2761, lr:1.0994e-04 dt: 3331.98ms, tok/sec:157350.18
step 12399, loss: 3.204159, norm:0.2549, lr:1.0990e-04 dt: 3332.09ms, tok/sec:157345.13
validation loss: 3.2186
Model and optimizer state saved.
HellaSwag accuracy:6936814122689594449/-2=-3468407061344797184.0000
rank 1 sample 0: Hello, I'm a language model, and my approach is to put students reading into words that match their interests. By using this approach and the vocabulary that I
rank 1 sample 1: Hello, I'm a language model, a teacher. Today I'm learning my alphabet. I also learned Spanish. My main teaching tip for the year is to
rank 1 sample 2: Hello, I'm a language model, but actually the best way to learn it is to see what they do in the language. It really makes it much more
rank 1 sample 3: Hello, I'm a language model, and I'm pretty good at it. Every time I add some color to my code — think of it as a color
rank 0 sample 0: Hello, I'm a language model, and I think I can teach the children these phrases. I think kids tend to be the kids and not the adults that
rank 0 sample 1: Hello, I'm a language model, so to speak. So, you said, you do something, that people have a way to do and to do,
rank 0 sample 2: Hello, I'm a language model, I'm being a speaker or a person.”
It seems like the term "language" is a little awkward
rank 0 sample 3: Hello, I'm a language model, and one of the things I do.
Do you have a class, and could you show me about the process involved
step 12400, loss: 3.221019, norm:0.2805, lr:1.0987e-04 dt: 56054.60ms, tok/sec:9353.17
step 12401, loss: 3.175384, norm:0.2768, lr:1.0984e-04 dt: 3332.32ms, tok/sec:157334.39
step 12402, loss: 3.259472, norm:0.2724, lr:1.0980e-04 dt: 3331.83ms, tok/sec:157357.48
step 12403, loss: 3.167987, norm:0.2683, lr:1.0977e-04 dt: 3331.93ms, tok/sec:157352.65
step 12404, loss: 3.192221, norm:0.2904, lr:1.0974e-04 dt: 3332.32ms, tok/sec:157334.10
step 12405, loss: 3.192963, norm:0.2914, lr:1.0970e-04 dt: 3332.03ms, tok/sec:157348.11
step 12406, loss: 3.159687, norm:0.2746, lr:1.0967e-04 dt: 3332.14ms, tok/sec:157342.51
step 12407, loss: 3.196709, norm:0.2885, lr:1.0963e-04 dt: 3331.91ms, tok/sec:157353.65
step 12408, loss: 3.197736, norm:0.2799, lr:1.0960e-04 dt: 3332.07ms, tok/sec:157346.09
step 12409, loss: 3.265568, norm:0.2894, lr:1.0957e-04 dt: 3332.09ms, tok/sec:157344.90
step 12410, loss: 3.278485, norm:0.3667, lr:1.0953e-04 dt: 3331.77ms, tok/sec:157360.26
step 12411, loss: 3.190971, norm:0.2818, lr:1.0950e-04 dt: 3331.95ms, tok/sec:157351.57
step 12412, loss: 3.207919, norm:0.2845, lr:1.0947e-04 dt: 3332.16ms, tok/sec:157341.69
step 12413, loss: 3.206454, norm:0.2774, lr:1.0943e-04 dt: 3332.10ms, tok/sec:157344.67
step 12414, loss: 3.226471, norm:0.2882, lr:1.0940e-04 dt: 3332.14ms, tok/sec:157342.56
step 12415, loss: 3.215919, norm:0.2709, lr:1.0937e-04 dt: 3331.94ms, tok/sec:157352.00
step 12416, loss: 3.234905, norm:0.2880, lr:1.0933e-04 dt: 3332.00ms, tok/sec:157349.32
step 12417, loss: 3.284918, norm:0.2829, lr:1.0930e-04 dt: 3332.31ms, tok/sec:157334.56
step 12418, loss: 3.239726, norm:0.2871, lr:1.0926e-04 dt: 3331.93ms, tok/sec:157352.54
step 12419, loss: 3.232905, norm:0.2665, lr:1.0923e-04 dt: 3331.89ms, tok/sec:157354.73
step 12420, loss: 3.202695, norm:0.3443, lr:1.0920e-04 dt: 3332.04ms, tok/sec:157347.58
step 12421, loss: 3.156536, norm:0.2799, lr:1.0916e-04 dt: 3332.31ms, tok/sec:157334.93
step 12422, loss: 3.250327, norm:0.2759, lr:1.0913e-04 dt: 3332.00ms, tok/sec:157349.44
step 12423, loss: 3.180392, norm:0.2505, lr:1.0910e-04 dt: 3332.12ms, tok/sec:157343.65
step 12424, loss: 3.189320, norm:0.2813, lr:1.0906e-04 dt: 3332.12ms, tok/sec:157343.52
step 12425, loss: 3.217455, norm:0.2719, lr:1.0903e-04 dt: 3332.22ms, tok/sec:157339.13
step 12426, loss: 3.243130, norm:0.2715, lr:1.0900e-04 dt: 3331.96ms, tok/sec:157351.33
step 12427, loss: 3.188280, norm:0.2535, lr:1.0896e-04 dt: 3331.89ms, tok/sec:157354.69
step 12428, loss: 3.165405, norm:0.2666, lr:1.0893e-04 dt: 3332.04ms, tok/sec:157347.38
step 12429, loss: 3.241136, norm:0.2663, lr:1.0890e-04 dt: 3332.13ms, tok/sec:157343.36
step 12430, loss: 3.225404, norm:0.2423, lr:1.0886e-04 dt: 3332.29ms, tok/sec:157335.54
step 12431, loss: 3.204199, norm:0.2594, lr:1.0883e-04 dt: 3332.00ms, tok/sec:157349.41
step 12432, loss: 3.184921, norm:0.2646, lr:1.0880e-04 dt: 3332.14ms, tok/sec:157342.64
step 12433, loss: 3.196404, norm:0.2592, lr:1.0876e-04 dt: 3332.13ms, tok/sec:157343.41
step 12434, loss: 3.156748, norm:0.2518, lr:1.0873e-04 dt: 3331.94ms, tok/sec:157352.20
step 12435, loss: 3.119118, norm:0.2651, lr:1.0870e-04 dt: 3332.05ms, tok/sec:157347.02
step 12436, loss: 3.202946, norm:0.2608, lr:1.0866e-04 dt: 3332.11ms, tok/sec:157344.18
step 12437, loss: 3.294785, norm:0.2816, lr:1.0863e-04 dt: 3331.98ms, tok/sec:157350.27
step 12438, loss: 3.202689, norm:0.2546, lr:1.0860e-04 dt: 3331.89ms, tok/sec:157354.65
step 12439, loss: 3.242599, norm:0.2675, lr:1.0856e-04 dt: 3331.96ms, tok/sec:157351.05
step 12440, loss: 3.194445, norm:0.2568, lr:1.0853e-04 dt: 3332.17ms, tok/sec:157341.14
step 12441, loss: 3.195962, norm:0.2553, lr:1.0849e-04 dt: 3332.07ms, tok/sec:157345.84
step 12442, loss: 3.170131, norm:0.2479, lr:1.0846e-04 dt: 3331.85ms, tok/sec:157356.33
step 12443, loss: 3.182324, norm:0.2568, lr:1.0843e-04 dt: 3332.07ms, tok/sec:157345.88
step 12444, loss: 3.226843, norm:0.2727, lr:1.0839e-04 dt: 3331.93ms, tok/sec:157352.54
step 12445, loss: 3.245478, norm:0.2672, lr:1.0836e-04 dt: 3332.14ms, tok/sec:157342.89
step 12446, loss: 3.257565, norm:0.2690, lr:1.0833e-04 dt: 3332.16ms, tok/sec:157341.92
step 12447, loss: 3.168211, norm:0.2637, lr:1.0830e-04 dt: 3332.09ms, tok/sec:157344.98
step 12448, loss: 3.236320, norm:0.2564, lr:1.0826e-04 dt: 3332.05ms, tok/sec:157347.03
step 12449, loss: 3.290748, norm:0.3103, lr:1.0823e-04 dt: 3332.37ms, tok/sec:157331.80
HellaSwag accuracy:-2286417176676563887/-2=1143208588338281984.0000
rank 1 sample 0: Hello, I'm a language model, right? Now, in this post… we see the potential for a language model to work for you. I'm not
rank 1 sample 1: Hello, I'm a language model, which means I need to know some language terminology. This way, I can make your writing or study easier.
I
rank 1 sample 2: Hello, I'm a language model, but still, I'm not sure how to use it.".<|endoftext|>In the early days of computerization and programming there
rank 1 sample 3: Hello, I'm a language model, and I'm interested to use a newbie to show users how to add English to standard Android devices.
"I
rank 0 sample 0: Hello, I'm a language model, and I think I can say this a completely wild, old-fashioned way:
You and I have no idea.
rank 0 sample 1: Hello, I'm a language model, I like to do stuff. So even though we say hello, there is some stuff in my language modeling language that makes
rank 0 sample 2: Hello, I'm a language model, but I understand it (if you don't already know it then, don't take a class).
I know I
rank 0 sample 3: Hello, I'm a language model, you say, a language model, a model made up of "analogies of objects." You talk about "An
step 12450, loss: 3.181208, norm:0.2599, lr:1.0820e-04 dt: 48388.77ms, tok/sec:10834.91
step 12451, loss: 3.242115, norm:0.2799, lr:1.0816e-04 dt: 3332.20ms, tok/sec:157339.95
step 12452, loss: 3.160328, norm:0.2916, lr:1.0813e-04 dt: 3332.07ms, tok/sec:157345.92
step 12453, loss: 3.231349, norm:0.2830, lr:1.0810e-04 dt: 3331.82ms, tok/sec:157357.74
step 12454, loss: 3.242168, norm:0.2750, lr:1.0806e-04 dt: 3332.14ms, tok/sec:157342.52
step 12455, loss: 3.244960, norm:0.2604, lr:1.0803e-04 dt: 3331.93ms, tok/sec:157352.73
step 12456, loss: 3.224172, norm:0.2578, lr:1.0800e-04 dt: 3332.04ms, tok/sec:157347.45
step 12457, loss: 3.146060, norm:0.2604, lr:1.0796e-04 dt: 3331.85ms, tok/sec:157356.27
step 12458, loss: 3.191667, norm:0.2588, lr:1.0793e-04 dt: 3332.04ms, tok/sec:157347.50
step 12459, loss: 3.204520, norm:0.2664, lr:1.0790e-04 dt: 3332.02ms, tok/sec:157348.48
step 12460, loss: 3.190347, norm:0.2611, lr:1.0786e-04 dt: 3332.48ms, tok/sec:157326.80
step 12461, loss: 3.173560, norm:0.2541, lr:1.0783e-04 dt: 3332.09ms, tok/sec:157345.16
step 12462, loss: 3.195480, norm:0.2561, lr:1.0780e-04 dt: 3332.14ms, tok/sec:157342.66
step 12463, loss: 3.200812, norm:0.2531, lr:1.0776e-04 dt: 3332.14ms, tok/sec:157342.89
step 12464, loss: 3.144449, norm:0.2751, lr:1.0773e-04 dt: 3332.20ms, tok/sec:157339.92
step 12465, loss: 3.209359, norm:0.2560, lr:1.0770e-04 dt: 3331.92ms, tok/sec:157353.17
step 12466, loss: 3.182442, norm:0.2605, lr:1.0766e-04 dt: 3332.01ms, tok/sec:157348.94
step 12467, loss: 3.183098, norm:0.2518, lr:1.0763e-04 dt: 3331.82ms, tok/sec:157357.69
step 12468, loss: 3.159967, norm:0.2586, lr:1.0760e-04 dt: 3332.15ms, tok/sec:157342.39
step 12469, loss: 3.204207, norm:0.2606, lr:1.0757e-04 dt: 3332.39ms, tok/sec:157330.74
step 12470, loss: 3.186324, norm:0.2490, lr:1.0753e-04 dt: 3332.04ms, tok/sec:157347.55
step 12471, loss: 3.196674, norm:0.2709, lr:1.0750e-04 dt: 3332.04ms, tok/sec:157347.61
step 12472, loss: 3.226559, norm:0.2545, lr:1.0747e-04 dt: 3332.10ms, tok/sec:157344.50
step 12473, loss: 3.174289, norm:0.3247, lr:1.0743e-04 dt: 3332.00ms, tok/sec:157349.47
step 12474, loss: 3.225191, norm:0.2751, lr:1.0740e-04 dt: 3331.93ms, tok/sec:157352.74
step 12475, loss: 3.161152, norm:0.2683, lr:1.0737e-04 dt: 3332.09ms, tok/sec:157344.90
step 12476, loss: 3.193781, norm:0.2611, lr:1.0733e-04 dt: 3332.11ms, tok/sec:157344.06
step 12477, loss: 3.198671, norm:0.2582, lr:1.0730e-04 dt: 3331.84ms, tok/sec:157356.85
step 12478, loss: 3.240992, norm:0.2841, lr:1.0727e-04 dt: 3332.27ms, tok/sec:157336.77
step 12479, loss: 3.232838, norm:0.2602, lr:1.0724e-04 dt: 3331.90ms, tok/sec:157353.98
step 12480, loss: 3.265911, norm:0.3030, lr:1.0720e-04 dt: 3332.05ms, tok/sec:157347.08
step 12481, loss: 3.258753, norm:0.2652, lr:1.0717e-04 dt: 3331.82ms, tok/sec:157358.01
step 12482, loss: 3.223725, norm:0.2826, lr:1.0714e-04 dt: 3331.92ms, tok/sec:157353.11
step 12483, loss: 3.231965, norm:0.2825, lr:1.0710e-04 dt: 3331.97ms, tok/sec:157350.81
step 12484, loss: 3.325863, norm:0.2737, lr:1.0707e-04 dt: 3332.21ms, tok/sec:157339.40
step 12485, loss: 3.239908, norm:0.2742, lr:1.0704e-04 dt: 3332.01ms, tok/sec:157348.84
step 12486, loss: 3.261595, norm:0.2800, lr:1.0700e-04 dt: 3332.05ms, tok/sec:157347.19
step 12487, loss: 3.226798, norm:0.2817, lr:1.0697e-04 dt: 3332.14ms, tok/sec:157342.78
step 12488, loss: 3.233438, norm:0.2690, lr:1.0694e-04 dt: 3332.40ms, tok/sec:157330.36
step 12489, loss: 3.158888, norm:0.2754, lr:1.0691e-04 dt: 3332.11ms, tok/sec:157344.20
step 12490, loss: 3.149626, norm:0.2821, lr:1.0687e-04 dt: 3332.42ms, tok/sec:157329.55
step 12491, loss: 3.201905, norm:0.2626, lr:1.0684e-04 dt: 3332.57ms, tok/sec:157322.24
step 12492, loss: 3.195144, norm:0.2670, lr:1.0681e-04 dt: 3332.32ms, tok/sec:157334.15
step 12493, loss: 3.162142, norm:0.2515, lr:1.0677e-04 dt: 3331.81ms, tok/sec:157358.54
step 12494, loss: 3.218158, norm:0.2674, lr:1.0674e-04 dt: 3331.86ms, tok/sec:157355.75
step 12495, loss: 3.186223, norm:0.2911, lr:1.0671e-04 dt: 3332.03ms, tok/sec:157347.91
step 12496, loss: 3.175077, norm:0.2500, lr:1.0668e-04 dt: 3332.10ms, tok/sec:157344.53
step 12497, loss: 3.144125, norm:0.2545, lr:1.0664e-04 dt: 3332.29ms, tok/sec:157335.67
step 12498, loss: 3.184589, norm:0.2616, lr:1.0661e-04 dt: 3331.97ms, tok/sec:157350.52
step 12499, loss: 3.162335, norm:0.2659, lr:1.0658e-04 dt: 3332.06ms, tok/sec:157346.31
validation loss: 3.2160
Model and optimizer state saved.
HellaSwag accuracy:-2286575506351225775/-2=1143287753175612928.0000
rank 1 sample 0: Hello, I'm a language model, and so this is how I did how it went:
To start with, I have been reading the story of the
rank 1 sample 1: Hello, I'm a language model, a language trainer, and a language tutor. I like your language model, the way your model provides you with the language
rank 1 sample 2: Hello, I'm a language model, but learning the rules of the language is a very nice job - and that's why I have always thought of languages like
rank 1 sample 3: Hello, I'm a language model, and I'm really excited about it. They're excited to add more. A lot of work to do, but I
rank 0 sample 0: Hello, I'm a language model, and I'll be talking about that as soon as I hear about. Can anybody help me? Can you give me more
rank 0 sample 1: Hello, I'm a language model, I'd like to be able to interact with people when I speak a new language, I've got access to a lot
rank 0 sample 2: Hello, I'm a language model, so I really think 'you're human' won't be easy."
"And that's the reality of my time
rank 0 sample 3: Hello, I'm a language model, and here's what I want. In my program, I'm going to write as long as possible; I'll let
step 12500, loss: 3.202343, norm:0.2829, lr:1.0655e-04 dt: 56101.34ms, tok/sec:9345.37
step 12501, loss: 3.189265, norm:0.2606, lr:1.0651e-04 dt: 3331.94ms, tok/sec:157352.29
step 12502, loss: 3.180254, norm:0.2552, lr:1.0648e-04 dt: 3332.31ms, tok/sec:157334.86
step 12503, loss: 3.187623, norm:0.2936, lr:1.0645e-04 dt: 3332.14ms, tok/sec:157342.61
step 12504, loss: 3.205358, norm:0.2482, lr:1.0641e-04 dt: 3332.02ms, tok/sec:157348.38
step 12505, loss: 3.124242, norm:0.3035, lr:1.0638e-04 dt: 3331.88ms, tok/sec:157354.89
step 12506, loss: 3.182464, norm:0.2771, lr:1.0635e-04 dt: 3332.01ms, tok/sec:157348.82
step 12507, loss: 3.202784, norm:0.2554, lr:1.0632e-04 dt: 3331.98ms, tok/sec:157350.06
step 12508, loss: 3.183270, norm:0.2632, lr:1.0628e-04 dt: 3332.10ms, tok/sec:157344.49
step 12509, loss: 3.207711, norm:0.2465, lr:1.0625e-04 dt: 3331.99ms, tok/sec:157349.73
step 12510, loss: 3.144676, norm:0.2469, lr:1.0622e-04 dt: 3332.17ms, tok/sec:157341.12
step 12511, loss: 3.229345, norm:0.3061, lr:1.0619e-04 dt: 3332.13ms, tok/sec:157343.15
step 12512, loss: 3.146138, norm:0.2532, lr:1.0615e-04 dt: 3332.02ms, tok/sec:157348.28
step 12513, loss: 3.221045, norm:0.2986, lr:1.0612e-04 dt: 3331.81ms, tok/sec:157358.34
step 12514, loss: 3.246414, norm:0.2761, lr:1.0609e-04 dt: 3332.00ms, tok/sec:157349.56
step 12515, loss: 3.232915, norm:0.2890, lr:1.0606e-04 dt: 3332.10ms, tok/sec:157344.79
step 12516, loss: 3.237575, norm:0.2833, lr:1.0602e-04 dt: 3332.15ms, tok/sec:157342.29
step 12517, loss: 3.210603, norm:0.2700, lr:1.0599e-04 dt: 3332.25ms, tok/sec:157337.69
step 12518, loss: 3.259665, norm:0.2644, lr:1.0596e-04 dt: 3331.94ms, tok/sec:157352.38
step 12519, loss: 3.233869, norm:0.2820, lr:1.0593e-04 dt: 3332.26ms, tok/sec:157336.85
step 12520, loss: 3.195089, norm:0.2736, lr:1.0589e-04 dt: 3332.09ms, tok/sec:157345.11
step 12521, loss: 3.172589, norm:0.2747, lr:1.0586e-04 dt: 3331.97ms, tok/sec:157350.67
step 12522, loss: 3.215417, norm:0.2662, lr:1.0583e-04 dt: 3332.11ms, tok/sec:157344.01
step 12523, loss: 3.235554, norm:0.2764, lr:1.0580e-04 dt: 3332.02ms, tok/sec:157348.35
step 12524, loss: 3.202648, norm:0.2907, lr:1.0576e-04 dt: 3332.10ms, tok/sec:157344.55
step 12525, loss: 3.222867, norm:0.2672, lr:1.0573e-04 dt: 3332.06ms, tok/sec:157346.63
step 12526, loss: 3.189850, norm:0.2708, lr:1.0570e-04 dt: 3331.85ms, tok/sec:157356.36
step 12527, loss: 3.175994, norm:0.2582, lr:1.0567e-04 dt: 3332.28ms, tok/sec:157336.30
step 12528, loss: 3.277478, norm:0.3679, lr:1.0563e-04 dt: 3331.96ms, tok/sec:157351.34
step 12529, loss: 3.194482, norm:0.2784, lr:1.0560e-04 dt: 3332.38ms, tok/sec:157331.62
step 12530, loss: 3.197837, norm:0.2812, lr:1.0557e-04 dt: 3332.25ms, tok/sec:157337.70
step 12531, loss: 3.236113, norm:0.2859, lr:1.0554e-04 dt: 3331.85ms, tok/sec:157356.56
step 12532, loss: 3.195745, norm:0.2741, lr:1.0550e-04 dt: 3331.97ms, tok/sec:157350.78
step 12533, loss: 3.228140, norm:0.2593, lr:1.0547e-04 dt: 3332.06ms, tok/sec:157346.63
step 12534, loss: 3.263561, norm:0.2969, lr:1.0544e-04 dt: 3331.95ms, tok/sec:157351.65
step 12535, loss: 3.163645, norm:0.2531, lr:1.0541e-04 dt: 3332.11ms, tok/sec:157343.99
step 12536, loss: 3.171047, norm:0.2641, lr:1.0537e-04 dt: 3331.89ms, tok/sec:157354.50
step 12537, loss: 3.182753, norm:0.2606, lr:1.0534e-04 dt: 3332.31ms, tok/sec:157334.79
step 12538, loss: 3.196358, norm:0.2612, lr:1.0531e-04 dt: 3332.15ms, tok/sec:157342.37
step 12539, loss: 3.189297, norm:0.2603, lr:1.0528e-04 dt: 3332.12ms, tok/sec:157343.88
step 12540, loss: 3.181261, norm:0.2631, lr:1.0524e-04 dt: 3332.05ms, tok/sec:157346.88
step 12541, loss: 3.181041, norm:0.2536, lr:1.0521e-04 dt: 3332.06ms, tok/sec:157346.45
step 12542, loss: 3.225913, norm:0.2639, lr:1.0518e-04 dt: 3331.90ms, tok/sec:157353.96
step 12543, loss: 3.202293, norm:0.2587, lr:1.0515e-04 dt: 3332.08ms, tok/sec:157345.72
step 12544, loss: 3.196918, norm:0.2546, lr:1.0512e-04 dt: 3331.95ms, tok/sec:157351.71
step 12545, loss: 3.256551, norm:0.2413, lr:1.0508e-04 dt: 3332.17ms, tok/sec:157341.47
step 12546, loss: 3.189547, norm:0.2508, lr:1.0505e-04 dt: 3332.02ms, tok/sec:157348.34
step 12547, loss: 3.182720, norm:0.2574, lr:1.0502e-04 dt: 3331.94ms, tok/sec:157352.14
step 12548, loss: 3.139257, norm:0.2724, lr:1.0499e-04 dt: 3332.07ms, tok/sec:157345.89
step 12549, loss: 3.166218, norm:0.2725, lr:1.0495e-04 dt: 3331.91ms, tok/sec:157353.71
HellaSwag accuracy:-2295582705605966767/-2=1147791352802983424.0000
rank 1 sample 0: Hello, I'm a language model, and here is a fun video with no grammar problems in class as well as a few questions for a quick reference.

rank 1 sample 1: Hello, I'm a language model, a teacher. One of the reasons it's so interesting so far is that many people talk about how to teach the language
rank 1 sample 2: Hello, I'm a language model, but am still interested in the language. I'm sure there’s a lot of people around who have no technical
rank 1 sample 3: Hello, I'm a language model, and I'm interested to use my existing environment to design environments. Now, just like here, there's a huge community
rank 0 sample 0: Hello, I'm a language model, and I love to use English for learning."
"Not so quick if your child needs to do more advanced tasks."
rank 0 sample 1: Hello, I'm a language model, so to speak. So, if I want you to remember what's I am doing in that sentence, and what's
rank 0 sample 2: Hello, I'm a language model, so I had no idea it was one of thousands of languages or any number of languages, or any number of languages,
rank 0 sample 3: Hello, I'm a language model, you use a language model to understand what a lot of people are saying at this webinar. I guess that you probably
step 12550, loss: 3.267179, norm:0.2746, lr:1.0492e-04 dt: 48382.16ms, tok/sec:10836.39
step 12551, loss: 3.226163, norm:0.2709, lr:1.0489e-04 dt: 3331.99ms, tok/sec:157349.85
step 12552, loss: 3.149030, norm:0.2524, lr:1.0486e-04 dt: 3331.95ms, tok/sec:157351.57
step 12553, loss: 3.218654, norm:0.3123, lr:1.0483e-04 dt: 3331.93ms, tok/sec:157352.84
step 12554, loss: 3.162773, norm:0.2620, lr:1.0479e-04 dt: 3332.36ms, tok/sec:157332.42
step 12555, loss: 3.196111, norm:0.2552, lr:1.0476e-04 dt: 3331.91ms, tok/sec:157353.53
step 12556, loss: 3.243501, norm:0.2768, lr:1.0473e-04 dt: 3331.91ms, tok/sec:157353.64
step 12557, loss: 3.276710, norm:0.2849, lr:1.0470e-04 dt: 3332.20ms, tok/sec:157339.97
step 12558, loss: 3.219684, norm:0.2703, lr:1.0466e-04 dt: 3332.09ms, tok/sec:157345.03
step 12559, loss: 3.200293, norm:0.2691, lr:1.0463e-04 dt: 3331.75ms, tok/sec:157360.91
step 12560, loss: 3.211504, norm:0.2928, lr:1.0460e-04 dt: 3331.89ms, tok/sec:157354.73
step 12561, loss: 3.222368, norm:0.2997, lr:1.0457e-04 dt: 3331.92ms, tok/sec:157352.95
step 12562, loss: 3.197126, norm:0.2629, lr:1.0454e-04 dt: 3332.07ms, tok/sec:157345.94
step 12563, loss: 3.200126, norm:0.2639, lr:1.0450e-04 dt: 3332.08ms, tok/sec:157345.76
step 12564, loss: 3.251336, norm:0.2843, lr:1.0447e-04 dt: 3331.79ms, tok/sec:157359.20
step 12565, loss: 3.203645, norm:0.2589, lr:1.0444e-04 dt: 3332.07ms, tok/sec:157345.98
step 12566, loss: 3.236104, norm:0.2559, lr:1.0441e-04 dt: 3332.04ms, tok/sec:157347.48
step 12567, loss: 3.190850, norm:0.2455, lr:1.0438e-04 dt: 3331.74ms, tok/sec:157361.52
step 12568, loss: 3.185368, norm:0.2563, lr:1.0434e-04 dt: 3331.98ms, tok/sec:157350.06
step 12569, loss: 3.225411, norm:0.2657, lr:1.0431e-04 dt: 3331.98ms, tok/sec:157350.14
step 12570, loss: 3.230395, norm:0.2548, lr:1.0428e-04 dt: 3332.37ms, tok/sec:157331.70
step 12571, loss: 3.213709, norm:0.2559, lr:1.0425e-04 dt: 3331.96ms, tok/sec:157351.06
step 12572, loss: 3.212452, norm:0.2739, lr:1.0422e-04 dt: 3333.96ms, tok/sec:157256.97
step 12573, loss: 3.198220, norm:0.2635, lr:1.0418e-04 dt: 3332.00ms, tok/sec:157349.27
step 12574, loss: 3.191904, norm:0.2517, lr:1.0415e-04 dt: 3332.02ms, tok/sec:157348.57
step 12575, loss: 3.175531, norm:0.2625, lr:1.0412e-04 dt: 3331.88ms, tok/sec:157355.23
step 12576, loss: 3.220900, norm:0.2707, lr:1.0409e-04 dt: 3332.06ms, tok/sec:157346.31
step 12577, loss: 3.169248, norm:0.2525, lr:1.0406e-04 dt: 3331.93ms, tok/sec:157352.54
step 12578, loss: 3.181273, norm:0.2669, lr:1.0402e-04 dt: 3332.43ms, tok/sec:157329.08
step 12579, loss: 3.204553, norm:0.2781, lr:1.0399e-04 dt: 3332.02ms, tok/sec:157348.33
step 12580, loss: 3.159475, norm:0.2697, lr:1.0396e-04 dt: 3332.04ms, tok/sec:157347.24
step 12581, loss: 3.178319, norm:0.2573, lr:1.0393e-04 dt: 3332.03ms, tok/sec:157347.83
step 12582, loss: 3.227446, norm:0.3119, lr:1.0390e-04 dt: 3332.02ms, tok/sec:157348.39
step 12583, loss: 3.197767, norm:0.2693, lr:1.0386e-04 dt: 3332.12ms, tok/sec:157343.54
step 12584, loss: 3.173944, norm:0.2997, lr:1.0383e-04 dt: 3332.04ms, tok/sec:157347.36
step 12585, loss: 3.240703, norm:0.2660, lr:1.0380e-04 dt: 3331.91ms, tok/sec:157353.54
step 12586, loss: 3.151745, norm:0.2981, lr:1.0377e-04 dt: 3332.08ms, tok/sec:157345.70
step 12587, loss: 3.183847, norm:0.2830, lr:1.0374e-04 dt: 3331.94ms, tok/sec:157352.16
step 12588, loss: 3.192681, norm:0.2843, lr:1.0371e-04 dt: 3332.04ms, tok/sec:157347.39
step 12589, loss: 3.222808, norm:0.2975, lr:1.0367e-04 dt: 3332.20ms, tok/sec:157340.04
step 12590, loss: 3.209062, norm:0.3115, lr:1.0364e-04 dt: 3331.99ms, tok/sec:157349.69
step 12591, loss: 3.247062, norm:0.2910, lr:1.0361e-04 dt: 3332.18ms, tok/sec:157340.65
step 12592, loss: 3.154164, norm:0.3147, lr:1.0358e-04 dt: 3331.98ms, tok/sec:157350.41
step 12593, loss: 3.156429, norm:0.2768, lr:1.0355e-04 dt: 3332.00ms, tok/sec:157349.42
step 12594, loss: 3.174665, norm:0.2944, lr:1.0351e-04 dt: 3332.08ms, tok/sec:157345.68
step 12595, loss: 3.192218, norm:0.2863, lr:1.0348e-04 dt: 3332.43ms, tok/sec:157329.19
step 12596, loss: 3.222209, norm:0.2578, lr:1.0345e-04 dt: 3332.17ms, tok/sec:157341.41
step 12597, loss: 3.161803, norm:0.2729, lr:1.0342e-04 dt: 3331.96ms, tok/sec:157351.26
step 12598, loss: 3.197407, norm:0.2793, lr:1.0339e-04 dt: 3332.25ms, tok/sec:157337.51
step 12599, loss: 3.128346, norm:0.2625, lr:1.0336e-04 dt: 3332.18ms, tok/sec:157340.77
validation loss: 3.2140
Model and optimizer state saved.
HellaSwag accuracy:-2286593098537270191/-2=1143296549268635136.0000
rank 1 sample 0: Hello, I'm a language model, and the results are in my head…I'll never run on a computer again. But it seems like it's a
rank 1 sample 1: Hello, I'm a language model, a teacher of computer science. I know my students as much as I do because they'll appreciate my efforts.
I
rank 1 sample 2: Hello, I'm a language model, but like most language models, I can't understand grammar and usage of English. I'm just an old-world slang
rank 1 sample 3: Hello, I'm a language model, and I'm looking forward to teaching people there.
First, why isn't it an extension of the normal learning process
rank 0 sample 0: Hello, I'm a language model, and I'll be talking about this with little brother.
So when I see a picture of that language, I want
rank 0 sample 1: Hello, I'm a language model, so why start with "what if a language" before diving in? - by: "Language" + "Introduction"
rank 0 sample 2: Hello, I'm a language model, so I guess it's been a great year!"
"So we've been using a lot of different languages for the
rank 0 sample 3: Hello, I'm a language model, but can't use the same syntax with other languages.
Thanks to your feedback I am so glad you asked! Now
step 12600, loss: 3.206815, norm:0.2509, lr:1.0332e-04 dt: 56062.87ms, tok/sec:9351.79
step 12601, loss: 3.203004, norm:0.2587, lr:1.0329e-04 dt: 3332.14ms, tok/sec:157342.94
step 12602, loss: 3.182470, norm:0.2628, lr:1.0326e-04 dt: 3331.92ms, tok/sec:157352.99
step 12603, loss: 3.223864, norm:0.2648, lr:1.0323e-04 dt: 3331.94ms, tok/sec:157352.27
step 12604, loss: 3.188339, norm:0.2733, lr:1.0320e-04 dt: 3332.21ms, tok/sec:157339.52
step 12605, loss: 3.206928, norm:0.2711, lr:1.0317e-04 dt: 3332.29ms, tok/sec:157335.74
step 12606, loss: 3.177066, norm:0.2699, lr:1.0313e-04 dt: 3331.88ms, tok/sec:157355.13
step 12607, loss: 3.192066, norm:0.2784, lr:1.0310e-04 dt: 3331.97ms, tok/sec:157350.62
step 12608, loss: 3.241132, norm:0.2750, lr:1.0307e-04 dt: 3331.99ms, tok/sec:157350.03
step 12609, loss: 3.187958, norm:0.2804, lr:1.0304e-04 dt: 3332.09ms, tok/sec:157345.07
step 12610, loss: 3.205564, norm:0.2781, lr:1.0301e-04 dt: 3332.00ms, tok/sec:157349.20
step 12611, loss: 3.167281, norm:0.2796, lr:1.0298e-04 dt: 3332.01ms, tok/sec:157349.02
step 12612, loss: 3.190063, norm:0.2633, lr:1.0294e-04 dt: 3331.77ms, tok/sec:157360.04
step 12613, loss: 3.198871, norm:0.2792, lr:1.0291e-04 dt: 3332.06ms, tok/sec:157346.49
step 12614, loss: 3.202240, norm:0.2621, lr:1.0288e-04 dt: 3331.89ms, tok/sec:157354.46
step 12615, loss: 3.202692, norm:0.2691, lr:1.0285e-04 dt: 3331.99ms, tok/sec:157349.63
step 12616, loss: 3.194389, norm:0.2636, lr:1.0282e-04 dt: 3332.05ms, tok/sec:157346.81
step 12617, loss: 3.227732, norm:0.2630, lr:1.0279e-04 dt: 3331.97ms, tok/sec:157350.58
step 12618, loss: 3.213298, norm:0.2685, lr:1.0276e-04 dt: 3332.08ms, tok/sec:157345.78
step 12619, loss: 3.245886, norm:0.3102, lr:1.0272e-04 dt: 3332.00ms, tok/sec:157349.37
step 12620, loss: 3.214542, norm:0.2873, lr:1.0269e-04 dt: 3332.01ms, tok/sec:157348.88
step 12621, loss: 3.197286, norm:0.3122, lr:1.0266e-04 dt: 3332.22ms, tok/sec:157338.87
step 12622, loss: 3.214405, norm:0.2796, lr:1.0263e-04 dt: 3331.91ms, tok/sec:157353.57
step 12623, loss: 3.147003, norm:0.2906, lr:1.0260e-04 dt: 3332.01ms, tok/sec:157348.89
step 12624, loss: 3.216247, norm:0.2769, lr:1.0257e-04 dt: 3331.98ms, tok/sec:157350.09
step 12625, loss: 3.181451, norm:0.2675, lr:1.0254e-04 dt: 3332.16ms, tok/sec:157341.77
step 12626, loss: 3.182916, norm:0.2790, lr:1.0250e-04 dt: 3332.07ms, tok/sec:157345.92
step 12627, loss: 3.252202, norm:0.3034, lr:1.0247e-04 dt: 3332.05ms, tok/sec:157346.94
step 12628, loss: 3.256804, norm:0.2935, lr:1.0244e-04 dt: 3332.03ms, tok/sec:157348.07
step 12629, loss: 3.246473, norm:0.3159, lr:1.0241e-04 dt: 3332.00ms, tok/sec:157349.11
step 12630, loss: 3.235551, norm:0.2907, lr:1.0238e-04 dt: 3331.93ms, tok/sec:157352.50
step 12631, loss: 3.245557, norm:0.2857, lr:1.0235e-04 dt: 3332.13ms, tok/sec:157343.15
step 12632, loss: 3.172777, norm:0.2743, lr:1.0232e-04 dt: 3332.15ms, tok/sec:157342.22
step 12633, loss: 3.220216, norm:0.2687, lr:1.0228e-04 dt: 3332.31ms, tok/sec:157334.50
step 12634, loss: 3.190345, norm:0.2740, lr:1.0225e-04 dt: 3331.96ms, tok/sec:157351.44
step 12635, loss: 3.163159, norm:0.2737, lr:1.0222e-04 dt: 3331.83ms, tok/sec:157357.45
step 12636, loss: 3.209016, norm:0.2546, lr:1.0219e-04 dt: 3332.20ms, tok/sec:157339.96
step 12637, loss: 3.201592, norm:0.2617, lr:1.0216e-04 dt: 3332.07ms, tok/sec:157346.03
step 12638, loss: 3.210378, norm:0.2728, lr:1.0213e-04 dt: 3332.04ms, tok/sec:157347.50
step 12639, loss: 3.229551, norm:0.2585, lr:1.0210e-04 dt: 3331.92ms, tok/sec:157352.90
step 12640, loss: 3.186657, norm:0.2536, lr:1.0207e-04 dt: 3332.09ms, tok/sec:157345.26
step 12641, loss: 3.243505, norm:0.2954, lr:1.0203e-04 dt: 3332.02ms, tok/sec:157348.31
step 12642, loss: 3.197314, norm:0.2671, lr:1.0200e-04 dt: 3331.93ms, tok/sec:157352.62
step 12643, loss: 3.170446, norm:0.2671, lr:1.0197e-04 dt: 3331.84ms, tok/sec:157357.06
step 12644, loss: 3.189468, norm:0.2628, lr:1.0194e-04 dt: 3332.00ms, tok/sec:157349.52
step 12645, loss: 3.175125, norm:0.2508, lr:1.0191e-04 dt: 3332.10ms, tok/sec:157344.70
step 12646, loss: 3.135644, norm:0.2561, lr:1.0188e-04 dt: 3331.88ms, tok/sec:157355.08
step 12647, loss: 3.189053, norm:0.2552, lr:1.0185e-04 dt: 3331.99ms, tok/sec:157349.76
step 12648, loss: 3.207317, norm:0.2897, lr:1.0182e-04 dt: 3332.03ms, tok/sec:157348.09
step 12649, loss: 3.149980, norm:0.2586, lr:1.0178e-04 dt: 3332.26ms, tok/sec:157337.19
HellaSwag accuracy:-2295565113419922351/-2=1147782556709961216.0000
rank 1 sample 0: Hello, I'm a language model, so when you're thinking of languages design and design thinking, do you need to know about language model?
I'm
rank 1 sample 1: Hello, I'm a language model, a teacher of this country. I can explain to you here what I'm working with because I've got a lot of
rank 1 sample 2: Hello, I'm a language model, but some other languages are not.
I'm a teacher (I teach as a language, and not a person).
rank 1 sample 3: Hello, I'm a language model, so I'm using the PongPam app. Every time I try reading a bunch of text, I want to
rank 0 sample 0: Hello, I'm a language model, and I'll be talking about my code after seeing a live chat for another time. It's so good and I hope
rank 0 sample 1: Hello, I'm a language model, but have a lot of work and ideas. So yeah, what does this really do to a lot of people? I
rank 0 sample 2: Hello, I'm a language model, so I wanted to keep an eye out for this.
It seems like a perfect way to get a little bit more
rank 0 sample 3: Hello, I'm a language model, and not a language model.
We are living in a world of new technologies evolving from nothing. For example, what
step 12650, loss: 3.231166, norm:0.2636, lr:1.0175e-04 dt: 48388.89ms, tok/sec:10834.88
step 12651, loss: 3.208472, norm:0.2806, lr:1.0172e-04 dt: 3332.39ms, tok/sec:157330.96
step 12652, loss: 3.202899, norm:0.2800, lr:1.0169e-04 dt: 3332.00ms, tok/sec:157349.43
step 12653, loss: 3.176335, norm:0.2702, lr:1.0166e-04 dt: 3331.99ms, tok/sec:157349.83
step 12654, loss: 3.184700, norm:0.2637, lr:1.0163e-04 dt: 3332.04ms, tok/sec:157347.51
step 12655, loss: 3.132511, norm:0.2726, lr:1.0160e-04 dt: 3332.06ms, tok/sec:157346.28
step 12656, loss: 3.214570, norm:0.2815, lr:1.0157e-04 dt: 3332.39ms, tok/sec:157331.06
step 12657, loss: 3.216464, norm:0.2692, lr:1.0154e-04 dt: 3332.14ms, tok/sec:157342.76
step 12658, loss: 3.158168, norm:0.2684, lr:1.0150e-04 dt: 3332.04ms, tok/sec:157347.41
step 12659, loss: 3.138224, norm:0.2752, lr:1.0147e-04 dt: 3332.03ms, tok/sec:157347.81
step 12660, loss: 3.198397, norm:0.2740, lr:1.0144e-04 dt: 3332.29ms, tok/sec:157335.59
step 12661, loss: 3.213186, norm:0.2574, lr:1.0141e-04 dt: 3331.97ms, tok/sec:157350.60
step 12662, loss: 3.198936, norm:0.2637, lr:1.0138e-04 dt: 3331.99ms, tok/sec:157349.88
step 12663, loss: 3.218906, norm:0.2609, lr:1.0135e-04 dt: 3332.11ms, tok/sec:157344.00
step 12664, loss: 3.211061, norm:0.2506, lr:1.0132e-04 dt: 3332.05ms, tok/sec:157347.10
step 12665, loss: 3.221970, norm:0.2900, lr:1.0129e-04 dt: 3332.09ms, tok/sec:157344.93
step 12666, loss: 3.191446, norm:0.2664, lr:1.0126e-04 dt: 3331.92ms, tok/sec:157353.22
step 12667, loss: 3.227385, norm:0.2658, lr:1.0123e-04 dt: 3332.09ms, tok/sec:157345.17
step 12668, loss: 3.155764, norm:0.2612, lr:1.0119e-04 dt: 3331.82ms, tok/sec:157357.90
step 12669, loss: 3.183601, norm:0.2967, lr:1.0116e-04 dt: 3332.16ms, tok/sec:157341.66
step 12670, loss: 3.147229, norm:0.2679, lr:1.0113e-04 dt: 3331.91ms, tok/sec:157353.74
step 12671, loss: 3.176179, norm:0.2713, lr:1.0110e-04 dt: 3331.89ms, tok/sec:157354.57
step 12672, loss: 3.171475, norm:0.2644, lr:1.0107e-04 dt: 3332.12ms, tok/sec:157343.72
step 12673, loss: 3.224848, norm:0.2851, lr:1.0104e-04 dt: 3331.86ms, tok/sec:157355.85
step 12674, loss: 3.187996, norm:0.2500, lr:1.0101e-04 dt: 3331.99ms, tok/sec:157349.82
step 12675, loss: 3.165474, norm:0.2833, lr:1.0098e-04 dt: 3332.08ms, tok/sec:157345.50
step 12676, loss: 3.164148, norm:0.2720, lr:1.0095e-04 dt: 3332.02ms, tok/sec:157348.35
step 12677, loss: 3.212273, norm:0.2620, lr:1.0092e-04 dt: 3331.87ms, tok/sec:157355.36
step 12678, loss: 3.205413, norm:0.2587, lr:1.0088e-04 dt: 3331.98ms, tok/sec:157350.24
step 12679, loss: 3.214817, norm:0.2847, lr:1.0085e-04 dt: 3332.19ms, tok/sec:157340.44
step 12680, loss: 3.194308, norm:0.2536, lr:1.0082e-04 dt: 3332.05ms, tok/sec:157347.06
step 12681, loss: 3.180080, norm:0.2593, lr:1.0079e-04 dt: 3331.98ms, tok/sec:157350.48
step 12682, loss: 3.170884, norm:0.2615, lr:1.0076e-04 dt: 3331.97ms, tok/sec:157350.96
step 12683, loss: 3.194670, norm:0.2583, lr:1.0073e-04 dt: 3332.05ms, tok/sec:157346.79
step 12684, loss: 3.256574, norm:0.2822, lr:1.0070e-04 dt: 3332.01ms, tok/sec:157348.66
step 12685, loss: 3.207395, norm:0.2729, lr:1.0067e-04 dt: 3331.98ms, tok/sec:157350.19
step 12686, loss: 3.214983, norm:0.3000, lr:1.0064e-04 dt: 3331.96ms, tok/sec:157351.29
step 12687, loss: 3.213806, norm:0.2747, lr:1.0061e-04 dt: 3331.99ms, tok/sec:157349.88
step 12688, loss: 3.202119, norm:0.2537, lr:1.0058e-04 dt: 3332.52ms, tok/sec:157325.00
step 12689, loss: 3.245867, norm:0.2683, lr:1.0055e-04 dt: 3332.08ms, tok/sec:157345.63
step 12690, loss: 3.202022, norm:0.2799, lr:1.0052e-04 dt: 3332.05ms, tok/sec:157346.86
step 12691, loss: 3.141418, norm:0.2479, lr:1.0048e-04 dt: 3331.94ms, tok/sec:157352.15
step 12692, loss: 3.218149, norm:0.2737, lr:1.0045e-04 dt: 3332.03ms, tok/sec:157348.04
step 12693, loss: 3.223941, norm:0.2902, lr:1.0042e-04 dt: 3331.97ms, tok/sec:157350.96
step 12694, loss: 3.212895, norm:0.2689, lr:1.0039e-04 dt: 3332.04ms, tok/sec:157347.23
step 12695, loss: 3.184039, norm:0.2838, lr:1.0036e-04 dt: 3331.96ms, tok/sec:157351.35
step 12696, loss: 3.222290, norm:0.2682, lr:1.0033e-04 dt: 3332.17ms, tok/sec:157341.47
step 12697, loss: 3.206993, norm:0.2697, lr:1.0030e-04 dt: 3332.20ms, tok/sec:157340.10
step 12698, loss: 3.228753, norm:0.2696, lr:1.0027e-04 dt: 3332.14ms, tok/sec:157342.58
step 12699, loss: 3.195433, norm:0.2753, lr:1.0024e-04 dt: 3332.23ms, tok/sec:157338.29
validation loss: 3.2122
Model and optimizer state saved.
HellaSwag accuracy:2325110512076162129/-2=-1162555256038081024.0000
rank 1 sample 0: Hello, I'm a language model, and this is the one that I still need (in most lessons) to develop. This is one of the reasons I
rank 1 sample 1: Hello, I'm a language model, a computer programmer, and a teacher. One of the key things to remember is to put in parentheses, and to use
rank 1 sample 2: Hello, I'm a language model, but because you've been there, you've been there and all you need to do is get in a place you're
rank 1 sample 3: Hello, I'm a language model, and I'm using the vocabulary from language analysis to analyze content.<|endoftext|>Cleveland Canyon may be the most ancient canyon
rank 0 sample 0: Hello, I'm a language model, and I love it! :)
My own work is written by Steve DeMille (as is my friend, and
rank 0 sample 1: Hello, I'm a language model, so i'm not going to say why i don't love language learning... I don't want your to be a
rank 0 sample 2: Hello, I'm a language model, so I would say that in computer languages, that is the main purpose of the language.
I think that the core
rank 0 sample 3: Hello, I'm a language model, and like any language, can't just stand tall.
Of course, there are no more languages we can learn now
step 12700, loss: 3.402605, norm:0.3315, lr:1.0021e-04 dt: 56889.14ms, tok/sec:9215.96
step 12701, loss: 3.137341, norm:0.2702, lr:1.0018e-04 dt: 3332.09ms, tok/sec:157345.05
step 12702, loss: 3.207421, norm:0.2828, lr:1.0015e-04 dt: 3332.24ms, tok/sec:157338.11
step 12703, loss: 3.223369, norm:0.3116, lr:1.0012e-04 dt: 3332.00ms, tok/sec:157349.51
step 12704, loss: 3.158595, norm:0.2754, lr:1.0009e-04 dt: 3331.97ms, tok/sec:157350.61
step 12705, loss: 3.219219, norm:0.2780, lr:1.0006e-04 dt: 3331.94ms, tok/sec:157351.95
step 12706, loss: 3.130185, norm:0.2795, lr:1.0003e-04 dt: 3331.97ms, tok/sec:157350.84
step 12707, loss: 3.220395, norm:0.2958, lr:9.9995e-05 dt: 3332.24ms, tok/sec:157337.95
step 12708, loss: 3.224792, norm:0.3273, lr:9.9964e-05 dt: 3332.11ms, tok/sec:157343.92
step 12709, loss: 3.220678, norm:0.2630, lr:9.9934e-05 dt: 3332.07ms, tok/sec:157345.89
step 12710, loss: 3.202151, norm:0.2805, lr:9.9903e-05 dt: 3332.14ms, tok/sec:157342.78
step 12711, loss: 3.209459, norm:0.2687, lr:9.9873e-05 dt: 3332.29ms, tok/sec:157335.49
step 12712, loss: 3.176337, norm:0.2522, lr:9.9842e-05 dt: 3332.02ms, tok/sec:157348.55
step 12713, loss: 3.288284, norm:0.3214, lr:9.9812e-05 dt: 3332.04ms, tok/sec:157347.39
step 12714, loss: 3.198398, norm:0.2726, lr:9.9781e-05 dt: 3332.12ms, tok/sec:157343.47
step 12715, loss: 3.173442, norm:0.2695, lr:9.9751e-05 dt: 3332.10ms, tok/sec:157344.82
step 12716, loss: 3.187919, norm:0.2871, lr:9.9720e-05 dt: 3331.94ms, tok/sec:157351.98
step 12717, loss: 3.186984, norm:0.2716, lr:9.9690e-05 dt: 3331.99ms, tok/sec:157349.97
step 12718, loss: 3.163688, norm:0.2767, lr:9.9659e-05 dt: 3332.08ms, tok/sec:157345.74
step 12719, loss: 3.201414, norm:0.2682, lr:9.9629e-05 dt: 3332.18ms, tok/sec:157340.96
step 12720, loss: 3.196827, norm:0.2676, lr:9.9598e-05 dt: 3332.23ms, tok/sec:157338.49
step 12721, loss: 3.215677, norm:0.2839, lr:9.9568e-05 dt: 3332.01ms, tok/sec:157348.97
step 12722, loss: 3.262865, norm:0.2582, lr:9.9538e-05 dt: 3332.11ms, tok/sec:157344.10
step 12723, loss: 3.211382, norm:0.2696, lr:9.9507e-05 dt: 3332.33ms, tok/sec:157333.98
step 12724, loss: 3.220054, norm:0.2601, lr:9.9477e-05 dt: 3331.82ms, tok/sec:157357.81
step 12725, loss: 3.212101, norm:0.2594, lr:9.9447e-05 dt: 3332.10ms, tok/sec:157344.42
step 12726, loss: 3.169692, norm:0.2522, lr:9.9416e-05 dt: 3332.06ms, tok/sec:157346.46
step 12727, loss: 3.149694, norm:0.2566, lr:9.9386e-05 dt: 3331.97ms, tok/sec:157350.81
step 12728, loss: 3.233166, norm:0.2956, lr:9.9356e-05 dt: 3332.06ms, tok/sec:157346.42
step 12729, loss: 3.318004, norm:0.2796, lr:9.9325e-05 dt: 3332.23ms, tok/sec:157338.41
step 12730, loss: 3.209898, norm:0.2631, lr:9.9295e-05 dt: 3332.19ms, tok/sec:157340.38
step 12731, loss: 3.283287, norm:0.2731, lr:9.9265e-05 dt: 3332.10ms, tok/sec:157344.58
step 12732, loss: 3.209849, norm:0.2945, lr:9.9234e-05 dt: 3332.10ms, tok/sec:157344.40
step 12733, loss: 3.228474, norm:0.2723, lr:9.9204e-05 dt: 3332.37ms, tok/sec:157332.10
step 12734, loss: 3.237026, norm:0.2537, lr:9.9174e-05 dt: 3332.05ms, tok/sec:157347.14
step 12735, loss: 3.220963, norm:0.2705, lr:9.9144e-05 dt: 3332.32ms, tok/sec:157334.06
step 12736, loss: 3.251455, norm:0.2629, lr:9.9113e-05 dt: 3332.26ms, tok/sec:157336.95
step 12737, loss: 3.393098, norm:0.2813, lr:9.9083e-05 dt: 3332.09ms, tok/sec:157345.07
step 12738, loss: 3.196339, norm:0.2652, lr:9.9053e-05 dt: 3332.12ms, tok/sec:157343.57
step 12739, loss: 3.232265, norm:0.2740, lr:9.9023e-05 dt: 3332.27ms, tok/sec:157336.37
step 12740, loss: 3.185696, norm:0.2686, lr:9.8992e-05 dt: 3332.03ms, tok/sec:157347.79
step 12741, loss: 3.131934, norm:0.2478, lr:9.8962e-05 dt: 3331.97ms, tok/sec:157350.81
step 12742, loss: 3.187179, norm:0.2601, lr:9.8932e-05 dt: 3332.06ms, tok/sec:157346.56
step 12743, loss: 3.194376, norm:0.2658, lr:9.8902e-05 dt: 3332.26ms, tok/sec:157336.98
step 12744, loss: 3.202266, norm:0.2526, lr:9.8872e-05 dt: 3331.99ms, tok/sec:157349.82
step 12745, loss: 3.182322, norm:0.2504, lr:9.8842e-05 dt: 3332.08ms, tok/sec:157345.62
step 12746, loss: 3.163741, norm:0.2591, lr:9.8812e-05 dt: 3332.05ms, tok/sec:157347.00
step 12747, loss: 3.234318, norm:0.2750, lr:9.8781e-05 dt: 3332.14ms, tok/sec:157342.82
step 12748, loss: 3.210582, norm:0.2587, lr:9.8751e-05 dt: 3331.77ms, tok/sec:157359.99
step 12749, loss: 3.202342, norm:0.2610, lr:9.8721e-05 dt: 3332.04ms, tok/sec:157347.32
HellaSwag accuracy:6927806923434853457/-2=-3463903461717426688.0000
rank 1 sample 0: Hello, I'm a language model, and my vocabulary is English, so reading is really easy. What I am also doing is studying, and learning English.
rank 1 sample 1: Hello, I'm a language model, a teacher who works with students who can become fluent in every language. I know everyone on the net. I have a
rank 1 sample 2: Hello, I'm a language model, but some languages do not have a native speaker.
In an attempt to define a language as the best language of all
rank 1 sample 3: Hello, I'm a language model, and I'm really excited about this one!!
It gets interesting sometimes as far as one feels right now. So,
rank 0 sample 0: Hello, I'm a language model, and I love it!
"My new school is different, just it looks like my original name. What's a
rank 0 sample 1: Hello, I'm a language model, so there's a lot of different places to learn . Learn a second language!<|endoftext|>In the last blog we looked at
rank 0 sample 2: Hello, I'm a language model, so I had the power to actually implement the project.
My initial thought was the same, but I was wrong about
rank 0 sample 3: Hello, I'm a language model, and how do I get to that level. Thanks.
DNS and the MATH test are on the left here
step 12750, loss: 3.176396, norm:0.2903, lr:9.8691e-05 dt: 48392.00ms, tok/sec:10834.19
step 12751, loss: 3.243572, norm:0.2680, lr:9.8661e-05 dt: 3332.09ms, tok/sec:157345.31
step 12752, loss: 3.203560, norm:0.2566, lr:9.8631e-05 dt: 3332.11ms, tok/sec:157344.16
step 12753, loss: 3.185719, norm:0.2581, lr:9.8601e-05 dt: 3332.55ms, tok/sec:157323.37
step 12754, loss: 3.200222, norm:0.2608, lr:9.8571e-05 dt: 3332.06ms, tok/sec:157346.28
step 12755, loss: 3.192183, norm:0.2418, lr:9.8541e-05 dt: 3332.19ms, tok/sec:157340.21
step 12756, loss: 3.293800, norm:0.2861, lr:9.8511e-05 dt: 3331.99ms, tok/sec:157349.70
step 12757, loss: 3.186413, norm:0.2657, lr:9.8481e-05 dt: 3332.02ms, tok/sec:157348.47
step 12758, loss: 3.213599, norm:0.2593, lr:9.8451e-05 dt: 3332.12ms, tok/sec:157343.83
step 12759, loss: 3.147007, norm:0.2627, lr:9.8421e-05 dt: 3331.90ms, tok/sec:157354.12
step 12760, loss: 3.232918, norm:0.2644, lr:9.8391e-05 dt: 3331.91ms, tok/sec:157353.39
step 12761, loss: 3.164988, norm:0.2812, lr:9.8361e-05 dt: 3332.29ms, tok/sec:157335.72
step 12762, loss: 3.206648, norm:0.2711, lr:9.8331e-05 dt: 3332.27ms, tok/sec:157336.79
step 12763, loss: 3.296263, norm:0.2699, lr:9.8301e-05 dt: 3334.45ms, tok/sec:157233.85
step 12764, loss: 3.176923, norm:0.3156, lr:9.8271e-05 dt: 3332.14ms, tok/sec:157342.83
step 12765, loss: 3.182271, norm:0.2795, lr:9.8241e-05 dt: 3332.24ms, tok/sec:157338.24
step 12766, loss: 3.206944, norm:0.2665, lr:9.8211e-05 dt: 3332.33ms, tok/sec:157333.79
step 12767, loss: 3.119286, norm:0.2882, lr:9.8181e-05 dt: 3332.03ms, tok/sec:157347.70
step 12768, loss: 3.171609, norm:0.2898, lr:9.8151e-05 dt: 3332.04ms, tok/sec:157347.59
step 12769, loss: 3.243255, norm:0.2760, lr:9.8121e-05 dt: 3332.19ms, tok/sec:157340.25
step 12770, loss: 3.188695, norm:0.2974, lr:9.8092e-05 dt: 3332.18ms, tok/sec:157340.95
step 12771, loss: 3.244121, norm:0.2952, lr:9.8062e-05 dt: 3332.07ms, tok/sec:157346.09
step 12772, loss: 3.159602, norm:0.2641, lr:9.8032e-05 dt: 3331.89ms, tok/sec:157354.39
step 12773, loss: 3.219452, norm:0.2892, lr:9.8002e-05 dt: 3332.33ms, tok/sec:157333.98
step 12774, loss: 3.162323, norm:0.2807, lr:9.7972e-05 dt: 3331.99ms, tok/sec:157349.90
step 12775, loss: 3.170249, norm:0.2831, lr:9.7942e-05 dt: 3332.17ms, tok/sec:157341.40
step 12776, loss: 3.261494, norm:0.2793, lr:9.7913e-05 dt: 3332.19ms, tok/sec:157340.31
step 12777, loss: 3.210691, norm:0.3035, lr:9.7883e-05 dt: 3331.93ms, tok/sec:157352.48
step 12778, loss: 3.182696, norm:0.2939, lr:9.7853e-05 dt: 3332.06ms, tok/sec:157346.60
step 12779, loss: 3.201664, norm:0.2565, lr:9.7823e-05 dt: 3331.81ms, tok/sec:157358.21
step 12780, loss: 3.172419, norm:0.2842, lr:9.7793e-05 dt: 3332.09ms, tok/sec:157345.09
step 12781, loss: 3.238561, norm:0.2775, lr:9.7764e-05 dt: 3331.96ms, tok/sec:157351.44
step 12782, loss: 3.197564, norm:0.2590, lr:9.7734e-05 dt: 3332.44ms, tok/sec:157328.37
step 12783, loss: 3.203488, norm:0.2498, lr:9.7704e-05 dt: 3332.02ms, tok/sec:157348.38
step 12784, loss: 3.241495, norm:0.3024, lr:9.7675e-05 dt: 3332.06ms, tok/sec:157346.32
step 12785, loss: 3.208105, norm:0.2694, lr:9.7645e-05 dt: 3332.03ms, tok/sec:157348.04
step 12786, loss: 3.306538, norm:0.2773, lr:9.7615e-05 dt: 3332.17ms, tok/sec:157341.33
step 12787, loss: 3.129110, norm:0.2563, lr:9.7585e-05 dt: 3332.01ms, tok/sec:157349.01
step 12788, loss: 3.155720, norm:0.2750, lr:9.7556e-05 dt: 3332.10ms, tok/sec:157344.62
step 12789, loss: 3.164045, norm:0.2680, lr:9.7526e-05 dt: 3331.93ms, tok/sec:157352.64
step 12790, loss: 3.208758, norm:0.2567, lr:9.7496e-05 dt: 3332.01ms, tok/sec:157348.65
step 12791, loss: 3.143611, norm:0.2573, lr:9.7467e-05 dt: 3332.15ms, tok/sec:157342.26
step 12792, loss: 3.179127, norm:0.2526, lr:9.7437e-05 dt: 3332.20ms, tok/sec:157339.95
step 12793, loss: 3.195626, norm:0.2642, lr:9.7408e-05 dt: 3332.12ms, tok/sec:157343.51
step 12794, loss: 3.174877, norm:0.2623, lr:9.7378e-05 dt: 3332.23ms, tok/sec:157338.36
step 12795, loss: 3.187602, norm:0.2507, lr:9.7348e-05 dt: 3331.92ms, tok/sec:157352.91
step 12796, loss: 3.189795, norm:0.2706, lr:9.7319e-05 dt: 3332.02ms, tok/sec:157348.45
step 12797, loss: 3.257660, norm:0.2996, lr:9.7289e-05 dt: 3331.84ms, tok/sec:157357.11
step 12798, loss: 3.198564, norm:0.2533, lr:9.7260e-05 dt: 3332.29ms, tok/sec:157335.48
step 12799, loss: 3.189193, norm:0.2787, lr:9.7230e-05 dt: 3332.23ms, tok/sec:157338.29
validation loss: 3.2111
Model and optimizer state saved.
HellaSwag accuracy:-2295565113419922351/-2=1147782556709961216.0000
rank 1 sample 0: Hello, I'm a language model, and you can't learn a language (except, like some in the language category). But if someone is trying to learn
rank 1 sample 1: Hello, I'm a language model, a teacher. As a teacher, I’m in the act of trying to provide learning and teaching opportunities for my
rank 1 sample 2: Hello, I'm a language model, but even I was a language learner. I still had difficulties learning a language, but I can learn a different way
rank 1 sample 3: Hello, I'm a language model, and I'm writing this guide for you through the grammar guides in Hindi. They are comprehensive, step-by-step
rank 0 sample 0: Hello, I'm a language model, and I'll be talking about this topic over the next few days on November 11th, here are my comments. And
rank 0 sample 1: Hello, I'm a language model, so there's a lot of language building in it which would make this difficult and I don't want to do that.
rank 0 sample 2: Hello, I'm a language model, so I understand it differently for certain words. This is a lot, because I am a language model, and it sounds
rank 0 sample 3: Hello, I'm a language model, and what I'm doing is trying to make everything in my system. The way to do that is if I have,
step 12800, loss: 3.232030, norm:0.2770, lr:9.7200e-05 dt: 56052.85ms, tok/sec:9353.46
step 12801, loss: 3.161255, norm:0.2761, lr:9.7171e-05 dt: 3332.19ms, tok/sec:157340.22
step 12802, loss: 3.148005, norm:0.3511, lr:9.7141e-05 dt: 3331.97ms, tok/sec:157350.54
step 12803, loss: 3.257056, norm:0.2816, lr:9.7112e-05 dt: 3332.02ms, tok/sec:157348.27
step 12804, loss: 3.202445, norm:0.2724, lr:9.7082e-05 dt: 3332.00ms, tok/sec:157349.27
step 12805, loss: 3.160309, norm:0.2875, lr:9.7053e-05 dt: 3332.50ms, tok/sec:157325.95
step 12806, loss: 3.162262, norm:0.2757, lr:9.7023e-05 dt: 3332.19ms, tok/sec:157340.38
step 12807, loss: 3.226748, norm:0.2714, lr:9.6994e-05 dt: 3332.07ms, tok/sec:157345.80
step 12808, loss: 3.225039, norm:0.2632, lr:9.6964e-05 dt: 3332.01ms, tok/sec:157348.98
step 12809, loss: 3.256572, norm:0.2673, lr:9.6935e-05 dt: 3332.19ms, tok/sec:157340.23
step 12810, loss: 3.180027, norm:0.2653, lr:9.6906e-05 dt: 3331.86ms, tok/sec:157355.91
step 12811, loss: 3.197814, norm:0.2934, lr:9.6876e-05 dt: 3331.94ms, tok/sec:157352.05
step 12812, loss: 3.280374, norm:0.2780, lr:9.6847e-05 dt: 3332.03ms, tok/sec:157347.90
step 12813, loss: 3.223942, norm:0.2704, lr:9.6817e-05 dt: 3331.94ms, tok/sec:157352.32
step 12814, loss: 3.226896, norm:0.2679, lr:9.6788e-05 dt: 3332.27ms, tok/sec:157336.70
step 12815, loss: 3.231112, norm:0.2764, lr:9.6759e-05 dt: 3331.97ms, tok/sec:157350.84
step 12816, loss: 3.177547, norm:0.2432, lr:9.6729e-05 dt: 3332.04ms, tok/sec:157347.49
step 12817, loss: 3.254004, norm:0.2858, lr:9.6700e-05 dt: 3332.22ms, tok/sec:157339.12
step 12818, loss: 3.148339, norm:0.2642, lr:9.6670e-05 dt: 3331.97ms, tok/sec:157350.95
step 12819, loss: 3.190064, norm:0.2688, lr:9.6641e-05 dt: 3331.91ms, tok/sec:157353.57
step 12820, loss: 3.198809, norm:0.2771, lr:9.6612e-05 dt: 3332.17ms, tok/sec:157341.09
step 12821, loss: 3.263386, norm:0.3097, lr:9.6582e-05 dt: 3331.99ms, tok/sec:157349.73
step 12822, loss: 3.151597, norm:0.2794, lr:9.6553e-05 dt: 3332.11ms, tok/sec:157344.02
step 12823, loss: 3.162969, norm:0.2602, lr:9.6524e-05 dt: 3331.99ms, tok/sec:157349.87
step 12824, loss: 3.182607, norm:0.2636, lr:9.6495e-05 dt: 3332.16ms, tok/sec:157341.91
step 12825, loss: 3.192034, norm:0.2696, lr:9.6465e-05 dt: 3332.25ms, tok/sec:157337.62
step 12826, loss: 3.112811, norm:0.2552, lr:9.6436e-05 dt: 3331.82ms, tok/sec:157357.85
step 12827, loss: 3.150058, norm:0.2483, lr:9.6407e-05 dt: 3332.09ms, tok/sec:157344.99
step 12828, loss: 3.205091, norm:0.2594, lr:9.6378e-05 dt: 3332.03ms, tok/sec:157347.89
step 12829, loss: 3.182144, norm:0.2568, lr:9.6348e-05 dt: 3331.88ms, tok/sec:157354.92
step 12830, loss: 3.154823, norm:0.2658, lr:9.6319e-05 dt: 3332.13ms, tok/sec:157343.21
step 12831, loss: 3.188275, norm:0.2648, lr:9.6290e-05 dt: 3332.02ms, tok/sec:157348.19
step 12832, loss: 3.212568, norm:0.2668, lr:9.6261e-05 dt: 3331.94ms, tok/sec:157352.23
step 12833, loss: 3.159538, norm:0.2599, lr:9.6231e-05 dt: 3332.35ms, tok/sec:157332.85
step 12834, loss: 3.171082, norm:0.2737, lr:9.6202e-05 dt: 3332.00ms, tok/sec:157349.42
step 12835, loss: 3.246236, norm:0.3294, lr:9.6173e-05 dt: 3332.12ms, tok/sec:157343.69
step 12836, loss: 3.302846, norm:0.3089, lr:9.6144e-05 dt: 3332.03ms, tok/sec:157347.94
step 12837, loss: 3.225676, norm:0.3031, lr:9.6115e-05 dt: 3332.19ms, tok/sec:157340.26
step 12838, loss: 3.185692, norm:0.2801, lr:9.6086e-05 dt: 3332.03ms, tok/sec:157347.76
step 12839, loss: 3.236820, norm:0.2676, lr:9.6057e-05 dt: 3331.95ms, tok/sec:157351.62
step 12840, loss: 3.217072, norm:0.2795, lr:9.6027e-05 dt: 3331.97ms, tok/sec:157350.62
step 12841, loss: 3.209601, norm:0.2806, lr:9.5998e-05 dt: 3332.26ms, tok/sec:157337.27
step 12842, loss: 3.217807, norm:0.2743, lr:9.5969e-05 dt: 3332.09ms, tok/sec:157345.09
step 12843, loss: 3.136123, norm:0.2844, lr:9.5940e-05 dt: 3331.99ms, tok/sec:157349.95
step 12844, loss: 3.292179, norm:0.2807, lr:9.5911e-05 dt: 3332.22ms, tok/sec:157339.12
step 12845, loss: 3.226514, norm:0.2826, lr:9.5882e-05 dt: 3332.09ms, tok/sec:157345.09
step 12846, loss: 3.166769, norm:0.2652, lr:9.5853e-05 dt: 3332.15ms, tok/sec:157342.11
step 12847, loss: 3.161710, norm:0.2626, lr:9.5824e-05 dt: 3331.86ms, tok/sec:157356.08
step 12848, loss: 3.197623, norm:0.2562, lr:9.5795e-05 dt: 3332.14ms, tok/sec:157342.54
step 12849, loss: 3.206880, norm:0.2632, lr:9.5766e-05 dt: 3332.25ms, tok/sec:157337.34
HellaSwag accuracy:-2295459560303655855/-2=1147729780151827968.0000
rank 1 sample 0: Hello, I'm a language model, and so I'm sure that's probably a great way to practice.
When I say the term "language model"
rank 1 sample 1: Hello, I'm a language model, not an English professor. I want to find out what this is, and where I’d be able to find
rank 1 sample 2: Hello, I'm a language model, but then what does that mean?
I'm a language model
It's a language model in that I don't
rank 1 sample 3: Hello, I'm a language model, and I'm very happy with it. Every time I find that this is happening, people make fun of me. I
rank 0 sample 0: Hello, I'm a language model, and I am a model for learning language-on-a-chip. And I can't be the type of programmer
rank 0 sample 1: Hello, I'm a language model, I understand that a language model, though, doesn't guarantee the success of an algorithm or a set of instructions. It
rank 0 sample 2: Hello, I'm a language model, so I love the people's language and I hate to say this about people. I hate to say this about the women
rank 0 sample 3: Hello, I'm a language model, and now I'll be able to get into things.
Okay, we've discussed all about how programming languages can solve
step 12850, loss: 3.224643, norm:0.2535, lr:9.5737e-05 dt: 48387.83ms, tok/sec:10835.12
step 12851, loss: 3.197312, norm:0.2491, lr:9.5708e-05 dt: 3331.87ms, tok/sec:157355.62
step 12852, loss: 3.209951, norm:0.2530, lr:9.5679e-05 dt: 3332.16ms, tok/sec:157341.98
step 12853, loss: 3.168173, norm:0.2455, lr:9.5650e-05 dt: 3332.28ms, tok/sec:157335.99
step 12854, loss: 3.235464, norm:0.2781, lr:9.5621e-05 dt: 3331.90ms, tok/sec:157354.10
step 12855, loss: 3.171572, norm:0.2437, lr:9.5592e-05 dt: 3331.97ms, tok/sec:157350.53
step 12856, loss: 3.299181, norm:0.2775, lr:9.5563e-05 dt: 3332.08ms, tok/sec:157345.38
step 12857, loss: 3.194134, norm:0.2598, lr:9.5534e-05 dt: 3332.06ms, tok/sec:157346.69
step 12858, loss: 3.202582, norm:0.2613, lr:9.5505e-05 dt: 3332.00ms, tok/sec:157349.26
step 12859, loss: 3.133011, norm:0.2623, lr:9.5476e-05 dt: 3332.03ms, tok/sec:157347.73
step 12860, loss: 3.120472, norm:0.2800, lr:9.5448e-05 dt: 3332.19ms, tok/sec:157340.23
step 12861, loss: 3.225608, norm:0.2660, lr:9.5419e-05 dt: 3331.92ms, tok/sec:157352.92
step 12862, loss: 3.178040, norm:0.2618, lr:9.5390e-05 dt: 3332.21ms, tok/sec:157339.20
step 12863, loss: 3.147418, norm:0.3061, lr:9.5361e-05 dt: 3332.08ms, tok/sec:157345.40
step 12864, loss: 3.114614, norm:0.2691, lr:9.5332e-05 dt: 3332.07ms, tok/sec:157345.80
step 12865, loss: 3.142095, norm:0.2677, lr:9.5303e-05 dt: 3332.05ms, tok/sec:157346.79
step 12866, loss: 3.232810, norm:0.2883, lr:9.5274e-05 dt: 3332.15ms, tok/sec:157342.39
step 12867, loss: 3.169979, norm:0.2583, lr:9.5246e-05 dt: 3332.65ms, tok/sec:157318.53
step 12868, loss: 3.165514, norm:0.2545, lr:9.5217e-05 dt: 3332.15ms, tok/sec:157342.02
step 12869, loss: 3.145384, norm:0.2414, lr:9.5188e-05 dt: 3332.01ms, tok/sec:157349.00
step 12870, loss: 3.213594, norm:0.2958, lr:9.5159e-05 dt: 3332.04ms, tok/sec:157347.27
step 12871, loss: 3.193545, norm:0.2900, lr:9.5130e-05 dt: 3332.04ms, tok/sec:157347.36
step 12872, loss: 3.219666, norm:0.2714, lr:9.5102e-05 dt: 3332.50ms, tok/sec:157325.61
step 12873, loss: 3.249005, norm:0.3046, lr:9.5073e-05 dt: 3332.03ms, tok/sec:157348.12
step 12874, loss: 3.194254, norm:0.2790, lr:9.5044e-05 dt: 3332.30ms, tok/sec:157335.05
step 12875, loss: 3.358643, norm:0.3256, lr:9.5015e-05 dt: 3332.21ms, tok/sec:157339.38
step 12876, loss: 3.167516, norm:0.2761, lr:9.4987e-05 dt: 3332.15ms, tok/sec:157342.28
step 12877, loss: 3.177986, norm:0.2892, lr:9.4958e-05 dt: 3331.88ms, tok/sec:157355.21
step 12878, loss: 3.213170, norm:0.2977, lr:9.4929e-05 dt: 3332.05ms, tok/sec:157347.00
step 12879, loss: 3.211067, norm:0.3040, lr:9.4901e-05 dt: 3332.02ms, tok/sec:157348.49
step 12880, loss: 3.234511, norm:0.2955, lr:9.4872e-05 dt: 3332.07ms, tok/sec:157346.13
step 12881, loss: 3.203793, norm:0.4365, lr:9.4843e-05 dt: 3332.14ms, tok/sec:157342.58
step 12882, loss: 3.196067, norm:0.2750, lr:9.4815e-05 dt: 3332.00ms, tok/sec:157349.15
step 12883, loss: 3.215308, norm:0.2938, lr:9.4786e-05 dt: 3332.05ms, tok/sec:157346.76
step 12884, loss: 3.217294, norm:0.3021, lr:9.4757e-05 dt: 3332.10ms, tok/sec:157344.75
step 12885, loss: 3.247985, norm:0.2529, lr:9.4729e-05 dt: 3332.26ms, tok/sec:157336.97
step 12886, loss: 3.238045, norm:0.2948, lr:9.4700e-05 dt: 3332.04ms, tok/sec:157347.54
step 12887, loss: 3.184539, norm:0.2956, lr:9.4672e-05 dt: 3331.95ms, tok/sec:157351.88
step 12888, loss: 3.223460, norm:0.2811, lr:9.4643e-05 dt: 3332.21ms, tok/sec:157339.43
step 12889, loss: 3.176361, norm:0.2822, lr:9.4614e-05 dt: 3332.21ms, tok/sec:157339.43
step 12890, loss: 3.218843, norm:0.2683, lr:9.4586e-05 dt: 3332.02ms, tok/sec:157348.40
step 12891, loss: 3.175510, norm:0.2592, lr:9.4557e-05 dt: 3332.04ms, tok/sec:157347.40
step 12892, loss: 3.243522, norm:0.2742, lr:9.4529e-05 dt: 3332.01ms, tok/sec:157349.08
step 12893, loss: 3.233310, norm:0.2944, lr:9.4500e-05 dt: 3331.91ms, tok/sec:157353.41
step 12894, loss: 3.200714, norm:0.2871, lr:9.4472e-05 dt: 3332.04ms, tok/sec:157347.45
step 12895, loss: 3.279168, norm:0.2883, lr:9.4443e-05 dt: 3332.00ms, tok/sec:157349.32
step 12896, loss: 3.210002, norm:0.2633, lr:9.4415e-05 dt: 3332.29ms, tok/sec:157335.63
step 12897, loss: 3.131316, norm:0.2837, lr:9.4386e-05 dt: 3332.02ms, tok/sec:157348.51
step 12898, loss: 3.189019, norm:0.2631, lr:9.4358e-05 dt: 3332.42ms, tok/sec:157329.54
step 12899, loss: 3.208412, norm:0.3001, lr:9.4329e-05 dt: 3331.95ms, tok/sec:157351.77
validation loss: 3.2104
Model and optimizer state saved.
HellaSwag accuracy:6927806923434855505/-2=-3463903461717427712.0000
rank 1 sample 0: Hello, I'm a language model, and this is what I want to avoid."
He and Mark are currently finishing their project in preparation for the webinar
rank 1 sample 1: Hello, I'm a language model, a person with a great idea for learning things. I wanted to build a machine, how I create it, how I
rank 1 sample 2: Hello, I'm a language model, but sometimes, as I say, you can't write code yourself." (Emma, thank you, for you too
rank 1 sample 3: Hello, I'm a language model, and I'm really happy
In your brain, all good speakers were not written in Japanese. They were written in the
rank 0 sample 0: Hello, I'm a language model, and I think I can get my word with as much as I could say that I want to, I'll just go
rank 0 sample 1: Hello, I'm a language model, so i dont know where i'd feel if you know exactly how to look after the rest of my English Language.<|endoftext|>
rank 0 sample 2: Hello, I'm a language model, I'm also a professional voice or video player I'm a student in my class.
I'm a teacher in the
rank 0 sample 3: Hello, I'm a language model, and not a machine. I have no clue
- I should get into some weird, I don't know how big
step 12900, loss: 3.118280, norm:0.2640, lr:9.4301e-05 dt: 56120.88ms, tok/sec:9342.12
step 12901, loss: 3.119154, norm:0.2875, lr:9.4272e-05 dt: 3331.97ms, tok/sec:157350.77
step 12902, loss: 3.143547, norm:0.2585, lr:9.4244e-05 dt: 3332.31ms, tok/sec:157334.84
step 12903, loss: 3.173443, norm:0.2561, lr:9.4215e-05 dt: 3332.08ms, tok/sec:157345.61
step 12904, loss: 3.155256, norm:0.2613, lr:9.4187e-05 dt: 3331.95ms, tok/sec:157351.62
step 12905, loss: 3.208272, norm:0.2775, lr:9.4159e-05 dt: 3331.90ms, tok/sec:157354.08
step 12906, loss: 3.198385, norm:0.2966, lr:9.4130e-05 dt: 3332.28ms, tok/sec:157335.90
step 12907, loss: 3.275437, norm:0.2711, lr:9.4102e-05 dt: 3332.10ms, tok/sec:157344.80
step 12908, loss: 3.218833, norm:0.2942, lr:9.4074e-05 dt: 3332.18ms, tok/sec:157340.91
step 12909, loss: 3.219474, norm:0.2683, lr:9.4045e-05 dt: 3331.95ms, tok/sec:157351.55
step 12910, loss: 3.230491, norm:0.2673, lr:9.4017e-05 dt: 3331.97ms, tok/sec:157350.69
step 12911, loss: 3.178041, norm:0.2696, lr:9.3989e-05 dt: 3332.28ms, tok/sec:157335.90
step 12912, loss: 3.379020, norm:0.3154, lr:9.3960e-05 dt: 3331.93ms, tok/sec:157352.77
step 12913, loss: 3.218013, norm:0.2571, lr:9.3932e-05 dt: 3332.26ms, tok/sec:157336.91
step 12914, loss: 3.216242, norm:0.2628, lr:9.3904e-05 dt: 3332.13ms, tok/sec:157343.05
step 12915, loss: 3.262253, norm:0.2797, lr:9.3875e-05 dt: 3332.17ms, tok/sec:157341.44
step 12916, loss: 3.232631, norm:0.2876, lr:9.3847e-05 dt: 3332.05ms, tok/sec:157347.01
step 12917, loss: 3.201959, norm:0.2779, lr:9.3819e-05 dt: 3332.06ms, tok/sec:157346.49
step 12918, loss: 3.195358, norm:0.2783, lr:9.3791e-05 dt: 3332.13ms, tok/sec:157343.23
step 12919, loss: 3.212376, norm:0.2844, lr:9.3762e-05 dt: 3332.23ms, tok/sec:157338.53
step 12920, loss: 3.191519, norm:0.2737, lr:9.3734e-05 dt: 3332.20ms, tok/sec:157339.84
step 12921, loss: 3.188012, norm:0.2603, lr:9.3706e-05 dt: 3332.02ms, tok/sec:157348.36
step 12922, loss: 3.214954, norm:0.2699, lr:9.3678e-05 dt: 3332.15ms, tok/sec:157342.32
step 12923, loss: 3.171981, norm:0.2613, lr:9.3649e-05 dt: 3332.11ms, tok/sec:157344.23
step 12924, loss: 3.184413, norm:0.2620, lr:9.3621e-05 dt: 3332.00ms, tok/sec:157349.40
step 12925, loss: 3.223881, norm:0.2572, lr:9.3593e-05 dt: 3332.10ms, tok/sec:157344.63
step 12926, loss: 3.192017, norm:0.2657, lr:9.3565e-05 dt: 3332.09ms, tok/sec:157345.24
step 12927, loss: 3.189683, norm:0.2692, lr:9.3537e-05 dt: 3332.49ms, tok/sec:157325.99
step 12928, loss: 3.140201, norm:0.2602, lr:9.3509e-05 dt: 3331.94ms, tok/sec:157352.35
step 12929, loss: 3.144908, norm:0.2678, lr:9.3480e-05 dt: 3331.99ms, tok/sec:157350.04
step 12930, loss: 3.115901, norm:0.2596, lr:9.3452e-05 dt: 3332.10ms, tok/sec:157344.39
step 12931, loss: 3.182812, norm:0.2690, lr:9.3424e-05 dt: 3332.08ms, tok/sec:157345.47
step 12932, loss: 3.189994, norm:0.2694, lr:9.3396e-05 dt: 3332.11ms, tok/sec:157344.17
step 12933, loss: 3.178083, norm:0.2803, lr:9.3368e-05 dt: 3331.87ms, tok/sec:157355.61
step 12934, loss: 3.175574, norm:0.2630, lr:9.3340e-05 dt: 3331.99ms, tok/sec:157350.03
step 12935, loss: 3.107881, norm:0.2572, lr:9.3312e-05 dt: 3332.46ms, tok/sec:157327.71
step 12936, loss: 3.180498, norm:0.2716, lr:9.3284e-05 dt: 3331.95ms, tok/sec:157351.67
step 12937, loss: 3.158017, norm:0.2608, lr:9.3256e-05 dt: 3332.04ms, tok/sec:157347.29
step 12938, loss: 3.204161, norm:0.2534, lr:9.3228e-05 dt: 3332.11ms, tok/sec:157344.05
step 12939, loss: 3.210300, norm:0.2684, lr:9.3200e-05 dt: 3332.05ms, tok/sec:157347.03
step 12940, loss: 3.199577, norm:0.2560, lr:9.3172e-05 dt: 3332.11ms, tok/sec:157343.99
step 12941, loss: 3.215332, norm:0.2882, lr:9.3144e-05 dt: 3331.98ms, tok/sec:157350.26
step 12942, loss: 3.187006, norm:0.3021, lr:9.3116e-05 dt: 3332.00ms, tok/sec:157349.58
step 12943, loss: 3.256019, norm:0.3060, lr:9.3088e-05 dt: 3332.30ms, tok/sec:157335.13
step 12944, loss: 3.241291, norm:0.2961, lr:9.3060e-05 dt: 3331.82ms, tok/sec:157358.01
step 12945, loss: 3.252196, norm:0.2703, lr:9.3032e-05 dt: 3332.26ms, tok/sec:157336.88
step 12946, loss: 3.174206, norm:0.2888, lr:9.3004e-05 dt: 3332.23ms, tok/sec:157338.65
step 12947, loss: 3.218552, norm:0.2668, lr:9.2976e-05 dt: 3332.23ms, tok/sec:157338.43
step 12948, loss: 3.218678, norm:0.2816, lr:9.2948e-05 dt: 3332.23ms, tok/sec:157338.55
step 12949, loss: 3.226900, norm:0.3012, lr:9.2920e-05 dt: 3332.22ms, tok/sec:157339.17
HellaSwag accuracy:6927947660923210833/-2=-3463973830461605376.0000
rank 1 sample 0: Hello, I'm a language model, and a colleague in the company that understands it so that I can use it to make things more flexible.
I'm
rank 1 sample 1: Hello, I'm a language model, which means I think I'm a teacher. I'm doing a whole bunch of computer exercises and, of course, I
rank 1 sample 2: Hello, I'm a language model, but sometimes, that's not the case.
One day there were some small groups of people coming up and telling us
rank 1 sample 3: Hello, I'm a language model, and I'm looking at programming languages from within the family but do nothing at the same time. It's a pretty cool
rank 0 sample 0: Hello, I'm a language model, and I think I can teach people what should be happening when you want to talk to me. If someone doesn't have
rank 0 sample 1: Hello, I'm a language model, but now I'm learning to write (and learn...) on my own. It's just one thing and you can't
rank 0 sample 2: Hello, I'm a language model, so I should start speaking, since you're really limited to my mother tongue. That's a lot of people who really
rank 0 sample 3: Hello, I'm a language model, you get a lot of new words. So at the end of the day you want to create a machine language that�
step 12950, loss: 3.269509, norm:0.2696, lr:9.2892e-05 dt: 48399.20ms, tok/sec:10832.58
step 12951, loss: 3.188057, norm:0.3061, lr:9.2864e-05 dt: 3332.05ms, tok/sec:157347.20
step 12952, loss: 3.220767, norm:0.2711, lr:9.2836e-05 dt: 3332.04ms, tok/sec:157347.45
step 12953, loss: 3.254072, norm:0.2619, lr:9.2808e-05 dt: 3334.36ms, tok/sec:157237.82
step 12954, loss: 3.268978, norm:0.2740, lr:9.2781e-05 dt: 3332.27ms, tok/sec:157336.59
step 12955, loss: 3.156744, norm:0.2631, lr:9.2753e-05 dt: 3332.08ms, tok/sec:157345.59
step 12956, loss: 3.225616, norm:0.2684, lr:9.2725e-05 dt: 3331.92ms, tok/sec:157352.98
step 12957, loss: 3.203477, norm:0.2854, lr:9.2697e-05 dt: 3332.05ms, tok/sec:157347.20
step 12958, loss: 3.171742, norm:0.2600, lr:9.2669e-05 dt: 3332.26ms, tok/sec:157337.21
step 12959, loss: 3.184013, norm:0.2623, lr:9.2642e-05 dt: 3331.81ms, tok/sec:157358.40
step 12960, loss: 3.215187, norm:0.2641, lr:9.2614e-05 dt: 3332.05ms, tok/sec:157346.90
step 12961, loss: 3.203578, norm:0.2766, lr:9.2586e-05 dt: 3332.12ms, tok/sec:157343.47
step 12962, loss: 3.178844, norm:0.2654, lr:9.2558e-05 dt: 3331.90ms, tok/sec:157354.24
step 12963, loss: 3.260322, norm:0.2732, lr:9.2530e-05 dt: 3331.99ms, tok/sec:157349.69
step 12964, loss: 3.223155, norm:0.2557, lr:9.2503e-05 dt: 3332.12ms, tok/sec:157343.78
step 12965, loss: 3.163753, norm:0.2803, lr:9.2475e-05 dt: 3332.35ms, tok/sec:157332.98
step 12966, loss: 3.170277, norm:0.2505, lr:9.2447e-05 dt: 3332.00ms, tok/sec:157349.52
step 12967, loss: 3.141013, norm:0.2681, lr:9.2419e-05 dt: 3332.00ms, tok/sec:157349.38
step 12968, loss: 3.152997, norm:0.2636, lr:9.2392e-05 dt: 3331.95ms, tok/sec:157351.47
step 12969, loss: 3.197344, norm:0.2670, lr:9.2364e-05 dt: 3332.06ms, tok/sec:157346.54
step 12970, loss: 3.220836, norm:0.2750, lr:9.2336e-05 dt: 3332.04ms, tok/sec:157347.45
step 12971, loss: 3.096281, norm:0.2546, lr:9.2309e-05 dt: 3331.96ms, tok/sec:157351.22
step 12972, loss: 3.145957, norm:0.2573, lr:9.2281e-05 dt: 3332.00ms, tok/sec:157349.34
step 12973, loss: 3.137646, norm:0.2538, lr:9.2253e-05 dt: 3332.47ms, tok/sec:157327.06
step 12974, loss: 3.185507, norm:0.2641, lr:9.2226e-05 dt: 3332.10ms, tok/sec:157344.57
step 12975, loss: 3.161834, norm:0.2563, lr:9.2198e-05 dt: 3332.20ms, tok/sec:157339.81
step 12976, loss: 3.145785, norm:0.2524, lr:9.2171e-05 dt: 3331.83ms, tok/sec:157357.19
step 12977, loss: 3.208507, norm:0.2611, lr:9.2143e-05 dt: 3332.06ms, tok/sec:157346.74
step 12978, loss: 3.163261, norm:0.2598, lr:9.2115e-05 dt: 3332.18ms, tok/sec:157340.70
step 12979, loss: 3.207499, norm:0.2617, lr:9.2088e-05 dt: 3331.88ms, tok/sec:157355.07
step 12980, loss: 3.233422, norm:0.2815, lr:9.2060e-05 dt: 3331.85ms, tok/sec:157356.23
step 12981, loss: 3.186319, norm:0.2590, lr:9.2033e-05 dt: 3332.22ms, tok/sec:157339.05
step 12982, loss: 3.226147, norm:0.2565, lr:9.2005e-05 dt: 3332.22ms, tok/sec:157338.94
step 12983, loss: 3.233718, norm:0.2732, lr:9.1978e-05 dt: 3332.01ms, tok/sec:157348.98
step 12984, loss: 3.308638, norm:0.2987, lr:9.1950e-05 dt: 3332.20ms, tok/sec:157339.95
step 12985, loss: 3.229983, norm:0.2902, lr:9.1923e-05 dt: 3332.02ms, tok/sec:157348.34
step 12986, loss: 3.194691, norm:0.2884, lr:9.1895e-05 dt: 3332.33ms, tok/sec:157333.69
step 12987, loss: 3.283439, norm:0.2727, lr:9.1868e-05 dt: 3332.16ms, tok/sec:157341.69
step 12988, loss: 3.245512, norm:0.2879, lr:9.1840e-05 dt: 3332.07ms, tok/sec:157346.16
step 12989, loss: 3.198920, norm:0.2699, lr:9.1813e-05 dt: 3332.04ms, tok/sec:157347.66
step 12990, loss: 3.171063, norm:0.2654, lr:9.1785e-05 dt: 3332.15ms, tok/sec:157342.35
step 12991, loss: 3.181582, norm:0.2709, lr:9.1758e-05 dt: 3332.21ms, tok/sec:157339.26
step 12992, loss: 3.198752, norm:0.2853, lr:9.1730e-05 dt: 3332.03ms, tok/sec:157348.00
step 12993, loss: 3.189167, norm:0.2772, lr:9.1703e-05 dt: 3332.17ms, tok/sec:157341.09
step 12994, loss: 3.195262, norm:0.2706, lr:9.1675e-05 dt: 3331.99ms, tok/sec:157349.65
step 12995, loss: 3.249063, norm:0.2692, lr:9.1648e-05 dt: 3331.99ms, tok/sec:157349.60
step 12996, loss: 3.236220, norm:0.2890, lr:9.1621e-05 dt: 3331.92ms, tok/sec:157353.18
step 12997, loss: 3.204110, norm:0.2707, lr:9.1593e-05 dt: 3332.00ms, tok/sec:157349.43
step 12998, loss: 3.234703, norm:0.2654, lr:9.1566e-05 dt: 3332.06ms, tok/sec:157346.72
step 12999, loss: 3.221450, norm:0.2842, lr:9.1538e-05 dt: 3332.07ms, tok/sec:157346.01
validation loss: 3.2084
Model and optimizer state saved.
HellaSwag accuracy:6928932823341960273/-2=-3464466411670980096.0000
rank 1 sample 0: Hello, I'm a language model, and that's what I want to demonstrate to this blog. To do that, I've come down to you.

rank 1 sample 1: Hello, I'm a language model, a teacher. One of the first things I learned was writing a text I'm comfortable giving myself to the teacher. I
rank 1 sample 2: Hello, I'm a language model, but even though I'm not a language model, I am. So what I'm trying to do is, let me
rank 1 sample 3: Hello, I'm a language model, and I'm pretty good at making some. I'd get on Facebook as a guest as well.
I can't
rank 0 sample 0: Hello, I'm a language model, and I think I can teach myself more."
That's why one of those students was invited to come out of class
rank 0 sample 1: Hello, I'm a language model, I'd like to be able to go to college all alone, because one can learn and do things differently from the other
rank 0 sample 2: Hello, I'm a language model, so I do lots of grammar and understanding.<|endoftext|>Facts of World War II
World War II was a major industrial
rank 0 sample 3: Hello, I'm a language model, and how to get started. I need a dictionary to understand this. The best definition is a language model. It tells
step 13000, loss: 3.147230, norm:0.2611, lr:9.1511e-05 dt: 56123.25ms, tok/sec:9341.73
step 13001, loss: 3.159185, norm:0.2750, lr:9.1484e-05 dt: 3332.07ms, tok/sec:157346.07
step 13002, loss: 3.143613, norm:0.2558, lr:9.1456e-05 dt: 3332.18ms, tok/sec:157340.70
step 13003, loss: 3.226074, norm:0.2681, lr:9.1429e-05 dt: 3332.20ms, tok/sec:157339.70
step 13004, loss: 3.222347, norm:0.2681, lr:9.1402e-05 dt: 3332.15ms, tok/sec:157342.41
step 13005, loss: 3.150696, norm:0.2474, lr:9.1375e-05 dt: 3332.30ms, tok/sec:157335.15
step 13006, loss: 3.155779, norm:0.2543, lr:9.1347e-05 dt: 3331.96ms, tok/sec:157351.12
step 13007, loss: 3.180277, norm:0.2692, lr:9.1320e-05 dt: 3332.05ms, tok/sec:157346.91
step 13008, loss: 3.154869, norm:0.2566, lr:9.1293e-05 dt: 3331.96ms, tok/sec:157351.29
step 13009, loss: 3.187462, norm:0.2568, lr:9.1265e-05 dt: 3331.94ms, tok/sec:157352.15
step 13010, loss: 3.181696, norm:0.2700, lr:9.1238e-05 dt: 3331.93ms, tok/sec:157352.63
step 13011, loss: 3.223547, norm:0.2622, lr:9.1211e-05 dt: 3332.05ms, tok/sec:157346.81
step 13012, loss: 3.228184, norm:0.3047, lr:9.1184e-05 dt: 3332.09ms, tok/sec:157345.02
step 13013, loss: 3.166638, norm:0.2715, lr:9.1157e-05 dt: 3332.26ms, tok/sec:157337.15
step 13014, loss: 3.151103, norm:0.2630, lr:9.1129e-05 dt: 3331.97ms, tok/sec:157350.71
step 13015, loss: 3.337832, norm:0.3968, lr:9.1102e-05 dt: 3331.89ms, tok/sec:157354.35
step 13016, loss: 3.164212, norm:0.2800, lr:9.1075e-05 dt: 3332.29ms, tok/sec:157335.82
step 13017, loss: 3.243596, norm:0.2932, lr:9.1048e-05 dt: 3332.21ms, tok/sec:157339.38
step 13018, loss: 3.213143, norm:0.2912, lr:9.1021e-05 dt: 3331.94ms, tok/sec:157352.40
step 13019, loss: 3.186798, norm:0.2808, lr:9.0994e-05 dt: 3332.04ms, tok/sec:157347.67
step 13020, loss: 3.199246, norm:0.2854, lr:9.0966e-05 dt: 3332.10ms, tok/sec:157344.71
step 13021, loss: 3.154826, norm:0.2601, lr:9.0939e-05 dt: 3332.38ms, tok/sec:157331.41
step 13022, loss: 3.302909, norm:0.2911, lr:9.0912e-05 dt: 3332.22ms, tok/sec:157338.94
step 13023, loss: 3.225967, norm:0.2670, lr:9.0885e-05 dt: 3332.16ms, tok/sec:157342.01
step 13024, loss: 3.221636, norm:0.2871, lr:9.0858e-05 dt: 3332.18ms, tok/sec:157340.61
step 13025, loss: 3.224131, norm:0.2620, lr:9.0831e-05 dt: 3332.16ms, tok/sec:157341.76
step 13026, loss: 3.176055, norm:0.2646, lr:9.0804e-05 dt: 3331.84ms, tok/sec:157356.70
step 13027, loss: 3.204424, norm:0.2674, lr:9.0777e-05 dt: 3332.10ms, tok/sec:157344.52
step 13028, loss: 3.177365, norm:0.2793, lr:9.0750e-05 dt: 3332.10ms, tok/sec:157344.49
step 13029, loss: 3.242345, norm:0.2895, lr:9.0723e-05 dt: 3332.45ms, tok/sec:157328.12
step 13030, loss: 3.226015, norm:0.2775, lr:9.0696e-05 dt: 3331.94ms, tok/sec:157352.37
step 13031, loss: 3.164093, norm:0.2619, lr:9.0669e-05 dt: 3332.00ms, tok/sec:157349.17
step 13032, loss: 3.219618, norm:0.2631, lr:9.0642e-05 dt: 3332.08ms, tok/sec:157345.79
step 13033, loss: 3.229186, norm:0.2492, lr:9.0615e-05 dt: 3332.38ms, tok/sec:157331.28
step 13034, loss: 3.204906, norm:0.2753, lr:9.0588e-05 dt: 3332.08ms, tok/sec:157345.38
step 13035, loss: 3.177437, norm:0.2527, lr:9.0561e-05 dt: 3331.93ms, tok/sec:157352.56
step 13036, loss: 3.185113, norm:0.2581, lr:9.0534e-05 dt: 3332.09ms, tok/sec:157345.08
step 13037, loss: 3.137132, norm:0.2538, lr:9.0507e-05 dt: 3332.14ms, tok/sec:157342.69
step 13038, loss: 3.163235, norm:0.2558, lr:9.0480e-05 dt: 3332.10ms, tok/sec:157344.52
step 13039, loss: 3.179830, norm:0.2547, lr:9.0453e-05 dt: 3331.96ms, tok/sec:157351.21
step 13040, loss: 3.158446, norm:0.2488, lr:9.0426e-05 dt: 3331.97ms, tok/sec:157350.78
step 13041, loss: 3.147047, norm:0.2625, lr:9.0399e-05 dt: 3331.95ms, tok/sec:157351.87
step 13042, loss: 3.203805, norm:0.2555, lr:9.0373e-05 dt: 3332.30ms, tok/sec:157335.19
step 13043, loss: 3.171033, norm:0.2654, lr:9.0346e-05 dt: 3332.07ms, tok/sec:157346.05
step 13044, loss: 3.183727, norm:0.2553, lr:9.0319e-05 dt: 3332.14ms, tok/sec:157342.66
step 13045, loss: 3.176743, norm:0.2910, lr:9.0292e-05 dt: 3332.02ms, tok/sec:157348.27
step 13046, loss: 3.169948, norm:0.2468, lr:9.0265e-05 dt: 3331.89ms, tok/sec:157354.48
step 13047, loss: 3.153207, norm:0.2693, lr:9.0238e-05 dt: 3332.05ms, tok/sec:157347.10
step 13048, loss: 3.216449, norm:0.2824, lr:9.0212e-05 dt: 3331.99ms, tok/sec:157349.72
step 13049, loss: 3.244891, norm:0.2840, lr:9.0185e-05 dt: 3332.05ms, tok/sec:157347.06
HellaSwag accuracy:-6906125231940465583/-2=3453062615970232832.0000
rank 1 sample 0: Hello, I'm a language model, and so I'm always trying to provide the exact syntax for myself. I try to provide all syntax for me, but
rank 1 sample 1: Hello, I'm a language model, a teacher. So I'm just an American. I work in technology, teaching and helping kids to understand and communicate with
rank 1 sample 2: Hello, I'm a language model, but sometimes, that's not the case.
That was how we were dealing with the problem that the students came to
rank 1 sample 3: Hello, I'm a language model, and I'm looking at English, because from the end, I really enjoy language models even more.
I really like
rank 0 sample 0: Hello, I'm a language model, and I'll be talking about that a short time ago. I just used my first grade (first year and second-
rank 0 sample 1: Hello, I'm a language model, I'd like to learn a new new language and find all of the advantages. I've also learned languages and many languages
rank 0 sample 2: Hello, I'm a language model, so I’m in the minority. What I’re interested in is the language model. I think that
rank 0 sample 3: Hello, I'm a language model, and one of the most important tools in a culture is the need for communication between an instructor and students.
You do
step 13050, loss: 3.144939, norm:0.2524, lr:9.0158e-05 dt: 48393.79ms, tok/sec:10833.79
step 13051, loss: 3.242665, norm:0.3131, lr:9.0131e-05 dt: 3331.92ms, tok/sec:157353.12
step 13052, loss: 3.182642, norm:0.2793, lr:9.0104e-05 dt: 3331.94ms, tok/sec:157352.38
step 13053, loss: 3.172843, norm:0.3104, lr:9.0078e-05 dt: 3331.95ms, tok/sec:157351.56
step 13054, loss: 3.258495, norm:0.2750, lr:9.0051e-05 dt: 3332.33ms, tok/sec:157333.78
step 13055, loss: 3.207108, norm:0.2683, lr:9.0024e-05 dt: 3332.21ms, tok/sec:157339.31
step 13056, loss: 3.172025, norm:0.2731, lr:8.9997e-05 dt: 3332.25ms, tok/sec:157337.68
step 13057, loss: 3.212543, norm:0.3084, lr:8.9971e-05 dt: 3331.94ms, tok/sec:157352.02
step 13058, loss: 3.260940, norm:0.3077, lr:8.9944e-05 dt: 3332.01ms, tok/sec:157349.05
step 13059, loss: 3.199317, norm:0.2881, lr:8.9917e-05 dt: 3332.13ms, tok/sec:157343.10
step 13060, loss: 3.172802, norm:0.2871, lr:8.9891e-05 dt: 3332.03ms, tok/sec:157347.70
step 13061, loss: 3.193951, norm:0.2791, lr:8.9864e-05 dt: 3332.11ms, tok/sec:157344.36
step 13062, loss: 3.174270, norm:0.3126, lr:8.9837e-05 dt: 3332.08ms, tok/sec:157345.47
step 13063, loss: 3.186534, norm:0.2818, lr:8.9811e-05 dt: 3332.24ms, tok/sec:157337.90
step 13064, loss: 3.164440, norm:0.2738, lr:8.9784e-05 dt: 3332.21ms, tok/sec:157339.34
step 13065, loss: 3.217926, norm:0.2548, lr:8.9757e-05 dt: 3331.94ms, tok/sec:157352.18
step 13066, loss: 3.183228, norm:0.2690, lr:8.9731e-05 dt: 3331.94ms, tok/sec:157352.24
step 13067, loss: 3.201365, norm:0.2755, lr:8.9704e-05 dt: 3332.09ms, tok/sec:157345.20
step 13068, loss: 3.209461, norm:0.2676, lr:8.9678e-05 dt: 3331.90ms, tok/sec:157354.24
step 13069, loss: 3.185289, norm:0.2598, lr:8.9651e-05 dt: 3331.98ms, tok/sec:157350.08
step 13070, loss: 3.226129, norm:0.2672, lr:8.9624e-05 dt: 3332.10ms, tok/sec:157344.77
step 13071, loss: 3.115407, norm:0.2691, lr:8.9598e-05 dt: 3331.74ms, tok/sec:157361.39
step 13072, loss: 3.201700, norm:0.2716, lr:8.9571e-05 dt: 3332.45ms, tok/sec:157327.88
step 13073, loss: 3.197715, norm:0.2545, lr:8.9545e-05 dt: 3331.89ms, tok/sec:157354.30
step 13074, loss: 3.170125, norm:0.2534, lr:8.9518e-05 dt: 3331.88ms, tok/sec:157355.19
step 13075, loss: 3.150565, norm:0.2531, lr:8.9492e-05 dt: 3331.93ms, tok/sec:157352.63
step 13076, loss: 3.176081, norm:0.2474, lr:8.9465e-05 dt: 3331.99ms, tok/sec:157349.70
step 13077, loss: 3.194148, norm:0.2657, lr:8.9439e-05 dt: 3331.84ms, tok/sec:157356.87
step 13078, loss: 3.157149, norm:0.2480, lr:8.9412e-05 dt: 3332.04ms, tok/sec:157347.58
step 13079, loss: 3.174883, norm:0.2526, lr:8.9386e-05 dt: 3332.08ms, tok/sec:157345.76
step 13080, loss: 3.138635, norm:0.2535, lr:8.9359e-05 dt: 3332.13ms, tok/sec:157343.02
step 13081, loss: 3.157112, norm:0.2471, lr:8.9333e-05 dt: 3332.26ms, tok/sec:157337.20
step 13082, loss: 3.229420, norm:0.3487, lr:8.9307e-05 dt: 3332.00ms, tok/sec:157349.58
step 13083, loss: 3.217414, norm:0.3338, lr:8.9280e-05 dt: 3332.14ms, tok/sec:157342.89
step 13084, loss: 3.233852, norm:0.2901, lr:8.9254e-05 dt: 3332.10ms, tok/sec:157344.79
step 13085, loss: 3.171639, norm:0.2750, lr:8.9227e-05 dt: 3332.10ms, tok/sec:157344.45
step 13086, loss: 3.201948, norm:0.2886, lr:8.9201e-05 dt: 3332.13ms, tok/sec:157343.35
step 13087, loss: 3.277866, norm:0.3155, lr:8.9175e-05 dt: 3332.00ms, tok/sec:157349.26
step 13088, loss: 3.171244, norm:0.2641, lr:8.9148e-05 dt: 3332.32ms, tok/sec:157334.31
step 13089, loss: 3.171812, norm:0.2847, lr:8.9122e-05 dt: 3332.10ms, tok/sec:157344.59
step 13090, loss: 3.157310, norm:0.2815, lr:8.9096e-05 dt: 3332.20ms, tok/sec:157339.90
step 13091, loss: 3.204267, norm:0.2604, lr:8.9069e-05 dt: 3332.65ms, tok/sec:157318.86
step 13092, loss: 3.215749, norm:0.2724, lr:8.9043e-05 dt: 3332.12ms, tok/sec:157343.86
step 13093, loss: 3.191118, norm:0.2719, lr:8.9017e-05 dt: 3332.08ms, tok/sec:157345.44
step 13094, loss: 3.180447, norm:0.2693, lr:8.8990e-05 dt: 3331.97ms, tok/sec:157350.77
step 13095, loss: 3.206794, norm:0.2769, lr:8.8964e-05 dt: 3332.07ms, tok/sec:157346.11
step 13096, loss: 3.196944, norm:0.2979, lr:8.8938e-05 dt: 3332.14ms, tok/sec:157342.91
step 13097, loss: 3.260507, norm:0.2607, lr:8.8911e-05 dt: 3332.01ms, tok/sec:157349.01
step 13098, loss: 3.225318, norm:0.2844, lr:8.8885e-05 dt: 3332.23ms, tok/sec:157338.31
step 13099, loss: 3.165483, norm:0.2797, lr:8.8859e-05 dt: 3332.15ms, tok/sec:157342.18
validation loss: 3.2074
Model and optimizer state saved.
HellaSwag accuracy:2325092919890119761/-2=-1162546459945059840.0000
rank 1 sample 0: Hello, I'm a language model, and that's what I want to avoid, especially in order to get the information I want from that language model.

rank 1 sample 1: Hello, I'm a language model, a computer programmer, a programmer, a network engineer, software engineer, a database developer"
Answered by:
rank 1 sample 2: Hello, I'm a language model, but really a language model. I'm not sure what to get from it. I'm a little rusty. The real
rank 1 sample 3: Hello, I'm a language model, and I'm working with you, it probably is quite simplistic, but a number of tools such as the Web site,
rank 0 sample 0: Hello, I'm a language model, and I think I can teach people all over the world.
As in many other fields, we can make a big
rank 0 sample 1: Hello, I'm a language model, I'd like to be able to run a language as normal, or what we're talking about. My job is to
rank 0 sample 2: Hello, I'm a language model, so I really have to get at my own ways of thinking on a lot of different things. I'm a language developer
rank 0 sample 3: Hello, I'm a language model, and now I have a language. In the bottom left corner of this picture is also a language. Now I can define
step 13100, loss: 3.235271, norm:0.2811, lr:8.8833e-05 dt: 56099.37ms, tok/sec:9345.70
step 13101, loss: 3.162399, norm:0.2582, lr:8.8806e-05 dt: 3332.17ms, tok/sec:157341.22
step 13102, loss: 3.129832, norm:0.2613, lr:8.8780e-05 dt: 3332.36ms, tok/sec:157332.29
step 13103, loss: 3.254589, norm:0.2645, lr:8.8754e-05 dt: 3332.10ms, tok/sec:157344.42
step 13104, loss: 3.210352, norm:0.2765, lr:8.8728e-05 dt: 3331.86ms, tok/sec:157355.86
step 13105, loss: 3.179783, norm:0.2598, lr:8.8702e-05 dt: 3331.97ms, tok/sec:157350.55
step 13106, loss: 3.206627, norm:0.2897, lr:8.8676e-05 dt: 3332.09ms, tok/sec:157345.21
step 13107, loss: 3.163943, norm:0.2679, lr:8.8649e-05 dt: 3332.06ms, tok/sec:157346.54
step 13108, loss: 3.130067, norm:0.2635, lr:8.8623e-05 dt: 3331.82ms, tok/sec:157357.92
step 13109, loss: 3.190287, norm:0.2621, lr:8.8597e-05 dt: 3332.04ms, tok/sec:157347.68
step 13110, loss: 3.201348, norm:0.2668, lr:8.8571e-05 dt: 3332.26ms, tok/sec:157337.24
step 13111, loss: 3.183115, norm:0.2527, lr:8.8545e-05 dt: 3332.45ms, tok/sec:157327.89
step 13112, loss: 3.173858, norm:0.2702, lr:8.8519e-05 dt: 3332.09ms, tok/sec:157344.97
step 13113, loss: 3.206875, norm:0.2592, lr:8.8493e-05 dt: 3331.88ms, tok/sec:157355.08
step 13114, loss: 3.179113, norm:0.2764, lr:8.8467e-05 dt: 3332.08ms, tok/sec:157345.63
step 13115, loss: 3.128434, norm:0.2662, lr:8.8441e-05 dt: 3332.05ms, tok/sec:157347.09
step 13116, loss: 3.171787, norm:0.2671, lr:8.8415e-05 dt: 3332.08ms, tok/sec:157345.52
step 13117, loss: 3.216336, norm:0.2665, lr:8.8389e-05 dt: 3331.96ms, tok/sec:157350.99
step 13118, loss: 3.203119, norm:0.2538, lr:8.8363e-05 dt: 3332.19ms, tok/sec:157340.26
step 13119, loss: 3.256043, norm:0.3045, lr:8.8337e-05 dt: 3332.44ms, tok/sec:157328.65
step 13120, loss: 3.185685, norm:0.2751, lr:8.8310e-05 dt: 3332.14ms, tok/sec:157342.79
step 13121, loss: 3.190600, norm:0.2749, lr:8.8285e-05 dt: 3332.04ms, tok/sec:157347.64
step 13122, loss: 3.214486, norm:0.2678, lr:8.8259e-05 dt: 3332.09ms, tok/sec:157345.15
step 13123, loss: 3.288710, norm:0.3878, lr:8.8233e-05 dt: 3331.80ms, tok/sec:157358.85
step 13124, loss: 3.168542, norm:0.3104, lr:8.8207e-05 dt: 3332.00ms, tok/sec:157349.14
step 13125, loss: 3.163570, norm:0.3469, lr:8.8181e-05 dt: 3331.86ms, tok/sec:157356.03
step 13126, loss: 3.179962, norm:0.2880, lr:8.8155e-05 dt: 3332.27ms, tok/sec:157336.56
step 13127, loss: 3.234951, norm:0.2865, lr:8.8129e-05 dt: 3332.11ms, tok/sec:157343.97
step 13128, loss: 3.203759, norm:0.3119, lr:8.8103e-05 dt: 3331.97ms, tok/sec:157350.98
step 13129, loss: 3.243751, norm:0.2748, lr:8.8077e-05 dt: 3332.07ms, tok/sec:157345.91
step 13130, loss: 3.206140, norm:0.2953, lr:8.8051e-05 dt: 3331.89ms, tok/sec:157354.55
step 13131, loss: 3.181635, norm:0.2639, lr:8.8025e-05 dt: 3332.12ms, tok/sec:157343.81
step 13132, loss: 3.189159, norm:0.2742, lr:8.7999e-05 dt: 3331.83ms, tok/sec:157357.38
step 13133, loss: 3.217443, norm:0.2660, lr:8.7973e-05 dt: 3332.30ms, tok/sec:157335.12
step 13134, loss: 3.146607, norm:0.2913, lr:8.7948e-05 dt: 3332.29ms, tok/sec:157335.86
step 13135, loss: 3.220363, norm:0.2694, lr:8.7922e-05 dt: 3332.09ms, tok/sec:157345.13
step 13136, loss: 3.215799, norm:0.2825, lr:8.7896e-05 dt: 3332.09ms, tok/sec:157345.08
step 13137, loss: 3.211015, norm:0.2726, lr:8.7870e-05 dt: 3332.10ms, tok/sec:157344.72
step 13138, loss: 3.218191, norm:0.2628, lr:8.7844e-05 dt: 3331.97ms, tok/sec:157350.76
step 13139, loss: 3.202391, norm:0.2701, lr:8.7819e-05 dt: 3332.13ms, tok/sec:157342.97
step 13140, loss: 3.155284, norm:0.2696, lr:8.7793e-05 dt: 3331.89ms, tok/sec:157354.44
step 13141, loss: 3.194653, norm:0.2853, lr:8.7767e-05 dt: 3332.35ms, tok/sec:157332.62
step 13142, loss: 3.214659, norm:0.2697, lr:8.7741e-05 dt: 3331.97ms, tok/sec:157350.75
step 13143, loss: 3.067483, norm:0.2811, lr:8.7716e-05 dt: 3331.95ms, tok/sec:157351.48
step 13144, loss: 3.152188, norm:0.2587, lr:8.7690e-05 dt: 3334.66ms, tok/sec:157223.72
step 13145, loss: 3.166764, norm:0.2499, lr:8.7664e-05 dt: 3332.28ms, tok/sec:157336.14
step 13146, loss: 3.173304, norm:0.2783, lr:8.7638e-05 dt: 3332.18ms, tok/sec:157340.82
step 13147, loss: 3.201850, norm:0.2731, lr:8.7613e-05 dt: 3332.13ms, tok/sec:157343.41
step 13148, loss: 3.153724, norm:0.2442, lr:8.7587e-05 dt: 3332.18ms, tok/sec:157341.06
step 13149, loss: 3.142249, norm:0.2628, lr:8.7561e-05 dt: 3331.94ms, tok/sec:157351.98
HellaSwag accuracy:-2286557914165179311/-2=1143278957082589696.0000
rank 1 sample 0: Hello, I'm a language model, and so I'm very interested in studying languages at a future event. I'm a human, I'm very interested in
rank 1 sample 1: Hello, I'm a language model, a teacher. There are a lot of books out there trying to come up with some suggestions on how you can use the
rank 1 sample 2: Hello, I'm a language model, but because you know how to use the language model, how did it work?
I've tried it, because it
rank 1 sample 3: Hello, I'm a language model, and I'm really excited to be learning . . . And if you like language, maybe be interested in learning languages.
rank 0 sample 0: Hello, I'm a language model, and I think I can get to understand every situation with them, whether by speaking or writing.<|endoftext|>In this article I
rank 0 sample 1: Hello, I'm a language model, I was curious about this. So one of those kinds of questions. Does a language model show the grammar, or does
rank 0 sample 2: Hello, I'm a language model, so I like languages
whereby they are used to communicate the world with each other, so I can understand the interactions
rank 0 sample 3: Hello, I'm a language model, I could start at any time, I'd help you.
It's fun saying it so much fun, I am
step 13150, loss: 3.170228, norm:0.2899, lr:8.7536e-05 dt: 48403.07ms, tok/sec:10831.71
step 13151, loss: 3.174565, norm:0.2473, lr:8.7510e-05 dt: 3332.02ms, tok/sec:157348.53
step 13152, loss: 3.198296, norm:0.2476, lr:8.7484e-05 dt: 3332.14ms, tok/sec:157342.71
step 13153, loss: 3.122488, norm:0.2563, lr:8.7459e-05 dt: 3332.78ms, tok/sec:157312.71
step 13154, loss: 3.260783, norm:0.2922, lr:8.7433e-05 dt: 3332.40ms, tok/sec:157330.23
step 13155, loss: 3.166468, norm:0.2712, lr:8.7407e-05 dt: 3331.97ms, tok/sec:157350.98
step 13156, loss: 3.220840, norm:0.2845, lr:8.7382e-05 dt: 3332.03ms, tok/sec:157348.10
step 13157, loss: 3.209686, norm:0.2749, lr:8.7356e-05 dt: 3332.13ms, tok/sec:157343.30
step 13158, loss: 3.224157, norm:0.2658, lr:8.7331e-05 dt: 3332.31ms, tok/sec:157334.59
step 13159, loss: 3.139728, norm:0.2907, lr:8.7305e-05 dt: 3331.95ms, tok/sec:157351.62
step 13160, loss: 3.225013, norm:0.2618, lr:8.7280e-05 dt: 3332.11ms, tok/sec:157344.13
step 13161, loss: 3.262480, norm:0.2809, lr:8.7254e-05 dt: 3331.90ms, tok/sec:157354.11
step 13162, loss: 3.213304, norm:0.2906, lr:8.7229e-05 dt: 3332.45ms, tok/sec:157328.31
step 13163, loss: 3.255753, norm:0.2636, lr:8.7203e-05 dt: 3332.07ms, tok/sec:157346.02
step 13164, loss: 3.186914, norm:0.2505, lr:8.7178e-05 dt: 3332.07ms, tok/sec:157345.97
step 13165, loss: 3.205108, norm:0.2584, lr:8.7152e-05 dt: 3332.07ms, tok/sec:157346.23
step 13166, loss: 3.206061, norm:0.3070, lr:8.7127e-05 dt: 3331.87ms, tok/sec:157355.68
step 13167, loss: 3.171096, norm:0.2700, lr:8.7101e-05 dt: 3331.91ms, tok/sec:157353.69
step 13168, loss: 3.201122, norm:0.2720, lr:8.7076e-05 dt: 3332.12ms, tok/sec:157343.73
step 13169, loss: 3.201729, norm:0.2841, lr:8.7050e-05 dt: 3332.17ms, tok/sec:157341.55
step 13170, loss: 3.180562, norm:0.2926, lr:8.7025e-05 dt: 3332.10ms, tok/sec:157344.44
step 13171, loss: 3.201338, norm:0.2688, lr:8.6999e-05 dt: 3332.23ms, tok/sec:157338.61
step 13172, loss: 3.173756, norm:0.2671, lr:8.6974e-05 dt: 3331.94ms, tok/sec:157352.40
step 13173, loss: 3.225238, norm:0.2701, lr:8.6948e-05 dt: 3331.86ms, tok/sec:157355.79
step 13174, loss: 3.212367, norm:0.2707, lr:8.6923e-05 dt: 3332.13ms, tok/sec:157342.99
step 13175, loss: 3.204251, norm:0.2593, lr:8.6898e-05 dt: 3331.87ms, tok/sec:157355.36
step 13176, loss: 3.288667, norm:0.2759, lr:8.6872e-05 dt: 3332.11ms, tok/sec:157343.92
step 13177, loss: 3.210918, norm:0.2566, lr:8.6847e-05 dt: 3332.13ms, tok/sec:157343.24
step 13178, loss: 3.174973, norm:0.2891, lr:8.6822e-05 dt: 3331.94ms, tok/sec:157352.40
step 13179, loss: 3.202908, norm:0.2694, lr:8.6796e-05 dt: 3332.13ms, tok/sec:157342.98
step 13180, loss: 3.160431, norm:0.2865, lr:8.6771e-05 dt: 3331.89ms, tok/sec:157354.64
step 13181, loss: 3.155292, norm:0.2510, lr:8.6746e-05 dt: 3332.08ms, tok/sec:157345.41
step 13182, loss: 3.230620, norm:0.2595, lr:8.6720e-05 dt: 3332.20ms, tok/sec:157340.04
step 13183, loss: 3.127109, norm:0.2770, lr:8.6695e-05 dt: 3332.01ms, tok/sec:157348.83
step 13184, loss: 3.130580, norm:0.2455, lr:8.6670e-05 dt: 3331.96ms, tok/sec:157351.01
step 13185, loss: 3.177837, norm:0.2673, lr:8.6644e-05 dt: 3332.25ms, tok/sec:157337.74
step 13186, loss: 3.184604, norm:0.2572, lr:8.6619e-05 dt: 3332.06ms, tok/sec:157346.64
step 13187, loss: 3.183253, norm:0.3253, lr:8.6594e-05 dt: 3331.92ms, tok/sec:157353.01
step 13188, loss: 3.215405, norm:0.2739, lr:8.6569e-05 dt: 3332.14ms, tok/sec:157342.83
step 13189, loss: 3.106253, norm:0.2712, lr:8.6544e-05 dt: 3332.06ms, tok/sec:157346.69
step 13190, loss: 3.173535, norm:0.2963, lr:8.6518e-05 dt: 3331.98ms, tok/sec:157350.30
step 13191, loss: 3.218585, norm:0.2769, lr:8.6493e-05 dt: 3332.03ms, tok/sec:157348.13
step 13192, loss: 3.248423, norm:0.3035, lr:8.6468e-05 dt: 3332.11ms, tok/sec:157344.10
step 13193, loss: 3.177354, norm:0.2684, lr:8.6443e-05 dt: 3332.22ms, tok/sec:157338.97
step 13194, loss: 3.242667, norm:0.2955, lr:8.6418e-05 dt: 3332.07ms, tok/sec:157346.07
step 13195, loss: 3.255502, norm:0.2794, lr:8.6392e-05 dt: 3331.98ms, tok/sec:157350.26
step 13196, loss: 3.168731, norm:0.2796, lr:8.6367e-05 dt: 3332.07ms, tok/sec:157346.15
step 13197, loss: 3.210473, norm:0.2690, lr:8.6342e-05 dt: 3332.38ms, tok/sec:157331.23
step 13198, loss: 3.192119, norm:0.2831, lr:8.6317e-05 dt: 3332.06ms, tok/sec:157346.40
step 13199, loss: 3.227490, norm:0.2644, lr:8.6292e-05 dt: 3332.05ms, tok/sec:157346.96
validation loss: 3.2045
Model and optimizer state saved.
HellaSwag accuracy:-2295459560303653871/-2=1147729780151826944.0000
rank 1 sample 0: Hello, I'm a language model, and this is the only one I love that was designed for humans. I never thought it would help me in my life
rank 1 sample 1: Hello, I'm a language model, a computer program designed to teach you everything you need to use your languages. And most thanks, but not all.

rank 1 sample 2: Hello, I'm a language model, but did you know that the language model is not used by you to create a language model? Well, it just gives
rank 1 sample 3: Hello, I'm a language model, and I'm working with one of the native speakers of
Natives that they have no business working with. My goal
rank 0 sample 0: Hello, I'm a language model, and I'll be writing to help you guys figure out how to find different things. And a big help to you,
rank 0 sample 1: Hello, I'm a language model, but here's a good example: an input/output pair. In: the first two input and output pairs are given
rank 0 sample 2: Hello, I'm a language model, so I hope that makes an impact on the whole classroom. And it's a bit like the language model, and I
rank 0 sample 3: Hello, I'm a language model, and now I have a question about how you deal with it. In order to tell what it's for, we put
step 13200, loss: 3.264669, norm:0.2711, lr:8.6267e-05 dt: 56178.83ms, tok/sec:9332.48
step 13201, loss: 3.192646, norm:0.2876, lr:8.6242e-05 dt: 3332.21ms, tok/sec:157339.52
step 13202, loss: 3.143953, norm:0.2890, lr:8.6217e-05 dt: 3332.09ms, tok/sec:157344.88
step 13203, loss: 3.199547, norm:0.2686, lr:8.6192e-05 dt: 3331.89ms, tok/sec:157354.69
step 13204, loss: 3.187482, norm:0.2807, lr:8.6167e-05 dt: 3332.15ms, tok/sec:157342.30
step 13205, loss: 3.172721, norm:0.2505, lr:8.6142e-05 dt: 3332.16ms, tok/sec:157341.72
step 13206, loss: 3.178549, norm:0.2553, lr:8.6116e-05 dt: 3332.04ms, tok/sec:157347.58
step 13207, loss: 3.245157, norm:0.3026, lr:8.6091e-05 dt: 3332.01ms, tok/sec:157349.00
step 13208, loss: 3.209939, norm:0.2742, lr:8.6066e-05 dt: 3332.21ms, tok/sec:157339.63
step 13209, loss: 3.175938, norm:0.2662, lr:8.6041e-05 dt: 3332.05ms, tok/sec:157347.11
step 13210, loss: 3.240023, norm:0.2572, lr:8.6016e-05 dt: 3332.14ms, tok/sec:157342.82
step 13211, loss: 3.230018, norm:0.3380, lr:8.5992e-05 dt: 3332.02ms, tok/sec:157348.35
step 13212, loss: 3.207313, norm:0.2745, lr:8.5967e-05 dt: 3332.08ms, tok/sec:157345.33
step 13213, loss: 3.235474, norm:0.2740, lr:8.5942e-05 dt: 3332.23ms, tok/sec:157338.36
step 13214, loss: 3.189537, norm:0.2760, lr:8.5917e-05 dt: 3332.01ms, tok/sec:157348.71
step 13215, loss: 3.148685, norm:0.2718, lr:8.5892e-05 dt: 3332.15ms, tok/sec:157342.08
step 13216, loss: 3.123861, norm:0.2692, lr:8.5867e-05 dt: 3332.01ms, tok/sec:157349.00
step 13217, loss: 3.191033, norm:0.2582, lr:8.5842e-05 dt: 3332.53ms, tok/sec:157324.54
step 13218, loss: 3.131306, norm:0.2687, lr:8.5817e-05 dt: 3331.94ms, tok/sec:157352.26
step 13219, loss: 3.159025, norm:0.2751, lr:8.5792e-05 dt: 3331.93ms, tok/sec:157352.87
step 13220, loss: 3.206824, norm:0.2848, lr:8.5767e-05 dt: 3332.12ms, tok/sec:157343.68
step 13221, loss: 3.165567, norm:0.3015, lr:8.5742e-05 dt: 3332.09ms, tok/sec:157345.04
step 13222, loss: 3.157826, norm:0.2743, lr:8.5718e-05 dt: 3332.24ms, tok/sec:157337.81
step 13223, loss: 3.133973, norm:0.2602, lr:8.5693e-05 dt: 3331.86ms, tok/sec:157355.76
step 13224, loss: 3.162696, norm:0.2870, lr:8.5668e-05 dt: 3331.99ms, tok/sec:157349.95
step 13225, loss: 3.171094, norm:0.2723, lr:8.5643e-05 dt: 3332.05ms, tok/sec:157346.91
step 13226, loss: 3.195861, norm:0.3052, lr:8.5618e-05 dt: 3332.05ms, tok/sec:157346.85
step 13227, loss: 3.272059, norm:0.3008, lr:8.5593e-05 dt: 3331.92ms, tok/sec:157352.93
step 13228, loss: 3.227321, norm:0.2729, lr:8.5569e-05 dt: 3331.98ms, tok/sec:157350.30
step 13229, loss: 3.203455, norm:0.2762, lr:8.5544e-05 dt: 3332.43ms, tok/sec:157328.96
step 13230, loss: 3.251483, norm:0.2632, lr:8.5519e-05 dt: 3332.33ms, tok/sec:157333.81
step 13231, loss: 3.196521, norm:0.2828, lr:8.5494e-05 dt: 3332.15ms, tok/sec:157342.05
step 13232, loss: 3.213893, norm:0.2786, lr:8.5470e-05 dt: 3332.13ms, tok/sec:157343.05
step 13233, loss: 3.185452, norm:0.2660, lr:8.5445e-05 dt: 3331.98ms, tok/sec:157350.08
step 13234, loss: 3.245021, norm:0.2864, lr:8.5420e-05 dt: 3331.99ms, tok/sec:157349.87
step 13235, loss: 3.225824, norm:0.2768, lr:8.5396e-05 dt: 3332.05ms, tok/sec:157347.00
step 13236, loss: 3.304118, norm:0.3485, lr:8.5371e-05 dt: 3332.12ms, tok/sec:157343.47
step 13237, loss: 3.207291, norm:0.2676, lr:8.5346e-05 dt: 3332.07ms, tok/sec:157346.22
step 13238, loss: 3.184860, norm:0.2830, lr:8.5322e-05 dt: 3332.15ms, tok/sec:157342.39
step 13239, loss: 3.200448, norm:0.2844, lr:8.5297e-05 dt: 3332.13ms, tok/sec:157343.28
step 13240, loss: 3.151390, norm:0.2675, lr:8.5272e-05 dt: 3331.94ms, tok/sec:157352.30
step 13241, loss: 3.180143, norm:0.2648, lr:8.5248e-05 dt: 3332.12ms, tok/sec:157343.88
step 13242, loss: 3.193679, norm:0.2687, lr:8.5223e-05 dt: 3332.46ms, tok/sec:157327.65
step 13243, loss: 3.183698, norm:0.2752, lr:8.5198e-05 dt: 3332.21ms, tok/sec:157339.20
step 13244, loss: 3.192295, norm:0.2815, lr:8.5174e-05 dt: 3332.03ms, tok/sec:157347.73
step 13245, loss: 3.158767, norm:0.2731, lr:8.5149e-05 dt: 3332.14ms, tok/sec:157342.83
step 13246, loss: 3.162764, norm:0.2661, lr:8.5125e-05 dt: 3331.89ms, tok/sec:157354.70
step 13247, loss: 3.219273, norm:0.2613, lr:8.5100e-05 dt: 3332.04ms, tok/sec:157347.53
step 13248, loss: 3.190342, norm:0.2661, lr:8.5076e-05 dt: 3331.83ms, tok/sec:157357.18
step 13249, loss: 3.191234, norm:0.2642, lr:8.5051e-05 dt: 3332.30ms, tok/sec:157335.40
HellaSwag accuracy:-2295565113419920303/-2=1147782556709960192.0000
rank 1 sample 0: Hello, I'm a language model, and that is what I want to express to other programmers. A language model is a language model.
What is a
rank 1 sample 1: Hello, I'm a language model, a computer, and a computer programming language. I was writing a small program on the phone. Can I use it?
rank 1 sample 2: Hello, I'm a language model, but where does a language model come from?
There are other ways that a language model is a tool. One way
rank 1 sample 3: Hello, I'm a language model, and I'm looking at lots of ideas what I might become of and maybe ideas that this approach will help me understand.
rank 0 sample 0: Hello, I'm a language model, and I love to use them as well. Here's what I can do, and why I need it:
First
rank 0 sample 1: Hello, I'm a language model, so to speak. So, to some extent you define our language model from an actual, natural, one-to-
rank 0 sample 2: Hello, I'm a language model, so I didn't start a dictionary in my computer. I'd rather go to the Internet, and I'd be just
rank 0 sample 3: Hello, I'm a language model, and one of the things I want to teach our students is to become better learners! The question that was asked in first
step 13250, loss: 3.175275, norm:0.2725, lr:8.5026e-05 dt: 48521.66ms, tok/sec:10805.24
step 13251, loss: 3.162908, norm:0.2692, lr:8.5002e-05 dt: 3332.14ms, tok/sec:157342.58
step 13252, loss: 3.109427, norm:0.2700, lr:8.4977e-05 dt: 3332.06ms, tok/sec:157346.52
step 13253, loss: 3.123353, norm:0.2525, lr:8.4953e-05 dt: 3331.74ms, tok/sec:157361.49
step 13254, loss: 3.114698, norm:0.2624, lr:8.4928e-05 dt: 3331.80ms, tok/sec:157358.71
step 13255, loss: 3.167699, norm:0.3275, lr:8.4904e-05 dt: 3332.03ms, tok/sec:157347.99
step 13256, loss: 3.143880, norm:0.2667, lr:8.4880e-05 dt: 3331.78ms, tok/sec:157359.61
step 13257, loss: 3.140814, norm:0.2850, lr:8.4855e-05 dt: 3331.85ms, tok/sec:157356.61
step 13258, loss: 3.102240, norm:0.2680, lr:8.4831e-05 dt: 3331.77ms, tok/sec:157360.38
step 13259, loss: 3.149801, norm:0.2479, lr:8.4806e-05 dt: 3331.95ms, tok/sec:157351.79
step 13260, loss: 3.193704, norm:0.2683, lr:8.4782e-05 dt: 3332.30ms, tok/sec:157335.21
step 13261, loss: 3.143307, norm:0.2501, lr:8.4757e-05 dt: 3331.75ms, tok/sec:157360.95
step 13262, loss: 3.165721, norm:0.2583, lr:8.4733e-05 dt: 3331.98ms, tok/sec:157350.09
step 13263, loss: 3.194532, norm:0.2852, lr:8.4709e-05 dt: 3332.19ms, tok/sec:157340.22
step 13264, loss: 3.156011, norm:0.2736, lr:8.4684e-05 dt: 3331.96ms, tok/sec:157351.01
step 13265, loss: 3.247288, norm:0.2984, lr:8.4660e-05 dt: 3331.70ms, tok/sec:157363.28
step 13266, loss: 3.200291, norm:0.2705, lr:8.4636e-05 dt: 3331.97ms, tok/sec:157350.96
step 13267, loss: 3.173607, norm:0.2770, lr:8.4611e-05 dt: 3331.83ms, tok/sec:157357.20
step 13268, loss: 3.177708, norm:0.2635, lr:8.4587e-05 dt: 3332.24ms, tok/sec:157337.90
step 13269, loss: 3.166394, norm:0.2610, lr:8.4563e-05 dt: 3332.11ms, tok/sec:157344.33
step 13270, loss: 3.247811, norm:0.2743, lr:8.4538e-05 dt: 3332.10ms, tok/sec:157344.75
step 13271, loss: 3.226628, norm:0.2642, lr:8.4514e-05 dt: 3332.00ms, tok/sec:157349.15
step 13272, loss: 3.281643, norm:0.2662, lr:8.4490e-05 dt: 3331.98ms, tok/sec:157350.05
step 13273, loss: 3.201401, norm:0.2668, lr:8.4465e-05 dt: 3331.93ms, tok/sec:157352.51
step 13274, loss: 3.261217, norm:0.2790, lr:8.4441e-05 dt: 3332.11ms, tok/sec:157344.13
step 13275, loss: 3.236713, norm:0.2831, lr:8.4417e-05 dt: 3332.06ms, tok/sec:157346.64
step 13276, loss: 3.212300, norm:0.2506, lr:8.4393e-05 dt: 3331.99ms, tok/sec:157349.73
step 13277, loss: 3.193207, norm:0.2736, lr:8.4369e-05 dt: 3331.83ms, tok/sec:157357.21
step 13278, loss: 3.297932, norm:0.2864, lr:8.4344e-05 dt: 3332.26ms, tok/sec:157337.09
step 13279, loss: 3.175029, norm:0.2590, lr:8.4320e-05 dt: 3332.09ms, tok/sec:157345.13
step 13280, loss: 3.154668, norm:0.2634, lr:8.4296e-05 dt: 3331.97ms, tok/sec:157350.69
step 13281, loss: 3.180248, norm:0.2562, lr:8.4272e-05 dt: 3332.07ms, tok/sec:157345.91
step 13282, loss: 3.245845, norm:0.2694, lr:8.4248e-05 dt: 3332.07ms, tok/sec:157346.22
step 13283, loss: 3.231721, norm:0.2638, lr:8.4223e-05 dt: 3331.95ms, tok/sec:157351.92
step 13284, loss: 3.200624, norm:0.2647, lr:8.4199e-05 dt: 3332.02ms, tok/sec:157348.20
step 13285, loss: 3.135947, norm:0.2656, lr:8.4175e-05 dt: 3331.80ms, tok/sec:157358.56
step 13286, loss: 3.203009, norm:0.2705, lr:8.4151e-05 dt: 3331.82ms, tok/sec:157357.84
step 13287, loss: 3.165606, norm:0.2520, lr:8.4127e-05 dt: 3332.22ms, tok/sec:157339.13
step 13288, loss: 3.087892, norm:0.2507, lr:8.4103e-05 dt: 3332.09ms, tok/sec:157344.93
step 13289, loss: 3.136951, norm:0.2574, lr:8.4079e-05 dt: 3331.94ms, tok/sec:157352.40
step 13290, loss: 3.147417, norm:0.2728, lr:8.4055e-05 dt: 3332.04ms, tok/sec:157347.35
step 13291, loss: 3.203647, norm:0.2514, lr:8.4031e-05 dt: 3331.92ms, tok/sec:157353.16
step 13292, loss: 3.163107, norm:0.2748, lr:8.4007e-05 dt: 3331.96ms, tok/sec:157351.06
step 13293, loss: 3.169231, norm:0.2805, lr:8.3983e-05 dt: 3331.88ms, tok/sec:157354.88
step 13294, loss: 3.179678, norm:0.2811, lr:8.3959e-05 dt: 3331.84ms, tok/sec:157356.86
step 13295, loss: 3.197777, norm:0.2695, lr:8.3935e-05 dt: 3331.94ms, tok/sec:157352.01
step 13296, loss: 3.182746, norm:0.2688, lr:8.3911e-05 dt: 3332.04ms, tok/sec:157347.46
step 13297, loss: 3.203173, norm:0.2745, lr:8.3887e-05 dt: 3332.20ms, tok/sec:157339.69
step 13298, loss: 3.141834, norm:0.2651, lr:8.3863e-05 dt: 3332.10ms, tok/sec:157344.54
step 13299, loss: 3.213360, norm:0.2612, lr:8.3839e-05 dt: 3331.89ms, tok/sec:157354.38
validation loss: 3.2036
Model and optimizer state saved.
HellaSwag accuracy:6927806923434853457/-2=-3463903461717426688.0000
rank 1 sample 0: Hello, I'm a language model, and my son is a native speaker even when he doesn't seem to know his native language. The reason why I'm
rank 1 sample 1: Hello, I'm a language model, you know. The way I want to create a function has to be similar to how we wrote our program.
I
rank 1 sample 2: Hello, I'm a language model, but at this point I'm not sure what I'm trying to teach myself. I'm just starting from scratch on the
rank 1 sample 3: Hello, I'm a language model, and I'm writing code because I've forgotten the basics now. However, thanks to IBM X10, I could write
rank 0 sample 0: Hello, I'm a language model, and I'll be writing something in about my 20th century, that's not a word too far off:
So
rank 0 sample 1: Hello, I'm a language model, so there's a lot of work a language engineer had to do for my work, so I've seen that I need
rank 0 sample 2: Hello, I'm a language model, so I wanted to look at the definition of how I was used by other languages.
So, I'm using a
rank 0 sample 3: Hello, I'm a language model, and not a language model.
It was I who decided to work out the vocabulary to describe a world in the new
step 13300, loss: 3.177435, norm:0.2637, lr:8.3815e-05 dt: 56433.94ms, tok/sec:9290.30
step 13301, loss: 3.235642, norm:0.2966, lr:8.3791e-05 dt: 3332.25ms, tok/sec:157337.38
step 13302, loss: 3.202850, norm:0.2862, lr:8.3767e-05 dt: 3332.05ms, tok/sec:157346.94
step 13303, loss: 3.253095, norm:0.2735, lr:8.3743e-05 dt: 3332.05ms, tok/sec:157346.88
step 13304, loss: 3.215571, norm:0.2660, lr:8.3719e-05 dt: 3331.94ms, tok/sec:157352.31
step 13305, loss: 3.182994, norm:0.2647, lr:8.3695e-05 dt: 3331.91ms, tok/sec:157353.81
step 13306, loss: 3.211413, norm:0.2611, lr:8.3671e-05 dt: 3332.03ms, tok/sec:157347.74
step 13307, loss: 3.229168, norm:0.2919, lr:8.3647e-05 dt: 3332.04ms, tok/sec:157347.61
step 13308, loss: 3.244606, norm:0.2901, lr:8.3623e-05 dt: 3332.00ms, tok/sec:157349.20
step 13309, loss: 3.204083, norm:0.2986, lr:8.3600e-05 dt: 3331.83ms, tok/sec:157357.21
step 13310, loss: 3.214810, norm:0.2759, lr:8.3576e-05 dt: 3332.11ms, tok/sec:157344.17
step 13311, loss: 3.162602, norm:0.2755, lr:8.3552e-05 dt: 3332.10ms, tok/sec:157344.79
step 13312, loss: 3.219160, norm:0.2937, lr:8.3528e-05 dt: 3331.99ms, tok/sec:157349.94
step 13313, loss: 3.186242, norm:0.2547, lr:8.3504e-05 dt: 3332.09ms, tok/sec:157345.25
step 13314, loss: 3.195766, norm:0.2631, lr:8.3481e-05 dt: 3332.14ms, tok/sec:157342.87
step 13315, loss: 3.205377, norm:0.2651, lr:8.3457e-05 dt: 3332.10ms, tok/sec:157344.55
step 13316, loss: 3.291492, norm:0.2820, lr:8.3433e-05 dt: 3331.92ms, tok/sec:157353.34
step 13317, loss: 3.205174, norm:0.2725, lr:8.3409e-05 dt: 3332.24ms, tok/sec:157338.01
step 13318, loss: 3.202241, norm:0.2753, lr:8.3385e-05 dt: 3332.03ms, tok/sec:157348.10
step 13319, loss: 3.168330, norm:0.2776, lr:8.3362e-05 dt: 3332.25ms, tok/sec:157337.65
step 13320, loss: 3.163040, norm:0.2718, lr:8.3338e-05 dt: 3332.00ms, tok/sec:157349.42
step 13321, loss: 3.239992, norm:0.3740, lr:8.3314e-05 dt: 3331.95ms, tok/sec:157351.65
step 13322, loss: 3.160955, norm:0.2678, lr:8.3291e-05 dt: 3332.06ms, tok/sec:157346.72
step 13323, loss: 3.131489, norm:0.2863, lr:8.3267e-05 dt: 3332.26ms, tok/sec:157337.10
step 13324, loss: 3.134942, norm:0.2903, lr:8.3243e-05 dt: 3332.17ms, tok/sec:157341.45
step 13325, loss: 3.102314, norm:0.2647, lr:8.3220e-05 dt: 3331.80ms, tok/sec:157358.75
step 13326, loss: 3.166079, norm:0.2651, lr:8.3196e-05 dt: 3331.76ms, tok/sec:157360.56
step 13327, loss: 3.152596, norm:0.3358, lr:8.3172e-05 dt: 3332.09ms, tok/sec:157344.90
step 13328, loss: 3.144668, norm:0.2616, lr:8.3149e-05 dt: 3331.94ms, tok/sec:157352.15
step 13329, loss: 3.141084, norm:0.2693, lr:8.3125e-05 dt: 3331.84ms, tok/sec:157357.06
step 13330, loss: 3.115653, norm:0.2630, lr:8.3101e-05 dt: 3331.93ms, tok/sec:157352.51
step 13331, loss: 3.173473, norm:0.2727, lr:8.3078e-05 dt: 3332.02ms, tok/sec:157348.26
step 13332, loss: 3.166607, norm:0.2505, lr:8.3054e-05 dt: 3332.16ms, tok/sec:157341.59
step 13333, loss: 3.262075, norm:0.2818, lr:8.3031e-05 dt: 3331.80ms, tok/sec:157358.57
step 13334, loss: 3.169981, norm:0.2704, lr:8.3007e-05 dt: 3334.02ms, tok/sec:157253.80
step 13335, loss: 3.212572, norm:0.2776, lr:8.2983e-05 dt: 3332.15ms, tok/sec:157342.34
step 13336, loss: 3.177777, norm:0.2530, lr:8.2960e-05 dt: 3331.93ms, tok/sec:157352.51
step 13337, loss: 3.241861, norm:0.2827, lr:8.2936e-05 dt: 3332.10ms, tok/sec:157344.55
step 13338, loss: 3.175707, norm:0.2773, lr:8.2913e-05 dt: 3332.17ms, tok/sec:157341.20
step 13339, loss: 3.152955, norm:0.2629, lr:8.2889e-05 dt: 3332.09ms, tok/sec:157345.21
step 13340, loss: 3.215089, norm:0.2689, lr:8.2866e-05 dt: 3331.94ms, tok/sec:157352.01
step 13341, loss: 3.178855, norm:0.2691, lr:8.2842e-05 dt: 3332.16ms, tok/sec:157341.66
step 13342, loss: 3.219759, norm:0.2868, lr:8.2819e-05 dt: 3332.02ms, tok/sec:157348.19
step 13343, loss: 3.227739, norm:0.2664, lr:8.2795e-05 dt: 3332.21ms, tok/sec:157339.35
step 13344, loss: 3.214553, norm:0.2801, lr:8.2772e-05 dt: 3331.80ms, tok/sec:157358.92
step 13345, loss: 3.200175, norm:0.2601, lr:8.2749e-05 dt: 3332.00ms, tok/sec:157349.58
step 13346, loss: 3.196201, norm:0.2703, lr:8.2725e-05 dt: 3332.09ms, tok/sec:157344.90
step 13347, loss: 3.192200, norm:0.2788, lr:8.2702e-05 dt: 3332.14ms, tok/sec:157342.76
step 13348, loss: 3.184652, norm:0.2541, lr:8.2678e-05 dt: 3331.95ms, tok/sec:157351.77
step 13349, loss: 3.195551, norm:0.2591, lr:8.2655e-05 dt: 3332.17ms, tok/sec:157341.24
HellaSwag accuracy:-2286417176676826031/-2=1143208588338413056.0000
rank 1 sample 0: Hello, I'm a language model, and this is the second time I came up with an HTML browser. I would like to show how to build a web
rank 1 sample 1: Hello, I'm a language model, a person. To me, language is about the meaning a person might have as they try to articulate it. I'm
rank 1 sample 2: Hello, I'm a language model, but at this point I'm not sure what to do about how it's going to work. What is it doing,
rank 1 sample 3: Hello, I'm a language model, and I'm working with native English speakers:
Thanks, Mike. My friends are good language modelers, and I
rank 0 sample 0: Hello, I'm a language model, and I'll be writing in English tomorrow; a second at a seminar meeting on the Internet, on my computer, my
rank 0 sample 1: Hello, I'm a language model, so to speak. So I'll know what's interesting about that. :)
So what about you? Can you think
rank 0 sample 2: Hello, I'm a language model, so I had the good friend out here. My good friend and friend were the people that I was talking to, so
rank 0 sample 3: Hello, I'm a language model, and one of the things I do is make changes to my language. The reason being that I am working on this kind
step 13350, loss: 3.210434, norm:0.2606, lr:8.2632e-05 dt: 48387.58ms, tok/sec:10835.18
step 13351, loss: 3.208007, norm:0.2642, lr:8.2608e-05 dt: 3332.00ms, tok/sec:157349.36
step 13352, loss: 3.182731, norm:0.2588, lr:8.2585e-05 dt: 3332.04ms, tok/sec:157347.57
step 13353, loss: 3.161218, norm:0.2585, lr:8.2562e-05 dt: 3332.34ms, tok/sec:157333.21
step 13354, loss: 3.176955, norm:0.2698, lr:8.2538e-05 dt: 3331.99ms, tok/sec:157349.69
step 13355, loss: 3.249322, norm:0.2591, lr:8.2515e-05 dt: 3331.95ms, tok/sec:157351.66
step 13356, loss: 3.201541, norm:0.2598, lr:8.2492e-05 dt: 3331.87ms, tok/sec:157355.34
step 13357, loss: 3.180108, norm:0.2765, lr:8.2468e-05 dt: 3332.07ms, tok/sec:157346.16
step 13358, loss: 3.220697, norm:0.2750, lr:8.2445e-05 dt: 3331.89ms, tok/sec:157354.65
step 13359, loss: 3.156634, norm:0.2652, lr:8.2422e-05 dt: 3331.78ms, tok/sec:157359.57
step 13360, loss: 3.195142, norm:0.2553, lr:8.2398e-05 dt: 3331.97ms, tok/sec:157350.67
step 13361, loss: 3.181015, norm:0.2754, lr:8.2375e-05 dt: 3331.93ms, tok/sec:157352.57
step 13362, loss: 3.175895, norm:0.2694, lr:8.2352e-05 dt: 3332.24ms, tok/sec:157338.17
step 13363, loss: 3.167264, norm:0.2796, lr:8.2329e-05 dt: 3332.02ms, tok/sec:157348.28
step 13364, loss: 3.197511, norm:0.2649, lr:8.2305e-05 dt: 3331.98ms, tok/sec:157350.32
step 13365, loss: 3.174571, norm:0.2623, lr:8.2282e-05 dt: 3332.05ms, tok/sec:157346.76
step 13366, loss: 3.216753, norm:0.2908, lr:8.2259e-05 dt: 3332.14ms, tok/sec:157342.76
step 13367, loss: 3.168767, norm:0.2568, lr:8.2236e-05 dt: 3332.23ms, tok/sec:157338.31
step 13368, loss: 3.197859, norm:0.2913, lr:8.2213e-05 dt: 3331.91ms, tok/sec:157353.39
step 13369, loss: 3.220609, norm:0.2784, lr:8.2190e-05 dt: 3331.92ms, tok/sec:157353.23
step 13370, loss: 3.165156, norm:0.2620, lr:8.2166e-05 dt: 3332.33ms, tok/sec:157333.60
step 13371, loss: 3.229475, norm:0.2674, lr:8.2143e-05 dt: 3332.07ms, tok/sec:157345.98
step 13372, loss: 3.208905, norm:0.2603, lr:8.2120e-05 dt: 3332.13ms, tok/sec:157343.17
step 13373, loss: 3.211708, norm:0.2905, lr:8.2097e-05 dt: 3332.10ms, tok/sec:157344.67
step 13374, loss: 3.219016, norm:0.2653, lr:8.2074e-05 dt: 3332.09ms, tok/sec:157344.87
step 13375, loss: 3.192141, norm:0.2748, lr:8.2051e-05 dt: 3332.01ms, tok/sec:157348.69
step 13376, loss: 3.207326, norm:0.2705, lr:8.2028e-05 dt: 3331.92ms, tok/sec:157353.16
step 13377, loss: 3.212360, norm:0.2535, lr:8.2005e-05 dt: 3332.16ms, tok/sec:157341.91
step 13378, loss: 3.184582, norm:0.2609, lr:8.1982e-05 dt: 3332.19ms, tok/sec:157340.46
step 13379, loss: 3.183497, norm:0.3035, lr:8.1959e-05 dt: 3331.89ms, tok/sec:157354.70
step 13380, loss: 3.168277, norm:0.2923, lr:8.1936e-05 dt: 3331.91ms, tok/sec:157353.58
step 13381, loss: 3.228589, norm:0.2548, lr:8.1913e-05 dt: 3331.91ms, tok/sec:157353.76
step 13382, loss: 3.156037, norm:0.2581, lr:8.1890e-05 dt: 3332.06ms, tok/sec:157346.34
step 13383, loss: 3.196444, norm:0.2714, lr:8.1867e-05 dt: 3332.06ms, tok/sec:157346.73
step 13384, loss: 3.165032, norm:0.2704, lr:8.1844e-05 dt: 3332.03ms, tok/sec:157348.10
step 13385, loss: 3.181037, norm:0.2768, lr:8.1821e-05 dt: 3332.13ms, tok/sec:157343.09
step 13386, loss: 3.147052, norm:0.2765, lr:8.1798e-05 dt: 3332.07ms, tok/sec:157346.27
step 13387, loss: 3.178923, norm:0.2857, lr:8.1775e-05 dt: 3331.90ms, tok/sec:157353.98
step 13388, loss: 3.160733, norm:0.2666, lr:8.1752e-05 dt: 3331.96ms, tok/sec:157351.41
step 13389, loss: 3.051040, norm:0.3135, lr:8.1729e-05 dt: 3331.90ms, tok/sec:157354.09
step 13390, loss: 3.177952, norm:0.2741, lr:8.1706e-05 dt: 3332.23ms, tok/sec:157338.32
step 13391, loss: 3.130867, norm:0.2685, lr:8.1683e-05 dt: 3331.96ms, tok/sec:157351.25
step 13392, loss: 3.148934, norm:0.2718, lr:8.1660e-05 dt: 3331.89ms, tok/sec:157354.41
step 13393, loss: 3.168478, norm:0.2630, lr:8.1637e-05 dt: 3331.88ms, tok/sec:157354.91
step 13394, loss: 3.184902, norm:0.2662, lr:8.1614e-05 dt: 3331.99ms, tok/sec:157349.61
step 13395, loss: 3.117004, norm:0.2516, lr:8.1591e-05 dt: 3331.94ms, tok/sec:157352.12
step 13396, loss: 3.195846, norm:0.2624, lr:8.1569e-05 dt: 3331.77ms, tok/sec:157360.34
step 13397, loss: 3.133004, norm:0.2705, lr:8.1546e-05 dt: 3332.15ms, tok/sec:157342.21
step 13398, loss: 3.198828, norm:0.2610, lr:8.1523e-05 dt: 3331.91ms, tok/sec:157353.57
step 13399, loss: 3.128443, norm:0.2732, lr:8.1500e-05 dt: 3331.97ms, tok/sec:157350.53
validation loss: 3.2038
Model and optimizer state saved.
HellaSwag accuracy:-2295565113419922351/-2=1147782556709961216.0000
rank 1 sample 0: Hello, I'm a language model, so I need to start my computer building and build the applications it needs to process. It's very simple and I'm
rank 1 sample 1: Hello, I'm a language model, a teacher.<|endoftext|>The first time I asked my student (or child) the answer first, she was not sure what
rank 1 sample 2: Hello, I'm a language model, but because you've got a lot of different languages, there has always been a lot of overlap with one another because everyone
rank 1 sample 3: Hello, I'm a language model, so I'm working with two languages together all the time just to share their learning. These ideas are going to go a
rank 0 sample 0: Hello, I'm a language model, and I'll be using them all day one hour in the future... [and] the others... I'll be going
rank 0 sample 1: Hello, I'm a language model, so how do I do that? Have you heard them first? How do you know if the answer is correct?

rank 0 sample 2: Hello, I'm a language model, so I didn't know much about programming languages that I'd come back to. So, I'm going to try and
rank 0 sample 3: Hello, I'm a language model, and one of the things I do with the JVM is to look at things on the screen and get a look right
step 13400, loss: 3.150072, norm:0.2603, lr:8.1477e-05 dt: 56048.53ms, tok/sec:9354.18
step 13401, loss: 3.122508, norm:0.2631, lr:8.1455e-05 dt: 3332.17ms, tok/sec:157341.51
step 13402, loss: 3.179838, norm:0.2540, lr:8.1432e-05 dt: 3332.73ms, tok/sec:157314.85
step 13403, loss: 3.178566, norm:0.2831, lr:8.1409e-05 dt: 3332.14ms, tok/sec:157342.71
step 13404, loss: 3.212550, norm:0.2711, lr:8.1386e-05 dt: 3332.12ms, tok/sec:157343.90
step 13405, loss: 3.142749, norm:0.2855, lr:8.1364e-05 dt: 3332.10ms, tok/sec:157344.85
step 13406, loss: 3.231873, norm:0.2721, lr:8.1341e-05 dt: 3332.06ms, tok/sec:157346.49
step 13407, loss: 3.278284, norm:0.2719, lr:8.1318e-05 dt: 3331.98ms, tok/sec:157350.51
step 13408, loss: 3.238819, norm:0.2747, lr:8.1295e-05 dt: 3332.28ms, tok/sec:157336.14
step 13409, loss: 3.160985, norm:0.2577, lr:8.1273e-05 dt: 3332.18ms, tok/sec:157340.75
step 13410, loss: 3.223046, norm:0.2829, lr:8.1250e-05 dt: 3331.94ms, tok/sec:157352.31
step 13411, loss: 3.240599, norm:0.2754, lr:8.1227e-05 dt: 3332.10ms, tok/sec:157344.75
step 13412, loss: 3.261704, norm:0.2986, lr:8.1205e-05 dt: 3332.10ms, tok/sec:157344.72
step 13413, loss: 3.205515, norm:0.2653, lr:8.1182e-05 dt: 3332.10ms, tok/sec:157344.62
step 13414, loss: 3.192770, norm:0.2724, lr:8.1159e-05 dt: 3332.08ms, tok/sec:157345.66
step 13415, loss: 3.169183, norm:0.2658, lr:8.1137e-05 dt: 3331.97ms, tok/sec:157350.71
step 13416, loss: 3.175198, norm:0.2816, lr:8.1114e-05 dt: 3331.92ms, tok/sec:157353.21
step 13417, loss: 3.214702, norm:0.2774, lr:8.1092e-05 dt: 3332.26ms, tok/sec:157337.19
step 13418, loss: 3.305278, norm:0.2724, lr:8.1069e-05 dt: 3331.95ms, tok/sec:157351.75
step 13419, loss: 3.176811, norm:0.2763, lr:8.1046e-05 dt: 3331.86ms, tok/sec:157355.99
step 13420, loss: 3.195421, norm:0.2721, lr:8.1024e-05 dt: 3332.05ms, tok/sec:157346.83
step 13421, loss: 3.184131, norm:0.2603, lr:8.1001e-05 dt: 3332.09ms, tok/sec:157345.02
step 13422, loss: 3.189515, norm:0.3249, lr:8.0979e-05 dt: 3331.90ms, tok/sec:157353.87
step 13423, loss: 3.207003, norm:0.2800, lr:8.0956e-05 dt: 3332.04ms, tok/sec:157347.39
step 13424, loss: 3.199645, norm:0.2966, lr:8.0934e-05 dt: 3331.95ms, tok/sec:157351.59
step 13425, loss: 3.230678, norm:0.2836, lr:8.0911e-05 dt: 3332.10ms, tok/sec:157344.85
step 13426, loss: 3.130975, norm:0.2706, lr:8.0889e-05 dt: 3331.95ms, tok/sec:157351.66
step 13427, loss: 3.173678, norm:0.2781, lr:8.0866e-05 dt: 3332.02ms, tok/sec:157348.24
step 13428, loss: 3.164466, norm:0.2531, lr:8.0844e-05 dt: 3331.97ms, tok/sec:157350.86
step 13429, loss: 3.095296, norm:0.2666, lr:8.0821e-05 dt: 3332.08ms, tok/sec:157345.49
step 13430, loss: 3.182078, norm:0.2631, lr:8.0799e-05 dt: 3332.15ms, tok/sec:157342.39
step 13431, loss: 3.150426, norm:0.3041, lr:8.0776e-05 dt: 3331.92ms, tok/sec:157353.13
step 13432, loss: 3.174664, norm:0.2752, lr:8.0754e-05 dt: 3332.04ms, tok/sec:157347.24
step 13433, loss: 3.131073, norm:0.2647, lr:8.0731e-05 dt: 3331.96ms, tok/sec:157351.25
step 13434, loss: 3.147831, norm:0.2585, lr:8.0709e-05 dt: 3332.09ms, tok/sec:157344.95
step 13435, loss: 3.150240, norm:0.2628, lr:8.0687e-05 dt: 3332.35ms, tok/sec:157332.93
step 13436, loss: 3.161389, norm:0.2706, lr:8.0664e-05 dt: 3331.96ms, tok/sec:157351.44
step 13437, loss: 3.190674, norm:0.2747, lr:8.0642e-05 dt: 3332.08ms, tok/sec:157345.43
step 13438, loss: 3.137349, norm:0.2682, lr:8.0620e-05 dt: 3331.90ms, tok/sec:157353.91
step 13439, loss: 3.232611, norm:0.2764, lr:8.0597e-05 dt: 3332.01ms, tok/sec:157349.10
step 13440, loss: 3.205114, norm:0.2688, lr:8.0575e-05 dt: 3332.00ms, tok/sec:157349.28
step 13441, loss: 3.288252, norm:0.3142, lr:8.0553e-05 dt: 3332.00ms, tok/sec:157349.17
step 13442, loss: 3.277641, norm:0.2933, lr:8.0530e-05 dt: 3331.77ms, tok/sec:157360.24
step 13443, loss: 3.253857, norm:0.2823, lr:8.0508e-05 dt: 3332.01ms, tok/sec:157348.89
step 13444, loss: 3.258760, norm:0.2813, lr:8.0486e-05 dt: 3332.04ms, tok/sec:157347.26
step 13445, loss: 3.244921, norm:0.3197, lr:8.0463e-05 dt: 3332.51ms, tok/sec:157325.25
step 13446, loss: 3.249518, norm:0.2913, lr:8.0441e-05 dt: 3332.27ms, tok/sec:157336.37
step 13447, loss: 3.234865, norm:0.2824, lr:8.0419e-05 dt: 3332.11ms, tok/sec:157344.07
step 13448, loss: 3.253207, norm:0.3113, lr:8.0397e-05 dt: 3332.01ms, tok/sec:157348.78
step 13449, loss: 3.142517, norm:0.2687, lr:8.0374e-05 dt: 3332.18ms, tok/sec:157340.92
HellaSwag accuracy:6927754146876722257/-2=-3463877073438361088.0000
rank 1 sample 0: Hello, I'm a language model, and this is the question that came upon us during this lesson.)
This has been my last day of school. I
rank 1 sample 1: Hello, I'm a language model, a computer program designed to teach you some complex programming concepts on a day-to-day basis.
I'm a
rank 1 sample 2: Hello, I'm a language model, but who's in the world?
I'm a teacher; my name is Bill. I want to be part of
rank 1 sample 3: Hello, I'm a language model, and I'm interested to introduce you to "language models;" some simple programming instructions. Most I'll be talking about are
rank 0 sample 0: Hello, I'm a language model, and I am a model builder of software environments. I grew up listening to people talk. I can tell how much people
rank 0 sample 1: Hello, I'm a language model, I'd like to learn how to tell the story a little more and know where the words come from; how to say
rank 0 sample 2: Hello, I'm a language model, so I will say ... (because the ... - for example the first sentence is ... ... (because the second is the
rank 0 sample 3: Hello, I'm a language model, not a one-time program, and you only need to work with an environment for debugging (this is not an area
step 13450, loss: 3.205624, norm:0.2820, lr:8.0352e-05 dt: 48400.24ms, tok/sec:10832.34
step 13451, loss: 3.210235, norm:0.2738, lr:8.0330e-05 dt: 3332.00ms, tok/sec:157349.14
step 13452, loss: 3.198666, norm:0.2687, lr:8.0308e-05 dt: 3331.98ms, tok/sec:157350.34
step 13453, loss: 3.262918, norm:0.2605, lr:8.0286e-05 dt: 3331.95ms, tok/sec:157351.69
step 13454, loss: 3.271244, norm:0.2750, lr:8.0263e-05 dt: 3331.82ms, tok/sec:157357.67
step 13455, loss: 3.147449, norm:0.2946, lr:8.0241e-05 dt: 3331.93ms, tok/sec:157352.47
step 13456, loss: 3.207807, norm:0.2553, lr:8.0219e-05 dt: 3332.07ms, tok/sec:157345.98
step 13457, loss: 3.211658, norm:0.2673, lr:8.0197e-05 dt: 3332.08ms, tok/sec:157345.50
step 13458, loss: 3.203586, norm:0.2564, lr:8.0175e-05 dt: 3331.99ms, tok/sec:157349.79
step 13459, loss: 3.165694, norm:0.2600, lr:8.0153e-05 dt: 3331.94ms, tok/sec:157352.07
step 13460, loss: 3.211305, norm:0.2545, lr:8.0131e-05 dt: 3331.92ms, tok/sec:157352.93
step 13461, loss: 3.184159, norm:0.2622, lr:8.0108e-05 dt: 3331.93ms, tok/sec:157352.80
step 13462, loss: 3.213228, norm:0.2673, lr:8.0086e-05 dt: 3332.03ms, tok/sec:157348.12
step 13463, loss: 3.122764, norm:0.2584, lr:8.0064e-05 dt: 3331.84ms, tok/sec:157356.69
step 13464, loss: 3.175142, norm:0.2572, lr:8.0042e-05 dt: 3331.87ms, tok/sec:157355.26
step 13465, loss: 3.143600, norm:0.2553, lr:8.0020e-05 dt: 3331.81ms, tok/sec:157358.18
step 13466, loss: 3.163363, norm:0.2613, lr:7.9998e-05 dt: 3332.24ms, tok/sec:157338.24
step 13467, loss: 3.209703, norm:0.2903, lr:7.9976e-05 dt: 3331.96ms, tok/sec:157351.01
step 13468, loss: 3.125068, norm:0.2669, lr:7.9954e-05 dt: 3332.18ms, tok/sec:157341.03
step 13469, loss: 3.200808, norm:0.2721, lr:7.9932e-05 dt: 3331.85ms, tok/sec:157356.48
step 13470, loss: 3.124930, norm:0.2585, lr:7.9910e-05 dt: 3331.97ms, tok/sec:157350.92
step 13471, loss: 3.130328, norm:0.2697, lr:7.9888e-05 dt: 3331.86ms, tok/sec:157355.76
step 13472, loss: 3.116913, norm:0.2767, lr:7.9866e-05 dt: 3332.05ms, tok/sec:157347.09
step 13473, loss: 3.132782, norm:0.2833, lr:7.9844e-05 dt: 3331.83ms, tok/sec:157357.48
step 13474, loss: 3.152031, norm:0.2879, lr:7.9822e-05 dt: 3331.86ms, tok/sec:157355.82
step 13475, loss: 3.185448, norm:0.2788, lr:7.9800e-05 dt: 3332.26ms, tok/sec:157337.27
step 13476, loss: 3.138612, norm:0.2653, lr:7.9779e-05 dt: 3331.92ms, tok/sec:157353.20
step 13477, loss: 3.192453, norm:0.2947, lr:7.9757e-05 dt: 3331.82ms, tok/sec:157357.70
step 13478, loss: 3.169026, norm:0.2773, lr:7.9735e-05 dt: 3331.88ms, tok/sec:157355.10
step 13479, loss: 3.175554, norm:0.2713, lr:7.9713e-05 dt: 3331.84ms, tok/sec:157356.83
step 13480, loss: 3.212087, norm:0.2897, lr:7.9691e-05 dt: 3332.19ms, tok/sec:157340.29
step 13481, loss: 3.272935, norm:0.2825, lr:7.9669e-05 dt: 3331.87ms, tok/sec:157355.63
step 13482, loss: 3.167104, norm:0.2668, lr:7.9647e-05 dt: 3331.88ms, tok/sec:157354.99
step 13483, loss: 3.211771, norm:0.2643, lr:7.9625e-05 dt: 3331.99ms, tok/sec:157349.70
step 13484, loss: 3.197616, norm:0.2739, lr:7.9604e-05 dt: 3332.20ms, tok/sec:157339.99
step 13485, loss: 3.197856, norm:0.2741, lr:7.9582e-05 dt: 3331.87ms, tok/sec:157355.26
step 13486, loss: 3.197592, norm:0.2878, lr:7.9560e-05 dt: 3331.94ms, tok/sec:157352.10
step 13487, loss: 3.221533, norm:0.2897, lr:7.9538e-05 dt: 3331.87ms, tok/sec:157355.25
step 13488, loss: 3.246525, norm:0.3101, lr:7.9516e-05 dt: 3332.03ms, tok/sec:157347.79
step 13489, loss: 3.198212, norm:0.2963, lr:7.9495e-05 dt: 3332.05ms, tok/sec:157346.94
step 13490, loss: 3.229866, norm:0.2780, lr:7.9473e-05 dt: 3331.72ms, tok/sec:157362.70
step 13491, loss: 3.198215, norm:0.2657, lr:7.9451e-05 dt: 3331.95ms, tok/sec:157351.56
step 13492, loss: 3.195354, norm:0.2689, lr:7.9429e-05 dt: 3331.83ms, tok/sec:157357.57
step 13493, loss: 3.144282, norm:0.2655, lr:7.9408e-05 dt: 3332.18ms, tok/sec:157340.74
step 13494, loss: 3.193209, norm:0.2557, lr:7.9386e-05 dt: 3331.88ms, tok/sec:157355.19
step 13495, loss: 3.233312, norm:0.2535, lr:7.9364e-05 dt: 3331.93ms, tok/sec:157352.75
step 13496, loss: 3.171587, norm:0.2502, lr:7.9343e-05 dt: 3331.92ms, tok/sec:157353.21
step 13497, loss: 3.133712, norm:0.2482, lr:7.9321e-05 dt: 3331.88ms, tok/sec:157354.96
step 13498, loss: 3.168253, norm:0.2494, lr:7.9299e-05 dt: 3332.03ms, tok/sec:157347.94
step 13499, loss: 3.166565, norm:0.2412, lr:7.9278e-05 dt: 3332.05ms, tok/sec:157347.20
validation loss: 3.2019
Model and optimizer state saved.
HellaSwag accuracy:6927806923434853393/-2=-3463903461717426688.0000
rank 1 sample 0: Hello, I'm a language model, in this case, just like how computer programming does for software; it's much more fun and useful than just programming.
rank 1 sample 1: Hello, I'm a language model, not an English-speaking person. You'd be able of any and all other situations are based on your own language.
rank 1 sample 2: Hello, I'm a language model, but who is my audience?
The first question is the audience: "What is the difference between "English"? Do
rank 1 sample 3: Hello, I'm a language model, and I'm looking at one of the concepts that I understand myself."
At the height of the pandemic, the
rank 0 sample 0: Hello, I'm a language model, and I need to understand some languages for teaching that I want to see taught by a teacher. If that happens, the
rank 0 sample 1: Hello, I'm a language model, I teach an English-speaking learner. It uses what's called "word" as it is often called. This
rank 0 sample 2: Hello, I'm a language model, so I would have learned more about English and my accent. The language model is really about learning how to speak and not
rank 0 sample 3: Hello, I'm a language model, and one of the things I do is write them out. I know you are there when you are ready to go because
step 13500, loss: 3.195852, norm:0.2485, lr:7.9256e-05 dt: 56199.71ms, tok/sec:9329.02
step 13501, loss: 3.104564, norm:0.2547, lr:7.9234e-05 dt: 3331.98ms, tok/sec:157350.36
step 13502, loss: 3.137474, norm:0.2487, lr:7.9213e-05 dt: 3331.93ms, tok/sec:157352.75
step 13503, loss: 3.189518, norm:0.2698, lr:7.9191e-05 dt: 3331.89ms, tok/sec:157354.73
step 13504, loss: 3.164376, norm:0.2527, lr:7.9170e-05 dt: 3332.05ms, tok/sec:157347.12
step 13505, loss: 3.165215, norm:0.2569, lr:7.9148e-05 dt: 3331.91ms, tok/sec:157353.50
step 13506, loss: 3.124006, norm:0.2569, lr:7.9126e-05 dt: 3331.99ms, tok/sec:157349.65
step 13507, loss: 3.190533, norm:0.2605, lr:7.9105e-05 dt: 3332.25ms, tok/sec:157337.52
step 13508, loss: 3.242086, norm:0.2617, lr:7.9083e-05 dt: 3332.09ms, tok/sec:157345.08
step 13509, loss: 3.132543, norm:0.2658, lr:7.9062e-05 dt: 3332.07ms, tok/sec:157345.87
step 13510, loss: 3.180992, norm:0.2665, lr:7.9040e-05 dt: 3332.06ms, tok/sec:157346.38
step 13511, loss: 3.192046, norm:0.2648, lr:7.9019e-05 dt: 3331.96ms, tok/sec:157351.14
step 13512, loss: 3.184528, norm:0.2640, lr:7.8997e-05 dt: 3332.12ms, tok/sec:157343.51
step 13513, loss: 3.193367, norm:0.2627, lr:7.8976e-05 dt: 3332.26ms, tok/sec:157336.99
step 13514, loss: 3.154877, norm:0.2801, lr:7.8954e-05 dt: 3332.01ms, tok/sec:157348.78
step 13515, loss: 3.345485, norm:0.2805, lr:7.8933e-05 dt: 3332.17ms, tok/sec:157341.13
step 13516, loss: 3.211677, norm:0.2754, lr:7.8911e-05 dt: 3331.72ms, tok/sec:157362.77
step 13517, loss: 3.133532, norm:0.2738, lr:7.8890e-05 dt: 3331.90ms, tok/sec:157353.89
step 13518, loss: 3.159398, norm:0.2761, lr:7.8869e-05 dt: 3331.96ms, tok/sec:157351.33
step 13519, loss: 3.214695, norm:0.2696, lr:7.8847e-05 dt: 3331.80ms, tok/sec:157358.70
step 13520, loss: 3.184995, norm:0.2828, lr:7.8826e-05 dt: 3332.27ms, tok/sec:157336.44
step 13521, loss: 3.166823, norm:0.2655, lr:7.8804e-05 dt: 3332.08ms, tok/sec:157345.59
step 13522, loss: 3.183559, norm:0.2612, lr:7.8783e-05 dt: 3331.90ms, tok/sec:157354.03
step 13523, loss: 3.222458, norm:0.2571, lr:7.8762e-05 dt: 3332.15ms, tok/sec:157342.25
step 13524, loss: 3.184655, norm:0.2594, lr:7.8740e-05 dt: 3331.80ms, tok/sec:157358.72
step 13525, loss: 3.239872, norm:0.2999, lr:7.8719e-05 dt: 3334.53ms, tok/sec:157229.97
step 13526, loss: 3.222322, norm:0.2635, lr:7.8698e-05 dt: 3332.16ms, tok/sec:157341.82
step 13527, loss: 3.207443, norm:0.2531, lr:7.8676e-05 dt: 3332.26ms, tok/sec:157337.27
step 13528, loss: 3.186766, norm:0.2706, lr:7.8655e-05 dt: 3332.05ms, tok/sec:157346.76
step 13529, loss: 3.220932, norm:0.2603, lr:7.8634e-05 dt: 3331.98ms, tok/sec:157350.14
step 13530, loss: 3.214551, norm:0.2625, lr:7.8612e-05 dt: 3331.98ms, tok/sec:157350.13
step 13531, loss: 3.155302, norm:0.2682, lr:7.8591e-05 dt: 3332.21ms, tok/sec:157339.32
step 13532, loss: 3.131377, norm:0.2665, lr:7.8570e-05 dt: 3331.86ms, tok/sec:157355.97
step 13533, loss: 3.187667, norm:0.2460, lr:7.8549e-05 dt: 3332.09ms, tok/sec:157345.23
step 13534, loss: 3.122654, norm:0.2587, lr:7.8527e-05 dt: 3332.21ms, tok/sec:157339.24
step 13535, loss: 3.186808, norm:0.3010, lr:7.8506e-05 dt: 3332.16ms, tok/sec:157341.92
step 13536, loss: 3.129944, norm:0.2524, lr:7.8485e-05 dt: 3331.86ms, tok/sec:157356.06
step 13537, loss: 3.156857, norm:0.2627, lr:7.8464e-05 dt: 3331.95ms, tok/sec:157351.65
step 13538, loss: 3.145864, norm:0.2737, lr:7.8443e-05 dt: 3332.19ms, tok/sec:157340.43
step 13539, loss: 3.163003, norm:0.2696, lr:7.8421e-05 dt: 3332.26ms, tok/sec:157337.06
step 13540, loss: 3.245654, norm:0.2648, lr:7.8400e-05 dt: 3331.91ms, tok/sec:157353.65
step 13541, loss: 3.170280, norm:0.2658, lr:7.8379e-05 dt: 3331.89ms, tok/sec:157354.69
step 13542, loss: 3.160384, norm:0.2550, lr:7.8358e-05 dt: 3332.08ms, tok/sec:157345.44
step 13543, loss: 3.237527, norm:0.2837, lr:7.8337e-05 dt: 3332.04ms, tok/sec:157347.41
step 13544, loss: 3.119862, norm:0.2762, lr:7.8316e-05 dt: 3332.04ms, tok/sec:157347.30
step 13545, loss: 3.213775, norm:0.2723, lr:7.8295e-05 dt: 3332.16ms, tok/sec:157342.00
step 13546, loss: 3.200973, norm:0.2580, lr:7.8273e-05 dt: 3331.98ms, tok/sec:157350.39
step 13547, loss: 3.154834, norm:0.2915, lr:7.8252e-05 dt: 3332.14ms, tok/sec:157342.75
step 13548, loss: 3.185628, norm:0.2649, lr:7.8231e-05 dt: 3332.14ms, tok/sec:157342.88
step 13549, loss: 3.204106, norm:0.2734, lr:7.8210e-05 dt: 3331.89ms, tok/sec:157354.59
HellaSwag accuracy:-2286575506350963631/-2=1143287753175481856.0000
rank 1 sample 0: Hello, I'm a language model, and so I'm gonna start with semantics and you know when you're gonna look at a new syntax. And I'm
rank 1 sample 1: Hello, I'm a language model, a computer program. I'm an intern and I'm reading about "Hello, My Internet Cafe", "Hello, My
rank 1 sample 2: Hello, I'm a language model, but your question is, "What is the difference between Java class and class definitions?"
The most interesting question, yes
rank 1 sample 3: Hello, I'm a language model, and I'm pretty much of a Language Programming Language. Even though I haven't been familiar with Python, I like the
rank 0 sample 0: Hello, I'm a language model, and I'll be talking about this from other classes.
- Language Group 1: Introduction to Language Modeling
-
rank 0 sample 1: Hello, I'm a language model, so don't worry if you don't know the differences between these two people. I've made some comments to share about
rank 0 sample 2: Hello, I'm a language model, so I learned it to my parents from my high school. They have an amazing community, and I'm a great mentor
rank 0 sample 3: Hello, I'm a language model, and what I mean is that you do what’s called a language learning experiment to develop your skill set, language
step 13550, loss: 3.199049, norm:0.2782, lr:7.8189e-05 dt: 48471.43ms, tok/sec:10816.43
step 13551, loss: 3.178761, norm:0.2730, lr:7.8168e-05 dt: 3332.30ms, tok/sec:157335.14
step 13552, loss: 3.192442, norm:0.3168, lr:7.8147e-05 dt: 3331.91ms, tok/sec:157353.50
step 13553, loss: 3.224503, norm:0.2722, lr:7.8126e-05 dt: 3332.06ms, tok/sec:157346.29
step 13554, loss: 3.226831, norm:0.2765, lr:7.8105e-05 dt: 3332.19ms, tok/sec:157340.31
step 13555, loss: 3.205245, norm:0.2928, lr:7.8084e-05 dt: 3332.08ms, tok/sec:157345.67
step 13556, loss: 3.174579, norm:0.2646, lr:7.8063e-05 dt: 3331.93ms, tok/sec:157352.75
step 13557, loss: 3.199226, norm:0.2748, lr:7.8042e-05 dt: 3331.98ms, tok/sec:157350.32
step 13558, loss: 3.186742, norm:0.2689, lr:7.8021e-05 dt: 3332.06ms, tok/sec:157346.39
step 13559, loss: 3.210157, norm:0.2646, lr:7.8000e-05 dt: 3332.12ms, tok/sec:157343.64
step 13560, loss: 3.198569, norm:0.2622, lr:7.7979e-05 dt: 3332.48ms, tok/sec:157326.75
step 13561, loss: 3.204209, norm:0.2645, lr:7.7958e-05 dt: 3331.97ms, tok/sec:157350.85
step 13562, loss: 3.169455, norm:0.2596, lr:7.7937e-05 dt: 3332.04ms, tok/sec:157347.26
step 13563, loss: 3.220207, norm:0.2656, lr:7.7917e-05 dt: 3331.88ms, tok/sec:157354.84
step 13564, loss: 3.202939, norm:0.2669, lr:7.7896e-05 dt: 3332.04ms, tok/sec:157347.23
step 13565, loss: 3.193774, norm:0.2680, lr:7.7875e-05 dt: 3332.05ms, tok/sec:157346.94
step 13566, loss: 3.164111, norm:0.2819, lr:7.7854e-05 dt: 3332.17ms, tok/sec:157341.53
step 13567, loss: 3.156370, norm:0.2802, lr:7.7833e-05 dt: 3331.70ms, tok/sec:157363.64
step 13568, loss: 3.112499, norm:0.3044, lr:7.7812e-05 dt: 3332.00ms, tok/sec:157349.31
step 13569, loss: 3.107837, norm:0.2718, lr:7.7791e-05 dt: 3332.23ms, tok/sec:157338.55
step 13570, loss: 3.120944, norm:0.2882, lr:7.7771e-05 dt: 3332.14ms, tok/sec:157342.53
step 13571, loss: 3.162748, norm:0.2599, lr:7.7750e-05 dt: 3332.04ms, tok/sec:157347.53
step 13572, loss: 3.124427, norm:0.2575, lr:7.7729e-05 dt: 3332.07ms, tok/sec:157345.87
step 13573, loss: 3.220128, norm:0.2772, lr:7.7708e-05 dt: 3331.92ms, tok/sec:157353.18
step 13574, loss: 3.173220, norm:0.2835, lr:7.7687e-05 dt: 3332.31ms, tok/sec:157334.89
step 13575, loss: 3.156712, norm:0.2685, lr:7.7667e-05 dt: 3331.87ms, tok/sec:157355.62
step 13576, loss: 3.248101, norm:0.4388, lr:7.7646e-05 dt: 3331.91ms, tok/sec:157353.65
step 13577, loss: 3.120158, norm:0.2983, lr:7.7625e-05 dt: 3332.15ms, tok/sec:157342.30
step 13578, loss: 3.217622, norm:0.3232, lr:7.7605e-05 dt: 3332.26ms, tok/sec:157336.86
step 13579, loss: 3.226126, norm:0.2962, lr:7.7584e-05 dt: 3331.93ms, tok/sec:157352.57
step 13580, loss: 3.204602, norm:0.2828, lr:7.7563e-05 dt: 3331.77ms, tok/sec:157360.08
step 13581, loss: 3.158249, norm:0.3444, lr:7.7542e-05 dt: 3332.13ms, tok/sec:157343.25
step 13582, loss: 3.220356, norm:0.3121, lr:7.7522e-05 dt: 3332.15ms, tok/sec:157342.34
step 13583, loss: 3.185263, norm:0.3133, lr:7.7501e-05 dt: 3331.86ms, tok/sec:157356.03
step 13584, loss: 3.263122, norm:0.2964, lr:7.7480e-05 dt: 3332.22ms, tok/sec:157339.12
step 13585, loss: 3.221985, norm:0.2828, lr:7.7460e-05 dt: 3332.12ms, tok/sec:157343.72
step 13586, loss: 3.198956, norm:0.2853, lr:7.7439e-05 dt: 3332.17ms, tok/sec:157341.23
step 13587, loss: 3.154130, norm:0.2862, lr:7.7419e-05 dt: 3332.49ms, tok/sec:157326.37
step 13588, loss: 3.135795, norm:0.2755, lr:7.7398e-05 dt: 3332.25ms, tok/sec:157337.74
step 13589, loss: 3.182489, norm:0.2765, lr:7.7377e-05 dt: 3332.04ms, tok/sec:157347.42
step 13590, loss: 3.194275, norm:0.2778, lr:7.7357e-05 dt: 3332.10ms, tok/sec:157344.73
step 13591, loss: 3.154421, norm:0.2621, lr:7.7336e-05 dt: 3332.02ms, tok/sec:157348.21
step 13592, loss: 3.190482, norm:0.2791, lr:7.7316e-05 dt: 3332.36ms, tok/sec:157332.50
step 13593, loss: 3.269370, norm:0.3375, lr:7.7295e-05 dt: 3331.97ms, tok/sec:157350.58
step 13594, loss: 3.175351, norm:0.2632, lr:7.7275e-05 dt: 3331.81ms, tok/sec:157358.53
step 13595, loss: 3.212206, norm:0.2738, lr:7.7254e-05 dt: 3332.09ms, tok/sec:157344.99
step 13596, loss: 3.220804, norm:0.3110, lr:7.7234e-05 dt: 3332.27ms, tok/sec:157336.80
step 13597, loss: 3.256192, norm:0.2940, lr:7.7213e-05 dt: 3332.33ms, tok/sec:157333.61
step 13598, loss: 3.182950, norm:0.2785, lr:7.7193e-05 dt: 3332.19ms, tok/sec:157340.21
step 13599, loss: 3.223378, norm:0.2832, lr:7.7172e-05 dt: 3332.03ms, tok/sec:157348.15
validation loss: 3.2003
Model and optimizer state saved.
HellaSwag accuracy:6936778938317505617/-2=-3468389469158752768.0000
rank 1 sample 0: Hello, I'm a language model, and my vocabulary is English, so are they in my family...
So, I've spent time with a few of
rank 1 sample 1: Hello, I'm a language model, not an engineer, so I've done all the work by myself. I think it means that technology has to be a
rank 1 sample 2: Hello, I'm a language model, but is a computer science student. I'm a programming instructor myself and am in the process of learning the basics in the
rank 1 sample 3: Hello, I'm a language model, and I'm using the model from the second part of my job. But since I learned first grade, I am using
rank 0 sample 0: Hello, I'm a language model, and I love it! But you also will do more than that!"
This is one of a few ways that a
rank 0 sample 1: Hello, I'm a language model, so don't worry if you are asking for something; as a matter of fact, you'll have to know the language
rank 0 sample 2: Hello, I'm a language model, so I like that in every detail.
My daughter is reading English in a textbook. I'm a language model.
rank 0 sample 3: Hello, I'm a language model, and here's what I need:
I learned it in the beginning because my Spanish teacher did it as a kid,"
step 13600, loss: 3.168249, norm:0.2653, lr:7.7152e-05 dt: 56196.28ms, tok/sec:9329.59
step 13601, loss: 3.200538, norm:0.2742, lr:7.7131e-05 dt: 3332.22ms, tok/sec:157339.16
step 13602, loss: 3.142579, norm:0.2697, lr:7.7111e-05 dt: 3332.04ms, tok/sec:157347.24
step 13603, loss: 3.130294, norm:0.2607, lr:7.7090e-05 dt: 3332.00ms, tok/sec:157349.19
step 13604, loss: 3.130890, norm:0.2544, lr:7.7070e-05 dt: 3332.26ms, tok/sec:157337.21
step 13605, loss: 3.244552, norm:0.3154, lr:7.7050e-05 dt: 3331.98ms, tok/sec:157350.44
step 13606, loss: 3.184146, norm:0.2757, lr:7.7029e-05 dt: 3331.83ms, tok/sec:157357.24
step 13607, loss: 3.184624, norm:0.2732, lr:7.7009e-05 dt: 3331.86ms, tok/sec:157356.03
step 13608, loss: 3.141632, norm:0.2732, lr:7.6988e-05 dt: 3332.15ms, tok/sec:157342.28
step 13609, loss: 3.123735, norm:0.2492, lr:7.6968e-05 dt: 3332.01ms, tok/sec:157348.80
step 13610, loss: 3.198774, norm:0.2769, lr:7.6948e-05 dt: 3331.91ms, tok/sec:157353.45
step 13611, loss: 3.187729, norm:0.2568, lr:7.6927e-05 dt: 3332.04ms, tok/sec:157347.56
step 13612, loss: 3.100479, norm:0.2560, lr:7.6907e-05 dt: 3332.13ms, tok/sec:157342.97
step 13613, loss: 3.155594, norm:0.2655, lr:7.6887e-05 dt: 3332.33ms, tok/sec:157333.93
step 13614, loss: 3.207995, norm:0.2673, lr:7.6866e-05 dt: 3331.94ms, tok/sec:157352.37
step 13615, loss: 3.149111, norm:0.2680, lr:7.6846e-05 dt: 3332.07ms, tok/sec:157346.18
step 13616, loss: 3.171502, norm:0.2599, lr:7.6826e-05 dt: 3332.09ms, tok/sec:157344.94
step 13617, loss: 3.272565, norm:0.3069, lr:7.6806e-05 dt: 3332.11ms, tok/sec:157344.28
step 13618, loss: 3.150375, norm:0.2795, lr:7.6785e-05 dt: 3332.02ms, tok/sec:157348.20
step 13619, loss: 3.251885, norm:0.2608, lr:7.6765e-05 dt: 3331.94ms, tok/sec:157352.07
step 13620, loss: 3.218903, norm:0.2710, lr:7.6745e-05 dt: 3332.20ms, tok/sec:157339.85
step 13621, loss: 3.145953, norm:0.2665, lr:7.6725e-05 dt: 3332.20ms, tok/sec:157339.90
step 13622, loss: 3.174938, norm:0.2759, lr:7.6704e-05 dt: 3332.00ms, tok/sec:157349.15
step 13623, loss: 3.197565, norm:0.2683, lr:7.6684e-05 dt: 3332.01ms, tok/sec:157348.78
step 13624, loss: 3.199307, norm:0.2662, lr:7.6664e-05 dt: 3332.16ms, tok/sec:157341.62
step 13625, loss: 3.231691, norm:0.2786, lr:7.6644e-05 dt: 3332.20ms, tok/sec:157339.72
step 13626, loss: 3.153139, norm:0.2793, lr:7.6624e-05 dt: 3332.10ms, tok/sec:157344.57
step 13627, loss: 3.318964, norm:0.3657, lr:7.6604e-05 dt: 3331.98ms, tok/sec:157350.12
step 13628, loss: 3.215089, norm:0.2605, lr:7.6584e-05 dt: 3332.07ms, tok/sec:157346.23
step 13629, loss: 3.214988, norm:0.2893, lr:7.6563e-05 dt: 3332.12ms, tok/sec:157343.51
step 13630, loss: 3.163844, norm:0.2698, lr:7.6543e-05 dt: 3332.08ms, tok/sec:157345.45
step 13631, loss: 3.181879, norm:0.2667, lr:7.6523e-05 dt: 3332.05ms, tok/sec:157346.83
step 13632, loss: 3.173754, norm:0.2640, lr:7.6503e-05 dt: 3332.13ms, tok/sec:157342.99
step 13633, loss: 3.215671, norm:0.2630, lr:7.6483e-05 dt: 3332.08ms, tok/sec:157345.69
step 13634, loss: 3.177321, norm:0.2567, lr:7.6463e-05 dt: 3332.11ms, tok/sec:157344.16
step 13635, loss: 3.199566, norm:0.2891, lr:7.6443e-05 dt: 3331.75ms, tok/sec:157361.02
step 13636, loss: 3.177887, norm:0.2663, lr:7.6423e-05 dt: 3332.01ms, tok/sec:157348.88
step 13637, loss: 3.109524, norm:0.2696, lr:7.6403e-05 dt: 3331.80ms, tok/sec:157358.56
step 13638, loss: 3.148860, norm:0.2552, lr:7.6383e-05 dt: 3332.06ms, tok/sec:157346.39
step 13639, loss: 3.173221, norm:0.2746, lr:7.6363e-05 dt: 3331.94ms, tok/sec:157352.27
step 13640, loss: 3.178982, norm:0.2529, lr:7.6343e-05 dt: 3332.09ms, tok/sec:157345.07
step 13641, loss: 3.088396, norm:0.2611, lr:7.6323e-05 dt: 3332.45ms, tok/sec:157328.31
step 13642, loss: 3.140623, norm:0.2728, lr:7.6303e-05 dt: 3332.08ms, tok/sec:157345.38
step 13643, loss: 3.179331, norm:0.2669, lr:7.6283e-05 dt: 3331.92ms, tok/sec:157353.25
step 13644, loss: 3.117864, norm:0.2475, lr:7.6263e-05 dt: 3332.06ms, tok/sec:157346.40
step 13645, loss: 3.121435, norm:0.2612, lr:7.6243e-05 dt: 3332.09ms, tok/sec:157345.06
step 13646, loss: 3.133527, norm:0.2738, lr:7.6223e-05 dt: 3331.98ms, tok/sec:157350.19
step 13647, loss: 3.155214, norm:0.2602, lr:7.6203e-05 dt: 3332.18ms, tok/sec:157340.95
step 13648, loss: 3.208722, norm:0.2660, lr:7.6183e-05 dt: 3332.06ms, tok/sec:157346.39
step 13649, loss: 3.175675, norm:0.2719, lr:7.6164e-05 dt: 3333.17ms, tok/sec:157294.10
HellaSwag accuracy:-2286575506351225775/-2=1143287753175612928.0000
rank 1 sample 0: Hello, I'm a language model, and my program is Java, so much more in depth? In this post, I'm going to talk about the basics
rank 1 sample 1: Hello, I'm a language model, a person that thinks that I know nothing about the world! I’m not exactly a global language model, but
rank 1 sample 2: Hello, I'm a language model, but still want to learn how to write a program. Now when we talk about programming, it has been around forever,
rank 1 sample 3: Hello, I'm a language model, and I'm pretty much aware of a child's understanding around what the right age to spend lots of time with children.
rank 0 sample 0: Hello, I'm a language model, and I think it is interesting because language changes can be complicated, you won't be able to say it like that!
rank 0 sample 1: Hello, I'm a language model, I teach native English. I want these to become "speaker" students in my classroom.
A language model is
rank 0 sample 2: Hello, I'm a language model, so I really appreciate the comments. And I also am a bit excited about the work I did.
I think I
rank 0 sample 3: Hello, I'm a language model, and for the last few years, I have kept a lot of questions about what words you may have as a child from
step 13650, loss: 3.130491, norm:0.2769, lr:7.6144e-05 dt: 48515.43ms, tok/sec:10806.62
step 13651, loss: 3.250937, norm:0.2837, lr:7.6124e-05 dt: 3332.16ms, tok/sec:157341.85
step 13652, loss: 3.203356, norm:0.2867, lr:7.6104e-05 dt: 3332.17ms, tok/sec:157341.22
step 13653, loss: 3.155608, norm:0.2783, lr:7.6084e-05 dt: 3332.04ms, tok/sec:157347.50
step 13654, loss: 3.212771, norm:0.2585, lr:7.6064e-05 dt: 3332.04ms, tok/sec:157347.37
step 13655, loss: 3.213130, norm:0.2729, lr:7.6044e-05 dt: 3332.12ms, tok/sec:157343.79
step 13656, loss: 3.156390, norm:0.2775, lr:7.6025e-05 dt: 3332.18ms, tok/sec:157340.94
step 13657, loss: 3.156891, norm:0.2803, lr:7.6005e-05 dt: 3332.03ms, tok/sec:157347.72
step 13658, loss: 3.188475, norm:0.2764, lr:7.5985e-05 dt: 3332.07ms, tok/sec:157346.10
step 13659, loss: 3.177755, norm:0.2779, lr:7.5965e-05 dt: 3332.06ms, tok/sec:157346.31
step 13660, loss: 3.173361, norm:0.2906, lr:7.5946e-05 dt: 3332.26ms, tok/sec:157337.03
step 13661, loss: 3.087740, norm:0.2793, lr:7.5926e-05 dt: 3331.90ms, tok/sec:157353.89
step 13662, loss: 3.206821, norm:0.2722, lr:7.5906e-05 dt: 3332.03ms, tok/sec:157348.00
step 13663, loss: 3.208792, norm:0.2888, lr:7.5886e-05 dt: 3332.03ms, tok/sec:157347.84
step 13664, loss: 3.180232, norm:0.2756, lr:7.5867e-05 dt: 3331.88ms, tok/sec:157354.89
step 13665, loss: 3.230211, norm:0.2747, lr:7.5847e-05 dt: 3331.69ms, tok/sec:157363.77
step 13666, loss: 3.157927, norm:0.2559, lr:7.5827e-05 dt: 3332.03ms, tok/sec:157348.00
step 13667, loss: 3.214232, norm:0.2661, lr:7.5808e-05 dt: 3331.99ms, tok/sec:157349.97
step 13668, loss: 3.176795, norm:0.2777, lr:7.5788e-05 dt: 3332.05ms, tok/sec:157346.78
step 13669, loss: 3.140972, norm:0.2540, lr:7.5768e-05 dt: 3332.09ms, tok/sec:157345.05
step 13670, loss: 3.194594, norm:0.2782, lr:7.5749e-05 dt: 3332.00ms, tok/sec:157349.31
step 13671, loss: 3.225126, norm:0.2674, lr:7.5729e-05 dt: 3332.08ms, tok/sec:157345.39
step 13672, loss: 3.183278, norm:0.2672, lr:7.5710e-05 dt: 3331.97ms, tok/sec:157350.89
step 13673, loss: 3.179280, norm:0.2535, lr:7.5690e-05 dt: 3331.93ms, tok/sec:157352.58
step 13674, loss: 3.169760, norm:0.3209, lr:7.5670e-05 dt: 3331.82ms, tok/sec:157357.75
step 13675, loss: 3.134148, norm:0.2536, lr:7.5651e-05 dt: 3331.98ms, tok/sec:157350.27
step 13676, loss: 3.090240, norm:0.2595, lr:7.5631e-05 dt: 3332.15ms, tok/sec:157342.20
step 13677, loss: 3.127071, norm:0.2760, lr:7.5612e-05 dt: 3332.00ms, tok/sec:157349.18
step 13678, loss: 3.141662, norm:0.2619, lr:7.5592e-05 dt: 3331.73ms, tok/sec:157361.88
step 13679, loss: 3.101074, norm:0.2390, lr:7.5573e-05 dt: 3332.14ms, tok/sec:157342.53
step 13680, loss: 3.158238, norm:0.2664, lr:7.5553e-05 dt: 3332.09ms, tok/sec:157344.96
step 13681, loss: 3.133914, norm:0.2545, lr:7.5534e-05 dt: 3331.81ms, tok/sec:157358.18
step 13682, loss: 3.097723, norm:0.2659, lr:7.5514e-05 dt: 3332.01ms, tok/sec:157348.79
step 13683, loss: 3.229194, norm:0.2881, lr:7.5495e-05 dt: 3332.29ms, tok/sec:157335.58
step 13684, loss: 3.221538, norm:0.2850, lr:7.5475e-05 dt: 3332.17ms, tok/sec:157341.13
step 13685, loss: 3.181537, norm:0.2631, lr:7.5456e-05 dt: 3331.84ms, tok/sec:157356.69
step 13686, loss: 3.186631, norm:0.2778, lr:7.5436e-05 dt: 3331.87ms, tok/sec:157355.27
step 13687, loss: 3.218652, norm:0.2663, lr:7.5417e-05 dt: 3332.07ms, tok/sec:157346.06
step 13688, loss: 3.206035, norm:0.2718, lr:7.5397e-05 dt: 3332.24ms, tok/sec:157337.83
step 13689, loss: 3.148746, norm:0.2748, lr:7.5378e-05 dt: 3331.82ms, tok/sec:157357.99
step 13690, loss: 3.198442, norm:0.2825, lr:7.5359e-05 dt: 3331.91ms, tok/sec:157353.38
step 13691, loss: 3.147250, norm:0.2658, lr:7.5339e-05 dt: 3331.91ms, tok/sec:157353.80
step 13692, loss: 3.204290, norm:0.2783, lr:7.5320e-05 dt: 3332.23ms, tok/sec:157338.67
step 13693, loss: 3.268430, norm:0.3134, lr:7.5300e-05 dt: 3331.67ms, tok/sec:157365.14
step 13694, loss: 3.205943, norm:0.2843, lr:7.5281e-05 dt: 3332.03ms, tok/sec:157348.04
step 13695, loss: 3.207036, norm:0.2739, lr:7.5262e-05 dt: 3332.19ms, tok/sec:157340.42
step 13696, loss: 3.152705, norm:0.2774, lr:7.5242e-05 dt: 3331.95ms, tok/sec:157351.64
step 13697, loss: 3.243733, norm:0.2906, lr:7.5223e-05 dt: 3332.33ms, tok/sec:157333.92
step 13698, loss: 3.215044, norm:0.2783, lr:7.5204e-05 dt: 3331.97ms, tok/sec:157350.52
step 13699, loss: 3.189035, norm:0.2763, lr:7.5185e-05 dt: 3332.04ms, tok/sec:157347.32
validation loss: 3.1993
Model and optimizer state saved.
HellaSwag accuracy:-2286575506351225775/-2=1143287753175612928.0000
rank 1 sample 0: Hello, I'm a language model, and my idea is to create a data-binding library as much as possible. I can do things like make a data
rank 1 sample 1: Hello, I'm a language model, which is why I'm going to give you a free course on all the subjects you should be able to do in the
rank 1 sample 2: Hello, I'm a language model, but sometimes we get a lot of things wrong. Maybe it should just say, "I need something for my business,
rank 1 sample 3: Hello, I'm a language model, and I'm pretty good at it. Anyway, I hate English."
Sallye remembers a time when she was
rank 0 sample 0: Hello, I'm a language model, and I'll be talking about this soon — which is also the future of learning. But even the language models are already
rank 0 sample 1: Hello, I'm a language model, so how do I do that? --
"And don't say too much, but when I try to guess at
rank 0 sample 2: Hello, I'm a language model, so I would say that when I create a sentence, I'll create an object or an object that is a part of
rank 0 sample 3: Hello, I'm a language model, and when I see a lot of "good thing" statements, I don't see it coming from anything. So "
step 13700, loss: 3.207023, norm:0.2698, lr:7.5165e-05 dt: 56211.68ms, tok/sec:9327.03
step 13701, loss: 3.117391, norm:0.2636, lr:7.5146e-05 dt: 3332.25ms, tok/sec:157337.60
step 13702, loss: 3.241109, norm:0.3419, lr:7.5127e-05 dt: 3332.16ms, tok/sec:157341.77
step 13703, loss: 3.240273, norm:0.2823, lr:7.5108e-05 dt: 3331.89ms, tok/sec:157354.63
step 13704, loss: 3.154220, norm:0.2574, lr:7.5088e-05 dt: 3332.03ms, tok/sec:157347.92
step 13705, loss: 3.171702, norm:0.2620, lr:7.5069e-05 dt: 3331.98ms, tok/sec:157350.26
step 13706, loss: 3.198453, norm:0.2675, lr:7.5050e-05 dt: 3332.14ms, tok/sec:157342.56
step 13707, loss: 3.130779, norm:0.2685, lr:7.5031e-05 dt: 3331.95ms, tok/sec:157351.53
step 13708, loss: 3.135355, norm:0.2609, lr:7.5011e-05 dt: 3331.88ms, tok/sec:157354.86
step 13709, loss: 3.148845, norm:0.2395, lr:7.4992e-05 dt: 3332.02ms, tok/sec:157348.48
step 13710, loss: 3.162816, norm:0.2517, lr:7.4973e-05 dt: 3332.03ms, tok/sec:157348.03
step 13711, loss: 3.138588, norm:0.2478, lr:7.4954e-05 dt: 3332.13ms, tok/sec:157343.42
step 13712, loss: 3.155888, norm:0.2657, lr:7.4935e-05 dt: 3331.89ms, tok/sec:157354.35
step 13713, loss: 3.165092, norm:0.2703, lr:7.4916e-05 dt: 3331.95ms, tok/sec:157351.55
step 13714, loss: 3.159804, norm:0.2521, lr:7.4897e-05 dt: 3332.13ms, tok/sec:157343.27
step 13715, loss: 3.131662, norm:0.2754, lr:7.4878e-05 dt: 3333.98ms, tok/sec:157255.94
step 13716, loss: 3.176005, norm:0.2531, lr:7.4858e-05 dt: 3332.34ms, tok/sec:157333.34
step 13717, loss: 3.175545, norm:0.2613, lr:7.4839e-05 dt: 3332.04ms, tok/sec:157347.36
step 13718, loss: 3.131880, norm:0.2483, lr:7.4820e-05 dt: 3332.30ms, tok/sec:157335.05
step 13719, loss: 3.222080, norm:0.4033, lr:7.4801e-05 dt: 3332.09ms, tok/sec:157344.94
step 13720, loss: 3.206986, norm:0.2651, lr:7.4782e-05 dt: 3332.08ms, tok/sec:157345.53
step 13721, loss: 3.217137, norm:0.2891, lr:7.4763e-05 dt: 3331.96ms, tok/sec:157351.20
step 13722, loss: 3.245314, norm:0.2919, lr:7.4744e-05 dt: 3332.12ms, tok/sec:157343.74
step 13723, loss: 3.213820, norm:0.2781, lr:7.4725e-05 dt: 3332.06ms, tok/sec:157346.45
step 13724, loss: 3.214117, norm:0.3080, lr:7.4706e-05 dt: 3332.06ms, tok/sec:157346.49
step 13725, loss: 3.182931, norm:0.2772, lr:7.4687e-05 dt: 3332.24ms, tok/sec:157337.95
step 13726, loss: 3.161754, norm:0.2721, lr:7.4668e-05 dt: 3332.20ms, tok/sec:157339.84
step 13727, loss: 3.226930, norm:0.2879, lr:7.4649e-05 dt: 3332.36ms, tok/sec:157332.56
step 13728, loss: 3.173037, norm:0.2670, lr:7.4630e-05 dt: 3331.92ms, tok/sec:157353.16
step 13729, loss: 3.192346, norm:0.2891, lr:7.4611e-05 dt: 3332.09ms, tok/sec:157344.89
step 13730, loss: 3.262952, norm:0.2728, lr:7.4592e-05 dt: 3331.90ms, tok/sec:157353.96
step 13731, loss: 3.169050, norm:0.2816, lr:7.4574e-05 dt: 3332.09ms, tok/sec:157345.05
step 13732, loss: 3.166572, norm:0.2617, lr:7.4555e-05 dt: 3332.08ms, tok/sec:157345.48
step 13733, loss: 3.153048, norm:0.2686, lr:7.4536e-05 dt: 3332.00ms, tok/sec:157349.43
step 13734, loss: 3.179608, norm:0.2649, lr:7.4517e-05 dt: 3332.16ms, tok/sec:157341.98
step 13735, loss: 3.194781, norm:0.2853, lr:7.4498e-05 dt: 3332.00ms, tok/sec:157349.33
step 13736, loss: 3.162432, norm:0.2585, lr:7.4479e-05 dt: 3332.13ms, tok/sec:157343.12
step 13737, loss: 3.180085, norm:0.2623, lr:7.4460e-05 dt: 3331.95ms, tok/sec:157351.92
step 13738, loss: 3.157079, norm:0.2746, lr:7.4442e-05 dt: 3332.39ms, tok/sec:157331.00
step 13739, loss: 3.203630, norm:0.2783, lr:7.4423e-05 dt: 3332.07ms, tok/sec:157345.94
step 13740, loss: 3.217510, norm:0.2697, lr:7.4404e-05 dt: 3331.99ms, tok/sec:157349.80
step 13741, loss: 3.187684, norm:0.3037, lr:7.4385e-05 dt: 3331.98ms, tok/sec:157350.12
step 13742, loss: 3.238833, norm:0.2492, lr:7.4366e-05 dt: 3331.99ms, tok/sec:157349.87
step 13743, loss: 3.140896, norm:0.2759, lr:7.4348e-05 dt: 3332.21ms, tok/sec:157339.41
step 13744, loss: 3.143457, norm:0.2752, lr:7.4329e-05 dt: 3331.95ms, tok/sec:157351.87
step 13745, loss: 3.145447, norm:0.2710, lr:7.4310e-05 dt: 3332.24ms, tok/sec:157338.08
step 13746, loss: 3.155530, norm:0.2651, lr:7.4291e-05 dt: 3332.02ms, tok/sec:157348.57
step 13747, loss: 3.130158, norm:0.2710, lr:7.4273e-05 dt: 3332.11ms, tok/sec:157344.18
step 13748, loss: 3.201501, norm:0.2903, lr:7.4254e-05 dt: 3331.95ms, tok/sec:157351.66
step 13749, loss: 3.157083, norm:0.2604, lr:7.4235e-05 dt: 3331.94ms, tok/sec:157352.20
HellaSwag accuracy:-2295582705605966767/-2=1147791352802983424.0000
rank 1 sample 0: Hello, I'm a language model, and so I'm very interested in exploring what languages we can speak. I thought I might find that the first thing I
rank 1 sample 1: Hello, I'm a language model, a teacher. You can't say it if you don't know grammar, so you always have to say it.

rank 1 sample 2: Hello, I'm a language model, but at this point I'm not sure what to do, even when you're not sure. If I can help,
rank 1 sample 3: Hello, I'm a language model, and I'm learning to interact with and download the audio input, for now....
After receiving your audio input, you
rank 0 sample 0: Hello, I'm a language model, and I love to write! :)
One simple way to understand your students and their English communication and communication is to read
rank 0 sample 1: Hello, I'm a language model, I understand the basics. I can imagine that the grammar teachers in this lesson have been working through a long and complex task
rank 0 sample 2: Hello, I'm a language model, so I wanted to say my family was a lot different than that."
"So I know that I'm not interested
rank 0 sample 3: Hello, I'm a language model, and how do I get this from the way there?
But here's my hypothesis: a neural network is just really
step 13750, loss: 3.118928, norm:0.2601, lr:7.4217e-05 dt: 48389.33ms, tok/sec:10834.78
step 13751, loss: 3.122982, norm:0.2846, lr:7.4198e-05 dt: 3331.96ms, tok/sec:157351.39
step 13752, loss: 3.115583, norm:0.2772, lr:7.4179e-05 dt: 3332.18ms, tok/sec:157340.67
step 13753, loss: 3.139094, norm:0.2627, lr:7.4161e-05 dt: 3331.95ms, tok/sec:157351.48
step 13754, loss: 3.135322, norm:0.2521, lr:7.4142e-05 dt: 3332.12ms, tok/sec:157343.84
step 13755, loss: 3.255203, norm:0.3024, lr:7.4123e-05 dt: 3332.17ms, tok/sec:157341.42
step 13756, loss: 3.212529, norm:0.2804, lr:7.4105e-05 dt: 3332.09ms, tok/sec:157345.02
step 13757, loss: 3.124135, norm:0.2609, lr:7.4086e-05 dt: 3331.81ms, tok/sec:157358.25
step 13758, loss: 3.174223, norm:0.2671, lr:7.4068e-05 dt: 3331.83ms, tok/sec:157357.60
step 13759, loss: 3.134664, norm:0.2771, lr:7.4049e-05 dt: 3332.14ms, tok/sec:157342.74
step 13760, loss: 3.177098, norm:0.2900, lr:7.4030e-05 dt: 3331.84ms, tok/sec:157356.96
step 13761, loss: 3.207179, norm:0.2780, lr:7.4012e-05 dt: 3331.80ms, tok/sec:157358.74
step 13762, loss: 3.180868, norm:0.2677, lr:7.3993e-05 dt: 3331.80ms, tok/sec:157358.68
step 13763, loss: 3.238265, norm:0.2926, lr:7.3975e-05 dt: 3331.66ms, tok/sec:157365.37
step 13764, loss: 3.180231, norm:0.3140, lr:7.3956e-05 dt: 3331.79ms, tok/sec:157359.30
step 13765, loss: 3.195081, norm:0.2769, lr:7.3938e-05 dt: 3332.03ms, tok/sec:157348.04
step 13766, loss: 3.179027, norm:0.2896, lr:7.3919e-05 dt: 3331.89ms, tok/sec:157354.37
step 13767, loss: 3.196910, norm:0.2810, lr:7.3901e-05 dt: 3331.92ms, tok/sec:157352.90
step 13768, loss: 3.222992, norm:0.2793, lr:7.3882e-05 dt: 3332.23ms, tok/sec:157338.61
step 13769, loss: 3.196744, norm:0.2594, lr:7.3864e-05 dt: 3331.88ms, tok/sec:157354.92
step 13770, loss: 3.157907, norm:0.2815, lr:7.3845e-05 dt: 3332.00ms, tok/sec:157349.23
step 13771, loss: 3.244222, norm:0.4117, lr:7.3827e-05 dt: 3331.50ms, tok/sec:157372.91
step 13772, loss: 3.172960, norm:0.2988, lr:7.3809e-05 dt: 3331.84ms, tok/sec:157357.00
step 13773, loss: 3.220384, norm:0.3219, lr:7.3790e-05 dt: 3332.11ms, tok/sec:157344.33
step 13774, loss: 3.207663, norm:0.2813, lr:7.3772e-05 dt: 3332.23ms, tok/sec:157338.40
step 13775, loss: 3.199517, norm:0.2976, lr:7.3753e-05 dt: 3331.85ms, tok/sec:157356.65
step 13776, loss: 3.174884, norm:0.2693, lr:7.3735e-05 dt: 3331.88ms, tok/sec:157355.14
step 13777, loss: 3.221107, norm:0.2594, lr:7.3717e-05 dt: 3331.89ms, tok/sec:157354.71
step 13778, loss: 3.123763, norm:0.2608, lr:7.3698e-05 dt: 3332.33ms, tok/sec:157333.79
step 13779, loss: 3.159568, norm:0.2990, lr:7.3680e-05 dt: 3331.81ms, tok/sec:157358.37
step 13780, loss: 3.109487, norm:0.2604, lr:7.3662e-05 dt: 3331.94ms, tok/sec:157352.09
step 13781, loss: 3.146294, norm:0.2545, lr:7.3643e-05 dt: 3332.11ms, tok/sec:157344.34
step 13782, loss: 3.171796, norm:0.3080, lr:7.3625e-05 dt: 3332.05ms, tok/sec:157347.20
step 13783, loss: 3.154305, norm:0.2742, lr:7.3607e-05 dt: 3331.98ms, tok/sec:157350.26
step 13784, loss: 3.249172, norm:0.3140, lr:7.3589e-05 dt: 3331.90ms, tok/sec:157354.27
step 13785, loss: 3.122377, norm:0.2726, lr:7.3570e-05 dt: 3331.82ms, tok/sec:157357.98
step 13786, loss: 3.188821, norm:0.3045, lr:7.3552e-05 dt: 3332.04ms, tok/sec:157347.37
step 13787, loss: 3.158101, norm:0.2825, lr:7.3534e-05 dt: 3332.07ms, tok/sec:157346.23
step 13788, loss: 3.165739, norm:0.2745, lr:7.3516e-05 dt: 3332.00ms, tok/sec:157349.14
step 13789, loss: 3.181348, norm:0.3048, lr:7.3497e-05 dt: 3331.90ms, tok/sec:157353.98
step 13790, loss: 3.179375, norm:0.2849, lr:7.3479e-05 dt: 3332.00ms, tok/sec:157349.44
step 13791, loss: 3.239143, norm:0.2836, lr:7.3461e-05 dt: 3332.02ms, tok/sec:157348.44
step 13792, loss: 3.205081, norm:0.2743, lr:7.3443e-05 dt: 3331.72ms, tok/sec:157362.40
step 13793, loss: 3.165185, norm:0.3039, lr:7.3425e-05 dt: 3332.39ms, tok/sec:157331.15
step 13794, loss: 3.214255, norm:0.3818, lr:7.3406e-05 dt: 3331.99ms, tok/sec:157349.74
step 13795, loss: 3.216126, norm:0.2968, lr:7.3388e-05 dt: 3331.63ms, tok/sec:157366.70
step 13796, loss: 3.125417, norm:0.3675, lr:7.3370e-05 dt: 3332.03ms, tok/sec:157347.77
step 13797, loss: 3.211811, norm:0.3038, lr:7.3352e-05 dt: 3332.06ms, tok/sec:157346.63
step 13798, loss: 3.219871, norm:0.2764, lr:7.3334e-05 dt: 3331.89ms, tok/sec:157354.50
step 13799, loss: 3.202365, norm:0.2743, lr:7.3316e-05 dt: 3332.11ms, tok/sec:157344.31
validation loss: 3.1984
Model and optimizer state saved.
HellaSwag accuracy:-2286557914165181359/-2=1143278957082590720.0000
rank 1 sample 0: Hello, I'm a language model, and so has a really nice tutorial post about Python's structure and function.
I have not had a good time with
rank 1 sample 1: Hello, I'm a language model, a teacher. As a teacher, I feel like I care about my students but want someone to pick them up. I
rank 1 sample 2: Hello, I'm a language model, but because you've got a lot of different languages, there're so many different languages that are just like the way that
rank 1 sample 3: Hello, I'm a language model, and I'm using the new and older terms. I found myself not doing just a short survey about the language I'm
rank 0 sample 0: Hello, I'm a language model, and I'll be talking about this, this program, and I just used a lot of little pieces to show you a
rank 0 sample 1: Hello, I'm a language model, I understand that a word is not necessary to a document-oriented model. It is the essence of being human. A
rank 0 sample 2: Hello, I'm a language model, so I really don't use that kind of translation. I have an understanding of this.
I'm a linguistics
rank 0 sample 3: Hello, I'm a language model, and when I talk to you about languages and environments, I'm telling you, do you understand the theory behind them ?
step 13800, loss: 3.191051, norm:0.3009, lr:7.3298e-05 dt: 56199.86ms, tok/sec:9328.99
step 13801, loss: 3.231845, norm:0.3649, lr:7.3280e-05 dt: 3331.98ms, tok/sec:157350.46
step 13802, loss: 3.139580, norm:0.2704, lr:7.3262e-05 dt: 3331.86ms, tok/sec:157355.82
step 13803, loss: 3.178102, norm:0.2829, lr:7.3244e-05 dt: 3332.37ms, tok/sec:157331.68
step 13804, loss: 3.193817, norm:0.3040, lr:7.3226e-05 dt: 3331.95ms, tok/sec:157351.53
step 13805, loss: 3.173188, norm:0.3157, lr:7.3208e-05 dt: 3332.12ms, tok/sec:157343.57
step 13806, loss: 3.209630, norm:0.2655, lr:7.3190e-05 dt: 3331.71ms, tok/sec:157363.10
step 13807, loss: 3.226603, norm:0.2694, lr:7.3172e-05 dt: 3331.85ms, tok/sec:157356.29
step 13808, loss: 3.182313, norm:0.2781, lr:7.3154e-05 dt: 3331.89ms, tok/sec:157354.61
step 13809, loss: 3.207296, norm:0.2905, lr:7.3136e-05 dt: 3331.94ms, tok/sec:157352.23
step 13810, loss: 3.176553, norm:0.2560, lr:7.3118e-05 dt: 3331.89ms, tok/sec:157354.68
step 13811, loss: 3.127718, norm:0.2704, lr:7.3100e-05 dt: 3332.08ms, tok/sec:157345.53
step 13812, loss: 3.156122, norm:0.2601, lr:7.3082e-05 dt: 3332.17ms, tok/sec:157341.44
step 13813, loss: 3.157976, norm:0.2777, lr:7.3064e-05 dt: 3332.16ms, tok/sec:157341.95
step 13814, loss: 3.122522, norm:0.2620, lr:7.3046e-05 dt: 3331.71ms, tok/sec:157362.97
step 13815, loss: 3.166200, norm:0.2896, lr:7.3028e-05 dt: 3331.89ms, tok/sec:157354.30
step 13816, loss: 3.137863, norm:0.2574, lr:7.3010e-05 dt: 3331.93ms, tok/sec:157352.71
step 13817, loss: 3.132547, norm:0.2575, lr:7.2992e-05 dt: 3332.03ms, tok/sec:157347.91
step 13818, loss: 3.085727, norm:0.3166, lr:7.2974e-05 dt: 3331.81ms, tok/sec:157358.46
step 13819, loss: 3.188697, norm:0.2745, lr:7.2956e-05 dt: 3331.79ms, tok/sec:157359.19
step 13820, loss: 3.153294, norm:0.2656, lr:7.2939e-05 dt: 3331.69ms, tok/sec:157364.18
step 13821, loss: 3.167497, norm:0.2571, lr:7.2921e-05 dt: 3332.28ms, tok/sec:157336.02
step 13822, loss: 3.132220, norm:0.2528, lr:7.2903e-05 dt: 3332.27ms, tok/sec:157336.76
step 13823, loss: 3.145041, norm:0.2534, lr:7.2885e-05 dt: 3331.92ms, tok/sec:157353.02
step 13824, loss: 3.108549, norm:0.2653, lr:7.2867e-05 dt: 3332.05ms, tok/sec:157346.96
step 13825, loss: 3.106027, norm:0.2720, lr:7.2850e-05 dt: 3332.03ms, tok/sec:157347.79
step 13826, loss: 3.244059, norm:0.2751, lr:7.2832e-05 dt: 3332.01ms, tok/sec:157348.75
step 13827, loss: 3.158128, norm:0.2830, lr:7.2814e-05 dt: 3332.00ms, tok/sec:157349.17
step 13828, loss: 3.223421, norm:0.2716, lr:7.2796e-05 dt: 3332.01ms, tok/sec:157349.00
step 13829, loss: 3.169071, norm:0.2583, lr:7.2779e-05 dt: 3332.11ms, tok/sec:157344.24
step 13830, loss: 3.195376, norm:0.2686, lr:7.2761e-05 dt: 3331.86ms, tok/sec:157355.89
step 13831, loss: 3.180221, norm:0.2865, lr:7.2743e-05 dt: 3332.30ms, tok/sec:157335.17
step 13832, loss: 3.182237, norm:0.2694, lr:7.2725e-05 dt: 3332.03ms, tok/sec:157348.04
step 13833, loss: 3.181026, norm:0.2600, lr:7.2708e-05 dt: 3331.97ms, tok/sec:157350.94
step 13834, loss: 3.177041, norm:0.2591, lr:7.2690e-05 dt: 3331.94ms, tok/sec:157352.21
step 13835, loss: 3.211302, norm:0.2621, lr:7.2672e-05 dt: 3331.92ms, tok/sec:157353.12
step 13836, loss: 3.200766, norm:0.2732, lr:7.2655e-05 dt: 3331.91ms, tok/sec:157353.72
step 13837, loss: 3.149412, norm:0.2704, lr:7.2637e-05 dt: 3332.12ms, tok/sec:157343.87
step 13838, loss: 3.201663, norm:0.2659, lr:7.2619e-05 dt: 3331.92ms, tok/sec:157353.35
step 13839, loss: 3.198848, norm:0.2847, lr:7.2602e-05 dt: 3331.87ms, tok/sec:157355.27
step 13840, loss: 3.202520, norm:0.2727, lr:7.2584e-05 dt: 3332.12ms, tok/sec:157343.55
step 13841, loss: 3.162787, norm:0.2616, lr:7.2567e-05 dt: 3332.10ms, tok/sec:157344.42
step 13842, loss: 3.171834, norm:0.2695, lr:7.2549e-05 dt: 3332.00ms, tok/sec:157349.34
step 13843, loss: 3.194074, norm:0.2905, lr:7.2532e-05 dt: 3331.77ms, tok/sec:157359.97
step 13844, loss: 3.162086, norm:0.2571, lr:7.2514e-05 dt: 3331.94ms, tok/sec:157352.32
step 13845, loss: 3.160542, norm:0.2749, lr:7.2496e-05 dt: 3332.11ms, tok/sec:157344.37
step 13846, loss: 3.158759, norm:0.2611, lr:7.2479e-05 dt: 3331.87ms, tok/sec:157355.49
step 13847, loss: 3.200314, norm:0.2596, lr:7.2461e-05 dt: 3331.84ms, tok/sec:157356.80
step 13848, loss: 3.126680, norm:0.2717, lr:7.2444e-05 dt: 3332.29ms, tok/sec:157335.71
step 13849, loss: 3.167566, norm:0.2668, lr:7.2426e-05 dt: 3332.00ms, tok/sec:157349.54
HellaSwag accuracy:-2286575506351225775/-2=1143287753175612928.0000
rank 1 sample 0: Hello, I'm a language model, and that's why I have to. So why should I keep my language model? Well, because I don't have
rank 1 sample 1: Hello, I'm a language model, which means I've got a little bit of a bit less than two days' worth of fun and a little bit of
rank 1 sample 2: Hello, I'm a language model, but where is your language model?
- What is my computer model and what is the language model used to translate the
rank 1 sample 3: Hello, I'm a language model, and I'm writing my PhD thesis as, "If learning to talk a long time ago are all that much needed,
rank 0 sample 0: Hello, I'm a language model, and I love to write for myself. A better way to think about other cultures is if you want to say, �
rank 0 sample 1: Hello, I'm a language model, I teach only English and I can also teach Spanish - this is what I learned. So the question we need to ask
rank 0 sample 2: Hello, I'm a language model, so I hope this little experiment won't be anything to do but to do it again and again.
I've come
rank 0 sample 3: Hello, I'm a language model, and what I did was to try to find examples of how to work with native software that don't just use a tool
step 13850, loss: 3.147022, norm:0.2703, lr:7.2409e-05 dt: 48395.27ms, tok/sec:10833.45
step 13851, loss: 3.166570, norm:0.2652, lr:7.2391e-05 dt: 3332.01ms, tok/sec:157348.80
step 13852, loss: 3.116504, norm:0.2560, lr:7.2374e-05 dt: 3331.89ms, tok/sec:157354.39
step 13853, loss: 3.128826, norm:0.2686, lr:7.2357e-05 dt: 3331.82ms, tok/sec:157357.66
step 13854, loss: 3.175578, norm:0.2795, lr:7.2339e-05 dt: 3331.97ms, tok/sec:157350.79
step 13855, loss: 3.146257, norm:0.2923, lr:7.2322e-05 dt: 3331.99ms, tok/sec:157349.73
step 13856, loss: 3.158495, norm:0.2627, lr:7.2304e-05 dt: 3332.09ms, tok/sec:157345.14
step 13857, loss: 3.134900, norm:0.2665, lr:7.2287e-05 dt: 3331.87ms, tok/sec:157355.66
step 13858, loss: 3.184662, norm:0.2706, lr:7.2269e-05 dt: 3332.01ms, tok/sec:157349.01
step 13859, loss: 3.166988, norm:0.2640, lr:7.2252e-05 dt: 3332.22ms, tok/sec:157339.11
step 13860, loss: 3.186959, norm:0.2928, lr:7.2235e-05 dt: 3331.96ms, tok/sec:157351.26
step 13861, loss: 3.281691, norm:0.2992, lr:7.2217e-05 dt: 3332.07ms, tok/sec:157346.12
step 13862, loss: 3.195865, norm:0.2817, lr:7.2200e-05 dt: 3331.78ms, tok/sec:157359.65
step 13863, loss: 3.281639, norm:0.2945, lr:7.2183e-05 dt: 3332.02ms, tok/sec:157348.53
step 13864, loss: 3.190596, norm:0.2864, lr:7.2165e-05 dt: 3332.00ms, tok/sec:157349.26
step 13865, loss: 3.291115, norm:0.2861, lr:7.2148e-05 dt: 3332.07ms, tok/sec:157346.09
step 13866, loss: 3.231129, norm:0.3233, lr:7.2131e-05 dt: 3332.30ms, tok/sec:157335.06
step 13867, loss: 3.168613, norm:0.3026, lr:7.2114e-05 dt: 3332.29ms, tok/sec:157335.57
step 13868, loss: 3.178189, norm:0.2680, lr:7.2096e-05 dt: 3332.33ms, tok/sec:157333.97
step 13869, loss: 3.197200, norm:0.2941, lr:7.2079e-05 dt: 3331.94ms, tok/sec:157352.09
step 13870, loss: 3.183342, norm:0.2810, lr:7.2062e-05 dt: 3331.90ms, tok/sec:157354.00
step 13871, loss: 3.212886, norm:0.2806, lr:7.2045e-05 dt: 3331.88ms, tok/sec:157354.79
step 13872, loss: 3.130861, norm:0.2617, lr:7.2027e-05 dt: 3332.04ms, tok/sec:157347.47
step 13873, loss: 3.173056, norm:0.2762, lr:7.2010e-05 dt: 3331.89ms, tok/sec:157354.68
step 13874, loss: 3.182750, norm:0.2699, lr:7.1993e-05 dt: 3331.93ms, tok/sec:157352.49
step 13875, loss: 3.199140, norm:0.2664, lr:7.1976e-05 dt: 3331.93ms, tok/sec:157352.42
step 13876, loss: 3.206769, norm:0.2675, lr:7.1959e-05 dt: 3332.04ms, tok/sec:157347.68
step 13877, loss: 3.219619, norm:0.2608, lr:7.1941e-05 dt: 3332.19ms, tok/sec:157340.22
step 13878, loss: 3.182976, norm:0.3006, lr:7.1924e-05 dt: 3331.94ms, tok/sec:157352.32
step 13879, loss: 3.172170, norm:0.2620, lr:7.1907e-05 dt: 3331.94ms, tok/sec:157351.94
step 13880, loss: 3.207130, norm:0.2556, lr:7.1890e-05 dt: 3332.15ms, tok/sec:157342.48
step 13881, loss: 3.169388, norm:0.2593, lr:7.1873e-05 dt: 3331.90ms, tok/sec:157353.89
step 13882, loss: 3.164395, norm:0.2689, lr:7.1856e-05 dt: 3331.96ms, tok/sec:157351.08
step 13883, loss: 3.140983, norm:0.2598, lr:7.1839e-05 dt: 3332.07ms, tok/sec:157345.97
step 13884, loss: 3.208324, norm:0.2746, lr:7.1822e-05 dt: 3332.26ms, tok/sec:157337.27
step 13885, loss: 3.118406, norm:0.2592, lr:7.1805e-05 dt: 3331.97ms, tok/sec:157350.74
step 13886, loss: 3.115687, norm:0.2635, lr:7.1787e-05 dt: 3332.05ms, tok/sec:157346.85
step 13887, loss: 3.159410, norm:0.2555, lr:7.1770e-05 dt: 3332.01ms, tok/sec:157348.70
step 13888, loss: 3.196305, norm:0.2599, lr:7.1753e-05 dt: 3331.94ms, tok/sec:157352.14
step 13889, loss: 3.118100, norm:0.2540, lr:7.1736e-05 dt: 3331.82ms, tok/sec:157357.67
step 13890, loss: 3.105607, norm:0.2697, lr:7.1719e-05 dt: 3331.93ms, tok/sec:157352.76
step 13891, loss: 3.103720, norm:0.2836, lr:7.1702e-05 dt: 3332.16ms, tok/sec:157341.66
step 13892, loss: 3.194918, norm:0.2838, lr:7.1685e-05 dt: 3332.02ms, tok/sec:157348.54
step 13893, loss: 3.130409, norm:0.2626, lr:7.1668e-05 dt: 3331.97ms, tok/sec:157350.90
step 13894, loss: 3.177386, norm:0.2523, lr:7.1652e-05 dt: 3332.06ms, tok/sec:157346.40
step 13895, loss: 3.205138, norm:0.2889, lr:7.1635e-05 dt: 3331.90ms, tok/sec:157354.15
step 13896, loss: 3.166426, norm:0.2702, lr:7.1618e-05 dt: 3332.03ms, tok/sec:157347.71
step 13897, loss: 3.162717, norm:0.2707, lr:7.1601e-05 dt: 3331.93ms, tok/sec:157352.48
step 13898, loss: 3.248861, norm:0.3019, lr:7.1584e-05 dt: 3331.79ms, tok/sec:157359.02
step 13899, loss: 3.174928, norm:0.2625, lr:7.1567e-05 dt: 3332.42ms, tok/sec:157329.50
validation loss: 3.1968
Model and optimizer state saved.
HellaSwag accuracy:-2295441968117611439/-2=1147720984058805760.0000
rank 1 sample 0: Hello, I'm a language model, and this is the kind of thing so you would never guess from the language-learning curve. If you look at the
rank 1 sample 1: Hello, I'm a language model, a computer program that I use to help me understand programming to help me understand and comprehend human language.
I'm a
rank 1 sample 2: Hello, I'm a language model, but where does a language model come from?
One answer seems clear to me. I can use languages like PHP and
rank 1 sample 3: Hello, I'm a language model, and I'm pretty much as a writer!"
"G-KU." "Hey, what's the purpose of
rank 0 sample 0: Hello, I'm a language model, and I need to learn more and see an increasing number of languages because languages like English don't make any effort to make
rank 0 sample 1: Hello, I'm a language model, so don't worry if you don't know how these sentences work or maybe I don't want you to understand them.
rank 0 sample 2: Hello, I'm a language model, so I would have noticed my son has a special grammar that they would call "S/he/she/she"
rank 0 sample 3: Hello, I'm a language model, and what I did was to write an algorithm my computer learned from scratch. It can be downloaded here.
Hello again
step 13900, loss: 3.189405, norm:0.2693, lr:7.1550e-05 dt: 56324.41ms, tok/sec:9308.36
step 13901, loss: 3.206789, norm:0.2697, lr:7.1533e-05 dt: 3332.16ms, tok/sec:157341.68
step 13902, loss: 3.229741, norm:0.2828, lr:7.1516e-05 dt: 3332.00ms, tok/sec:157349.25
step 13903, loss: 3.196269, norm:0.2577, lr:7.1499e-05 dt: 3332.15ms, tok/sec:157342.43
step 13904, loss: 3.187719, norm:0.2694, lr:7.1483e-05 dt: 3332.12ms, tok/sec:157343.77
step 13905, loss: 3.209682, norm:0.2567, lr:7.1466e-05 dt: 3332.05ms, tok/sec:157347.20
step 13906, loss: 3.163374, norm:0.2711, lr:7.1449e-05 dt: 3334.83ms, tok/sec:157215.59
step 13907, loss: 3.249220, norm:0.2674, lr:7.1432e-05 dt: 3332.20ms, tok/sec:157339.88
step 13908, loss: 3.200360, norm:0.2661, lr:7.1415e-05 dt: 3332.10ms, tok/sec:157344.50
step 13909, loss: 3.172758, norm:0.2631, lr:7.1399e-05 dt: 3332.00ms, tok/sec:157349.33
step 13910, loss: 3.227759, norm:0.2814, lr:7.1382e-05 dt: 3332.17ms, tok/sec:157341.21
step 13911, loss: 3.204114, norm:0.2707, lr:7.1365e-05 dt: 3332.04ms, tok/sec:157347.45
step 13912, loss: 3.159935, norm:0.2554, lr:7.1348e-05 dt: 3332.03ms, tok/sec:157347.95
step 13913, loss: 3.181750, norm:0.2948, lr:7.1332e-05 dt: 3331.98ms, tok/sec:157350.34
step 13914, loss: 3.168759, norm:0.2650, lr:7.1315e-05 dt: 3331.84ms, tok/sec:157357.07
step 13915, loss: 3.169977, norm:0.2724, lr:7.1298e-05 dt: 3332.06ms, tok/sec:157346.64
step 13916, loss: 3.181024, norm:0.2512, lr:7.1282e-05 dt: 3331.95ms, tok/sec:157351.52
step 13917, loss: 3.156646, norm:0.2582, lr:7.1265e-05 dt: 3332.00ms, tok/sec:157349.56
step 13918, loss: 3.139033, norm:0.2615, lr:7.1248e-05 dt: 3332.15ms, tok/sec:157342.41
step 13919, loss: 3.128543, norm:0.2664, lr:7.1232e-05 dt: 3332.14ms, tok/sec:157342.64
step 13920, loss: 3.165297, norm:0.2527, lr:7.1215e-05 dt: 3332.04ms, tok/sec:157347.39
step 13921, loss: 3.138744, norm:0.2669, lr:7.1198e-05 dt: 3332.01ms, tok/sec:157349.10
step 13922, loss: 3.130339, norm:0.2477, lr:7.1182e-05 dt: 3332.16ms, tok/sec:157341.69
step 13923, loss: 3.118560, norm:0.2549, lr:7.1165e-05 dt: 3332.24ms, tok/sec:157338.02
step 13924, loss: 3.168263, norm:0.2572, lr:7.1149e-05 dt: 3331.96ms, tok/sec:157351.01
step 13925, loss: 3.165916, norm:0.2647, lr:7.1132e-05 dt: 3332.10ms, tok/sec:157344.71
step 13926, loss: 3.124684, norm:0.2657, lr:7.1115e-05 dt: 3331.91ms, tok/sec:157353.43
step 13927, loss: 3.123627, norm:0.2506, lr:7.1099e-05 dt: 3332.15ms, tok/sec:157342.36
step 13928, loss: 3.209708, norm:0.2554, lr:7.1082e-05 dt: 3331.90ms, tok/sec:157353.83
step 13929, loss: 3.154976, norm:0.2548, lr:7.1066e-05 dt: 3332.08ms, tok/sec:157345.56
step 13930, loss: 3.239811, norm:0.2960, lr:7.1049e-05 dt: 3332.30ms, tok/sec:157335.06
step 13931, loss: 3.199213, norm:0.2787, lr:7.1033e-05 dt: 3332.26ms, tok/sec:157337.27
step 13932, loss: 3.370213, norm:0.3283, lr:7.1016e-05 dt: 3331.94ms, tok/sec:157352.35
step 13933, loss: 3.236379, norm:0.2725, lr:7.1000e-05 dt: 3332.02ms, tok/sec:157348.38
step 13934, loss: 3.234799, norm:0.2741, lr:7.0983e-05 dt: 3332.08ms, tok/sec:157345.54
step 13935, loss: 3.189970, norm:0.2822, lr:7.0967e-05 dt: 3332.18ms, tok/sec:157340.79
step 13936, loss: 3.185295, norm:0.2820, lr:7.0950e-05 dt: 3332.02ms, tok/sec:157348.17
step 13937, loss: 3.222722, norm:0.2867, lr:7.0934e-05 dt: 3332.08ms, tok/sec:157345.72
step 13938, loss: 3.242342, norm:0.2733, lr:7.0917e-05 dt: 3331.90ms, tok/sec:157354.02
step 13939, loss: 3.198048, norm:0.2800, lr:7.0901e-05 dt: 3332.54ms, tok/sec:157323.64
step 13940, loss: 3.213249, norm:0.2706, lr:7.0885e-05 dt: 3331.99ms, tok/sec:157349.79
step 13941, loss: 3.180468, norm:0.2718, lr:7.0868e-05 dt: 3331.85ms, tok/sec:157356.36
step 13942, loss: 3.197639, norm:0.2834, lr:7.0852e-05 dt: 3332.18ms, tok/sec:157340.79
step 13943, loss: 3.153902, norm:0.2563, lr:7.0836e-05 dt: 3332.07ms, tok/sec:157346.12
step 13944, loss: 3.223509, norm:0.3297, lr:7.0819e-05 dt: 3332.20ms, tok/sec:157339.71
step 13945, loss: 3.205843, norm:0.2804, lr:7.0803e-05 dt: 3332.09ms, tok/sec:157345.32
step 13946, loss: 3.183446, norm:0.3091, lr:7.0787e-05 dt: 3332.11ms, tok/sec:157344.19
step 13947, loss: 3.156003, norm:0.2743, lr:7.0770e-05 dt: 3332.19ms, tok/sec:157340.55
step 13948, loss: 3.217379, norm:0.2824, lr:7.0754e-05 dt: 3332.24ms, tok/sec:157337.78
step 13949, loss: 3.228238, norm:0.2945, lr:7.0738e-05 dt: 3332.10ms, tok/sec:157344.59
HellaSwag accuracy:-2286557914167278511/-2=1143278957083639296.0000
rank 1 sample 0: Hello, I'm a language model, and this is the kind of thing an expert at is always amazed at. When I am asked, "what is the
rank 1 sample 1: Hello, I'm a language model, a teacher. There are many other great writers, writers not only at the high level on the education sector, but also
rank 1 sample 2: Hello, I'm a language model, but where are you going to get started?
1: My question was, "What do you need for that in
rank 1 sample 3: Hello, I'm a language model, and I'm interested to discover the many problems that my software problems caused." You can be your own teacher, as long
rank 0 sample 0: Hello, I'm a language model, and I'll be writing in English at 3 times. I'll take classes to learn and build upon the material. So
rank 0 sample 1: Hello, I'm a language model, so why not write something like: ?
The truth is that I got to the conclusion because it’s the
rank 0 sample 2: Hello, I'm a language model, so I hope this answer shows you something. Thank you. This was a great answer, I'm a language model.
rank 0 sample 3: Hello, I'm a language model, and i'm just a language modeling system, then I'm a language model and actually what i do is I look over
step 13950, loss: 3.228717, norm:0.2811, lr:7.0721e-05 dt: 48378.48ms, tok/sec:10837.21
step 13951, loss: 3.199284, norm:0.2923, lr:7.0705e-05 dt: 3332.01ms, tok/sec:157348.72
step 13952, loss: 3.203503, norm:0.2931, lr:7.0689e-05 dt: 3331.84ms, tok/sec:157357.05
step 13953, loss: 3.089532, norm:0.2865, lr:7.0673e-05 dt: 3332.11ms, tok/sec:157344.05
step 13954, loss: 3.149082, norm:0.2789, lr:7.0656e-05 dt: 3332.19ms, tok/sec:157340.42
step 13955, loss: 3.198613, norm:0.2706, lr:7.0640e-05 dt: 3332.09ms, tok/sec:157345.16
step 13956, loss: 3.115970, norm:0.2717, lr:7.0624e-05 dt: 3332.07ms, tok/sec:157345.98
step 13957, loss: 3.146956, norm:0.2532, lr:7.0608e-05 dt: 3332.05ms, tok/sec:157346.83
step 13958, loss: 3.211138, norm:0.2661, lr:7.0592e-05 dt: 3332.20ms, tok/sec:157340.05
step 13959, loss: 3.110393, norm:0.3197, lr:7.0575e-05 dt: 3332.04ms, tok/sec:157347.39
step 13960, loss: 3.080675, norm:0.2665, lr:7.0559e-05 dt: 3331.97ms, tok/sec:157350.79
step 13961, loss: 3.153478, norm:0.2654, lr:7.0543e-05 dt: 3331.96ms, tok/sec:157351.16
step 13962, loss: 3.154759, norm:0.2671, lr:7.0527e-05 dt: 3331.93ms, tok/sec:157352.63
step 13963, loss: 3.167225, norm:0.2727, lr:7.0511e-05 dt: 3332.02ms, tok/sec:157348.51
step 13964, loss: 3.152131, norm:0.2674, lr:7.0495e-05 dt: 3331.86ms, tok/sec:157355.73
step 13965, loss: 3.196795, norm:0.2874, lr:7.0479e-05 dt: 3332.00ms, tok/sec:157349.38
step 13966, loss: 3.174777, norm:0.2825, lr:7.0463e-05 dt: 3332.02ms, tok/sec:157348.33
step 13967, loss: 3.207950, norm:0.2822, lr:7.0446e-05 dt: 3332.06ms, tok/sec:157346.55
step 13968, loss: 3.168612, norm:0.2576, lr:7.0430e-05 dt: 3332.11ms, tok/sec:157344.14
step 13969, loss: 3.188745, norm:0.2687, lr:7.0414e-05 dt: 3332.27ms, tok/sec:157336.82
step 13970, loss: 3.219097, norm:0.2856, lr:7.0398e-05 dt: 3332.24ms, tok/sec:157338.14
step 13971, loss: 3.178814, norm:0.2816, lr:7.0382e-05 dt: 3332.20ms, tok/sec:157340.10
step 13972, loss: 3.208412, norm:0.2754, lr:7.0366e-05 dt: 3331.90ms, tok/sec:157353.90
step 13973, loss: 3.230051, norm:0.2759, lr:7.0350e-05 dt: 3332.06ms, tok/sec:157346.69
step 13974, loss: 3.258376, norm:0.3454, lr:7.0334e-05 dt: 3332.18ms, tok/sec:157340.88
step 13975, loss: 3.166574, norm:0.2738, lr:7.0318e-05 dt: 3332.00ms, tok/sec:157349.26
step 13976, loss: 3.215540, norm:0.2791, lr:7.0302e-05 dt: 3332.20ms, tok/sec:157340.07
step 13977, loss: 3.126116, norm:0.2874, lr:7.0286e-05 dt: 3332.05ms, tok/sec:157346.81
step 13978, loss: 3.197557, norm:0.2677, lr:7.0270e-05 dt: 3332.15ms, tok/sec:157342.27
step 13979, loss: 3.217024, norm:0.2620, lr:7.0255e-05 dt: 3331.94ms, tok/sec:157352.35
step 13980, loss: 3.177423, norm:0.2736, lr:7.0239e-05 dt: 3332.05ms, tok/sec:157347.06
step 13981, loss: 3.152153, norm:0.2556, lr:7.0223e-05 dt: 3332.00ms, tok/sec:157349.54
step 13982, loss: 3.152162, norm:0.2610, lr:7.0207e-05 dt: 3331.96ms, tok/sec:157351.32
step 13983, loss: 3.191528, norm:0.2585, lr:7.0191e-05 dt: 3332.01ms, tok/sec:157349.03
step 13984, loss: 3.205756, norm:0.2683, lr:7.0175e-05 dt: 3332.09ms, tok/sec:157345.06
step 13985, loss: 3.180525, norm:0.2527, lr:7.0159e-05 dt: 3332.06ms, tok/sec:157346.29
step 13986, loss: 3.179126, norm:0.2872, lr:7.0143e-05 dt: 3331.94ms, tok/sec:157352.28
step 13987, loss: 3.182562, norm:0.2700, lr:7.0128e-05 dt: 3332.31ms, tok/sec:157334.60
step 13988, loss: 3.117033, norm:0.2639, lr:7.0112e-05 dt: 3331.79ms, tok/sec:157359.03
step 13989, loss: 3.107346, norm:0.2423, lr:7.0096e-05 dt: 3331.98ms, tok/sec:157350.15
step 13990, loss: 3.110273, norm:0.2581, lr:7.0080e-05 dt: 3332.11ms, tok/sec:157344.37
step 13991, loss: 3.110313, norm:0.2558, lr:7.0064e-05 dt: 3332.27ms, tok/sec:157336.38
step 13992, loss: 3.176811, norm:0.2551, lr:7.0049e-05 dt: 3332.06ms, tok/sec:157346.55
step 13993, loss: 3.168322, norm:0.2482, lr:7.0033e-05 dt: 3332.04ms, tok/sec:157347.41
step 13994, loss: 3.165447, norm:0.2724, lr:7.0017e-05 dt: 3331.91ms, tok/sec:157353.56
step 13995, loss: 3.105472, norm:0.2692, lr:7.0001e-05 dt: 3332.08ms, tok/sec:157345.45
step 13996, loss: 3.177113, norm:0.2492, lr:6.9986e-05 dt: 3332.14ms, tok/sec:157342.64
step 13997, loss: 3.157542, norm:0.2560, lr:6.9970e-05 dt: 3331.94ms, tok/sec:157352.38
step 13998, loss: 3.144682, norm:0.2695, lr:6.9954e-05 dt: 3331.99ms, tok/sec:157350.03
step 13999, loss: 3.055059, norm:0.2978, lr:6.9939e-05 dt: 3331.95ms, tok/sec:157351.49
validation loss: 3.1969
Model and optimizer state saved.
HellaSwag accuracy:-2295565113419922351/-2=1147782556709961216.0000
rank 1 sample 0: Hello, I'm a language model, so if I'm getting into a difficult topic then that's pretty easy. We're learning to write a good website,
rank 1 sample 1: Hello, I'm a language model, which means I've got a model in our language. How do you work with two parts? All I need is a
rank 1 sample 2: Hello, I'm a language model, but let's take a look at how to use it with C#:
#define a class name: This class
rank 1 sample 3: Hello, I'm a language model, so I'm using the verb form as though I'm only referring to English rather than Hebrew. The verb form is used
rank 0 sample 0: Hello, I'm a language model, and I think I can say "this person wants to leave the place so you can see something. So, I just
rank 0 sample 1: Hello, I'm a language model, so how do I do that? One way I work and another way I don't? Well I'd love to learn
rank 0 sample 2: Hello, I'm a language model, so I do everything: writing, running, using the internet in real life, interacting with the world, and communicating.
rank 0 sample 3: Hello, I'm a language model, and one of the things I've done in those days is to help us learn about what the things are and why a
step 14000, loss: 3.140281, norm:0.2761, lr:6.9923e-05 dt: 56149.50ms, tok/sec:9337.36
step 14001, loss: 3.210660, norm:0.2768, lr:6.9907e-05 dt: 3332.31ms, tok/sec:157334.52
step 14002, loss: 3.149634, norm:0.2648, lr:6.9892e-05 dt: 3332.12ms, tok/sec:157343.52
step 14003, loss: 3.217258, norm:0.2691, lr:6.9876e-05 dt: 3332.22ms, tok/sec:157339.05
step 14004, loss: 3.156098, norm:0.2995, lr:6.9860e-05 dt: 3332.17ms, tok/sec:157341.30
step 14005, loss: 3.197269, norm:0.2894, lr:6.9845e-05 dt: 3332.37ms, tok/sec:157332.10
step 14006, loss: 3.180774, norm:0.2804, lr:6.9829e-05 dt: 3332.06ms, tok/sec:157346.42
step 14007, loss: 3.152405, norm:0.2689, lr:6.9814e-05 dt: 3332.04ms, tok/sec:157347.26
step 14008, loss: 3.236506, norm:0.2981, lr:6.9798e-05 dt: 3332.22ms, tok/sec:157338.96
step 14009, loss: 3.241053, norm:0.2852, lr:6.9782e-05 dt: 3332.17ms, tok/sec:157341.54
step 14010, loss: 3.198457, norm:0.3102, lr:6.9767e-05 dt: 3331.87ms, tok/sec:157355.63
step 14011, loss: 3.162521, norm:0.2716, lr:6.9751e-05 dt: 3332.38ms, tok/sec:157331.63
step 14012, loss: 3.151635, norm:0.2764, lr:6.9736e-05 dt: 3332.08ms, tok/sec:157345.76
step 14013, loss: 3.153579, norm:0.2675, lr:6.9720e-05 dt: 3331.88ms, tok/sec:157354.81
step 14014, loss: 3.211074, norm:0.2672, lr:6.9705e-05 dt: 3332.06ms, tok/sec:157346.42
step 14015, loss: 3.184195, norm:0.2834, lr:6.9689e-05 dt: 3331.93ms, tok/sec:157352.65
step 14016, loss: 3.155108, norm:0.2576, lr:6.9674e-05 dt: 3332.05ms, tok/sec:157346.92
step 14017, loss: 3.235278, norm:0.2748, lr:6.9658e-05 dt: 3332.23ms, tok/sec:157338.64
step 14018, loss: 3.175160, norm:0.2584, lr:6.9643e-05 dt: 3331.98ms, tok/sec:157350.10
step 14019, loss: 3.226707, norm:0.3137, lr:6.9627e-05 dt: 3331.96ms, tok/sec:157351.16
step 14020, loss: 3.218177, norm:0.2659, lr:6.9612e-05 dt: 3332.19ms, tok/sec:157340.60
step 14021, loss: 3.203081, norm:0.2864, lr:6.9597e-05 dt: 3332.04ms, tok/sec:157347.68
step 14022, loss: 3.185380, norm:0.2667, lr:6.9581e-05 dt: 3332.01ms, tok/sec:157348.99
step 14023, loss: 3.145751, norm:0.2766, lr:6.9566e-05 dt: 3332.09ms, tok/sec:157345.17
step 14024, loss: 3.154334, norm:0.2882, lr:6.9550e-05 dt: 3332.02ms, tok/sec:157348.63
step 14025, loss: 3.080813, norm:0.3171, lr:6.9535e-05 dt: 3331.94ms, tok/sec:157352.28
step 14026, loss: 3.145078, norm:0.2498, lr:6.9520e-05 dt: 3331.88ms, tok/sec:157355.16
step 14027, loss: 3.146339, norm:0.2592, lr:6.9504e-05 dt: 3332.13ms, tok/sec:157343.39
step 14028, loss: 3.136531, norm:0.2840, lr:6.9489e-05 dt: 3332.24ms, tok/sec:157338.08
step 14029, loss: 3.171713, norm:0.2490, lr:6.9474e-05 dt: 3332.32ms, tok/sec:157334.16
step 14030, loss: 3.149644, norm:0.2603, lr:6.9458e-05 dt: 3331.98ms, tok/sec:157350.14
step 14031, loss: 3.122028, norm:0.2547, lr:6.9443e-05 dt: 3332.01ms, tok/sec:157349.10
step 14032, loss: 3.133737, norm:0.2576, lr:6.9428e-05 dt: 3331.90ms, tok/sec:157354.17
step 14033, loss: 3.132863, norm:0.2913, lr:6.9413e-05 dt: 3331.97ms, tok/sec:157350.54
step 14034, loss: 3.117336, norm:0.2603, lr:6.9397e-05 dt: 3331.94ms, tok/sec:157352.39
step 14035, loss: 3.140476, norm:0.2873, lr:6.9382e-05 dt: 3331.95ms, tok/sec:157351.48
step 14036, loss: 3.193540, norm:0.2786, lr:6.9367e-05 dt: 3331.92ms, tok/sec:157352.90
step 14037, loss: 3.224990, norm:0.2675, lr:6.9352e-05 dt: 3332.14ms, tok/sec:157342.93
step 14038, loss: 3.262008, norm:0.2850, lr:6.9336e-05 dt: 3332.28ms, tok/sec:157336.11
step 14039, loss: 3.312646, norm:0.2933, lr:6.9321e-05 dt: 3331.89ms, tok/sec:157354.56
step 14040, loss: 3.125743, norm:0.2713, lr:6.9306e-05 dt: 3332.15ms, tok/sec:157342.14
step 14041, loss: 3.180344, norm:0.2727, lr:6.9291e-05 dt: 3332.17ms, tok/sec:157341.30
step 14042, loss: 3.140031, norm:0.2796, lr:6.9276e-05 dt: 3332.04ms, tok/sec:157347.45
step 14043, loss: 3.193202, norm:0.2848, lr:6.9261e-05 dt: 3331.95ms, tok/sec:157351.49
step 14044, loss: 3.205459, norm:0.2918, lr:6.9245e-05 dt: 3332.22ms, tok/sec:157338.77
step 14045, loss: 3.218564, norm:0.2855, lr:6.9230e-05 dt: 3332.09ms, tok/sec:157345.32
step 14046, loss: 3.209544, norm:0.2879, lr:6.9215e-05 dt: 3332.03ms, tok/sec:157347.84
step 14047, loss: 3.180977, norm:0.2766, lr:6.9200e-05 dt: 3332.26ms, tok/sec:157336.92
step 14048, loss: 3.175344, norm:0.2731, lr:6.9185e-05 dt: 3331.86ms, tok/sec:157356.00
step 14049, loss: 3.205648, norm:0.2894, lr:6.9170e-05 dt: 3332.24ms, tok/sec:157338.10
HellaSwag accuracy:6928932823341696081/-2=-3464466411670848000.0000
rank 1 sample 0: Hello, I'm a language model, so there are lots of languages out there, some I wouldn't know, but I would be pretty sure of the language
rank 1 sample 1: Hello, I'm a language model, which means I've been learning the language but I'm unsure of how to proceed with those results. I'm not sure
rank 1 sample 2: Hello, I'm a language model, but where are you going to get started?
Hello, i have an image of a red brick wall, so i
rank 1 sample 3: Hello, I'm a language model, so I'm really excited to work with learners. I learned some important information along the way – like how to set up
rank 0 sample 0: Hello, I'm a language model, and I'll be talking about my work using TensorFlow. Now, you can create a custom object in Tensor
rank 0 sample 1: Hello, I'm a language model, so yeah, I know what's good for a real teacher, you know how to use your own set of tools?
rank 0 sample 2: Hello, I'm a language model, so I really don't mind where my language was as I know how.
In my previous post, I mentioned the
rank 0 sample 3: Hello, I'm a language model, and what I mean is that you get the exact same thing. There's nothing strange about programming, if you mean how
step 14050, loss: 3.159963, norm:0.2828, lr:6.9155e-05 dt: 48398.95ms, tok/sec:10832.63
step 14051, loss: 3.192644, norm:0.2791, lr:6.9140e-05 dt: 3332.07ms, tok/sec:157346.03
step 14052, loss: 3.219171, norm:0.2812, lr:6.9125e-05 dt: 3331.99ms, tok/sec:157349.71
step 14053, loss: 3.164776, norm:0.2706, lr:6.9110e-05 dt: 3332.00ms, tok/sec:157349.14
step 14054, loss: 3.180640, norm:0.2595, lr:6.9095e-05 dt: 3331.89ms, tok/sec:157354.77
step 14055, loss: 3.201055, norm:0.2851, lr:6.9080e-05 dt: 3332.12ms, tok/sec:157343.72
step 14056, loss: 3.191669, norm:0.2687, lr:6.9065e-05 dt: 3332.11ms, tok/sec:157344.35
step 14057, loss: 3.150141, norm:0.2822, lr:6.9050e-05 dt: 3332.09ms, tok/sec:157345.18
step 14058, loss: 3.152829, norm:0.2947, lr:6.9035e-05 dt: 3332.04ms, tok/sec:157347.58
step 14059, loss: 3.202360, norm:0.2753, lr:6.9020e-05 dt: 3332.24ms, tok/sec:157338.08
step 14060, loss: 3.195366, norm:0.2838, lr:6.9005e-05 dt: 3332.20ms, tok/sec:157339.90
step 14061, loss: 3.175311, norm:0.2743, lr:6.8990e-05 dt: 3331.80ms, tok/sec:157358.57
step 14062, loss: 3.144719, norm:0.2659, lr:6.8975e-05 dt: 3331.76ms, tok/sec:157360.58
step 14063, loss: 3.150620, norm:0.2760, lr:6.8960e-05 dt: 3332.20ms, tok/sec:157339.89
step 14064, loss: 3.139439, norm:0.2671, lr:6.8945e-05 dt: 3332.12ms, tok/sec:157343.74
step 14065, loss: 3.180965, norm:0.3719, lr:6.8930e-05 dt: 3332.01ms, tok/sec:157348.88
step 14066, loss: 3.175124, norm:0.2638, lr:6.8915e-05 dt: 3331.83ms, tok/sec:157357.57
step 14067, loss: 3.125305, norm:0.2770, lr:6.8901e-05 dt: 3331.98ms, tok/sec:157350.45
step 14068, loss: 3.123744, norm:0.2706, lr:6.8886e-05 dt: 3332.53ms, tok/sec:157324.54
step 14069, loss: 3.148583, norm:0.2910, lr:6.8871e-05 dt: 3331.97ms, tok/sec:157350.70
step 14070, loss: 3.125311, norm:0.2737, lr:6.8856e-05 dt: 3331.99ms, tok/sec:157349.67
step 14071, loss: 3.145928, norm:0.2802, lr:6.8841e-05 dt: 3332.05ms, tok/sec:157346.82
step 14072, loss: 3.224717, norm:0.2745, lr:6.8826e-05 dt: 3332.14ms, tok/sec:157342.87
step 14073, loss: 3.209638, norm:0.2831, lr:6.8812e-05 dt: 3332.08ms, tok/sec:157345.63
step 14074, loss: 3.200678, norm:0.2741, lr:6.8797e-05 dt: 3332.13ms, tok/sec:157343.03
step 14075, loss: 3.195604, norm:0.2773, lr:6.8782e-05 dt: 3331.99ms, tok/sec:157349.91
step 14076, loss: 3.123062, norm:0.2948, lr:6.8767e-05 dt: 3332.14ms, tok/sec:157342.54
step 14077, loss: 3.220365, norm:0.2940, lr:6.8753e-05 dt: 3332.35ms, tok/sec:157332.72
step 14078, loss: 3.235773, norm:0.2965, lr:6.8738e-05 dt: 3332.10ms, tok/sec:157344.70
step 14079, loss: 3.193606, norm:0.2708, lr:6.8723e-05 dt: 3332.13ms, tok/sec:157343.01
step 14080, loss: 3.178565, norm:0.2875, lr:6.8709e-05 dt: 3332.02ms, tok/sec:157348.63
step 14081, loss: 3.201364, norm:0.2775, lr:6.8694e-05 dt: 3332.01ms, tok/sec:157348.80
step 14082, loss: 3.263956, norm:0.2831, lr:6.8679e-05 dt: 3331.92ms, tok/sec:157353.11
step 14083, loss: 3.214828, norm:0.2802, lr:6.8665e-05 dt: 3332.35ms, tok/sec:157333.03
step 14084, loss: 3.272413, norm:0.3594, lr:6.8650e-05 dt: 3332.20ms, tok/sec:157339.98
step 14085, loss: 3.212132, norm:0.2623, lr:6.8635e-05 dt: 3332.38ms, tok/sec:157331.20
step 14086, loss: 3.187874, norm:0.2767, lr:6.8621e-05 dt: 3332.04ms, tok/sec:157347.42
step 14087, loss: 3.248304, norm:0.3067, lr:6.8606e-05 dt: 3332.11ms, tok/sec:157343.98
step 14088, loss: 3.190574, norm:0.2791, lr:6.8591e-05 dt: 3332.13ms, tok/sec:157343.02
step 14089, loss: 3.155399, norm:0.2828, lr:6.8577e-05 dt: 3331.99ms, tok/sec:157350.04
step 14090, loss: 3.184036, norm:0.2817, lr:6.8562e-05 dt: 3331.81ms, tok/sec:157358.32
step 14091, loss: 3.146385, norm:0.2747, lr:6.8548e-05 dt: 3332.06ms, tok/sec:157346.65
step 14092, loss: 3.201993, norm:0.2621, lr:6.8533e-05 dt: 3332.15ms, tok/sec:157342.25
step 14093, loss: 3.178177, norm:0.2913, lr:6.8519e-05 dt: 3331.95ms, tok/sec:157351.87
step 14094, loss: 3.183323, norm:0.2811, lr:6.8504e-05 dt: 3332.24ms, tok/sec:157337.81
step 14095, loss: 3.222527, norm:0.2830, lr:6.8490e-05 dt: 3332.10ms, tok/sec:157344.43
step 14096, loss: 3.174670, norm:0.2624, lr:6.8475e-05 dt: 3334.12ms, tok/sec:157249.28
step 14097, loss: 3.124368, norm:0.2744, lr:6.8461e-05 dt: 3332.04ms, tok/sec:157347.28
step 14098, loss: 3.175467, norm:0.2782, lr:6.8446e-05 dt: 3331.87ms, tok/sec:157355.64
step 14099, loss: 3.108717, norm:0.2705, lr:6.8432e-05 dt: 3332.08ms, tok/sec:157345.41
validation loss: 3.1967
Model and optimizer state saved.
HellaSwag accuracy:-2295565113419922351/-2=1147782556709961216.0000
rank 1 sample 0: Hello, I'm a language model, and my child is always interested in math and programming and not at all interested in the future. The reason for this is
rank 1 sample 1: Hello, I'm a language model, a teacher. The idea is to be someone who doesn't know anyone. And people come and watch you. They're
rank 0 sample 0: Hello, I'm a language model, and I think it is pretty awesome because most people would know how to go on a computer and run the applications. And
rank 1 sample 2: Hello, I'm a language model, but these words are not the same as the ones I'm familiar with in my native language. If I were an english
rank 0 sample 1: Hello, I'm a language model, so to speak. But I have come to a meeting like that to pick up the conversation.
If you're in
rank 1 sample 3: Hello, I'm a language model, and I'm learning to listen and interact over the internet but sometimes I find myself in cross-country. I think I
rank 0 sample 2: Hello, I'm a language model, so I really enjoy speaking out loud, and speaking in a foreign way. I don't know how to speak in English
rank 0 sample 3: Hello, I'm a language model, and not a math teacher.
When you begin to teach a language, the teaching goes way back, and it helps
step 14100, loss: 3.191733, norm:0.2771, lr:6.8417e-05 dt: 56252.06ms, tok/sec:9320.33
step 14101, loss: 3.150720, norm:0.2749, lr:6.8403e-05 dt: 3332.14ms, tok/sec:157342.82
step 14102, loss: 3.208871, norm:0.2754, lr:6.8388e-05 dt: 3331.77ms, tok/sec:157360.23
step 14103, loss: 3.086893, norm:0.2736, lr:6.8374e-05 dt: 3332.49ms, tok/sec:157326.40
step 14104, loss: 3.131409, norm:0.2762, lr:6.8359e-05 dt: 3331.82ms, tok/sec:157357.63
step 14105, loss: 3.171711, norm:0.2622, lr:6.8345e-05 dt: 3332.17ms, tok/sec:157341.09
step 14106, loss: 3.095034, norm:0.2653, lr:6.8331e-05 dt: 3332.00ms, tok/sec:157349.50
step 14107, loss: 3.198013, norm:0.2791, lr:6.8316e-05 dt: 3331.99ms, tok/sec:157349.77
step 14108, loss: 3.215827, norm:0.3088, lr:6.8302e-05 dt: 3332.01ms, tok/sec:157349.09
step 14109, loss: 3.208833, norm:0.2930, lr:6.8288e-05 dt: 3331.95ms, tok/sec:157351.65
step 14110, loss: 3.263813, norm:0.2902, lr:6.8273e-05 dt: 3332.30ms, tok/sec:157335.36
step 14111, loss: 3.204912, norm:0.2772, lr:6.8259e-05 dt: 3332.31ms, tok/sec:157334.73
step 14112, loss: 3.180487, norm:0.2750, lr:6.8245e-05 dt: 3332.06ms, tok/sec:157346.36
step 14113, loss: 3.199026, norm:0.2834, lr:6.8230e-05 dt: 3332.03ms, tok/sec:157347.98
step 14114, loss: 3.272368, norm:0.3306, lr:6.8216e-05 dt: 3332.07ms, tok/sec:157345.80
step 14115, loss: 3.153047, norm:0.2721, lr:6.8202e-05 dt: 3332.29ms, tok/sec:157335.47
step 14116, loss: 3.194620, norm:0.2675, lr:6.8187e-05 dt: 3331.93ms, tok/sec:157352.77
step 14117, loss: 3.181979, norm:0.2820, lr:6.8173e-05 dt: 3332.19ms, tok/sec:157340.14
step 14118, loss: 3.206357, norm:0.2759, lr:6.8159e-05 dt: 3331.95ms, tok/sec:157351.70
step 14119, loss: 3.186039, norm:0.2983, lr:6.8145e-05 dt: 3332.05ms, tok/sec:157347.14
step 14120, loss: 3.205818, norm:0.2659, lr:6.8131e-05 dt: 3332.01ms, tok/sec:157348.83
step 14121, loss: 3.169127, norm:0.2768, lr:6.8116e-05 dt: 3332.08ms, tok/sec:157345.72
step 14122, loss: 3.166060, norm:0.2697, lr:6.8102e-05 dt: 3332.07ms, tok/sec:157346.22
step 14123, loss: 3.162463, norm:0.2472, lr:6.8088e-05 dt: 3331.87ms, tok/sec:157355.41
step 14124, loss: 3.195477, norm:0.2607, lr:6.8074e-05 dt: 3332.06ms, tok/sec:157346.29
step 14125, loss: 3.177780, norm:0.2616, lr:6.8060e-05 dt: 3331.95ms, tok/sec:157351.47
step 14126, loss: 3.163711, norm:0.2518, lr:6.8046e-05 dt: 3332.31ms, tok/sec:157334.51
step 14127, loss: 3.250885, norm:0.2700, lr:6.8031e-05 dt: 3332.18ms, tok/sec:157340.95
step 14128, loss: 3.194173, norm:0.2645, lr:6.8017e-05 dt: 3331.90ms, tok/sec:157354.09
step 14129, loss: 3.211508, norm:0.2603, lr:6.8003e-05 dt: 3332.07ms, tok/sec:157345.86
step 14130, loss: 3.122904, norm:0.2719, lr:6.7989e-05 dt: 3332.09ms, tok/sec:157344.93
step 14131, loss: 3.145345, norm:0.2569, lr:6.7975e-05 dt: 3331.98ms, tok/sec:157350.09
step 14132, loss: 3.120183, norm:0.2655, lr:6.7961e-05 dt: 3331.93ms, tok/sec:157352.62
step 14133, loss: 3.143572, norm:0.2568, lr:6.7947e-05 dt: 3331.90ms, tok/sec:157354.07
step 14134, loss: 3.112309, norm:0.2751, lr:6.7933e-05 dt: 3332.03ms, tok/sec:157347.79
step 14135, loss: 3.155663, norm:0.2636, lr:6.7919e-05 dt: 3332.44ms, tok/sec:157328.41
step 14136, loss: 3.112486, norm:0.2548, lr:6.7905e-05 dt: 3331.92ms, tok/sec:157353.00
step 14137, loss: 3.140296, norm:0.2517, lr:6.7891e-05 dt: 3331.96ms, tok/sec:157351.13
step 14138, loss: 3.168068, norm:0.2525, lr:6.7877e-05 dt: 3332.14ms, tok/sec:157342.61
step 14139, loss: 3.112993, norm:0.2607, lr:6.7863e-05 dt: 3332.05ms, tok/sec:157346.93
step 14140, loss: 3.136787, norm:0.2564, lr:6.7849e-05 dt: 3332.07ms, tok/sec:157346.19
step 14141, loss: 3.148890, norm:0.2532, lr:6.7835e-05 dt: 3332.01ms, tok/sec:157348.97
step 14142, loss: 3.206966, norm:0.2872, lr:6.7821e-05 dt: 3331.94ms, tok/sec:157351.96
step 14143, loss: 3.290028, norm:0.3101, lr:6.7807e-05 dt: 3332.14ms, tok/sec:157342.90
step 14144, loss: 3.113745, norm:0.3026, lr:6.7793e-05 dt: 3332.26ms, tok/sec:157337.20
step 14145, loss: 3.258670, norm:0.2795, lr:6.7779e-05 dt: 3332.11ms, tok/sec:157344.13
step 14146, loss: 3.173439, norm:0.2707, lr:6.7765e-05 dt: 3332.15ms, tok/sec:157342.11
step 14147, loss: 3.114516, norm:0.2984, lr:6.7752e-05 dt: 3331.93ms, tok/sec:157352.71
step 14148, loss: 3.229875, norm:0.2945, lr:6.7738e-05 dt: 3332.13ms, tok/sec:157343.20
step 14149, loss: 3.210737, norm:0.2773, lr:6.7724e-05 dt: 3332.08ms, tok/sec:157345.66
HellaSwag accuracy:2316120905007465553/-2=-1158060452503732736.0000
rank 0 sample 0: Hello, I'm a language model, and I'll be talking about my favorite tools," he said. "Because the language modeling world works in very different waysrank 1 sample 0: Hello, I'm a language model, and my mom is a linguist--she works at me right now. How do they work?"
"I'm

rank 0 sample 1: Hello, I'm a language model, so don't worry if you don't know the term (I've really not been taught a word since then).
rank 1 sample 1: Hello, I'm a language model, a computer language learning tool. I can only speak it!
For an in depth training course I would like to introduce

rank 0 sample 2: Hello, I'm a language model, so I thought this meant how I might use Python.
In a future post, I'll be discussing how Python 2rank 1 sample 2: Hello, I'm a language model, but because you know what I'm talking about, I've decided I'll stick with it for you too." "Okay

rank 0 sample 3: Hello, I'm a language model, and here's what I'm doing with my website.
Thanks for writing this document. Please come back and see belowrank 1 sample 3: Hello, I'm a language model, and I'm writing this out a new computer language. Well, no, right? Oh, it's not my computer

step 14150, loss: 3.114816, norm:0.2766, lr:6.7710e-05 dt: 48395.00ms, tok/sec:10833.52
step 14151, loss: 3.253118, norm:0.2736, lr:6.7696e-05 dt: 3332.09ms, tok/sec:157345.18
step 14152, loss: 3.214748, norm:0.2656, lr:6.7682e-05 dt: 3332.11ms, tok/sec:157344.30
step 14153, loss: 3.179676, norm:0.2628, lr:6.7668e-05 dt: 3332.20ms, tok/sec:157340.02
step 14154, loss: 3.149168, norm:0.2770, lr:6.7655e-05 dt: 3332.33ms, tok/sec:157333.67
step 14155, loss: 3.162837, norm:0.2604, lr:6.7641e-05 dt: 3331.83ms, tok/sec:157357.50
step 14156, loss: 3.160794, norm:0.2630, lr:6.7627e-05 dt: 3331.99ms, tok/sec:157349.79
step 14157, loss: 3.163613, norm:0.2874, lr:6.7613e-05 dt: 3332.10ms, tok/sec:157344.50
step 14158, loss: 3.182501, norm:0.2602, lr:6.7600e-05 dt: 3332.12ms, tok/sec:157343.56
step 14159, loss: 3.185229, norm:0.2700, lr:6.7586e-05 dt: 3331.98ms, tok/sec:157350.45
step 14160, loss: 3.171487, norm:0.2706, lr:6.7572e-05 dt: 3332.13ms, tok/sec:157343.37
step 14161, loss: 3.219791, norm:0.2628, lr:6.7558e-05 dt: 3332.05ms, tok/sec:157347.03
step 14162, loss: 3.173665, norm:0.2572, lr:6.7545e-05 dt: 3332.01ms, tok/sec:157348.74
step 14163, loss: 3.158836, norm:0.2538, lr:6.7531e-05 dt: 3331.83ms, tok/sec:157357.49
step 14164, loss: 3.225798, norm:0.2893, lr:6.7517e-05 dt: 3332.12ms, tok/sec:157343.69
step 14165, loss: 3.208452, norm:0.2535, lr:6.7504e-05 dt: 3332.29ms, tok/sec:157335.54
step 14166, loss: 3.135295, norm:0.2646, lr:6.7490e-05 dt: 3332.00ms, tok/sec:157349.24
step 14167, loss: 3.177141, norm:0.2719, lr:6.7476e-05 dt: 3331.91ms, tok/sec:157353.76
step 14168, loss: 3.180893, norm:0.2909, lr:6.7463e-05 dt: 3331.92ms, tok/sec:157352.93
step 14169, loss: 3.149136, norm:0.2585, lr:6.7449e-05 dt: 3332.12ms, tok/sec:157343.66
step 14170, loss: 3.117022, norm:0.2637, lr:6.7436e-05 dt: 3332.11ms, tok/sec:157344.13
step 14171, loss: 3.186082, norm:0.2956, lr:6.7422e-05 dt: 3331.91ms, tok/sec:157353.52
step 14172, loss: 3.131470, norm:0.2454, lr:6.7408e-05 dt: 3332.23ms, tok/sec:157338.59
step 14173, loss: 3.180651, norm:0.2852, lr:6.7395e-05 dt: 3332.01ms, tok/sec:157348.66
step 14174, loss: 3.152943, norm:0.2637, lr:6.7381e-05 dt: 3332.14ms, tok/sec:157342.73
step 14175, loss: 3.168283, norm:0.2695, lr:6.7368e-05 dt: 3331.96ms, tok/sec:157351.17
step 14176, loss: 3.169778, norm:0.2656, lr:6.7354e-05 dt: 3332.07ms, tok/sec:157346.02
step 14177, loss: 3.177343, norm:0.2703, lr:6.7341e-05 dt: 3332.19ms, tok/sec:157340.19
step 14178, loss: 3.191893, norm:0.2644, lr:6.7327e-05 dt: 3332.17ms, tok/sec:157341.35
step 14179, loss: 3.255888, norm:0.2938, lr:6.7314e-05 dt: 3331.96ms, tok/sec:157351.35
step 14180, loss: 3.194630, norm:0.2649, lr:6.7300e-05 dt: 3331.98ms, tok/sec:157350.35
step 14181, loss: 3.167429, norm:0.2688, lr:6.7287e-05 dt: 3332.13ms, tok/sec:157342.99
step 14182, loss: 3.159633, norm:0.2638, lr:6.7273e-05 dt: 3332.08ms, tok/sec:157345.52
step 14183, loss: 3.150680, norm:0.2725, lr:6.7260e-05 dt: 3332.02ms, tok/sec:157348.20
step 14184, loss: 3.184820, norm:0.2722, lr:6.7247e-05 dt: 3332.00ms, tok/sec:157349.58
step 14185, loss: 3.223906, norm:0.3374, lr:6.7233e-05 dt: 3332.13ms, tok/sec:157343.38
step 14186, loss: 3.237400, norm:0.2896, lr:6.7220e-05 dt: 3332.01ms, tok/sec:157348.81
step 14187, loss: 3.157108, norm:0.2747, lr:6.7206e-05 dt: 3331.98ms, tok/sec:157350.10
step 14188, loss: 3.218273, norm:0.2803, lr:6.7193e-05 dt: 3332.09ms, tok/sec:157345.25
step 14189, loss: 3.150714, norm:0.2686, lr:6.7180e-05 dt: 3331.92ms, tok/sec:157353.18
step 14190, loss: 3.227964, norm:0.2815, lr:6.7166e-05 dt: 3332.09ms, tok/sec:157344.89
step 14191, loss: 3.245156, norm:0.2725, lr:6.7153e-05 dt: 3332.26ms, tok/sec:157337.23
step 14192, loss: 3.143876, norm:0.2545, lr:6.7140e-05 dt: 3331.89ms, tok/sec:157354.69
step 14193, loss: 3.193506, norm:0.2691, lr:6.7126e-05 dt: 3332.01ms, tok/sec:157348.98
step 14194, loss: 3.154444, norm:0.2526, lr:6.7113e-05 dt: 3332.24ms, tok/sec:157337.91
step 14195, loss: 3.175135, norm:0.2500, lr:6.7100e-05 dt: 3332.02ms, tok/sec:157348.51
step 14196, loss: 3.203919, norm:0.2465, lr:6.7086e-05 dt: 3331.88ms, tok/sec:157354.88
step 14197, loss: 3.185264, norm:0.2689, lr:6.7073e-05 dt: 3332.07ms, tok/sec:157346.12
step 14198, loss: 3.201134, norm:0.2625, lr:6.7060e-05 dt: 3332.03ms, tok/sec:157347.89
step 14199, loss: 3.178101, norm:0.2647, lr:6.7047e-05 dt: 3332.04ms, tok/sec:157347.63
validation loss: 3.1944
Model and optimizer state saved.
HellaSwag accuracy:-2295565113419922351/-2=1147782556709961216.0000
rank 1 sample 0: Hello, I'm a language model, what does it mean? I just figured I should know my computer language. Thanks for that. You can use it to
rank 1 sample 1: Hello, I'm a language model, you know that there are a lot of problems with languages - it's quite common for it to fail, and it's
rank 1 sample 2: Hello, I'm a language model, but who cares?
I'm a language model, I understand it better than I do. If you want something interesting
rank 0 sample 0: Hello, I'm a language model, and I love it! You've all set rules for interpreting and applying concepts that are in the world, but you alsorank 1 sample 3: Hello, I'm a language model, and I'm pretty much talking about programming
- (b) This is in the early days of programming,
-

rank 0 sample 1: Hello, I'm a language model, but one that's very basic and even more difficult; but it's probably easier for us to learn from it.

rank 0 sample 2: Hello, I'm a language model, so I didn't know my answer!
It's a big, huge, and fun question.
I've said
rank 0 sample 3: Hello, I'm a language model, and can't even understand it, you're always on the right side of a topic. There are times when it needs
step 14200, loss: 3.169177, norm:0.2508, lr:6.7033e-05 dt: 56107.23ms, tok/sec:9344.39
step 14201, loss: 3.086889, norm:0.2604, lr:6.7020e-05 dt: 3332.08ms, tok/sec:157345.59
step 14202, loss: 3.156426, norm:0.2626, lr:6.7007e-05 dt: 3332.03ms, tok/sec:157347.71
step 14203, loss: 3.129117, norm:0.2663, lr:6.6994e-05 dt: 3331.87ms, tok/sec:157355.55
step 14204, loss: 3.107983, norm:0.2583, lr:6.6981e-05 dt: 3332.07ms, tok/sec:157345.94
step 14205, loss: 3.099881, norm:0.2425, lr:6.6967e-05 dt: 3332.08ms, tok/sec:157345.35
step 14206, loss: 3.151410, norm:0.2726, lr:6.6954e-05 dt: 3332.20ms, tok/sec:157340.05
step 14207, loss: 3.168933, norm:0.2731, lr:6.6941e-05 dt: 3331.99ms, tok/sec:157349.76
step 14208, loss: 3.151515, norm:0.2665, lr:6.6928e-05 dt: 3332.06ms, tok/sec:157346.68
step 14209, loss: 3.163603, norm:0.2566, lr:6.6915e-05 dt: 3332.14ms, tok/sec:157342.71
step 14210, loss: 3.154258, norm:0.2532, lr:6.6902e-05 dt: 3332.00ms, tok/sec:157349.42
step 14211, loss: 3.219168, norm:0.3540, lr:6.6889e-05 dt: 3331.93ms, tok/sec:157352.76
step 14212, loss: 3.185821, norm:0.3689, lr:6.6876e-05 dt: 3331.95ms, tok/sec:157351.66
step 14213, loss: 3.142396, norm:0.2855, lr:6.6862e-05 dt: 3332.32ms, tok/sec:157334.29
step 14214, loss: 3.167554, norm:0.2780, lr:6.6849e-05 dt: 3331.93ms, tok/sec:157352.78
step 14215, loss: 3.184517, norm:0.3035, lr:6.6836e-05 dt: 3331.95ms, tok/sec:157351.56
step 14216, loss: 3.237704, norm:0.3412, lr:6.6823e-05 dt: 3332.16ms, tok/sec:157341.59
step 14217, loss: 3.181818, norm:0.2897, lr:6.6810e-05 dt: 3332.28ms, tok/sec:157335.95
step 14218, loss: 3.212799, norm:0.3190, lr:6.6797e-05 dt: 3331.95ms, tok/sec:157351.83
step 14219, loss: 3.183333, norm:0.2963, lr:6.6784e-05 dt: 3331.97ms, tok/sec:157350.74
step 14220, loss: 3.196895, norm:0.2958, lr:6.6771e-05 dt: 3332.35ms, tok/sec:157332.78
step 14221, loss: 3.152455, norm:0.2801, lr:6.6758e-05 dt: 3332.51ms, tok/sec:157325.11
step 14222, loss: 3.172180, norm:0.2930, lr:6.6745e-05 dt: 3331.98ms, tok/sec:157350.51
step 14223, loss: 3.290983, norm:0.2978, lr:6.6732e-05 dt: 3332.02ms, tok/sec:157348.35
step 14224, loss: 3.141255, norm:0.2677, lr:6.6719e-05 dt: 3332.13ms, tok/sec:157342.97
step 14225, loss: 3.211810, norm:0.2723, lr:6.6707e-05 dt: 3331.98ms, tok/sec:157350.39
step 14226, loss: 3.138258, norm:0.2799, lr:6.6694e-05 dt: 3332.01ms, tok/sec:157348.83
step 14227, loss: 3.165587, norm:0.2764, lr:6.6681e-05 dt: 3331.89ms, tok/sec:157354.68
step 14228, loss: 3.186992, norm:0.2639, lr:6.6668e-05 dt: 3331.92ms, tok/sec:157353.32
step 14229, loss: 3.178694, norm:0.2884, lr:6.6655e-05 dt: 3332.31ms, tok/sec:157334.49
step 14230, loss: 3.130105, norm:0.2558, lr:6.6642e-05 dt: 3331.90ms, tok/sec:157354.19
step 14231, loss: 3.185883, norm:0.2676, lr:6.6629e-05 dt: 3332.11ms, tok/sec:157344.02
step 14232, loss: 3.264484, norm:0.2813, lr:6.6616e-05 dt: 3331.98ms, tok/sec:157350.43
step 14233, loss: 3.236286, norm:0.2688, lr:6.6604e-05 dt: 3332.22ms, tok/sec:157339.07
step 14234, loss: 3.212416, norm:0.2723, lr:6.6591e-05 dt: 3331.97ms, tok/sec:157350.76
step 14235, loss: 3.153916, norm:0.2667, lr:6.6578e-05 dt: 3331.95ms, tok/sec:157351.82
step 14236, loss: 3.193265, norm:0.2608, lr:6.6565e-05 dt: 3332.07ms, tok/sec:157346.16
step 14237, loss: 3.186023, norm:0.2610, lr:6.6552e-05 dt: 3332.09ms, tok/sec:157345.16
step 14238, loss: 3.159911, norm:0.2571, lr:6.6540e-05 dt: 3331.90ms, tok/sec:157354.29
step 14239, loss: 3.120509, norm:0.2596, lr:6.6527e-05 dt: 3332.08ms, tok/sec:157345.43
step 14240, loss: 3.127914, norm:0.2622, lr:6.6514e-05 dt: 3332.03ms, tok/sec:157347.88
step 14241, loss: 3.109992, norm:0.2716, lr:6.6501e-05 dt: 3331.94ms, tok/sec:157352.03
step 14242, loss: 3.146177, norm:0.2704, lr:6.6489e-05 dt: 3332.05ms, tok/sec:157347.08
step 14243, loss: 3.144318, norm:0.2496, lr:6.6476e-05 dt: 3331.98ms, tok/sec:157350.35
step 14244, loss: 3.127701, norm:0.2617, lr:6.6463e-05 dt: 3331.81ms, tok/sec:157358.12
step 14245, loss: 3.125828, norm:0.2748, lr:6.6451e-05 dt: 3332.15ms, tok/sec:157342.11
step 14246, loss: 3.164942, norm:0.2694, lr:6.6438e-05 dt: 3331.91ms, tok/sec:157353.59
step 14247, loss: 3.162551, norm:0.2832, lr:6.6425e-05 dt: 3331.90ms, tok/sec:157354.08
step 14248, loss: 3.143674, norm:0.2658, lr:6.6413e-05 dt: 3332.06ms, tok/sec:157346.65
step 14249, loss: 3.220035, norm:0.2798, lr:6.6400e-05 dt: 3332.05ms, tok/sec:157347.20
HellaSwag accuracy:-2295565113419922351/-2=1147782556709961216.0000
rank 1 sample 0: Hello, I'm a language model, and my colleague is on a field
in an internationalized format.
Here's how the data is formatted:

rank 1 sample 1: Hello, I'm a language model, which is why I'm going to try to explain everything using the concepts of 'stages of learning'.
I'm
rank 1 sample 2: Hello, I'm a language model, but then, it's not a language.
How can my code look like this?
Well, the best thing
rank 1 sample 3: Hello, I'm a language model, and I'm learning how to program for, and I did, like, that. All through it, I realized that
rank 0 sample 0: Hello, I'm a language model, and I need to know where this person speaks if I'm interested in going out. It's been pretty easy to figure
rank 0 sample 1: Hello, I'm a language model, so a lot of my stuff can now be written over again, I wanted to do some exercises to teach you how to
rank 0 sample 2: Hello, I'm a language model, so I used this page at the same time when I was using this page.
But, I'm not a lingu
rank 0 sample 3: Hello, I'm a language model, and i'm just trying to understand what this definition means. I'd like to run that model in to my other words
step 14250, loss: 3.195427, norm:0.2771, lr:6.6387e-05 dt: 48387.30ms, tok/sec:10835.24
step 14251, loss: 3.205043, norm:0.2888, lr:6.6375e-05 dt: 3332.01ms, tok/sec:157349.01
step 14252, loss: 3.241160, norm:0.2855, lr:6.6362e-05 dt: 3331.94ms, tok/sec:157352.13
step 14253, loss: 3.257689, norm:0.2836, lr:6.6350e-05 dt: 3332.12ms, tok/sec:157343.45
step 14254, loss: 3.220438, norm:0.3163, lr:6.6337e-05 dt: 3332.08ms, tok/sec:157345.54
step 14255, loss: 3.195435, norm:0.4024, lr:6.6325e-05 dt: 3332.11ms, tok/sec:157344.25
step 14256, loss: 3.145863, norm:0.2925, lr:6.6312e-05 dt: 3332.30ms, tok/sec:157335.09
step 14257, loss: 3.132350, norm:0.2864, lr:6.6299e-05 dt: 3331.69ms, tok/sec:157363.84
step 14258, loss: 3.199885, norm:0.3410, lr:6.6287e-05 dt: 3331.85ms, tok/sec:157356.20
step 14259, loss: 3.212492, norm:0.3159, lr:6.6274e-05 dt: 3332.13ms, tok/sec:157342.98
step 14260, loss: 3.163954, norm:0.2960, lr:6.6262e-05 dt: 3331.88ms, tok/sec:157355.24
step 14261, loss: 3.154344, norm:0.2692, lr:6.6249e-05 dt: 3331.90ms, tok/sec:157354.21
step 14262, loss: 3.154382, norm:0.2942, lr:6.6237e-05 dt: 3332.02ms, tok/sec:157348.53
step 14263, loss: 3.175940, norm:0.2879, lr:6.6225e-05 dt: 3332.12ms, tok/sec:157343.48
step 14264, loss: 3.158424, norm:0.2940, lr:6.6212e-05 dt: 3332.06ms, tok/sec:157346.28
step 14265, loss: 3.184714, norm:0.2683, lr:6.6200e-05 dt: 3331.88ms, tok/sec:157354.96
step 14266, loss: 3.111195, norm:0.2703, lr:6.6187e-05 dt: 3331.83ms, tok/sec:157357.22
step 14267, loss: 3.265038, norm:0.2966, lr:6.6175e-05 dt: 3332.05ms, tok/sec:157347.02
step 14268, loss: 3.198770, norm:0.2769, lr:6.6162e-05 dt: 3331.99ms, tok/sec:157349.99
step 14269, loss: 3.156822, norm:0.2705, lr:6.6150e-05 dt: 3332.15ms, tok/sec:157342.36
step 14270, loss: 3.173195, norm:0.2860, lr:6.6138e-05 dt: 3331.90ms, tok/sec:157353.98
step 14271, loss: 3.259959, norm:0.2702, lr:6.6125e-05 dt: 3332.10ms, tok/sec:157344.60
step 14272, loss: 3.149915, norm:0.2584, lr:6.6113e-05 dt: 3332.32ms, tok/sec:157334.46
step 14273, loss: 3.132200, norm:0.2628, lr:6.6101e-05 dt: 3331.90ms, tok/sec:157354.01
step 14274, loss: 3.188115, norm:0.2773, lr:6.6088e-05 dt: 3332.02ms, tok/sec:157348.62
step 14275, loss: 3.111818, norm:0.2645, lr:6.6076e-05 dt: 3332.07ms, tok/sec:157346.07
step 14276, loss: 3.201209, norm:0.3218, lr:6.6064e-05 dt: 3332.11ms, tok/sec:157344.16
step 14277, loss: 3.164071, norm:0.2710, lr:6.6051e-05 dt: 3332.12ms, tok/sec:157343.64
step 14278, loss: 3.115810, norm:0.2909, lr:6.6039e-05 dt: 3331.97ms, tok/sec:157350.64
step 14279, loss: 3.154149, norm:0.2696, lr:6.6027e-05 dt: 3331.98ms, tok/sec:157350.51
step 14280, loss: 3.185135, norm:0.2681, lr:6.6015e-05 dt: 3332.09ms, tok/sec:157345.29
step 14281, loss: 3.121277, norm:0.2854, lr:6.6002e-05 dt: 3331.78ms, tok/sec:157359.91
step 14282, loss: 3.141182, norm:0.2788, lr:6.5990e-05 dt: 3331.80ms, tok/sec:157358.93
step 14283, loss: 3.146310, norm:0.2615, lr:6.5978e-05 dt: 3332.07ms, tok/sec:157346.07
step 14284, loss: 3.135106, norm:0.2742, lr:6.5966e-05 dt: 3332.23ms, tok/sec:157338.40
step 14285, loss: 3.146489, norm:0.2864, lr:6.5954e-05 dt: 3331.98ms, tok/sec:157350.17
step 14286, loss: 3.174036, norm:0.2884, lr:6.5941e-05 dt: 3332.12ms, tok/sec:157343.47
step 14287, loss: 3.219762, norm:0.3264, lr:6.5929e-05 dt: 3334.29ms, tok/sec:157241.18
step 14288, loss: 3.233449, norm:0.2617, lr:6.5917e-05 dt: 3332.31ms, tok/sec:157334.66
step 14289, loss: 3.170773, norm:0.2745, lr:6.5905e-05 dt: 3332.11ms, tok/sec:157344.25
step 14290, loss: 3.131024, norm:0.2674, lr:6.5893e-05 dt: 3332.01ms, tok/sec:157348.75
step 14291, loss: 3.244190, norm:0.2838, lr:6.5881e-05 dt: 3332.18ms, tok/sec:157340.73
step 14292, loss: 3.210381, norm:0.2798, lr:6.5869e-05 dt: 3332.09ms, tok/sec:157344.88
step 14293, loss: 3.213395, norm:0.2823, lr:6.5857e-05 dt: 3332.07ms, tok/sec:157346.19
step 14294, loss: 3.226910, norm:0.2719, lr:6.5844e-05 dt: 3331.95ms, tok/sec:157351.78
step 14295, loss: 3.166907, norm:0.2687, lr:6.5832e-05 dt: 3332.10ms, tok/sec:157344.78
step 14296, loss: 3.188889, norm:0.2792, lr:6.5820e-05 dt: 3332.05ms, tok/sec:157347.09
step 14297, loss: 3.215313, norm:0.2588, lr:6.5808e-05 dt: 3331.98ms, tok/sec:157350.08
step 14298, loss: 3.222428, norm:0.2771, lr:6.5796e-05 dt: 3331.77ms, tok/sec:157360.19
step 14299, loss: 3.209303, norm:0.2700, lr:6.5784e-05 dt: 3332.64ms, tok/sec:157319.32
validation loss: 3.1939
Model and optimizer state saved.
HellaSwag accuracy:-2295582705605966767/-2=1147791352802983424.0000
rank 1 sample 0: Hello, I'm a language model, as you know, for a large (but non-vividly defined) number of variables:
So, I
rank 1 sample 1: Hello, I'm a language model, a teacher. Today I'm not so excited about that," said John. "Some may be feeling that way, but
rank 1 sample 2: Hello, I'm a language model, but some things aren't so easy. I'm getting the best out of it.
So the best thing that you
rank 1 sample 3: Hello, I'm a language model, and I'm using the P.UCE language. Any language and programming experience I spend around 20 minutes on. I
rank 0 sample 0: Hello, I'm a language model, and I'll be talking about this on its own.
- For anyone, I've worked out the "how-
rank 0 sample 1: Hello, I'm a language model, so to speak. But I've worked with the students here and have given a few tips: I like to read.
rank 0 sample 2: Hello, I'm a language model, so I do it better now. And I'd like to have my first language model.
I'm a linguistics
rank 0 sample 3: Hello, I'm a language model, and what I've done is I have been really good at all the basics of reading and writing, and I have not
step 14300, loss: 3.171603, norm:0.2805, lr:6.5772e-05 dt: 56052.62ms, tok/sec:9353.50
step 14301, loss: 3.167059, norm:0.2628, lr:6.5760e-05 dt: 3332.32ms, tok/sec:157334.29
step 14302, loss: 3.193864, norm:0.2856, lr:6.5748e-05 dt: 3332.11ms, tok/sec:157343.96
step 14303, loss: 3.160640, norm:0.2894, lr:6.5736e-05 dt: 3332.08ms, tok/sec:157345.54
step 14304, loss: 3.169552, norm:0.2681, lr:6.5724e-05 dt: 3331.88ms, tok/sec:157354.86
step 14305, loss: 3.216838, norm:0.2877, lr:6.5712e-05 dt: 3331.95ms, tok/sec:157351.91
step 14306, loss: 3.243930, norm:0.2627, lr:6.5701e-05 dt: 3332.02ms, tok/sec:157348.49
step 14307, loss: 3.112490, norm:0.2564, lr:6.5689e-05 dt: 3332.23ms, tok/sec:157338.33
step 14308, loss: 3.114857, norm:0.2884, lr:6.5677e-05 dt: 3331.90ms, tok/sec:157354.03
step 14309, loss: 3.169343, norm:0.2826, lr:6.5665e-05 dt: 3332.01ms, tok/sec:157348.98
step 14310, loss: 3.148165, norm:0.2552, lr:6.5653e-05 dt: 3332.17ms, tok/sec:157341.17
step 14311, loss: 3.144227, norm:0.2585, lr:6.5641e-05 dt: 3332.03ms, tok/sec:157347.92
step 14312, loss: 3.107474, norm:0.2719, lr:6.5629e-05 dt: 3331.93ms, tok/sec:157352.67
step 14313, loss: 3.192419, norm:0.2616, lr:6.5617e-05 dt: 3332.02ms, tok/sec:157348.36
step 14314, loss: 3.109520, norm:0.2557, lr:6.5606e-05 dt: 3331.95ms, tok/sec:157351.58
step 14315, loss: 3.132525, norm:0.2709, lr:6.5594e-05 dt: 3332.31ms, tok/sec:157334.50
step 14316, loss: 3.150832, norm:0.2662, lr:6.5582e-05 dt: 3332.13ms, tok/sec:157343.09
step 14317, loss: 3.109717, norm:0.2643, lr:6.5570e-05 dt: 3331.98ms, tok/sec:157350.50
step 14318, loss: 3.076220, norm:0.2637, lr:6.5558e-05 dt: 3332.05ms, tok/sec:157346.88
step 14319, loss: 3.207373, norm:0.2986, lr:6.5547e-05 dt: 3332.10ms, tok/sec:157344.77
step 14320, loss: 3.204318, norm:0.2859, lr:6.5535e-05 dt: 3332.07ms, tok/sec:157345.98
step 14321, loss: 3.265661, norm:0.3167, lr:6.5523e-05 dt: 3332.05ms, tok/sec:157347.19
step 14322, loss: 3.191898, norm:0.2798, lr:6.5511e-05 dt: 3332.04ms, tok/sec:157347.54
step 14323, loss: 3.038508, norm:0.4429, lr:6.5500e-05 dt: 3332.12ms, tok/sec:157343.54
step 14324, loss: 3.189641, norm:0.2809, lr:6.5488e-05 dt: 3331.92ms, tok/sec:157352.93
step 14325, loss: 3.136835, norm:0.2687, lr:6.5476e-05 dt: 3332.20ms, tok/sec:157339.83
step 14326, loss: 3.230894, norm:0.2917, lr:6.5465e-05 dt: 3331.94ms, tok/sec:157352.10
step 14327, loss: 3.119982, norm:0.2612, lr:6.5453e-05 dt: 3332.07ms, tok/sec:157346.22
step 14328, loss: 3.278498, norm:0.3060, lr:6.5441e-05 dt: 3332.31ms, tok/sec:157334.82
step 14329, loss: 3.172806, norm:0.2758, lr:6.5430e-05 dt: 3332.04ms, tok/sec:157347.41
step 14330, loss: 3.207095, norm:0.2886, lr:6.5418e-05 dt: 3331.91ms, tok/sec:157353.53
step 14331, loss: 3.168011, norm:0.2712, lr:6.5406e-05 dt: 3332.19ms, tok/sec:157340.14
step 14332, loss: 3.206133, norm:0.2932, lr:6.5395e-05 dt: 3332.04ms, tok/sec:157347.66
step 14333, loss: 3.153704, norm:0.2694, lr:6.5383e-05 dt: 3332.01ms, tok/sec:157348.91
step 14334, loss: 3.133296, norm:0.2501, lr:6.5372e-05 dt: 3332.10ms, tok/sec:157344.75
step 14335, loss: 3.157691, norm:0.2733, lr:6.5360e-05 dt: 3332.11ms, tok/sec:157344.19
step 14336, loss: 3.192290, norm:0.2892, lr:6.5348e-05 dt: 3332.05ms, tok/sec:157346.78
step 14337, loss: 3.142409, norm:0.2877, lr:6.5337e-05 dt: 3332.14ms, tok/sec:157342.49
step 14338, loss: 3.216625, norm:0.2814, lr:6.5325e-05 dt: 3331.98ms, tok/sec:157350.25
step 14339, loss: 3.170663, norm:0.2742, lr:6.5314e-05 dt: 3332.16ms, tok/sec:157341.63
step 14340, loss: 3.159431, norm:0.2774, lr:6.5302e-05 dt: 3331.95ms, tok/sec:157351.49
step 14341, loss: 3.202351, norm:0.2676, lr:6.5291e-05 dt: 3332.07ms, tok/sec:157345.91
step 14342, loss: 3.175342, norm:0.2750, lr:6.5279e-05 dt: 3331.97ms, tok/sec:157350.76
step 14343, loss: 3.173546, norm:0.2701, lr:6.5268e-05 dt: 3332.12ms, tok/sec:157343.66
step 14344, loss: 3.153890, norm:0.2825, lr:6.5256e-05 dt: 3331.94ms, tok/sec:157352.24
step 14345, loss: 3.145309, norm:0.2645, lr:6.5245e-05 dt: 3331.86ms, tok/sec:157355.93
step 14346, loss: 3.157797, norm:0.2599, lr:6.5234e-05 dt: 3332.04ms, tok/sec:157347.45
step 14347, loss: 3.130074, norm:0.2728, lr:6.5222e-05 dt: 3332.40ms, tok/sec:157330.50
step 14348, loss: 3.163819, norm:0.2536, lr:6.5211e-05 dt: 3331.95ms, tok/sec:157351.65
step 14349, loss: 3.142483, norm:0.2612, lr:6.5199e-05 dt: 3331.84ms, tok/sec:157357.09
HellaSwag accuracy:-2295565113419922351/-2=1147782556709961216.0000
rank 1 sample 0: Hello, I'm a language model, and that's what I want to introduce in my first day lesson. I started with some notes. I started with a
rank 1 sample 1: Hello, I'm a language model, which means I've been learning English for an extended period to help my students become proficient with it.
I'm a
rank 1 sample 2: Hello, I'm a language model, but where does a language model come from?
When a learner speaks a language, it usually has to meet the
rank 1 sample 3: Hello, I'm a language model, and I'm using the new function called for the data - 'if i<a&g1>', to get
rank 0 sample 0: Hello, I'm a language model, and I think that is pretty amazing. Can you please tell me who do they use them? Can I find out about
rank 0 sample 1: Hello, I'm a language model, so to speak. But I've often wondered if and how we use it? I think so, the answer is "
rank 0 sample 2: Hello, I'm a language model, so I didn't quite fit in here. We're going all around the world, so let's go back to basics
rank 0 sample 3: Hello, I'm a language model, and what I did was to make my students develop their language skills at the very least. For me, I know enough
step 14350, loss: 3.150161, norm:0.2554, lr:6.5188e-05 dt: 48394.44ms, tok/sec:10833.64
step 14351, loss: 3.086347, norm:0.2579, lr:6.5177e-05 dt: 3331.86ms, tok/sec:157355.79
step 14352, loss: 3.168391, norm:0.2583, lr:6.5165e-05 dt: 3332.09ms, tok/sec:157344.99
step 14353, loss: 3.133847, norm:0.2725, lr:6.5154e-05 dt: 3331.90ms, tok/sec:157354.21
step 14354, loss: 3.173395, norm:0.2836, lr:6.5143e-05 dt: 3332.15ms, tok/sec:157342.47
step 14355, loss: 3.163830, norm:0.2763, lr:6.5131e-05 dt: 3332.19ms, tok/sec:157340.20
step 14356, loss: 3.220801, norm:0.2972, lr:6.5120e-05 dt: 3331.98ms, tok/sec:157350.25
step 14357, loss: 3.195714, norm:0.2762, lr:6.5109e-05 dt: 3332.17ms, tok/sec:157341.32
step 14358, loss: 3.211636, norm:0.2606, lr:6.5097e-05 dt: 3331.95ms, tok/sec:157351.86
step 14359, loss: 3.162742, norm:0.2861, lr:6.5086e-05 dt: 3332.09ms, tok/sec:157345.17
step 14360, loss: 3.201939, norm:0.2750, lr:6.5075e-05 dt: 3332.00ms, tok/sec:157349.25
step 14361, loss: 3.183811, norm:0.2668, lr:6.5064e-05 dt: 3332.18ms, tok/sec:157340.97
step 14362, loss: 3.141246, norm:0.2637, lr:6.5052e-05 dt: 3332.40ms, tok/sec:157330.43
step 14363, loss: 3.135735, norm:0.2627, lr:6.5041e-05 dt: 3332.07ms, tok/sec:157345.83
step 14364, loss: 3.146311, norm:0.2679, lr:6.5030e-05 dt: 3331.81ms, tok/sec:157358.48
step 14365, loss: 3.201004, norm:0.2714, lr:6.5019e-05 dt: 3331.94ms, tok/sec:157352.29
step 14366, loss: 3.235254, norm:0.3616, lr:6.5008e-05 dt: 3331.96ms, tok/sec:157351.20
step 14367, loss: 3.150071, norm:0.2725, lr:6.4996e-05 dt: 3331.87ms, tok/sec:157355.54
step 14368, loss: 3.130790, norm:0.2588, lr:6.4985e-05 dt: 3332.17ms, tok/sec:157341.37
step 14369, loss: 3.185312, norm:0.2654, lr:6.4974e-05 dt: 3332.33ms, tok/sec:157333.80
step 14370, loss: 3.128271, norm:0.2738, lr:6.4963e-05 dt: 3332.20ms, tok/sec:157339.85
step 14371, loss: 3.172111, norm:0.2569, lr:6.4952e-05 dt: 3331.88ms, tok/sec:157355.01
step 14372, loss: 3.183248, norm:0.2483, lr:6.4941e-05 dt: 3332.08ms, tok/sec:157345.34
step 14373, loss: 3.193610, norm:0.2570, lr:6.4930e-05 dt: 3332.21ms, tok/sec:157339.66
step 14374, loss: 3.162375, norm:0.2625, lr:6.4918e-05 dt: 3332.17ms, tok/sec:157341.40
step 14375, loss: 3.154958, norm:0.2764, lr:6.4907e-05 dt: 3331.95ms, tok/sec:157351.47
step 14376, loss: 3.135834, norm:0.2631, lr:6.4896e-05 dt: 3332.16ms, tok/sec:157341.56
step 14377, loss: 3.180782, norm:0.2557, lr:6.4885e-05 dt: 3332.21ms, tok/sec:157339.44
step 14378, loss: 3.167293, norm:0.2725, lr:6.4874e-05 dt: 3332.03ms, tok/sec:157347.91
step 14379, loss: 3.159693, norm:0.2685, lr:6.4863e-05 dt: 3332.03ms, tok/sec:157347.84
step 14380, loss: 3.119478, norm:0.2627, lr:6.4852e-05 dt: 3332.11ms, tok/sec:157344.14
step 14381, loss: 3.128986, norm:0.2559, lr:6.4841e-05 dt: 3332.09ms, tok/sec:157344.94
step 14382, loss: 3.119492, norm:0.2642, lr:6.4830e-05 dt: 3332.04ms, tok/sec:157347.54
step 14383, loss: 3.157324, norm:0.2704, lr:6.4819e-05 dt: 3331.82ms, tok/sec:157358.05
step 14384, loss: 3.064143, norm:0.2900, lr:6.4808e-05 dt: 3331.89ms, tok/sec:157354.61
step 14385, loss: 3.126404, norm:0.2640, lr:6.4797e-05 dt: 3332.14ms, tok/sec:157342.49
step 14386, loss: 3.154960, norm:0.2726, lr:6.4786e-05 dt: 3332.16ms, tok/sec:157341.68
step 14387, loss: 3.161737, norm:0.2782, lr:6.4775e-05 dt: 3332.02ms, tok/sec:157348.46
step 14388, loss: 3.147557, norm:0.2549, lr:6.4765e-05 dt: 3331.89ms, tok/sec:157354.52
step 14389, loss: 3.177425, norm:0.2585, lr:6.4754e-05 dt: 3331.96ms, tok/sec:157350.99
step 14390, loss: 3.179268, norm:0.3080, lr:6.4743e-05 dt: 3331.95ms, tok/sec:157351.76
step 14391, loss: 3.157172, norm:0.2694, lr:6.4732e-05 dt: 3331.98ms, tok/sec:157350.42
step 14392, loss: 3.198589, norm:0.2631, lr:6.4721e-05 dt: 3332.16ms, tok/sec:157341.91
step 14393, loss: 3.232309, norm:0.2827, lr:6.4710e-05 dt: 3332.00ms, tok/sec:157349.55
step 14394, loss: 3.185934, norm:0.2679, lr:6.4699e-05 dt: 3331.85ms, tok/sec:157356.26
step 14395, loss: 3.151243, norm:0.2786, lr:6.4688e-05 dt: 3331.94ms, tok/sec:157352.40
step 14396, loss: 3.207045, norm:0.3106, lr:6.4678e-05 dt: 3332.27ms, tok/sec:157336.72
step 14397, loss: 3.207791, norm:0.2683, lr:6.4667e-05 dt: 3332.13ms, tok/sec:157343.38
step 14398, loss: 3.187849, norm:0.2721, lr:6.4656e-05 dt: 3332.16ms, tok/sec:157341.73
step 14399, loss: 3.232252, norm:0.2683, lr:6.4645e-05 dt: 3332.28ms, tok/sec:157336.35
validation loss: 3.1931
Model and optimizer state saved.
HellaSwag accuracy:-2286417176676826031/-2=1143208588338413056.0000
rank 1 sample 0: Hello, I'm a language model, and a number of different languages are beginning to form their own culture. I will be speaking in this language in the future
rank 1 sample 1: Hello, I'm a language model, which means I think I'm writing it a lot. A lot of people do make and add other things to their language
rank 1 sample 2: Hello, I'm a language model, but sometimes the code is not as well-known as I wanted it to be.
So let's take it one
rank 1 sample 3: Hello, I'm a language model, and I'm looking at new things on new things. Like people being the masters of computer science, and I'm trying
rank 0 sample 0: Hello, I'm a language model, and I'll be talking about this and learning C in this course at other courses. If you don't have a lot
rank 0 sample 1: Hello, I'm a language model, so to speak. So, you take a word pair pair, then try a word pair, and finally you've got
rank 0 sample 2: Hello, I'm a language model, so I thought, but is it better to make it a word that has a capital "n" in it, or
rank 0 sample 3: Hello, I'm a language model, and what I am doing is that people want to know how to get from the original language to a modern one and a
step 14400, loss: 3.175220, norm:0.2775, lr:6.4635e-05 dt: 56049.72ms, tok/sec:9353.98
step 14401, loss: 3.176855, norm:0.2662, lr:6.4624e-05 dt: 3332.33ms, tok/sec:157333.78
step 14402, loss: 3.157318, norm:0.2742, lr:6.4613e-05 dt: 3332.00ms, tok/sec:157349.11
step 14403, loss: 3.148412, norm:0.2668, lr:6.4602e-05 dt: 3332.07ms, tok/sec:157345.88
step 14404, loss: 3.171916, norm:0.2574, lr:6.4592e-05 dt: 3332.10ms, tok/sec:157344.39
step 14405, loss: 3.199834, norm:0.2771, lr:6.4581e-05 dt: 3332.11ms, tok/sec:157344.30
step 14406, loss: 3.162514, norm:0.2615, lr:6.4570e-05 dt: 3332.01ms, tok/sec:157349.01
step 14407, loss: 3.188809, norm:0.2661, lr:6.4560e-05 dt: 3331.87ms, tok/sec:157355.52
step 14408, loss: 3.189936, norm:0.2500, lr:6.4549e-05 dt: 3332.03ms, tok/sec:157348.10
step 14409, loss: 3.174049, norm:0.2564, lr:6.4538e-05 dt: 3332.11ms, tok/sec:157344.10
step 14410, loss: 3.162075, norm:0.2594, lr:6.4528e-05 dt: 3332.06ms, tok/sec:157346.61
step 14411, loss: 3.210652, norm:0.2810, lr:6.4517e-05 dt: 3331.94ms, tok/sec:157352.07
step 14412, loss: 3.156304, norm:0.2528, lr:6.4506e-05 dt: 3332.03ms, tok/sec:157347.98
step 14413, loss: 3.134314, norm:0.2615, lr:6.4496e-05 dt: 3332.15ms, tok/sec:157342.09
step 14414, loss: 3.137660, norm:0.2624, lr:6.4485e-05 dt: 3332.00ms, tok/sec:157349.29
step 14415, loss: 3.155472, norm:0.3441, lr:6.4475e-05 dt: 3331.75ms, tok/sec:157361.18
step 14416, loss: 3.121124, norm:0.2669, lr:6.4464e-05 dt: 3332.20ms, tok/sec:157339.67
step 14417, loss: 3.137255, norm:0.2682, lr:6.4453e-05 dt: 3332.05ms, tok/sec:157346.85
step 14418, loss: 3.121209, norm:0.2602, lr:6.4443e-05 dt: 3331.96ms, tok/sec:157351.19
step 14419, loss: 3.084268, norm:0.2625, lr:6.4432e-05 dt: 3332.20ms, tok/sec:157339.87
step 14420, loss: 3.115082, norm:0.2614, lr:6.4422e-05 dt: 3332.04ms, tok/sec:157347.64
step 14421, loss: 3.154880, norm:0.2666, lr:6.4411e-05 dt: 3331.96ms, tok/sec:157351.37
step 14422, loss: 3.142642, norm:0.2597, lr:6.4401e-05 dt: 3331.94ms, tok/sec:157351.96
step 14423, loss: 3.161861, norm:0.2570, lr:6.4390e-05 dt: 3331.90ms, tok/sec:157353.87
step 14424, loss: 3.284651, norm:0.2788, lr:6.4380e-05 dt: 3332.28ms, tok/sec:157335.93
step 14425, loss: 3.176305, norm:0.2864, lr:6.4369e-05 dt: 3332.33ms, tok/sec:157333.66
step 14426, loss: 3.232397, norm:0.2593, lr:6.4359e-05 dt: 3332.10ms, tok/sec:157344.54
step 14427, loss: 3.186360, norm:0.2630, lr:6.4349e-05 dt: 3332.13ms, tok/sec:157343.42
step 14428, loss: 3.326939, norm:0.5143, lr:6.4338e-05 dt: 3332.08ms, tok/sec:157345.60
step 14429, loss: 3.216945, norm:0.2989, lr:6.4328e-05 dt: 3331.92ms, tok/sec:157353.23
step 14430, loss: 3.172569, norm:0.3025, lr:6.4317e-05 dt: 3332.10ms, tok/sec:157344.52
step 14431, loss: 3.230700, norm:0.3171, lr:6.4307e-05 dt: 3331.86ms, tok/sec:157356.06
step 14432, loss: 3.212918, norm:0.2981, lr:6.4297e-05 dt: 3331.84ms, tok/sec:157356.69
step 14433, loss: 3.148878, norm:0.3157, lr:6.4286e-05 dt: 3332.21ms, tok/sec:157339.24
step 14434, loss: 3.167357, norm:0.2875, lr:6.4276e-05 dt: 3332.11ms, tok/sec:157344.00
step 14435, loss: 3.216874, norm:0.2761, lr:6.4266e-05 dt: 3332.12ms, tok/sec:157343.87
step 14436, loss: 3.133461, norm:0.2661, lr:6.4255e-05 dt: 3332.28ms, tok/sec:157336.11
step 14437, loss: 3.116966, norm:0.2817, lr:6.4245e-05 dt: 3332.38ms, tok/sec:157331.44
step 14438, loss: 3.148068, norm:0.2707, lr:6.4235e-05 dt: 3332.40ms, tok/sec:157330.68
step 14439, loss: 3.220862, norm:0.2865, lr:6.4224e-05 dt: 3332.02ms, tok/sec:157348.40
step 14440, loss: 3.260186, norm:0.3138, lr:6.4214e-05 dt: 3331.85ms, tok/sec:157356.51
step 14441, loss: 3.213442, norm:0.2797, lr:6.4204e-05 dt: 3332.10ms, tok/sec:157344.54
step 14442, loss: 3.171956, norm:0.2832, lr:6.4194e-05 dt: 3332.02ms, tok/sec:157348.54
step 14443, loss: 3.164752, norm:0.2791, lr:6.4183e-05 dt: 3331.98ms, tok/sec:157350.34
step 14444, loss: 3.163270, norm:0.2850, lr:6.4173e-05 dt: 3332.17ms, tok/sec:157341.35
step 14445, loss: 3.190703, norm:0.2706, lr:6.4163e-05 dt: 3332.19ms, tok/sec:157340.19
step 14446, loss: 3.200168, norm:0.2634, lr:6.4153e-05 dt: 3332.10ms, tok/sec:157344.71
step 14447, loss: 3.233907, norm:0.2680, lr:6.4143e-05 dt: 3332.33ms, tok/sec:157333.74
step 14448, loss: 3.107839, norm:0.2784, lr:6.4132e-05 dt: 3331.96ms, tok/sec:157351.35
step 14449, loss: 3.131245, norm:0.2945, lr:6.4122e-05 dt: 3331.91ms, tok/sec:157353.74
HellaSwag accuracy:-2286557914165181359/-2=1143278957082590720.0000
rank 1 sample 0: Hello, I'm a language model, and that's what I want to introduce. What do you find interesting about this?<|endoftext|>What if you can't find
rank 1 sample 1: Hello, I'm a language model, you know that what you're looking for is a bunch number of terms that will sort along your field.
Now,
rank 1 sample 2: Hello, I'm a language model, but are they good at it?
I'm a linguistics student and I'm a computer programming teacher, because what
rank 1 sample 3: Hello, I'm a language model, and I'm using the
tamat language to find something new. First, do a few basic math concepts.
rank 0 sample 0: Hello, I'm a language model, and I need to learn this new language!"
"For me, all that I've spent years trying to learn this
rank 0 sample 1: Hello, I'm a language model, so why not learn it? I used to teach Latin I wasn't fluent.
But then, for some reason,
rank 0 sample 2: Hello, I'm a language model, so I would have something to say:
My theory is this because language models are a kind of grammar, a way
rank 0 sample 3: Hello, I'm a language model, and when I get to know my audience and language, I immediately have to learn about language model, grammar, vocabulary-
step 14450, loss: 3.187030, norm:0.3340, lr:6.4112e-05 dt: 48390.94ms, tok/sec:10834.43
step 14451, loss: 3.098020, norm:0.3008, lr:6.4102e-05 dt: 3331.86ms, tok/sec:157356.17
step 14452, loss: 3.143222, norm:0.2752, lr:6.4092e-05 dt: 3332.14ms, tok/sec:157342.70
step 14453, loss: 3.084730, norm:0.2765, lr:6.4082e-05 dt: 3331.77ms, tok/sec:157360.28
step 14454, loss: 3.133884, norm:0.2703, lr:6.4072e-05 dt: 3332.03ms, tok/sec:157348.12
step 14455, loss: 3.121366, norm:0.2597, lr:6.4061e-05 dt: 3332.35ms, tok/sec:157332.96
step 14456, loss: 3.130969, norm:0.2779, lr:6.4051e-05 dt: 3332.14ms, tok/sec:157342.61
step 14457, loss: 3.126081, norm:0.2936, lr:6.4041e-05 dt: 3331.75ms, tok/sec:157361.31
step 14458, loss: 3.149779, norm:0.2720, lr:6.4031e-05 dt: 3331.74ms, tok/sec:157361.68
step 14459, loss: 3.136150, norm:0.2919, lr:6.4021e-05 dt: 3332.14ms, tok/sec:157342.87
step 14460, loss: 3.197415, norm:0.2963, lr:6.4011e-05 dt: 3332.13ms, tok/sec:157343.16
step 14461, loss: 3.168386, norm:0.2857, lr:6.4001e-05 dt: 3332.10ms, tok/sec:157344.45
step 14462, loss: 3.259323, norm:0.2856, lr:6.3991e-05 dt: 3332.12ms, tok/sec:157343.73
step 14463, loss: 3.165981, norm:0.2910, lr:6.3981e-05 dt: 3332.22ms, tok/sec:157339.00
step 14464, loss: 3.166233, norm:0.2968, lr:6.3971e-05 dt: 3332.13ms, tok/sec:157343.12
step 14465, loss: 3.193863, norm:0.2857, lr:6.3961e-05 dt: 3332.12ms, tok/sec:157343.48
step 14466, loss: 3.121137, norm:0.2660, lr:6.3951e-05 dt: 3332.03ms, tok/sec:157347.99
step 14467, loss: 3.257384, norm:0.2804, lr:6.3941e-05 dt: 3332.19ms, tok/sec:157340.43
step 14468, loss: 3.171429, norm:0.2855, lr:6.3932e-05 dt: 3332.23ms, tok/sec:157338.67
step 14469, loss: 3.172100, norm:0.2708, lr:6.3922e-05 dt: 3332.12ms, tok/sec:157343.90
step 14470, loss: 3.176044, norm:0.2783, lr:6.3912e-05 dt: 3332.24ms, tok/sec:157338.04
step 14471, loss: 3.243123, norm:0.2889, lr:6.3902e-05 dt: 3332.05ms, tok/sec:157346.86
step 14472, loss: 3.206941, norm:0.2779, lr:6.3892e-05 dt: 3332.18ms, tok/sec:157341.06
step 14473, loss: 3.218437, norm:0.2724, lr:6.3882e-05 dt: 3331.97ms, tok/sec:157350.90
step 14474, loss: 3.142984, norm:0.2734, lr:6.3872e-05 dt: 3331.91ms, tok/sec:157353.48
step 14475, loss: 3.212578, norm:0.2655, lr:6.3862e-05 dt: 3332.03ms, tok/sec:157348.01
step 14476, loss: 3.174891, norm:0.2690, lr:6.3853e-05 dt: 3332.03ms, tok/sec:157347.91
step 14477, loss: 3.178252, norm:0.2666, lr:6.3843e-05 dt: 3334.21ms, tok/sec:157244.82
step 14478, loss: 3.216095, norm:0.3043, lr:6.3833e-05 dt: 3332.27ms, tok/sec:157336.77
step 14479, loss: 3.174076, norm:0.2597, lr:6.3823e-05 dt: 3332.02ms, tok/sec:157348.29
step 14480, loss: 3.160722, norm:0.2573, lr:6.3813e-05 dt: 3332.24ms, tok/sec:157337.79
step 14481, loss: 3.160302, norm:0.2653, lr:6.3804e-05 dt: 3331.85ms, tok/sec:157356.57
step 14482, loss: 3.195583, norm:0.2587, lr:6.3794e-05 dt: 3331.93ms, tok/sec:157352.51
step 14483, loss: 3.110579, norm:0.2714, lr:6.3784e-05 dt: 3331.90ms, tok/sec:157354.03
step 14484, loss: 3.111179, norm:0.2568, lr:6.3774e-05 dt: 3332.36ms, tok/sec:157332.43
step 14485, loss: 3.117847, norm:0.2873, lr:6.3765e-05 dt: 3331.90ms, tok/sec:157354.19
step 14486, loss: 3.143234, norm:0.2628, lr:6.3755e-05 dt: 3331.90ms, tok/sec:157353.91
step 14487, loss: 3.114169, norm:0.2666, lr:6.3745e-05 dt: 3332.01ms, tok/sec:157348.97
step 14488, loss: 3.079713, norm:0.2861, lr:6.3736e-05 dt: 3332.13ms, tok/sec:157343.06
step 14489, loss: 3.164599, norm:0.2586, lr:6.3726e-05 dt: 3332.18ms, tok/sec:157341.04
step 14490, loss: 3.152302, norm:0.2541, lr:6.3716e-05 dt: 3332.16ms, tok/sec:157341.64
step 14491, loss: 3.098536, norm:0.2803, lr:6.3707e-05 dt: 3332.04ms, tok/sec:157347.62
step 14492, loss: 3.100947, norm:0.2669, lr:6.3697e-05 dt: 3332.00ms, tok/sec:157349.56
step 14493, loss: 3.094377, norm:0.2847, lr:6.3688e-05 dt: 3332.00ms, tok/sec:157349.15
step 14494, loss: 3.207050, norm:0.3036, lr:6.3678e-05 dt: 3332.02ms, tok/sec:157348.30
step 14495, loss: 3.227718, norm:0.2955, lr:6.3668e-05 dt: 3332.09ms, tok/sec:157345.18
step 14496, loss: 3.173533, norm:0.2683, lr:6.3659e-05 dt: 3332.30ms, tok/sec:157335.09
step 14497, loss: 3.178513, norm:0.2712, lr:6.3649e-05 dt: 3332.20ms, tok/sec:157339.75
step 14498, loss: 3.169085, norm:0.3132, lr:6.3640e-05 dt: 3332.01ms, tok/sec:157348.69
step 14499, loss: 3.151581, norm:0.2742, lr:6.3630e-05 dt: 3332.38ms, tok/sec:157331.60
validation loss: 3.1930
Model and optimizer state saved.
HellaSwag accuracy:-2286557914165181359/-2=1143278957082590720.0000
rank 1 sample 0: Hello, I'm a language model, and my new language is Python. "Hello, "a " is a noun, which is what you would call a
rank 1 sample 1: Hello, I'm a language model, a teacher. You've got to get through the basics...
Please join StudyMode to read the full document<|endoftext|>The
rank 1 sample 2: Hello, I'm a language model, but even in English, I'm not a language professor anymore—so why isn't my English?’ So,
rank 1 sample 3: Hello, I'm a language model, and I'm writing code because I love having to talk 'it' as sometimes I go at it. I like to
rank 0 sample 0: Hello, I'm a language model, and I love it! The idea is fun so I won't write your essay until it's perfect, but I hope
rank 0 sample 1: Hello, I'm a language model, so maybe I'm not an English scientist, so they want me to believe they can understand how the different languages work.
rank 0 sample 2: Hello, I'm a language model, so I used this script because I've been looking at the "solution" of a language.
I've gotten
rank 0 sample 3: Hello, I'm a language model, and what I am doing is trying to get these kinds of things down on paper--in my case, the English lear
step 14500, loss: 3.168145, norm:0.3016, lr:6.3621e-05 dt: 56085.77ms, tok/sec:9347.97
step 14501, loss: 3.150014, norm:0.3499, lr:6.3611e-05 dt: 3332.09ms, tok/sec:157344.94
step 14502, loss: 3.143637, norm:0.2992, lr:6.3602e-05 dt: 3332.14ms, tok/sec:157342.57
step 14503, loss: 3.142719, norm:0.2824, lr:6.3592e-05 dt: 3332.21ms, tok/sec:157339.24
step 14504, loss: 3.216820, norm:0.2723, lr:6.3583e-05 dt: 3332.10ms, tok/sec:157344.58
step 14505, loss: 3.129588, norm:0.2809, lr:6.3573e-05 dt: 3332.12ms, tok/sec:157343.88
step 14506, loss: 3.216016, norm:0.2901, lr:6.3564e-05 dt: 3331.94ms, tok/sec:157352.12
step 14507, loss: 3.219842, norm:0.2891, lr:6.3554e-05 dt: 3332.12ms, tok/sec:157343.61
step 14508, loss: 3.154791, norm:0.2928, lr:6.3545e-05 dt: 3332.00ms, tok/sec:157349.20
step 14509, loss: 3.217710, norm:0.2762, lr:6.3535e-05 dt: 3332.00ms, tok/sec:157349.17
step 14510, loss: 3.210060, norm:0.2683, lr:6.3526e-05 dt: 3332.04ms, tok/sec:157347.61
step 14511, loss: 3.142222, norm:0.2692, lr:6.3517e-05 dt: 3332.26ms, tok/sec:157336.98
step 14512, loss: 3.226955, norm:0.2682, lr:6.3507e-05 dt: 3332.02ms, tok/sec:157348.44
step 14513, loss: 3.224100, norm:0.2649, lr:6.3498e-05 dt: 3331.95ms, tok/sec:157351.52
step 14514, loss: 3.162890, norm:0.2554, lr:6.3489e-05 dt: 3332.36ms, tok/sec:157332.33
step 14515, loss: 3.234235, norm:0.2711, lr:6.3479e-05 dt: 3331.88ms, tok/sec:157355.08
step 14516, loss: 3.177512, norm:0.2818, lr:6.3470e-05 dt: 3332.03ms, tok/sec:157348.07
step 14517, loss: 3.177926, norm:0.2625, lr:6.3461e-05 dt: 3332.08ms, tok/sec:157345.41
step 14518, loss: 3.089406, norm:0.2638, lr:6.3451e-05 dt: 3332.12ms, tok/sec:157343.63
step 14519, loss: 3.143211, norm:0.2615, lr:6.3442e-05 dt: 3332.00ms, tok/sec:157349.33
step 14520, loss: 3.103975, norm:0.2633, lr:6.3433e-05 dt: 3332.07ms, tok/sec:157345.81
step 14521, loss: 3.135998, norm:0.2615, lr:6.3423e-05 dt: 3332.10ms, tok/sec:157344.58
step 14522, loss: 3.167656, norm:0.2688, lr:6.3414e-05 dt: 3331.98ms, tok/sec:157350.24
step 14523, loss: 3.140335, norm:0.2838, lr:6.3405e-05 dt: 3332.04ms, tok/sec:157347.63
step 14524, loss: 3.147492, norm:0.2730, lr:6.3396e-05 dt: 3331.95ms, tok/sec:157351.87
step 14525, loss: 3.126412, norm:0.2595, lr:6.3386e-05 dt: 3332.30ms, tok/sec:157335.05
step 14526, loss: 3.119630, norm:0.2594, lr:6.3377e-05 dt: 3332.18ms, tok/sec:157341.04
step 14527, loss: 3.127094, norm:0.2645, lr:6.3368e-05 dt: 3332.07ms, tok/sec:157346.20
step 14528, loss: 3.135772, norm:0.2899, lr:6.3359e-05 dt: 3331.88ms, tok/sec:157354.92
step 14529, loss: 3.127075, norm:0.2722, lr:6.3350e-05 dt: 3332.12ms, tok/sec:157343.68
step 14530, loss: 3.130738, norm:0.2770, lr:6.3341e-05 dt: 3332.18ms, tok/sec:157340.62
step 14531, loss: 3.220736, norm:0.2809, lr:6.3331e-05 dt: 3332.18ms, tok/sec:157340.78
step 14532, loss: 3.207863, norm:0.2757, lr:6.3322e-05 dt: 3331.98ms, tok/sec:157350.39
step 14533, loss: 3.196509, norm:0.2740, lr:6.3313e-05 dt: 3332.12ms, tok/sec:157343.77
step 14534, loss: 3.180465, norm:0.2841, lr:6.3304e-05 dt: 3332.47ms, tok/sec:157327.24
step 14535, loss: 3.239372, norm:0.2704, lr:6.3295e-05 dt: 3332.24ms, tok/sec:157337.89
step 14536, loss: 3.111835, norm:0.2741, lr:6.3286e-05 dt: 3332.09ms, tok/sec:157345.03
step 14537, loss: 3.178515, norm:0.2677, lr:6.3277e-05 dt: 3331.98ms, tok/sec:157350.10
step 14538, loss: 3.175739, norm:0.2689, lr:6.3268e-05 dt: 3332.06ms, tok/sec:157346.29
step 14539, loss: 3.294574, norm:0.2798, lr:6.3259e-05 dt: 3332.06ms, tok/sec:157346.39
step 14540, loss: 3.168847, norm:0.2884, lr:6.3250e-05 dt: 3332.09ms, tok/sec:157345.14
step 14541, loss: 3.148002, norm:0.3094, lr:6.3241e-05 dt: 3332.05ms, tok/sec:157347.17
step 14542, loss: 3.245335, norm:0.2737, lr:6.3232e-05 dt: 3331.94ms, tok/sec:157352.05
step 14543, loss: 3.124567, norm:0.2645, lr:6.3223e-05 dt: 3332.26ms, tok/sec:157336.99
step 14544, loss: 3.212981, norm:0.2809, lr:6.3214e-05 dt: 3331.95ms, tok/sec:157351.79
step 14545, loss: 3.237355, norm:0.2996, lr:6.3205e-05 dt: 3332.00ms, tok/sec:157349.41
step 14546, loss: 3.162728, norm:0.2779, lr:6.3196e-05 dt: 3332.01ms, tok/sec:157348.83
step 14547, loss: 3.212713, norm:0.2766, lr:6.3187e-05 dt: 3332.01ms, tok/sec:157348.73
step 14548, loss: 3.166122, norm:0.2737, lr:6.3178e-05 dt: 3332.01ms, tok/sec:157348.99
step 14549, loss: 3.189084, norm:0.2785, lr:6.3169e-05 dt: 3332.18ms, tok/sec:157340.88
HellaSwag accuracy:-2295565113419914159/-2=1147782556709957120.0000
rank 1 sample 0: Hello, I'm a language model, and you want to help me improve :) I was wondering if some of the lessons I have is wrong. I'm not
rank 1 sample 1: Hello, I'm a language model, a teacher. There are so many more books out there just waiting to be added. Now I have to go back and
rank 1 sample 2: Hello, I'm a language model, but because you know, I'm not a language model... (And, I'm not a language model, it is
rank 1 sample 3: Hello, I'm a language model, and I'm really excited to be using her as a person with something to refer to while not in her native language.
rank 0 sample 0: Hello, I'm a language model, and I'll be talking about this with them again.
So my initial point is: is the program run in my
rank 0 sample 1: Hello, I'm a language model, so i'm not going to go after the model; they just do a simple, but really complex...
How do
rank 0 sample 2: Hello, I'm a language model, so I wanted to think about how it would make a difference when a student is working in a foreign language. I asked
rank 0 sample 3: Hello, I'm a language model, and now I'll be talking to you about . . . .
And you guys should come to me, because at
step 14550, loss: 3.210104, norm:0.2653, lr:6.3160e-05 dt: 48395.77ms, tok/sec:10833.34
step 14551, loss: 3.108223, norm:0.2615, lr:6.3151e-05 dt: 3332.10ms, tok/sec:157344.53
step 14552, loss: 3.181584, norm:0.2803, lr:6.3142e-05 dt: 3331.87ms, tok/sec:157355.34
step 14553, loss: 3.166088, norm:0.2763, lr:6.3134e-05 dt: 3332.12ms, tok/sec:157343.63
step 14554, loss: 3.103408, norm:0.2785, lr:6.3125e-05 dt: 3332.10ms, tok/sec:157344.77
step 14555, loss: 3.115869, norm:0.3127, lr:6.3116e-05 dt: 3331.82ms, tok/sec:157358.02
step 14556, loss: 3.132133, norm:0.2521, lr:6.3107e-05 dt: 3331.97ms, tok/sec:157350.74
step 14557, loss: 3.101496, norm:0.2771, lr:6.3098e-05 dt: 3332.14ms, tok/sec:157342.58
step 14558, loss: 3.140489, norm:0.2739, lr:6.3089e-05 dt: 3332.07ms, tok/sec:157345.83
step 14559, loss: 3.104735, norm:0.2686, lr:6.3081e-05 dt: 3332.12ms, tok/sec:157343.54
step 14560, loss: 3.175591, norm:0.2603, lr:6.3072e-05 dt: 3331.93ms, tok/sec:157352.66
step 14561, loss: 3.104292, norm:0.2557, lr:6.3063e-05 dt: 3332.01ms, tok/sec:157348.97
step 14562, loss: 3.168999, norm:0.2780, lr:6.3054e-05 dt: 3332.26ms, tok/sec:157336.89
step 14563, loss: 3.152389, norm:0.2641, lr:6.3046e-05 dt: 3332.13ms, tok/sec:157343.17
step 14564, loss: 3.133059, norm:0.2704, lr:6.3037e-05 dt: 3331.96ms, tok/sec:157351.44
step 14565, loss: 3.240967, norm:0.3955, lr:6.3028e-05 dt: 3332.13ms, tok/sec:157343.05
step 14566, loss: 3.146430, norm:0.2846, lr:6.3019e-05 dt: 3332.11ms, tok/sec:157344.05
step 14567, loss: 3.178757, norm:0.3027, lr:6.3011e-05 dt: 3332.08ms, tok/sec:157345.53
step 14568, loss: 3.135038, norm:0.3102, lr:6.3002e-05 dt: 3332.19ms, tok/sec:157340.48
step 14569, loss: 3.155939, norm:0.2873, lr:6.2993e-05 dt: 3331.87ms, tok/sec:157355.48
step 14570, loss: 3.167791, norm:0.2742, lr:6.2985e-05 dt: 3332.01ms, tok/sec:157348.82
step 14571, loss: 3.136509, norm:0.2586, lr:6.2976e-05 dt: 3331.95ms, tok/sec:157351.48
step 14572, loss: 3.182421, norm:0.2726, lr:6.2967e-05 dt: 3331.94ms, tok/sec:157352.09
step 14573, loss: 3.216708, norm:0.2853, lr:6.2959e-05 dt: 3332.44ms, tok/sec:157328.70
step 14574, loss: 3.202074, norm:0.2647, lr:6.2950e-05 dt: 3332.44ms, tok/sec:157328.37
step 14575, loss: 3.182661, norm:0.2716, lr:6.2942e-05 dt: 3332.12ms, tok/sec:157343.82
step 14576, loss: 3.176997, norm:0.2802, lr:6.2933e-05 dt: 3332.11ms, tok/sec:157344.37
step 14577, loss: 3.206527, norm:0.2646, lr:6.2924e-05 dt: 3332.16ms, tok/sec:157342.01
step 14578, loss: 3.174250, norm:0.2638, lr:6.2916e-05 dt: 3332.16ms, tok/sec:157341.66
step 14579, loss: 3.179598, norm:0.2619, lr:6.2907e-05 dt: 3332.63ms, tok/sec:157319.72
step 14580, loss: 3.199090, norm:0.2829, lr:6.2899e-05 dt: 3331.91ms, tok/sec:157353.59
step 14581, loss: 3.133688, norm:0.2721, lr:6.2890e-05 dt: 3332.06ms, tok/sec:157346.66
step 14582, loss: 3.126017, norm:0.2507, lr:6.2882e-05 dt: 3332.12ms, tok/sec:157343.73
step 14583, loss: 3.188370, norm:0.2563, lr:6.2873e-05 dt: 3332.07ms, tok/sec:157346.13
step 14584, loss: 3.198667, norm:0.2695, lr:6.2865e-05 dt: 3332.23ms, tok/sec:157338.60
step 14585, loss: 3.125204, norm:0.2539, lr:6.2856e-05 dt: 3332.07ms, tok/sec:157346.00
step 14586, loss: 3.166842, norm:0.2579, lr:6.2848e-05 dt: 3332.24ms, tok/sec:157338.18
step 14587, loss: 3.168763, norm:0.2651, lr:6.2839e-05 dt: 3331.99ms, tok/sec:157349.92
step 14588, loss: 3.161684, norm:0.2490, lr:6.2831e-05 dt: 3331.90ms, tok/sec:157354.27
step 14589, loss: 3.138501, norm:0.2618, lr:6.2823e-05 dt: 3332.12ms, tok/sec:157343.64
step 14590, loss: 3.147655, norm:0.2619, lr:6.2814e-05 dt: 3332.23ms, tok/sec:157338.37
step 14591, loss: 3.070123, norm:0.2660, lr:6.2806e-05 dt: 3332.04ms, tok/sec:157347.40
step 14592, loss: 3.115400, norm:0.2711, lr:6.2797e-05 dt: 3331.95ms, tok/sec:157351.47
step 14593, loss: 3.081993, norm:0.2710, lr:6.2789e-05 dt: 3332.14ms, tok/sec:157342.89
step 14594, loss: 3.083970, norm:0.2631, lr:6.2781e-05 dt: 3332.16ms, tok/sec:157341.58
step 14595, loss: 3.144376, norm:0.2750, lr:6.2772e-05 dt: 3332.10ms, tok/sec:157344.45
step 14596, loss: 3.142526, norm:0.2784, lr:6.2764e-05 dt: 3331.98ms, tok/sec:157350.14
step 14597, loss: 3.118395, norm:0.2795, lr:6.2756e-05 dt: 3332.00ms, tok/sec:157349.38
step 14598, loss: 3.157013, norm:0.2839, lr:6.2747e-05 dt: 3332.04ms, tok/sec:157347.27
step 14599, loss: 3.156699, norm:0.3037, lr:6.2739e-05 dt: 3332.15ms, tok/sec:157342.29
validation loss: 3.1929
Model and optimizer state saved.
HellaSwag accuracy:-2295565113419922351/-2=1147782556709961216.0000
rank 1 sample 0: Hello, I'm a language model, so there are lots of ways to handle the various aspects of creating a language model. But as an example, I'm
rank 1 sample 1: Hello, I'm a language model, which means I've been learning the language so far. How do you build my own Python source code for this?

rank 1 sample 2: Hello, I'm a language model, but at first I'm not sure how to do it. Now that you know how to write and how to program it
rank 1 sample 3: Hello, I'm a language model, so I'm working with one language over the other. Right now, that goes for other languages and I'm thinking about
rank 0 sample 0: Hello, I'm a language model, and I don't have that in my other two languages.
A question: I know? Well, let's look
rank 0 sample 1: Hello, I'm a language model, so maybe I'm not doing this (I think language was the way it was used), but it really does put some
rank 0 sample 2: Hello, I'm a language model, so I thought we could build a compiler for Linux, but what's it?
As I said, I think we
rank 0 sample 3: Hello, I'm a language model, and when I do, I get the chance to talk to the computer about what your child sees and when you're learning
step 14600, loss: 3.122401, norm:0.2724, lr:6.2731e-05 dt: 56061.17ms, tok/sec:9352.07
step 14601, loss: 3.190259, norm:0.2906, lr:6.2723e-05 dt: 3332.09ms, tok/sec:157345.17
step 14602, loss: 3.161606, norm:0.2872, lr:6.2714e-05 dt: 3332.14ms, tok/sec:157342.58
step 14603, loss: 3.180310, norm:0.2983, lr:6.2706e-05 dt: 3332.20ms, tok/sec:157339.92
step 14604, loss: 3.147251, norm:0.2933, lr:6.2698e-05 dt: 3332.32ms, tok/sec:157334.29
step 14605, loss: 3.220649, norm:0.2768, lr:6.2690e-05 dt: 3332.25ms, tok/sec:157337.59
step 14606, loss: 3.135768, norm:0.2762, lr:6.2681e-05 dt: 3332.03ms, tok/sec:157347.80
step 14607, loss: 3.161091, norm:0.2707, lr:6.2673e-05 dt: 3332.01ms, tok/sec:157348.84
step 14608, loss: 3.142320, norm:0.2941, lr:6.2665e-05 dt: 3332.17ms, tok/sec:157341.22
step 14609, loss: 3.216127, norm:0.2731, lr:6.2657e-05 dt: 3332.05ms, tok/sec:157346.87
step 14610, loss: 3.176246, norm:0.2599, lr:6.2649e-05 dt: 3331.98ms, tok/sec:157350.37
step 14611, loss: 3.162036, norm:0.2699, lr:6.2641e-05 dt: 3331.95ms, tok/sec:157351.68
step 14612, loss: 3.174911, norm:0.2711, lr:6.2632e-05 dt: 3332.30ms, tok/sec:157334.95
step 14613, loss: 3.164200, norm:0.2560, lr:6.2624e-05 dt: 3332.13ms, tok/sec:157343.00
step 14614, loss: 3.154104, norm:0.2635, lr:6.2616e-05 dt: 3332.15ms, tok/sec:157342.02
step 14615, loss: 3.224789, norm:0.2734, lr:6.2608e-05 dt: 3332.05ms, tok/sec:157347.02
step 14616, loss: 3.248252, norm:0.2713, lr:6.2600e-05 dt: 3332.03ms, tok/sec:157348.01
step 14617, loss: 3.151813, norm:0.2511, lr:6.2592e-05 dt: 3332.11ms, tok/sec:157344.19
step 14618, loss: 3.149067, norm:0.2636, lr:6.2584e-05 dt: 3332.01ms, tok/sec:157348.89
step 14619, loss: 3.189756, norm:0.2681, lr:6.2576e-05 dt: 3331.94ms, tok/sec:157351.98
step 14620, loss: 3.168089, norm:0.2728, lr:6.2568e-05 dt: 3332.00ms, tok/sec:157349.49
step 14621, loss: 3.084830, norm:0.2577, lr:6.2560e-05 dt: 3332.33ms, tok/sec:157333.97
step 14622, loss: 3.135315, norm:0.2635, lr:6.2552e-05 dt: 3332.06ms, tok/sec:157346.41
step 14623, loss: 3.107206, norm:0.2772, lr:6.2544e-05 dt: 3331.78ms, tok/sec:157359.56
step 14624, loss: 3.144945, norm:0.2556, lr:6.2536e-05 dt: 3332.15ms, tok/sec:157342.04
step 14625, loss: 3.104736, norm:0.2485, lr:6.2528e-05 dt: 3332.08ms, tok/sec:157345.70
step 14626, loss: 3.102147, norm:0.2583, lr:6.2520e-05 dt: 3331.99ms, tok/sec:157349.73
step 14627, loss: 3.242701, norm:0.2780, lr:6.2512e-05 dt: 3331.94ms, tok/sec:157352.35
step 14628, loss: 3.212267, norm:0.3107, lr:6.2504e-05 dt: 3331.92ms, tok/sec:157353.31
step 14629, loss: 3.121320, norm:0.2592, lr:6.2496e-05 dt: 3332.43ms, tok/sec:157329.23
step 14630, loss: 3.142504, norm:0.2685, lr:6.2488e-05 dt: 3331.90ms, tok/sec:157354.19
step 14631, loss: 3.139237, norm:0.2630, lr:6.2480e-05 dt: 3332.11ms, tok/sec:157343.98
step 14632, loss: 3.143839, norm:0.2795, lr:6.2473e-05 dt: 3332.08ms, tok/sec:157345.47
step 14633, loss: 3.126122, norm:0.2811, lr:6.2465e-05 dt: 3331.97ms, tok/sec:157350.53
step 14634, loss: 3.164552, norm:0.2696, lr:6.2457e-05 dt: 3332.00ms, tok/sec:157349.52
step 14635, loss: 3.204534, norm:0.2930, lr:6.2449e-05 dt: 3332.08ms, tok/sec:157345.67
step 14636, loss: 3.175490, norm:0.2899, lr:6.2441e-05 dt: 3332.19ms, tok/sec:157340.56
step 14637, loss: 3.240619, norm:0.3061, lr:6.2433e-05 dt: 3332.67ms, tok/sec:157317.61
step 14638, loss: 3.209370, norm:0.2935, lr:6.2425e-05 dt: 3332.27ms, tok/sec:157336.36
step 14639, loss: 3.191421, norm:0.3037, lr:6.2418e-05 dt: 3332.18ms, tok/sec:157341.03
step 14640, loss: 3.194419, norm:0.2885, lr:6.2410e-05 dt: 3332.16ms, tok/sec:157341.59
step 14641, loss: 3.140535, norm:0.3050, lr:6.2402e-05 dt: 3332.20ms, tok/sec:157339.92
step 14642, loss: 3.152611, norm:0.2523, lr:6.2394e-05 dt: 3332.10ms, tok/sec:157344.66
step 14643, loss: 3.208305, norm:0.3428, lr:6.2387e-05 dt: 3331.94ms, tok/sec:157352.33
step 14644, loss: 3.134041, norm:0.2765, lr:6.2379e-05 dt: 3332.00ms, tok/sec:157349.20
step 14645, loss: 3.228281, norm:0.2822, lr:6.2371e-05 dt: 3332.21ms, tok/sec:157339.62
step 14646, loss: 3.153240, norm:0.2682, lr:6.2363e-05 dt: 3332.20ms, tok/sec:157339.92
step 14647, loss: 3.165405, norm:0.3261, lr:6.2356e-05 dt: 3332.03ms, tok/sec:157347.84
step 14648, loss: 3.177488, norm:0.2935, lr:6.2348e-05 dt: 3332.15ms, tok/sec:157342.07
step 14649, loss: 3.155769, norm:0.2658, lr:6.2340e-05 dt: 3332.08ms, tok/sec:157345.66
HellaSwag accuracy:-2286557914165181359/-2=1143278957082590720.0000
rank 1 sample 0: Hello, I'm a language model, and my son is reading. I get the hang of saying, "I'm a child. That's right, I
rank 1 sample 1: Hello, I'm a language model, not an object modeling system. I think this is a better way to work with class data, rather than a class model
rank 1 sample 2: Hello, I'm a language model, but some languages I'm not familiar with are not well known; for example, the English language does not make an equal
rank 1 sample 3: Hello, I'm a language model, and I'm using the data set and algorithm to help keep the current system flowing. "One problem is that many of
rank 0 sample 0: Hello, I'm a language model, and I'll be talking about this with little or no information about languages - the language of the day - that I have
rank 0 sample 1: Hello, I'm a language model, so how do I do that? As a teacher my opinion is a small, but it's not all that different.
rank 0 sample 2: Hello, I'm a language model, so I hope I made up my mind. This is a nice thing to do, and it's a great thing.
rank 0 sample 3: Hello, I'm a language model, and for that I'm going to put that stuff in a web page. So if you put a
index, say
step 14650, loss: 3.184171, norm:0.2747, lr:6.2333e-05 dt: 48474.55ms, tok/sec:10815.74
step 14651, loss: 3.187312, norm:0.2826, lr:6.2325e-05 dt: 3332.04ms, tok/sec:157347.57
step 14652, loss: 3.141822, norm:0.2650, lr:6.2318e-05 dt: 3332.05ms, tok/sec:157347.03
step 14653, loss: 3.170340, norm:0.2637, lr:6.2310e-05 dt: 3332.08ms, tok/sec:157345.38
step 14654, loss: 3.210506, norm:0.2649, lr:6.2302e-05 dt: 3332.07ms, tok/sec:157345.87
step 14655, loss: 3.239778, norm:0.2628, lr:6.2295e-05 dt: 3332.23ms, tok/sec:157338.54
step 14656, loss: 3.141338, norm:0.2668, lr:6.2287e-05 dt: 3332.24ms, tok/sec:157338.11
step 14657, loss: 3.096681, norm:0.2621, lr:6.2280e-05 dt: 3331.92ms, tok/sec:157353.27
step 14658, loss: 3.147087, norm:0.2912, lr:6.2272e-05 dt: 3331.99ms, tok/sec:157349.65
step 14659, loss: 3.103585, norm:0.2716, lr:6.2264e-05 dt: 3332.12ms, tok/sec:157343.55
step 14660, loss: 3.157255, norm:0.2675, lr:6.2257e-05 dt: 3331.92ms, tok/sec:157353.20
step 14661, loss: 3.123553, norm:0.2661, lr:6.2249e-05 dt: 3332.32ms, tok/sec:157334.41
step 14662, loss: 3.071676, norm:0.2687, lr:6.2242e-05 dt: 3331.86ms, tok/sec:157355.76
step 14663, loss: 3.087920, norm:0.2988, lr:6.2234e-05 dt: 3331.88ms, tok/sec:157354.91
step 14664, loss: 3.153077, norm:0.2530, lr:6.2227e-05 dt: 3332.19ms, tok/sec:157340.57
step 14665, loss: 3.106205, norm:0.2591, lr:6.2219e-05 dt: 3331.90ms, tok/sec:157353.85
step 14666, loss: 3.186167, norm:0.2786, lr:6.2212e-05 dt: 3332.06ms, tok/sec:157346.59
step 14667, loss: 3.074375, norm:0.2853, lr:6.2205e-05 dt: 3331.94ms, tok/sec:157352.13
step 14668, loss: 3.151670, norm:0.2623, lr:6.2197e-05 dt: 3334.56ms, tok/sec:157228.49
step 14669, loss: 3.130678, norm:0.2828, lr:6.2190e-05 dt: 3331.99ms, tok/sec:157349.91
step 14670, loss: 3.244714, norm:0.2991, lr:6.2182e-05 dt: 3332.15ms, tok/sec:157342.38
step 14671, loss: 3.138083, norm:0.2792, lr:6.2175e-05 dt: 3332.10ms, tok/sec:157344.42
step 14672, loss: 3.160719, norm:0.2771, lr:6.2168e-05 dt: 3332.11ms, tok/sec:157344.14
step 14673, loss: 3.164198, norm:0.2783, lr:6.2160e-05 dt: 3331.93ms, tok/sec:157352.68
step 14674, loss: 3.138447, norm:0.2884, lr:6.2153e-05 dt: 3332.08ms, tok/sec:157345.49
step 14675, loss: 3.181079, norm:0.3267, lr:6.2145e-05 dt: 3331.97ms, tok/sec:157350.90
step 14676, loss: 3.154430, norm:0.2792, lr:6.2138e-05 dt: 3332.20ms, tok/sec:157340.06
step 14677, loss: 3.153414, norm:0.2938, lr:6.2131e-05 dt: 3332.13ms, tok/sec:157343.27
step 14678, loss: 3.165296, norm:0.2697, lr:6.2123e-05 dt: 3332.05ms, tok/sec:157346.85
step 14679, loss: 3.150505, norm:0.2741, lr:6.2116e-05 dt: 3331.90ms, tok/sec:157354.28
step 14680, loss: 3.193399, norm:0.3137, lr:6.2109e-05 dt: 3332.15ms, tok/sec:157342.27
step 14681, loss: 3.198040, norm:0.2744, lr:6.2102e-05 dt: 3332.28ms, tok/sec:157336.12
step 14682, loss: 3.192580, norm:0.2614, lr:6.2094e-05 dt: 3332.02ms, tok/sec:157348.35
step 14683, loss: 3.234331, norm:0.2681, lr:6.2087e-05 dt: 3331.99ms, tok/sec:157349.71
step 14684, loss: 3.174541, norm:0.2694, lr:6.2080e-05 dt: 3332.03ms, tok/sec:157347.99
step 14685, loss: 3.189462, norm:0.2580, lr:6.2073e-05 dt: 3332.05ms, tok/sec:157347.15
step 14686, loss: 3.160741, norm:0.3413, lr:6.2065e-05 dt: 3332.00ms, tok/sec:157349.15
step 14687, loss: 3.143118, norm:0.2700, lr:6.2058e-05 dt: 3332.08ms, tok/sec:157345.38
step 14688, loss: 3.203928, norm:0.2734, lr:6.2051e-05 dt: 3332.09ms, tok/sec:157345.02
step 14689, loss: 3.185828, norm:0.2794, lr:6.2044e-05 dt: 3331.69ms, tok/sec:157363.81
step 14690, loss: 3.166080, norm:0.2600, lr:6.2037e-05 dt: 3332.46ms, tok/sec:157327.62
step 14691, loss: 3.143590, norm:0.2756, lr:6.2030e-05 dt: 3332.12ms, tok/sec:157343.52
step 14692, loss: 3.039527, norm:0.3235, lr:6.2023e-05 dt: 3331.96ms, tok/sec:157351.19
step 14693, loss: 3.119061, norm:0.2755, lr:6.2015e-05 dt: 3331.97ms, tok/sec:157350.87
step 14694, loss: 3.092894, norm:0.2902, lr:6.2008e-05 dt: 3331.88ms, tok/sec:157355.21
step 14695, loss: 3.111095, norm:0.2746, lr:6.2001e-05 dt: 3331.94ms, tok/sec:157352.13
step 14696, loss: 3.129840, norm:0.2699, lr:6.1994e-05 dt: 3332.21ms, tok/sec:157339.45
step 14697, loss: 3.130884, norm:0.2654, lr:6.1987e-05 dt: 3332.04ms, tok/sec:157347.24
step 14698, loss: 3.119920, norm:0.2708, lr:6.1980e-05 dt: 3331.85ms, tok/sec:157356.25
step 14699, loss: 3.130912, norm:0.2923, lr:6.1973e-05 dt: 3332.18ms, tok/sec:157340.66
validation loss: 3.1929
Model and optimizer state saved.
HellaSwag accuracy:-2295565113419922351/-2=1147782556709961216.0000
rank 1 sample 0: Hello, I'm a language model, so there are lots of ways to embed the text and create visualizations. Now, when I open a file, I
rank 1 sample 1: Hello, I'm a language model, not just an abstract language. I can get a lot smarter. I can also try everything out and get a lot more
rank 1 sample 2: Hello, I'm a language model, but learning the languages is not easy. I'm having difficulty remembering how many resources I have in my system, as I
rank 1 sample 3: Hello, I'm a language model, so I'm using the right and my current language model won't run until 30 minutes earlier. If I'm writing a
rank 0 sample 0: Hello, I'm a language model, and I need to learn more, not for example, learn how to play an instrument and when and where.
This
rank 0 sample 1: Hello, I'm a language model, so why not make the world as accessible as I might imagine. The World Wide Web, for instance, is a huge
rank 0 sample 2: Hello, I'm a language model, so I learned it better a few years ago!
I learned to speak English.
I learned to speak Spanish

rank 0 sample 3: Hello, I'm a language model, and what I've done is I have been wondering what you're interested in and, for me, "what do computers
step 14700, loss: 3.206965, norm:0.2840, lr:6.1966e-05 dt: 56327.92ms, tok/sec:9307.78
step 14701, loss: 3.151542, norm:0.2730, lr:6.1959e-05 dt: 3332.09ms, tok/sec:157344.91
step 14702, loss: 3.208765, norm:0.3150, lr:6.1952e-05 dt: 3331.96ms, tok/sec:157351.06
step 14703, loss: 3.113481, norm:0.2694, lr:6.1945e-05 dt: 3332.22ms, tok/sec:157338.85
step 14704, loss: 3.231031, norm:0.2965, lr:6.1938e-05 dt: 3332.38ms, tok/sec:157331.29
step 14705, loss: 3.088191, norm:0.2776, lr:6.1931e-05 dt: 3331.87ms, tok/sec:157355.53
step 14706, loss: 3.212457, norm:0.2811, lr:6.1924e-05 dt: 3332.07ms, tok/sec:157346.02
step 14707, loss: 3.141208, norm:0.2749, lr:6.1917e-05 dt: 3332.24ms, tok/sec:157337.97
step 14708, loss: 3.141378, norm:0.2949, lr:6.1910e-05 dt: 3332.07ms, tok/sec:157345.89
step 14709, loss: 3.221729, norm:0.2814, lr:6.1903e-05 dt: 3332.10ms, tok/sec:157344.63
step 14710, loss: 3.129059, norm:0.2716, lr:6.1896e-05 dt: 3332.03ms, tok/sec:157348.12
step 14711, loss: 3.223602, norm:0.2742, lr:6.1889e-05 dt: 3331.95ms, tok/sec:157351.82
step 14712, loss: 3.149672, norm:0.2721, lr:6.1883e-05 dt: 3332.13ms, tok/sec:157343.30
step 14713, loss: 3.242988, norm:0.3127, lr:6.1876e-05 dt: 3332.04ms, tok/sec:157347.49
step 14714, loss: 3.147308, norm:0.2716, lr:6.1869e-05 dt: 3332.14ms, tok/sec:157342.61
step 14715, loss: 3.115921, norm:0.3289, lr:6.1862e-05 dt: 3332.11ms, tok/sec:157344.18
step 14716, loss: 3.182223, norm:0.2785, lr:6.1855e-05 dt: 3331.89ms, tok/sec:157354.72
step 14717, loss: 3.134634, norm:0.2695, lr:6.1848e-05 dt: 3332.48ms, tok/sec:157326.69
step 14718, loss: 3.208591, norm:0.2688, lr:6.1841e-05 dt: 3332.37ms, tok/sec:157331.97
step 14719, loss: 3.158971, norm:0.2639, lr:6.1835e-05 dt: 3332.16ms, tok/sec:157341.90
step 14720, loss: 3.124932, norm:0.2597, lr:6.1828e-05 dt: 3332.01ms, tok/sec:157348.90
step 14721, loss: 3.200594, norm:0.2542, lr:6.1821e-05 dt: 3332.30ms, tok/sec:157335.18
step 14722, loss: 3.156754, norm:0.2595, lr:6.1814e-05 dt: 3332.07ms, tok/sec:157346.22
step 14723, loss: 3.167318, norm:0.2749, lr:6.1808e-05 dt: 3332.06ms, tok/sec:157346.30
step 14724, loss: 3.204995, norm:0.2708, lr:6.1801e-05 dt: 3332.04ms, tok/sec:157347.65
step 14725, loss: 3.195848, norm:0.2677, lr:6.1794e-05 dt: 3331.97ms, tok/sec:157350.68
step 14726, loss: 3.182803, norm:0.2621, lr:6.1787e-05 dt: 3332.14ms, tok/sec:157342.88
step 14727, loss: 3.154135, norm:0.2661, lr:6.1781e-05 dt: 3331.91ms, tok/sec:157353.49
step 14728, loss: 3.129536, norm:0.2732, lr:6.1774e-05 dt: 3331.73ms, tok/sec:157362.06
step 14729, loss: 3.140567, norm:0.2662, lr:6.1767e-05 dt: 3331.93ms, tok/sec:157352.51
step 14730, loss: 3.111653, norm:0.2590, lr:6.1761e-05 dt: 3332.11ms, tok/sec:157344.10
step 14731, loss: 3.107192, norm:0.2657, lr:6.1754e-05 dt: 3332.10ms, tok/sec:157344.71
step 14732, loss: 3.183237, norm:0.2750, lr:6.1748e-05 dt: 3332.24ms, tok/sec:157337.80
step 14733, loss: 3.148135, norm:0.2678, lr:6.1741e-05 dt: 3332.20ms, tok/sec:157340.10
step 14734, loss: 3.174320, norm:0.2517, lr:6.1734e-05 dt: 3332.12ms, tok/sec:157343.68
step 14735, loss: 3.092636, norm:0.2612, lr:6.1728e-05 dt: 3331.94ms, tok/sec:157351.97
step 14736, loss: 3.126968, norm:0.2641, lr:6.1721e-05 dt: 3331.97ms, tok/sec:157350.52
step 14737, loss: 3.119941, norm:0.2702, lr:6.1715e-05 dt: 3332.02ms, tok/sec:157348.48
step 14738, loss: 3.161631, norm:0.2533, lr:6.1708e-05 dt: 3332.11ms, tok/sec:157344.05
step 14739, loss: 3.276421, norm:0.3101, lr:6.1701e-05 dt: 3332.00ms, tok/sec:157349.20
step 14740, loss: 3.216809, norm:0.3225, lr:6.1695e-05 dt: 3332.27ms, tok/sec:157336.64
step 14741, loss: 3.169379, norm:0.3843, lr:6.1688e-05 dt: 3331.79ms, tok/sec:157359.12
step 14742, loss: 3.182578, norm:0.2783, lr:6.1682e-05 dt: 3332.74ms, tok/sec:157314.22
step 14743, loss: 3.175375, norm:0.2728, lr:6.1675e-05 dt: 3332.12ms, tok/sec:157343.54
step 14744, loss: 3.164010, norm:0.2912, lr:6.1669e-05 dt: 3332.09ms, tok/sec:157345.00
step 14745, loss: 3.191945, norm:0.2866, lr:6.1662e-05 dt: 3332.14ms, tok/sec:157342.52
step 14746, loss: 3.148364, norm:0.2886, lr:6.1656e-05 dt: 3331.91ms, tok/sec:157353.82
step 14747, loss: 3.156809, norm:0.2846, lr:6.1650e-05 dt: 3332.30ms, tok/sec:157335.03
step 14748, loss: 3.128617, norm:0.2700, lr:6.1643e-05 dt: 3331.79ms, tok/sec:157359.06
step 14749, loss: 3.135709, norm:0.2745, lr:6.1637e-05 dt: 3332.33ms, tok/sec:157333.96
HellaSwag accuracy:-2295565113419922351/-2=1147782556709961216.0000
rank 1 sample 0: Hello, I'm a language model, and a number of companies have a certain language or dialect within their product. Let's look at this.
The language
rank 1 sample 1: Hello, I'm a language model, which means I think I'm trying to design a computer as a community. That is different than anyone who is doing a
rank 1 sample 2: Hello, I'm a language model, but where is all the data?
I'm a programmer or programmer and I'm not just the developer, or computer
rank 1 sample 3: Hello, I'm a language model, and I'm pretty much here, as am I. Like, for all i'm still quite an expert, but I
rank 0 sample 0: Hello, I'm a language model, and I think I can be the type writer you want to write that the kids are using.<|endoftext|>In this article I
rank 0 sample 1: Hello, I'm a language model, so to speak. But I can give you an illustration so that when you get to that you're saying 'wow,
rank 0 sample 2: Hello, I'm a language model, so I wanted to explain everything I'm doing as a language programmer in an easy way, but I'm not a lingu
rank 0 sample 3: Hello, I'm a language model, and what I do is just a piece of that. I'm pretty sure you will get that if you're going at
step 14750, loss: 3.180979, norm:0.2581, lr:6.1630e-05 dt: 48392.45ms, tok/sec:10834.09
step 14751, loss: 3.120179, norm:0.2611, lr:6.1624e-05 dt: 3332.22ms, tok/sec:157339.14
step 14752, loss: 3.205437, norm:0.2718, lr:6.1618e-05 dt: 3331.83ms, tok/sec:157357.59
step 14753, loss: 3.213727, norm:0.3245, lr:6.1611e-05 dt: 3332.08ms, tok/sec:157345.50
step 14754, loss: 3.217677, norm:0.2709, lr:6.1605e-05 dt: 3332.25ms, tok/sec:157337.43
step 14755, loss: 3.195030, norm:0.2736, lr:6.1598e-05 dt: 3332.08ms, tok/sec:157345.39
step 14756, loss: 3.206983, norm:0.2861, lr:6.1592e-05 dt: 3332.01ms, tok/sec:157348.75
step 14757, loss: 3.253576, norm:0.3139, lr:6.1586e-05 dt: 3332.03ms, tok/sec:157348.08
step 14758, loss: 3.211105, norm:0.2737, lr:6.1579e-05 dt: 3331.88ms, tok/sec:157354.80
step 14759, loss: 3.204712, norm:0.2720, lr:6.1573e-05 dt: 3331.93ms, tok/sec:157352.42
step 14760, loss: 3.201303, norm:0.2703, lr:6.1567e-05 dt: 3332.21ms, tok/sec:157339.54
step 14761, loss: 3.168547, norm:0.2601, lr:6.1561e-05 dt: 3332.13ms, tok/sec:157343.29
step 14762, loss: 3.165410, norm:0.2891, lr:6.1554e-05 dt: 3331.94ms, tok/sec:157352.05
step 14763, loss: 3.141450, norm:0.2642, lr:6.1548e-05 dt: 3331.91ms, tok/sec:157353.37
step 14764, loss: 3.095778, norm:0.2733, lr:6.1542e-05 dt: 3331.96ms, tok/sec:157351.14
step 14765, loss: 3.099844, norm:0.2550, lr:6.1536e-05 dt: 3332.00ms, tok/sec:157349.24
step 14766, loss: 3.086721, norm:0.2583, lr:6.1529e-05 dt: 3332.25ms, tok/sec:157337.41
step 14767, loss: 3.131995, norm:0.2600, lr:6.1523e-05 dt: 3331.84ms, tok/sec:157357.01
step 14768, loss: 3.145796, norm:0.2781, lr:6.1517e-05 dt: 3331.94ms, tok/sec:157352.03
step 14769, loss: 3.111327, norm:0.2591, lr:6.1511e-05 dt: 3332.16ms, tok/sec:157341.99
step 14770, loss: 3.096553, norm:0.2644, lr:6.1505e-05 dt: 3332.06ms, tok/sec:157346.34
step 14771, loss: 3.111915, norm:0.2609, lr:6.1499e-05 dt: 3331.92ms, tok/sec:157353.12
step 14772, loss: 3.128451, norm:0.2860, lr:6.1493e-05 dt: 3331.94ms, tok/sec:157352.03
step 14773, loss: 3.121686, norm:0.2518, lr:6.1486e-05 dt: 3331.85ms, tok/sec:157356.20
step 14774, loss: 3.139292, norm:0.2614, lr:6.1480e-05 dt: 3331.98ms, tok/sec:157350.34
step 14775, loss: 3.168050, norm:0.2852, lr:6.1474e-05 dt: 3331.99ms, tok/sec:157349.76
step 14776, loss: 3.196622, norm:0.2856, lr:6.1468e-05 dt: 3332.06ms, tok/sec:157346.50
step 14777, loss: 3.165985, norm:0.2619, lr:6.1462e-05 dt: 3332.02ms, tok/sec:157348.42
step 14778, loss: 3.132755, norm:0.2695, lr:6.1456e-05 dt: 3332.25ms, tok/sec:157337.72
step 14779, loss: 3.195862, norm:0.2810, lr:6.1450e-05 dt: 3332.23ms, tok/sec:157338.56
step 14780, loss: 3.133208, norm:0.2668, lr:6.1444e-05 dt: 3331.95ms, tok/sec:157351.56
step 14781, loss: 3.195984, norm:0.2660, lr:6.1438e-05 dt: 3331.90ms, tok/sec:157354.09
step 14782, loss: 3.222189, norm:0.2753, lr:6.1432e-05 dt: 3331.97ms, tok/sec:157350.96
step 14783, loss: 3.196716, norm:0.2869, lr:6.1426e-05 dt: 3331.92ms, tok/sec:157352.96
step 14784, loss: 3.137561, norm:0.2710, lr:6.1420e-05 dt: 3331.98ms, tok/sec:157350.21
step 14785, loss: 3.267989, norm:0.2805, lr:6.1414e-05 dt: 3332.08ms, tok/sec:157345.75
step 14786, loss: 3.149887, norm:0.2775, lr:6.1408e-05 dt: 3332.05ms, tok/sec:157346.77
step 14787, loss: 3.148708, norm:0.2747, lr:6.1402e-05 dt: 3331.91ms, tok/sec:157353.66
step 14788, loss: 3.196778, norm:0.2741, lr:6.1396e-05 dt: 3332.25ms, tok/sec:157337.34
step 14789, loss: 3.194463, norm:0.2664, lr:6.1390e-05 dt: 3332.17ms, tok/sec:157341.53
step 14790, loss: 3.199059, norm:0.2662, lr:6.1384e-05 dt: 3332.15ms, tok/sec:157342.02
step 14791, loss: 3.159940, norm:0.2860, lr:6.1378e-05 dt: 3331.97ms, tok/sec:157350.89
step 14792, loss: 3.136260, norm:0.2678, lr:6.1373e-05 dt: 3332.01ms, tok/sec:157348.94
step 14793, loss: 3.097622, norm:0.2595, lr:6.1367e-05 dt: 3332.00ms, tok/sec:157349.23
step 14794, loss: 3.181254, norm:0.2748, lr:6.1361e-05 dt: 3332.01ms, tok/sec:157349.10
step 14795, loss: 3.161895, norm:0.2792, lr:6.1355e-05 dt: 3332.17ms, tok/sec:157341.22
step 14796, loss: 3.166789, norm:0.2855, lr:6.1349e-05 dt: 3332.27ms, tok/sec:157336.55
step 14797, loss: 3.182111, norm:0.3010, lr:6.1343e-05 dt: 3332.25ms, tok/sec:157337.75
step 14798, loss: 3.128854, norm:0.2747, lr:6.1338e-05 dt: 3332.07ms, tok/sec:157346.10
step 14799, loss: 3.104600, norm:0.2823, lr:6.1332e-05 dt: 3332.00ms, tok/sec:157349.33
validation loss: 3.1914
Model and optimizer state saved.
HellaSwag accuracy:-2295565113419922351/-2=1147782556709961216.0000
rank 1 sample 0: Hello, I'm a language model, and so as a language model, like I can, would look like this:
Now let's say we have a
rank 1 sample 1: Hello, I'm a language model, which means I've got a little bit of a bad childhood. A lot of my thoughts about building an effective language model
rank 1 sample 2: Hello, I'm a language model, but, you know, I'm a language model. And we've all been talking about it since then, because I
rank 1 sample 3: Hello, I'm a language model, and I'm writing my speech. This question is like some crazy grammar and my grammar got some pretty weirdly long answers
rank 0 sample 0: Hello, I'm a language model, and I think I can write an efficient C++ program using it here. It's very quick, it takes a little
rank 0 sample 1: Hello, I'm a language model, so don't worry if you don't know what someone here actually wants. You're just like, the other person has
rank 0 sample 2: Hello, I'm a language model, so I really don't even know a lot about it. That said, I am a language modeler.
For
rank 0 sample 3: Hello, I'm a language model, and what I did was to write an algorithm along the lines of I, you kind of say, get a tree on
step 14800, loss: 3.135894, norm:0.2640, lr:6.1326e-05 dt: 56075.97ms, tok/sec:9349.60
step 14801, loss: 3.101707, norm:0.2588, lr:6.1320e-05 dt: 3331.96ms, tok/sec:157351.21
step 14802, loss: 3.107532, norm:0.2481, lr:6.1314e-05 dt: 3332.13ms, tok/sec:157343.12
step 14803, loss: 3.213778, norm:0.2531, lr:6.1309e-05 dt: 3332.01ms, tok/sec:157349.03
step 14804, loss: 3.124515, norm:0.2729, lr:6.1303e-05 dt: 3332.06ms, tok/sec:157346.58
step 14805, loss: 3.172972, norm:0.2571, lr:6.1297e-05 dt: 3332.22ms, tok/sec:157338.86
step 14806, loss: 3.138817, norm:0.2659, lr:6.1292e-05 dt: 3332.05ms, tok/sec:157347.02
step 14807, loss: 3.133260, norm:0.2644, lr:6.1286e-05 dt: 3332.01ms, tok/sec:157348.97
step 14808, loss: 3.128564, norm:0.2500, lr:6.1280e-05 dt: 3332.07ms, tok/sec:157346.19
step 14809, loss: 3.137667, norm:0.2652, lr:6.1275e-05 dt: 3331.97ms, tok/sec:157350.54
step 14810, loss: 3.246344, norm:0.3012, lr:6.1269e-05 dt: 3332.05ms, tok/sec:157346.99
step 14811, loss: 3.294273, norm:0.2787, lr:6.1263e-05 dt: 3332.29ms, tok/sec:157335.67
step 14812, loss: 3.137111, norm:0.2557, lr:6.1258e-05 dt: 3332.05ms, tok/sec:157347.03
step 14813, loss: 3.227537, norm:0.2691, lr:6.1252e-05 dt: 3331.94ms, tok/sec:157352.10
step 14814, loss: 3.225517, norm:0.2752, lr:6.1246e-05 dt: 3332.03ms, tok/sec:157348.04
step 14815, loss: 3.233098, norm:0.2721, lr:6.1241e-05 dt: 3332.07ms, tok/sec:157346.15
step 14816, loss: 3.182496, norm:0.2703, lr:6.1235e-05 dt: 3332.06ms, tok/sec:157346.65
step 14817, loss: 3.195764, norm:0.2597, lr:6.1230e-05 dt: 3332.30ms, tok/sec:157335.00
step 14818, loss: 3.136607, norm:0.2731, lr:6.1224e-05 dt: 3331.96ms, tok/sec:157351.07
step 14819, loss: 3.187412, norm:0.2659, lr:6.1219e-05 dt: 3332.08ms, tok/sec:157345.69
step 14820, loss: 3.129905, norm:0.2687, lr:6.1213e-05 dt: 3332.00ms, tok/sec:157349.32
step 14821, loss: 3.143986, norm:0.2842, lr:6.1208e-05 dt: 3332.21ms, tok/sec:157339.42
step 14822, loss: 3.129195, norm:0.2597, lr:6.1202e-05 dt: 3332.05ms, tok/sec:157346.82
step 14823, loss: 3.142012, norm:0.2626, lr:6.1197e-05 dt: 3332.19ms, tok/sec:157340.49
step 14824, loss: 3.157526, norm:0.2928, lr:6.1191e-05 dt: 3331.91ms, tok/sec:157353.76
step 14825, loss: 3.115808, norm:0.2582, lr:6.1186e-05 dt: 3331.95ms, tok/sec:157351.82
step 14826, loss: 3.113850, norm:0.2490, lr:6.1180e-05 dt: 3332.02ms, tok/sec:157348.28
step 14827, loss: 3.180688, norm:0.2641, lr:6.1175e-05 dt: 3331.98ms, tok/sec:157350.28
step 14828, loss: 3.173721, norm:0.2706, lr:6.1169e-05 dt: 3332.05ms, tok/sec:157347.12
step 14829, loss: 3.160739, norm:0.2732, lr:6.1164e-05 dt: 3331.94ms, tok/sec:157352.22
step 14830, loss: 3.130490, norm:0.2766, lr:6.1158e-05 dt: 3332.26ms, tok/sec:157336.91
step 14831, loss: 3.144175, norm:0.2474, lr:6.1153e-05 dt: 3332.18ms, tok/sec:157340.79
step 14832, loss: 3.217075, norm:0.2703, lr:6.1148e-05 dt: 3332.02ms, tok/sec:157348.25
step 14833, loss: 3.147353, norm:0.2809, lr:6.1142e-05 dt: 3332.15ms, tok/sec:157342.28
step 14834, loss: 3.112009, norm:0.2691, lr:6.1137e-05 dt: 3332.08ms, tok/sec:157345.63
step 14835, loss: 3.081307, norm:0.2883, lr:6.1132e-05 dt: 3331.79ms, tok/sec:157359.35
step 14836, loss: 3.107076, norm:0.2830, lr:6.1126e-05 dt: 3332.07ms, tok/sec:157345.94
step 14837, loss: 3.152295, norm:0.2642, lr:6.1121e-05 dt: 3332.20ms, tok/sec:157339.84
step 14838, loss: 3.147942, norm:0.2707, lr:6.1116e-05 dt: 3331.85ms, tok/sec:157356.24
step 14839, loss: 3.104953, norm:0.2744, lr:6.1110e-05 dt: 3332.12ms, tok/sec:157343.81
step 14840, loss: 3.147779, norm:0.2789, lr:6.1105e-05 dt: 3331.93ms, tok/sec:157352.45
step 14841, loss: 3.230189, norm:0.3087, lr:6.1100e-05 dt: 3332.10ms, tok/sec:157344.49
step 14842, loss: 3.142754, norm:0.2657, lr:6.1095e-05 dt: 3331.72ms, tok/sec:157362.67
step 14843, loss: 3.168113, norm:0.3086, lr:6.1089e-05 dt: 3332.02ms, tok/sec:157348.25
step 14844, loss: 3.162198, norm:0.2753, lr:6.1084e-05 dt: 3331.95ms, tok/sec:157351.85
step 14845, loss: 3.136279, norm:0.2846, lr:6.1079e-05 dt: 3332.08ms, tok/sec:157345.38
step 14846, loss: 3.142793, norm:0.2900, lr:6.1074e-05 dt: 3332.10ms, tok/sec:157344.41
step 14847, loss: 3.214507, norm:0.3117, lr:6.1068e-05 dt: 3332.00ms, tok/sec:157349.29
step 14848, loss: 3.122863, norm:0.2899, lr:6.1063e-05 dt: 3332.35ms, tok/sec:157332.95
step 14849, loss: 3.173720, norm:0.2925, lr:6.1058e-05 dt: 3332.11ms, tok/sec:157344.23
HellaSwag accuracy:-2294298476024724399/-2=1147149238012362240.0000
rank 1 sample 0: Hello, I'm a language model, and a visual language.
You find that my model really follows the same grammar as a C.
You can see
rank 1 sample 1: Hello, I'm a language model, which means I don't have an answer when I get this question. I'll add whatever i want.
I'm
rank 1 sample 2: Hello, I'm a language model, but where is all the data?
- I'm looking for data.
- I'm going to be working with
rank 1 sample 3: Hello, I'm a language model, and I'm writing this a lot and actually I'm kind of done for what I understand - that's how I think
rank 0 sample 0: Hello, I'm a language model, and I need to learn more to know another language.
- This class teaches you about computers, the human brain,
rank 0 sample 1: Hello, I'm a language model, so to work with.
So yeah, it took hours, so time for me, it was nice and exciting to
rank 0 sample 2: Hello, I'm a language model, so I hope this piece gives you a little fun.
In fact, I don't really know what you mean.
rank 0 sample 3: Hello, I'm a language model, and not a linguist.
And, not that I'm wrong: the original word-type machine was not quite
step 14850, loss: 3.188790, norm:0.2929, lr:6.1053e-05 dt: 48388.77ms, tok/sec:10834.91
step 14851, loss: 3.161613, norm:0.2857, lr:6.1048e-05 dt: 3332.19ms, tok/sec:157340.46
step 14852, loss: 3.211102, norm:0.2887, lr:6.1043e-05 dt: 3332.14ms, tok/sec:157342.94
step 14853, loss: 3.191856, norm:0.3012, lr:6.1038e-05 dt: 3331.97ms, tok/sec:157350.71
step 14854, loss: 3.200241, norm:0.2861, lr:6.1033e-05 dt: 3332.10ms, tok/sec:157344.84
step 14855, loss: 3.147985, norm:0.2814, lr:6.1027e-05 dt: 3331.92ms, tok/sec:157352.99
step 14856, loss: 3.218402, norm:0.3815, lr:6.1022e-05 dt: 3332.07ms, tok/sec:157346.15
step 14857, loss: 3.117131, norm:0.2686, lr:6.1017e-05 dt: 3331.88ms, tok/sec:157355.10
step 14858, loss: 3.183341, norm:0.2797, lr:6.1012e-05 dt: 3334.33ms, tok/sec:157239.18
step 14859, loss: 3.371734, norm:0.3340, lr:6.1007e-05 dt: 3332.08ms, tok/sec:157345.71
step 14860, loss: 3.173850, norm:0.2821, lr:6.1002e-05 dt: 3332.10ms, tok/sec:157344.41
step 14861, loss: 3.240125, norm:0.3079, lr:6.0997e-05 dt: 3332.29ms, tok/sec:157335.83
step 14862, loss: 3.170090, norm:0.2809, lr:6.0992e-05 dt: 3332.43ms, tok/sec:157329.08
step 14863, loss: 3.157481, norm:0.2841, lr:6.0987e-05 dt: 3332.13ms, tok/sec:157343.01
step 14864, loss: 3.201004, norm:0.2674, lr:6.0982e-05 dt: 3332.08ms, tok/sec:157345.75
step 14865, loss: 3.123768, norm:0.2738, lr:6.0977e-05 dt: 3332.08ms, tok/sec:157345.47
step 14866, loss: 3.202127, norm:0.2892, lr:6.0972e-05 dt: 3331.89ms, tok/sec:157354.38
step 14867, loss: 3.170424, norm:0.2666, lr:6.0967e-05 dt: 3332.15ms, tok/sec:157342.32
step 14868, loss: 3.151926, norm:0.2735, lr:6.0962e-05 dt: 3332.11ms, tok/sec:157344.35
step 14869, loss: 3.079608, norm:0.2795, lr:6.0957e-05 dt: 3331.85ms, tok/sec:157356.26
step 14870, loss: 3.158937, norm:0.2786, lr:6.0953e-05 dt: 3332.02ms, tok/sec:157348.55
step 14871, loss: 3.159410, norm:0.2765, lr:6.0948e-05 dt: 3331.97ms, tok/sec:157350.61
step 14872, loss: 3.070673, norm:0.2646, lr:6.0943e-05 dt: 3332.30ms, tok/sec:157335.39
step 14873, loss: 3.114274, norm:0.2731, lr:6.0938e-05 dt: 3331.88ms, tok/sec:157355.04
step 14874, loss: 3.153100, norm:0.2584, lr:6.0933e-05 dt: 3331.78ms, tok/sec:157359.55
step 14875, loss: 3.116791, norm:0.2669, lr:6.0928e-05 dt: 3332.07ms, tok/sec:157346.05
step 14876, loss: 3.097734, norm:0.2569, lr:6.0923e-05 dt: 3332.22ms, tok/sec:157338.88
step 14877, loss: 3.114988, norm:0.2625, lr:6.0919e-05 dt: 3331.68ms, tok/sec:157364.54
step 14878, loss: 3.133104, norm:0.2610, lr:6.0914e-05 dt: 3331.93ms, tok/sec:157352.64
step 14879, loss: 3.133907, norm:0.2579, lr:6.0909e-05 dt: 3332.12ms, tok/sec:157343.51
step 14880, loss: 3.165299, norm:0.2915, lr:6.0904e-05 dt: 3332.11ms, tok/sec:157344.07
step 14881, loss: 3.146775, norm:0.2725, lr:6.0900e-05 dt: 3332.03ms, tok/sec:157347.99
step 14882, loss: 3.247484, norm:0.2928, lr:6.0895e-05 dt: 3332.13ms, tok/sec:157343.34
step 14883, loss: 3.162087, norm:0.2629, lr:6.0890e-05 dt: 3332.17ms, tok/sec:157341.36
step 14884, loss: 3.139756, norm:0.2650, lr:6.0885e-05 dt: 3332.11ms, tok/sec:157343.95
step 14885, loss: 3.162280, norm:0.2781, lr:6.0881e-05 dt: 3332.07ms, tok/sec:157345.96
step 14886, loss: 3.197156, norm:0.2798, lr:6.0876e-05 dt: 3332.01ms, tok/sec:157348.81
step 14887, loss: 3.190232, norm:0.2950, lr:6.0871e-05 dt: 3332.01ms, tok/sec:157349.00
step 14888, loss: 3.199090, norm:0.2663, lr:6.0867e-05 dt: 3332.29ms, tok/sec:157335.71
step 14889, loss: 3.219156, norm:0.2794, lr:6.0862e-05 dt: 3332.07ms, tok/sec:157346.00
step 14890, loss: 3.250922, norm:0.3034, lr:6.0857e-05 dt: 3332.01ms, tok/sec:157349.01
step 14891, loss: 3.183761, norm:0.2793, lr:6.0853e-05 dt: 3331.91ms, tok/sec:157353.49
step 14892, loss: 3.181981, norm:0.2999, lr:6.0848e-05 dt: 3332.12ms, tok/sec:157343.65
step 14893, loss: 3.154947, norm:0.2800, lr:6.0843e-05 dt: 3331.94ms, tok/sec:157352.11
step 14894, loss: 3.119193, norm:0.2731, lr:6.0839e-05 dt: 3332.07ms, tok/sec:157346.09
step 14895, loss: 3.148781, norm:0.2705, lr:6.0834e-05 dt: 3331.87ms, tok/sec:157355.28
step 14896, loss: 3.174345, norm:0.2721, lr:6.0830e-05 dt: 3332.09ms, tok/sec:157345.22
step 14897, loss: 3.132985, norm:0.2776, lr:6.0825e-05 dt: 3332.21ms, tok/sec:157339.66
step 14898, loss: 3.164400, norm:0.2718, lr:6.0820e-05 dt: 3331.87ms, tok/sec:157355.52
step 14899, loss: 3.176932, norm:0.2682, lr:6.0816e-05 dt: 3332.08ms, tok/sec:157345.60
validation loss: 3.1907
Model and optimizer state saved.
HellaSwag accuracy:-2295441968117611439/-2=1147720984058805760.0000
rank 1 sample 0: Hello, I'm a language model, and that's what I want to develop for all kinds of computer languages. Now, what is your goal here?

rank 1 sample 1: Hello, I'm a language model, a computer programmer, and a teacher. For the purposes just mentioned, I'd say what a cool way to learn a
rank 1 sample 2: Hello, I'm a language model, but at first I'm not sure what I'm talking about about here."
"I don't have any good intentions
rank 1 sample 3: Hello, I'm a language model, and I'm using the most common word used in the last four hundred years?
Thanks for a great question.

rank 0 sample 0: Hello, I'm a language model, and I need to learn and learn some rules around what is called object-oriented programming and how this can be done using
rank 0 sample 1: Hello, I'm a language model, so why not have fun with that???
Well as to what it is called, the term is just that, you
rank 0 sample 2: Hello, I'm a language model, so I really don't believe that the language we're talking about will have a great deal of influence on the future.
rank 0 sample 3: Hello, I'm a language model, and what I've done is I have to test it. I was just a guy to have me out. So just
step 14900, loss: 3.230317, norm:0.2899, lr:6.0811e-05 dt: 56077.96ms, tok/sec:9349.27
step 14901, loss: 3.127252, norm:0.2964, lr:6.0807e-05 dt: 3332.04ms, tok/sec:157347.64
step 14902, loss: 3.198576, norm:0.2667, lr:6.0802e-05 dt: 3331.92ms, tok/sec:157353.18
step 14903, loss: 3.148422, norm:0.3044, lr:6.0798e-05 dt: 3332.08ms, tok/sec:157345.36
step 14904, loss: 3.195991, norm:0.2716, lr:6.0793e-05 dt: 3331.97ms, tok/sec:157350.89
step 14905, loss: 3.136752, norm:0.2807, lr:6.0789e-05 dt: 3331.95ms, tok/sec:157351.62
step 14906, loss: 3.116486, norm:0.2771, lr:6.0785e-05 dt: 3332.02ms, tok/sec:157348.49
step 14907, loss: 3.102475, norm:0.2769, lr:6.0780e-05 dt: 3332.29ms, tok/sec:157335.77
step 14908, loss: 3.107644, norm:0.2878, lr:6.0776e-05 dt: 3332.07ms, tok/sec:157346.14
step 14909, loss: 3.102955, norm:0.2585, lr:6.0771e-05 dt: 3331.98ms, tok/sec:157350.21
step 14910, loss: 3.132303, norm:0.2646, lr:6.0767e-05 dt: 3332.26ms, tok/sec:157337.02
step 14911, loss: 3.111142, norm:0.2587, lr:6.0762e-05 dt: 3332.40ms, tok/sec:157330.27
step 14912, loss: 3.170201, norm:0.2694, lr:6.0758e-05 dt: 3332.07ms, tok/sec:157346.18
step 14913, loss: 3.143428, norm:0.2712, lr:6.0754e-05 dt: 3332.06ms, tok/sec:157346.72
step 14914, loss: 3.184209, norm:0.2609, lr:6.0749e-05 dt: 3332.11ms, tok/sec:157344.32
step 14915, loss: 3.148229, norm:0.2877, lr:6.0745e-05 dt: 3331.88ms, tok/sec:157354.92
step 14916, loss: 3.195655, norm:0.2952, lr:6.0741e-05 dt: 3332.38ms, tok/sec:157331.62
step 14917, loss: 3.160286, norm:0.2795, lr:6.0736e-05 dt: 3331.99ms, tok/sec:157349.68
step 14918, loss: 3.148363, norm:0.2686, lr:6.0732e-05 dt: 3332.00ms, tok/sec:157349.55
step 14919, loss: 3.183743, norm:0.2805, lr:6.0728e-05 dt: 3332.25ms, tok/sec:157337.64
step 14920, loss: 3.115938, norm:0.2837, lr:6.0724e-05 dt: 3332.17ms, tok/sec:157341.33
step 14921, loss: 3.105937, norm:0.2705, lr:6.0719e-05 dt: 3332.02ms, tok/sec:157348.27
step 14922, loss: 3.164877, norm:0.2835, lr:6.0715e-05 dt: 3332.08ms, tok/sec:157345.63
step 14923, loss: 3.242094, norm:0.3318, lr:6.0711e-05 dt: 3331.98ms, tok/sec:157350.07
step 14924, loss: 3.192791, norm:0.2953, lr:6.0707e-05 dt: 3331.97ms, tok/sec:157350.75
step 14925, loss: 3.146875, norm:0.2909, lr:6.0702e-05 dt: 3332.06ms, tok/sec:157346.65
step 14926, loss: 3.155905, norm:0.2816, lr:6.0698e-05 dt: 3332.12ms, tok/sec:157343.66
step 14927, loss: 3.172328, norm:0.2638, lr:6.0694e-05 dt: 3332.09ms, tok/sec:157344.89
step 14928, loss: 3.191847, norm:0.2724, lr:6.0690e-05 dt: 3331.85ms, tok/sec:157356.33
step 14929, loss: 3.125170, norm:0.2806, lr:6.0686e-05 dt: 3332.06ms, tok/sec:157346.56
step 14930, loss: 3.150285, norm:0.2703, lr:6.0682e-05 dt: 3332.05ms, tok/sec:157347.04
step 14931, loss: 3.156546, norm:0.2661, lr:6.0677e-05 dt: 3332.16ms, tok/sec:157341.71
step 14932, loss: 3.110020, norm:0.2572, lr:6.0673e-05 dt: 3332.48ms, tok/sec:157326.54
step 14933, loss: 3.198286, norm:0.2690, lr:6.0669e-05 dt: 3331.96ms, tok/sec:157351.05
step 14934, loss: 3.182860, norm:0.2554, lr:6.0665e-05 dt: 3332.26ms, tok/sec:157337.05
step 14935, loss: 3.151045, norm:0.2589, lr:6.0661e-05 dt: 3332.32ms, tok/sec:157334.40
step 14936, loss: 3.133557, norm:0.2477, lr:6.0657e-05 dt: 3331.91ms, tok/sec:157353.46
step 14937, loss: 3.165099, norm:0.2832, lr:6.0653e-05 dt: 3331.83ms, tok/sec:157357.14
step 14938, loss: 3.188035, norm:0.2770, lr:6.0649e-05 dt: 3331.94ms, tok/sec:157352.03
step 14939, loss: 3.121315, norm:0.2787, lr:6.0645e-05 dt: 3332.25ms, tok/sec:157337.52
step 14940, loss: 3.138552, norm:0.2635, lr:6.0641e-05 dt: 3332.07ms, tok/sec:157345.85
step 14941, loss: 3.116425, norm:0.2658, lr:6.0637e-05 dt: 3331.75ms, tok/sec:157360.97
step 14942, loss: 3.185013, norm:0.2598, lr:6.0633e-05 dt: 3331.95ms, tok/sec:157351.56
step 14943, loss: 3.224464, norm:0.3026, lr:6.0629e-05 dt: 3332.18ms, tok/sec:157340.94
step 14944, loss: 3.128241, norm:0.2584, lr:6.0625e-05 dt: 3332.02ms, tok/sec:157348.45
step 14945, loss: 3.081153, norm:0.2895, lr:6.0621e-05 dt: 3332.10ms, tok/sec:157344.46
step 14946, loss: 3.115899, norm:0.2620, lr:6.0617e-05 dt: 3331.84ms, tok/sec:157356.70
step 14947, loss: 3.153304, norm:0.2569, lr:6.0613e-05 dt: 3332.17ms, tok/sec:157341.44
step 14948, loss: 3.111171, norm:0.2724, lr:6.0609e-05 dt: 3332.40ms, tok/sec:157330.63
step 14949, loss: 3.118725, norm:0.2669, lr:6.0605e-05 dt: 3332.12ms, tok/sec:157343.77
HellaSwag accuracy:-2294439213513079727/-2=1147219606756539904.0000
rank 1 sample 0: Hello, I'm a language model, and my favourite language is the English learner version. What’s in the language model I use?
I
rank 1 sample 1: Hello, I'm a language model, which means I've got a lot of resources to build languages. Let's go to get a deeper look at the language
rank 1 sample 2: Hello, I'm a language model, but because you know that you can't do anything because there can only be one language. I have been teaching my first
rank 1 sample 3: Hello, I'm a language model, and I'm really excited to talk about so many things connected with Java. When I come back to my class, I
rank 0 sample 0: Hello, I'm a language model, and I think I can get some inspiration (from my mistakes) here. And I hope you've found it. :)
rank 0 sample 1: Hello, I'm a language model, so there's no reason to say in English, only when I know Spanish is a foreign language, there's no reason
rank 0 sample 2: Hello, I'm a language model, so I really love helping them learn a lot when I'm trying to communicate with the kids.
I'm also having
rank 0 sample 3: Hello, I'm a language model, and what I've learned is how to communicate online. I've always felt I wasn't in the dark, so much
step 14950, loss: 3.176497, norm:0.4148, lr:6.0601e-05 dt: 48385.52ms, tok/sec:10835.64
step 14951, loss: 3.189463, norm:0.2916, lr:6.0597e-05 dt: 3332.06ms, tok/sec:157346.49
step 14952, loss: 3.181747, norm:0.2709, lr:6.0593e-05 dt: 3332.07ms, tok/sec:157346.16
step 14953, loss: 3.143114, norm:0.2726, lr:6.0590e-05 dt: 3332.20ms, tok/sec:157339.79
step 14954, loss: 3.169522, norm:0.2945, lr:6.0586e-05 dt: 3332.16ms, tok/sec:157341.75
step 14955, loss: 3.169903, norm:0.2844, lr:6.0582e-05 dt: 3332.01ms, tok/sec:157348.78
step 14956, loss: 3.167098, norm:0.2879, lr:6.0578e-05 dt: 3332.02ms, tok/sec:157348.42
step 14957, loss: 3.217734, norm:0.2771, lr:6.0574e-05 dt: 3332.23ms, tok/sec:157338.53
step 14958, loss: 3.155407, norm:0.2873, lr:6.0570e-05 dt: 3332.18ms, tok/sec:157340.86
step 14959, loss: 3.144488, norm:0.2809, lr:6.0567e-05 dt: 3332.14ms, tok/sec:157342.49
step 14960, loss: 3.234652, norm:0.2646, lr:6.0563e-05 dt: 3331.84ms, tok/sec:157356.76
step 14961, loss: 3.187779, norm:0.2775, lr:6.0559e-05 dt: 3331.89ms, tok/sec:157354.43
step 14962, loss: 3.172206, norm:0.2729, lr:6.0555e-05 dt: 3331.96ms, tok/sec:157351.43
step 14963, loss: 3.172340, norm:0.2618, lr:6.0552e-05 dt: 3332.00ms, tok/sec:157349.28
step 14964, loss: 3.178586, norm:0.2576, lr:6.0548e-05 dt: 3332.18ms, tok/sec:157340.96
step 14965, loss: 3.162559, norm:0.2581, lr:6.0544e-05 dt: 3332.02ms, tok/sec:157348.25
step 14966, loss: 3.138061, norm:0.2595, lr:6.0541e-05 dt: 3332.13ms, tok/sec:157343.11
step 14967, loss: 3.182796, norm:0.2641, lr:6.0537e-05 dt: 3332.16ms, tok/sec:157341.63
step 14968, loss: 3.180459, norm:0.2710, lr:6.0533e-05 dt: 3331.95ms, tok/sec:157351.66
step 14969, loss: 3.168544, norm:0.2522, lr:6.0530e-05 dt: 3331.92ms, tok/sec:157353.29
step 14970, loss: 3.217129, norm:0.2733, lr:6.0526e-05 dt: 3332.06ms, tok/sec:157346.49
step 14971, loss: 3.184142, norm:0.2585, lr:6.0522e-05 dt: 3331.99ms, tok/sec:157349.70
step 14972, loss: 3.181803, norm:0.2584, lr:6.0519e-05 dt: 3331.91ms, tok/sec:157353.67
step 14973, loss: 3.136765, norm:0.2711, lr:6.0515e-05 dt: 3332.15ms, tok/sec:157342.03
step 14974, loss: 3.136644, norm:0.2624, lr:6.0511e-05 dt: 3331.99ms, tok/sec:157349.79
step 14975, loss: 3.126574, norm:0.2645, lr:6.0508e-05 dt: 3332.06ms, tok/sec:157346.55
step 14976, loss: 3.074056, norm:0.2660, lr:6.0504e-05 dt: 3331.94ms, tok/sec:157352.14
step 14977, loss: 3.118874, norm:0.2613, lr:6.0501e-05 dt: 3332.11ms, tok/sec:157344.10
step 14978, loss: 3.078129, norm:0.2698, lr:6.0497e-05 dt: 3332.26ms, tok/sec:157337.18
step 14979, loss: 3.110133, norm:0.2492, lr:6.0494e-05 dt: 3332.07ms, tok/sec:157346.16
step 14980, loss: 3.172990, norm:0.2696, lr:6.0490e-05 dt: 3332.00ms, tok/sec:157349.41
step 14981, loss: 3.121506, norm:0.2716, lr:6.0487e-05 dt: 3332.38ms, tok/sec:157331.30
step 14982, loss: 3.113947, norm:0.2619, lr:6.0483e-05 dt: 3331.97ms, tok/sec:157350.64
step 14983, loss: 3.100959, norm:0.2744, lr:6.0480e-05 dt: 3332.02ms, tok/sec:157348.63
step 14984, loss: 3.123549, norm:0.2786, lr:6.0476e-05 dt: 3332.05ms, tok/sec:157346.81
step 14985, loss: 3.164019, norm:0.3017, lr:6.0473e-05 dt: 3332.02ms, tok/sec:157348.38
step 14986, loss: 3.161328, norm:0.2851, lr:6.0469e-05 dt: 3332.18ms, tok/sec:157340.73
step 14987, loss: 3.177416, norm:0.2815, lr:6.0466e-05 dt: 3331.97ms, tok/sec:157350.63
step 14988, loss: 3.218582, norm:0.2849, lr:6.0462e-05 dt: 3332.09ms, tok/sec:157345.22
step 14989, loss: 3.218589, norm:0.3402, lr:6.0459e-05 dt: 3332.20ms, tok/sec:157340.12
step 14990, loss: 3.200742, norm:0.2921, lr:6.0456e-05 dt: 3332.23ms, tok/sec:157338.34
step 14991, loss: 3.196244, norm:0.2763, lr:6.0452e-05 dt: 3331.96ms, tok/sec:157351.03
step 14992, loss: 3.195686, norm:0.2876, lr:6.0449e-05 dt: 3332.03ms, tok/sec:157348.08
step 14993, loss: 3.244006, norm:0.3095, lr:6.0446e-05 dt: 3331.90ms, tok/sec:157354.02
step 14994, loss: 3.274569, norm:0.3539, lr:6.0442e-05 dt: 3332.01ms, tok/sec:157348.64
step 14995, loss: 3.135128, norm:0.2751, lr:6.0439e-05 dt: 3332.07ms, tok/sec:157346.00
step 14996, loss: 3.161940, norm:0.2880, lr:6.0436e-05 dt: 3332.04ms, tok/sec:157347.50
step 14997, loss: 3.176132, norm:0.2860, lr:6.0432e-05 dt: 3332.25ms, tok/sec:157337.73
step 14998, loss: 3.159276, norm:0.2785, lr:6.0429e-05 dt: 3331.92ms, tok/sec:157353.32
step 14999, loss: 3.163368, norm:0.2716, lr:6.0426e-05 dt: 3332.11ms, tok/sec:157344.35
validation loss: 3.1898
Model and optimizer state saved.
HellaSwag accuracy:-2295441968117611439/-2=1147720984058805760.0000
rank 1 sample 0: Hello, I'm a language model, and this is the only language I own that makes sense about writing.
One of the biggest mistakes I make is that
rank 1 sample 1: Hello, I'm a language model, which means I think I'm just using what I understand what I see. If you really want to know what I'm
rank 1 sample 2: Hello, I'm a language model, but who can understand the language?
- I'm trying to teach the language to my children - but they need a
rank 1 sample 3: Hello, I'm a language model, and I'm interested to what can be, and how technology can inform a machine learning research for healthcare.
We have
rank 0 sample 0: Hello, I'm a language model, and I think I can say "The new definition of a language doesn't mean that we can call any language any more
rank 0 sample 1: Hello, I'm a language model, so now I'm gonna create some little exercises for grammar with the words a-z and d. What are the main
rank 0 sample 2: Hello, I'm a language model, so I do lots of learning, and I'll let you use that as a way of thinking.
I've actually
rank 0 sample 3: Hello, I'm a language model, and now I'm going to be speaking a simplified version of programming I did with two kids with my English. So first
step 15000, loss: 3.186883, norm:0.2808, lr:6.0422e-05 dt: 56243.31ms, tok/sec:9321.78
step 15001, loss: 3.163272, norm:0.2884, lr:6.0419e-05 dt: 3332.22ms, tok/sec:157338.83
step 15002, loss: 3.143796, norm:0.2829, lr:6.0416e-05 dt: 3332.22ms, tok/sec:157338.80
step 15003, loss: 3.145502, norm:0.2791, lr:6.0413e-05 dt: 3331.87ms, tok/sec:157355.54
step 15004, loss: 3.148824, norm:0.2623, lr:6.0409e-05 dt: 3332.03ms, tok/sec:157347.94
step 15005, loss: 3.156622, norm:0.2742, lr:6.0406e-05 dt: 3332.17ms, tok/sec:157341.24
step 15006, loss: 3.190794, norm:0.2879, lr:6.0403e-05 dt: 3332.06ms, tok/sec:157346.49
step 15007, loss: 3.194373, norm:0.2868, lr:6.0400e-05 dt: 3331.90ms, tok/sec:157354.16
step 15008, loss: 3.062093, norm:0.2782, lr:6.0397e-05 dt: 3332.32ms, tok/sec:157334.40
step 15009, loss: 3.095537, norm:0.2666, lr:6.0394e-05 dt: 3332.12ms, tok/sec:157343.87
step 15010, loss: 3.108367, norm:0.2816, lr:6.0390e-05 dt: 3331.79ms, tok/sec:157359.36
step 15011, loss: 3.121688, norm:0.2902, lr:6.0387e-05 dt: 3332.05ms, tok/sec:157346.93
step 15012, loss: 3.180314, norm:0.2689, lr:6.0384e-05 dt: 3332.25ms, tok/sec:157337.57
step 15013, loss: 3.136429, norm:0.2835, lr:6.0381e-05 dt: 3332.24ms, tok/sec:157338.15
step 15014, loss: 3.106071, norm:0.2698, lr:6.0378e-05 dt: 3331.96ms, tok/sec:157351.26
step 15015, loss: 3.098289, norm:0.2929, lr:6.0375e-05 dt: 3331.89ms, tok/sec:157354.47
step 15016, loss: 3.153422, norm:0.2579, lr:6.0372e-05 dt: 3332.15ms, tok/sec:157342.10
step 15017, loss: 3.117948, norm:0.2678, lr:6.0369e-05 dt: 3332.33ms, tok/sec:157333.70
step 15018, loss: 3.132358, norm:0.2793, lr:6.0366e-05 dt: 3332.04ms, tok/sec:157347.27
step 15019, loss: 3.151401, norm:0.2990, lr:6.0363e-05 dt: 3331.96ms, tok/sec:157351.40
step 15020, loss: 3.146796, norm:0.3676, lr:6.0360e-05 dt: 3332.31ms, tok/sec:157334.69
step 15021, loss: 3.180666, norm:0.3027, lr:6.0357e-05 dt: 3332.09ms, tok/sec:157345.06
step 15022, loss: 3.118793, norm:0.2920, lr:6.0354e-05 dt: 3331.81ms, tok/sec:157358.22
step 15023, loss: 3.157637, norm:0.2680, lr:6.0351e-05 dt: 3331.93ms, tok/sec:157352.85
step 15024, loss: 3.172642, norm:0.2712, lr:6.0348e-05 dt: 3332.46ms, tok/sec:157327.85
step 15025, loss: 3.162044, norm:0.2759, lr:6.0345e-05 dt: 3332.06ms, tok/sec:157346.59
step 15026, loss: 3.177904, norm:0.2749, lr:6.0342e-05 dt: 3331.89ms, tok/sec:157354.55
step 15027, loss: 3.171903, norm:0.2753, lr:6.0339e-05 dt: 3332.06ms, tok/sec:157346.69
step 15028, loss: 3.251963, norm:0.2811, lr:6.0336e-05 dt: 3331.99ms, tok/sec:157349.63
step 15029, loss: 3.184687, norm:0.2722, lr:6.0333e-05 dt: 3332.16ms, tok/sec:157342.01
step 15030, loss: 3.127560, norm:0.2680, lr:6.0330e-05 dt: 3331.95ms, tok/sec:157351.79
step 15031, loss: 3.114240, norm:0.2708, lr:6.0327e-05 dt: 3331.99ms, tok/sec:157349.69
step 15032, loss: 3.210110, norm:0.2734, lr:6.0325e-05 dt: 3332.07ms, tok/sec:157346.21
step 15033, loss: 3.166583, norm:0.2770, lr:6.0322e-05 dt: 3331.99ms, tok/sec:157349.69
step 15034, loss: 3.195906, norm:0.2728, lr:6.0319e-05 dt: 3331.90ms, tok/sec:157354.11
step 15035, loss: 3.178310, norm:0.2703, lr:6.0316e-05 dt: 3331.97ms, tok/sec:157350.80
step 15036, loss: 3.150407, norm:0.2718, lr:6.0313e-05 dt: 3331.97ms, tok/sec:157350.77
step 15037, loss: 3.294083, norm:0.3356, lr:6.0310e-05 dt: 3332.40ms, tok/sec:157330.35
step 15038, loss: 3.138519, norm:0.2754, lr:6.0308e-05 dt: 3332.08ms, tok/sec:157345.69
step 15039, loss: 3.176655, norm:0.3280, lr:6.0305e-05 dt: 3332.43ms, tok/sec:157328.99
step 15040, loss: 3.181177, norm:0.2716, lr:6.0302e-05 dt: 3332.04ms, tok/sec:157347.37
step 15041, loss: 3.173381, norm:0.2763, lr:6.0299e-05 dt: 3332.08ms, tok/sec:157345.58
step 15042, loss: 3.096147, norm:0.2663, lr:6.0297e-05 dt: 3331.86ms, tok/sec:157355.96
step 15043, loss: 3.147926, norm:0.2665, lr:6.0294e-05 dt: 3332.00ms, tok/sec:157349.11
step 15044, loss: 3.131812, norm:0.2658, lr:6.0291e-05 dt: 3332.02ms, tok/sec:157348.52
step 15045, loss: 3.121250, norm:0.2684, lr:6.0288e-05 dt: 3332.01ms, tok/sec:157348.93
step 15046, loss: 3.077919, norm:0.2661, lr:6.0286e-05 dt: 3332.06ms, tok/sec:157346.31
step 15047, loss: 3.098591, norm:0.2608, lr:6.0283e-05 dt: 3331.97ms, tok/sec:157350.55
step 15048, loss: 3.179570, norm:0.2625, lr:6.0280e-05 dt: 3331.93ms, tok/sec:157352.72
step 15049, loss: 3.143185, norm:0.2781, lr:6.0278e-05 dt: 3334.16ms, tok/sec:157247.51
HellaSwag accuracy:-2268543515655699375/-2=1134271757827849728.0000
rank 0 sample 0: Hello, I'm a language model, and I think I can teach people in future I will be able to go and teach people in that field of study in
rank 1 sample 0: Hello, I'm a language model, this is a language model, this belongs with this class. For example, language model 1, you can see that language
rank 0 sample 1: Hello, I'm a language model, so how do I do that? Just like any math model, there is something you should look at on how to do
rank 0 sample 2: Hello, I'm a language model, so I didn't know my audience's language; I just got the opportunity to show you how to use it in arank 1 sample 1: Hello, I'm a language model, which means I've got a good understanding of the English sentence. A lot of writing systems talk about how to write a

rank 0 sample 3: Hello, I'm a language model, so now I want to do something. So start with the idea of how I see what happens. We're talking...
rank 1 sample 2: Hello, I'm a language model, but actually a model of the world. I'm not an ESL speaker and I'm not even going to speak.

rank 1 sample 3: Hello, I'm a language model, and I'm pretty much everyone's. On the other hand, people's need to hear everything about the language of the
step 15050, loss: 3.089615, norm:0.2572, lr:6.0275e-05 dt: 48384.27ms, tok/sec:10835.92
step 15051, loss: 3.168918, norm:0.2654, lr:6.0272e-05 dt: 3332.00ms, tok/sec:157349.38
step 15052, loss: 3.157743, norm:0.2600, lr:6.0270e-05 dt: 3332.08ms, tok/sec:157345.58
step 15053, loss: 3.168137, norm:0.2727, lr:6.0267e-05 dt: 3331.91ms, tok/sec:157353.45
step 15054, loss: 3.191603, norm:0.2872, lr:6.0265e-05 dt: 3331.94ms, tok/sec:157352.13
step 15055, loss: 3.185402, norm:0.2837, lr:6.0262e-05 dt: 3331.97ms, tok/sec:157350.66
step 15056, loss: 3.092579, norm:0.2700, lr:6.0260e-05 dt: 3332.11ms, tok/sec:157344.36
step 15057, loss: 3.130539, norm:0.2761, lr:6.0257e-05 dt: 3331.97ms, tok/sec:157350.75
step 15058, loss: 3.166927, norm:0.2801, lr:6.0254e-05 dt: 3332.07ms, tok/sec:157345.87
step 15059, loss: 3.127990, norm:0.2805, lr:6.0252e-05 dt: 3331.87ms, tok/sec:157355.66
step 15060, loss: 3.091440, norm:0.2692, lr:6.0249e-05 dt: 3331.96ms, tok/sec:157351.42
step 15061, loss: 3.168355, norm:0.2769, lr:6.0247e-05 dt: 3331.99ms, tok/sec:157349.85
step 15062, loss: 3.200461, norm:0.2863, lr:6.0244e-05 dt: 3331.69ms, tok/sec:157363.88
step 15063, loss: 3.179857, norm:0.2943, lr:6.0242e-05 dt: 3331.93ms, tok/sec:157352.83
step 15064, loss: 3.201012, norm:0.2828, lr:6.0239e-05 dt: 3331.91ms, tok/sec:157353.37
step 15065, loss: 3.170656, norm:0.2795, lr:6.0237e-05 dt: 3331.86ms, tok/sec:157355.76
step 15066, loss: 3.133725, norm:0.2792, lr:6.0235e-05 dt: 3332.00ms, tok/sec:157349.58
step 15067, loss: 3.179919, norm:0.2744, lr:6.0232e-05 dt: 3332.03ms, tok/sec:157347.76
step 15068, loss: 3.210575, norm:0.3320, lr:6.0230e-05 dt: 3332.07ms, tok/sec:157345.84
step 15069, loss: 3.180639, norm:0.3068, lr:6.0227e-05 dt: 3331.91ms, tok/sec:157353.55
step 15070, loss: 3.177487, norm:0.2720, lr:6.0225e-05 dt: 3332.13ms, tok/sec:157343.32
step 15071, loss: 3.135123, norm:0.2657, lr:6.0223e-05 dt: 3332.07ms, tok/sec:157346.00
step 15072, loss: 3.093678, norm:0.2682, lr:6.0220e-05 dt: 3331.99ms, tok/sec:157349.85
step 15073, loss: 3.194044, norm:0.2841, lr:6.0218e-05 dt: 3331.73ms, tok/sec:157362.22
step 15074, loss: 3.176638, norm:0.2626, lr:6.0216e-05 dt: 3331.88ms, tok/sec:157354.98
step 15075, loss: 3.150842, norm:0.2547, lr:6.0213e-05 dt: 3332.22ms, tok/sec:157338.98
step 15076, loss: 3.150262, norm:0.2763, lr:6.0211e-05 dt: 3332.06ms, tok/sec:157346.41
step 15077, loss: 3.082300, norm:0.2803, lr:6.0209e-05 dt: 3331.64ms, tok/sec:157366.51
step 15078, loss: 3.159669, norm:0.2816, lr:6.0206e-05 dt: 3331.88ms, tok/sec:157354.99
step 15079, loss: 3.127504, norm:0.2587, lr:6.0204e-05 dt: 3332.16ms, tok/sec:157341.85
step 15080, loss: 3.186455, norm:0.2583, lr:6.0202e-05 dt: 3332.17ms, tok/sec:157341.39
step 15081, loss: 3.077459, norm:0.2695, lr:6.0200e-05 dt: 3331.65ms, tok/sec:157365.68
step 15082, loss: 3.152699, norm:0.2562, lr:6.0197e-05 dt: 3331.86ms, tok/sec:157355.73
step 15083, loss: 3.134371, norm:0.2703, lr:6.0195e-05 dt: 3331.92ms, tok/sec:157353.17
step 15084, loss: 3.120582, norm:0.3092, lr:6.0193e-05 dt: 3331.85ms, tok/sec:157356.47
step 15085, loss: 3.152567, norm:0.2777, lr:6.0191e-05 dt: 3331.90ms, tok/sec:157353.94
step 15086, loss: 3.170988, norm:0.2671, lr:6.0188e-05 dt: 3331.85ms, tok/sec:157356.33
step 15087, loss: 3.135007, norm:0.2709, lr:6.0186e-05 dt: 3331.92ms, tok/sec:157352.96
step 15088, loss: 3.190262, norm:0.2974, lr:6.0184e-05 dt: 3332.26ms, tok/sec:157337.23
step 15089, loss: 3.201842, norm:0.2684, lr:6.0182e-05 dt: 3332.13ms, tok/sec:157343.20
step 15090, loss: 3.205981, norm:0.3033, lr:6.0180e-05 dt: 3332.11ms, tok/sec:157344.37
step 15091, loss: 3.185936, norm:0.2931, lr:6.0178e-05 dt: 3331.85ms, tok/sec:157356.48
step 15092, loss: 3.111219, norm:0.2847, lr:6.0176e-05 dt: 3331.95ms, tok/sec:157351.79
step 15093, loss: 3.205488, norm:0.2751, lr:6.0174e-05 dt: 3332.04ms, tok/sec:157347.47
step 15094, loss: 3.178488, norm:0.2787, lr:6.0171e-05 dt: 3331.99ms, tok/sec:157349.91
step 15095, loss: 3.163593, norm:0.2794, lr:6.0169e-05 dt: 3332.06ms, tok/sec:157346.51
step 15096, loss: 3.205398, norm:0.3083, lr:6.0167e-05 dt: 3332.03ms, tok/sec:157347.98
step 15097, loss: 3.146172, norm:0.2648, lr:6.0165e-05 dt: 3332.09ms, tok/sec:157345.11
step 15098, loss: 3.234677, norm:0.2899, lr:6.0163e-05 dt: 3332.09ms, tok/sec:157345.09
step 15099, loss: 3.150819, norm:0.2706, lr:6.0161e-05 dt: 3332.15ms, tok/sec:157342.14
validation loss: 3.1887
Model and optimizer state saved.
HellaSwag accuracy:-2295424375931567023/-2=1147712187965783552.0000
rank 1 sample 0: Hello, I'm a language model, and so I'm interested to see as many other ways to understand the language. I'll write it down, and I
rank 0 sample 0: Hello, I'm a language model, and I think I can write more words now without much trouble. My other favorite language is _______ and this is 
rank 1 sample 1: Hello, I'm a language model, you can see a lot of interesting words such as "hello" and "bye" by people over there.
I
rank 0 sample 1: Hello, I'm a language model, so don't worry if you don't know the "i" sound /p/ sound. I am trying to find
rank 1 sample 2: Hello, I'm a language model, but who can you tell me?
I'm a teacher - you can tell me.
How did I start this
rank 0 sample 2: Hello, I'm a language model, so I hope I see these fun words. They are all words for learning.
It's a fun way of giving
rank 1 sample 3: Hello, I'm a language model, and I'm pretty much right. My teachers are great... It's cool as well :) Now, I'm thinking of
rank 0 sample 3: Hello, I'm a language model, and what I mean is that you have a particular language model that you need for future communication. So your language model in
step 15100, loss: 3.200359, norm:0.2909, lr:6.0159e-05 dt: 56079.79ms, tok/sec:9348.97
step 15101, loss: 3.154133, norm:0.2613, lr:6.0157e-05 dt: 3331.97ms, tok/sec:157350.85
step 15102, loss: 3.200640, norm:0.2628, lr:6.0155e-05 dt: 3331.98ms, tok/sec:157350.24
step 15103, loss: 3.140353, norm:0.2885, lr:6.0153e-05 dt: 3332.29ms, tok/sec:157335.80
step 15104, loss: 3.165823, norm:0.2794, lr:6.0151e-05 dt: 3331.91ms, tok/sec:157353.58
step 15105, loss: 3.151971, norm:0.2711, lr:6.0149e-05 dt: 3331.92ms, tok/sec:157353.01
step 15106, loss: 3.188044, norm:0.2838, lr:6.0147e-05 dt: 3331.99ms, tok/sec:157349.76
step 15107, loss: 3.225633, norm:0.3002, lr:6.0146e-05 dt: 3332.01ms, tok/sec:157348.81
step 15108, loss: 3.128921, norm:0.2717, lr:6.0144e-05 dt: 3332.17ms, tok/sec:157341.42
step 15109, loss: 3.131282, norm:0.2967, lr:6.0142e-05 dt: 3331.84ms, tok/sec:157357.03
step 15110, loss: 3.129147, norm:0.2789, lr:6.0140e-05 dt: 3331.99ms, tok/sec:157349.81
step 15111, loss: 3.126813, norm:0.2810, lr:6.0138e-05 dt: 3332.23ms, tok/sec:157338.54
step 15112, loss: 3.137803, norm:0.2696, lr:6.0136e-05 dt: 3332.16ms, tok/sec:157341.78
step 15113, loss: 3.151508, norm:0.2877, lr:6.0134e-05 dt: 3332.05ms, tok/sec:157347.10
step 15114, loss: 3.091102, norm:0.2722, lr:6.0132e-05 dt: 3331.95ms, tok/sec:157351.79
step 15115, loss: 3.166886, norm:0.2698, lr:6.0131e-05 dt: 3332.10ms, tok/sec:157344.40
step 15116, loss: 3.098485, norm:0.2657, lr:6.0129e-05 dt: 3331.90ms, tok/sec:157353.98
step 15117, loss: 3.116400, norm:0.2554, lr:6.0127e-05 dt: 3331.77ms, tok/sec:157359.99
step 15118, loss: 3.130975, norm:0.2753, lr:6.0125e-05 dt: 3331.99ms, tok/sec:157349.86
step 15119, loss: 3.130162, norm:0.2581, lr:6.0123e-05 dt: 3332.16ms, tok/sec:157341.64
step 15120, loss: 3.150425, norm:0.2727, lr:6.0122e-05 dt: 3331.80ms, tok/sec:157358.65
step 15121, loss: 3.114290, norm:0.2680, lr:6.0120e-05 dt: 3332.49ms, tok/sec:157326.30
step 15122, loss: 3.148484, norm:0.2867, lr:6.0118e-05 dt: 3332.05ms, tok/sec:157346.87
step 15123, loss: 3.181377, norm:0.2786, lr:6.0116e-05 dt: 3332.19ms, tok/sec:157340.50
step 15124, loss: 3.221863, norm:0.2741, lr:6.0115e-05 dt: 3331.94ms, tok/sec:157352.12
step 15125, loss: 3.189852, norm:0.2720, lr:6.0113e-05 dt: 3331.88ms, tok/sec:157354.92
step 15126, loss: 3.269588, norm:0.2841, lr:6.0111e-05 dt: 3331.96ms, tok/sec:157351.11
step 15127, loss: 3.166691, norm:0.2907, lr:6.0110e-05 dt: 3332.30ms, tok/sec:157335.14
step 15128, loss: 3.198080, norm:0.2855, lr:6.0108e-05 dt: 3332.07ms, tok/sec:157346.15
step 15129, loss: 3.124390, norm:0.2678, lr:6.0106e-05 dt: 3331.94ms, tok/sec:157352.32
step 15130, loss: 3.106034, norm:0.2746, lr:6.0105e-05 dt: 3332.30ms, tok/sec:157334.97
step 15131, loss: 3.169289, norm:0.2906, lr:6.0103e-05 dt: 3332.07ms, tok/sec:157345.93
step 15132, loss: 3.213259, norm:0.2839, lr:6.0102e-05 dt: 3331.84ms, tok/sec:157356.67
step 15133, loss: 3.142837, norm:0.2837, lr:6.0100e-05 dt: 3332.12ms, tok/sec:157343.84
step 15134, loss: 3.226593, norm:0.2792, lr:6.0098e-05 dt: 3332.02ms, tok/sec:157348.34
step 15135, loss: 3.168890, norm:0.3255, lr:6.0097e-05 dt: 3331.86ms, tok/sec:157355.95
step 15136, loss: 3.177572, norm:0.2654, lr:6.0095e-05 dt: 3331.86ms, tok/sec:157355.84
step 15137, loss: 3.173075, norm:0.2625, lr:6.0094e-05 dt: 3332.01ms, tok/sec:157349.06
step 15138, loss: 3.133033, norm:0.2761, lr:6.0092e-05 dt: 3331.88ms, tok/sec:157354.95
step 15139, loss: 3.165256, norm:0.2865, lr:6.0091e-05 dt: 3332.26ms, tok/sec:157337.14
step 15140, loss: 3.109121, norm:0.2754, lr:6.0089e-05 dt: 3331.80ms, tok/sec:157358.89
step 15141, loss: 3.181872, norm:0.2671, lr:6.0088e-05 dt: 3332.13ms, tok/sec:157343.39
step 15142, loss: 3.190626, norm:0.3481, lr:6.0086e-05 dt: 3332.14ms, tok/sec:157342.93
step 15143, loss: 3.200212, norm:0.2898, lr:6.0085e-05 dt: 3332.04ms, tok/sec:157347.31
step 15144, loss: 3.106071, norm:0.2829, lr:6.0083e-05 dt: 3331.93ms, tok/sec:157352.63
step 15145, loss: 3.137308, norm:0.2899, lr:6.0082e-05 dt: 3331.90ms, tok/sec:157354.06
step 15146, loss: 3.085455, norm:0.2723, lr:6.0080e-05 dt: 3331.93ms, tok/sec:157352.82
step 15147, loss: 3.114351, norm:0.2936, lr:6.0079e-05 dt: 3332.10ms, tok/sec:157344.66
step 15148, loss: 3.149642, norm:0.2693, lr:6.0078e-05 dt: 3332.24ms, tok/sec:157337.86
step 15149, loss: 3.088943, norm:0.2715, lr:6.0076e-05 dt: 3332.00ms, tok/sec:157349.26
HellaSwag accuracy:-2295424375931567023/-2=1147712187965783552.0000
rank 1 sample 0: Hello, I'm a language model, and so I'm getting my way—to do something better when I'm stuck.” The lesson in the book
rank 1 sample 1: Hello, I'm a language model, which means I've got a good understanding of the subject."
Celley notes (pictured right): "I'm
rank 1 sample 2: Hello, I'm a language model, but am really good at it. I'm a scientist who does science and loves to do science but can't think about
rank 1 sample 3: Hello, I'm a language model, and I'm looking forward to coming soon- I'm reading more about them but I got through one of my posts about
rank 0 sample 0: Hello, I'm a language model, and I love it! The idea is two completely different things: You say the word and you go on saying it with
rank 0 sample 1: Hello, I'm a language model, so how do I do that? Right now we'd like to have lots of people in the country take their language skills
rank 0 sample 2: Hello, I'm a language model, so I didn't mind what I wrote. However, I can now say that I have a lot of fun learning different
rank 0 sample 3: Hello, I'm a language model, and like it when I'm writing I need to know how to translate this language metaphor from my local language to English...
step 15150, loss: 3.116145, norm:0.2769, lr:6.0075e-05 dt: 48385.25ms, tok/sec:10835.70
step 15151, loss: 3.132187, norm:0.3183, lr:6.0073e-05 dt: 3332.32ms, tok/sec:157334.03
step 15152, loss: 3.092203, norm:0.2710, lr:6.0072e-05 dt: 3331.90ms, tok/sec:157354.28
step 15153, loss: 3.103537, norm:0.2637, lr:6.0071e-05 dt: 3331.91ms, tok/sec:157353.47
step 15154, loss: 3.107227, norm:0.2643, lr:6.0069e-05 dt: 3331.93ms, tok/sec:157352.57
step 15155, loss: 3.202248, norm:0.2850, lr:6.0068e-05 dt: 3332.04ms, tok/sec:157347.42
step 15156, loss: 3.088187, norm:0.2651, lr:6.0067e-05 dt: 3332.06ms, tok/sec:157346.68
step 15157, loss: 3.175144, norm:0.2809, lr:6.0066e-05 dt: 3331.94ms, tok/sec:157352.03
step 15158, loss: 3.120675, norm:0.2962, lr:6.0064e-05 dt: 3332.27ms, tok/sec:157336.71
step 15159, loss: 3.174397, norm:0.2908, lr:6.0063e-05 dt: 3331.89ms, tok/sec:157354.46
step 15160, loss: 3.147591, norm:0.2713, lr:6.0062e-05 dt: 3332.35ms, tok/sec:157332.65
step 15161, loss: 3.161917, norm:0.2755, lr:6.0060e-05 dt: 3332.02ms, tok/sec:157348.30
step 15162, loss: 3.178858, norm:0.2867, lr:6.0059e-05 dt: 3332.20ms, tok/sec:157339.86
step 15163, loss: 3.187641, norm:0.3115, lr:6.0058e-05 dt: 3331.94ms, tok/sec:157352.03
step 15164, loss: 3.095943, norm:0.2769, lr:6.0057e-05 dt: 3332.11ms, tok/sec:157343.97
step 15165, loss: 3.193128, norm:0.2867, lr:6.0056e-05 dt: 3331.91ms, tok/sec:157353.69
step 15166, loss: 3.148168, norm:0.2892, lr:6.0054e-05 dt: 3332.03ms, tok/sec:157347.85
step 15167, loss: 3.229358, norm:0.3291, lr:6.0053e-05 dt: 3331.84ms, tok/sec:157356.73
step 15168, loss: 3.120286, norm:0.2877, lr:6.0052e-05 dt: 3332.03ms, tok/sec:157347.81
step 15169, loss: 3.188836, norm:0.2938, lr:6.0051e-05 dt: 3332.22ms, tok/sec:157339.06
step 15170, loss: 3.155674, norm:0.2787, lr:6.0050e-05 dt: 3332.10ms, tok/sec:157344.75
step 15171, loss: 3.145263, norm:0.2668, lr:6.0049e-05 dt: 3332.14ms, tok/sec:157342.73
step 15172, loss: 3.120235, norm:0.2536, lr:6.0048e-05 dt: 3331.98ms, tok/sec:157350.13
step 15173, loss: 3.187107, norm:0.2745, lr:6.0047e-05 dt: 3332.11ms, tok/sec:157344.09
step 15174, loss: 3.162890, norm:0.2631, lr:6.0046e-05 dt: 3332.22ms, tok/sec:157339.02
step 15175, loss: 3.187213, norm:0.2832, lr:6.0044e-05 dt: 3331.85ms, tok/sec:157356.32
step 15176, loss: 3.167650, norm:0.2682, lr:6.0043e-05 dt: 3332.06ms, tok/sec:157346.57
step 15177, loss: 3.163950, norm:0.2733, lr:6.0042e-05 dt: 3332.21ms, tok/sec:157339.32
step 15178, loss: 3.110915, norm:0.2870, lr:6.0041e-05 dt: 3332.06ms, tok/sec:157346.43
step 15179, loss: 3.148734, norm:0.2612, lr:6.0040e-05 dt: 3331.90ms, tok/sec:157353.89
step 15180, loss: 3.155404, norm:0.2774, lr:6.0039e-05 dt: 3331.93ms, tok/sec:157352.62
step 15181, loss: 3.141319, norm:0.2697, lr:6.0038e-05 dt: 3331.90ms, tok/sec:157353.92
step 15182, loss: 3.143785, norm:0.2680, lr:6.0037e-05 dt: 3331.97ms, tok/sec:157350.95
step 15183, loss: 3.163513, norm:0.2632, lr:6.0036e-05 dt: 3331.90ms, tok/sec:157354.11
step 15184, loss: 3.098022, norm:0.2630, lr:6.0035e-05 dt: 3331.98ms, tok/sec:157350.41
step 15185, loss: 3.169111, norm:0.2702, lr:6.0034e-05 dt: 3332.03ms, tok/sec:157347.91
step 15186, loss: 3.108735, norm:0.2712, lr:6.0034e-05 dt: 3332.21ms, tok/sec:157339.54
step 15187, loss: 3.167982, norm:0.2792, lr:6.0033e-05 dt: 3332.00ms, tok/sec:157349.53
step 15188, loss: 3.135395, norm:0.2611, lr:6.0032e-05 dt: 3332.10ms, tok/sec:157344.79
step 15189, loss: 3.140849, norm:0.2693, lr:6.0031e-05 dt: 3331.85ms, tok/sec:157356.42
step 15190, loss: 3.123194, norm:0.2711, lr:6.0030e-05 dt: 3332.05ms, tok/sec:157346.82
step 15191, loss: 3.164052, norm:0.2812, lr:6.0029e-05 dt: 3331.88ms, tok/sec:157354.93
step 15192, loss: 3.135130, norm:0.2699, lr:6.0028e-05 dt: 3331.87ms, tok/sec:157355.50
step 15193, loss: 3.229432, norm:0.2907, lr:6.0027e-05 dt: 3332.03ms, tok/sec:157348.08
step 15194, loss: 3.166305, norm:0.2788, lr:6.0027e-05 dt: 3331.78ms, tok/sec:157359.96
step 15195, loss: 3.147386, norm:0.2911, lr:6.0026e-05 dt: 3332.31ms, tok/sec:157334.73
step 15196, loss: 3.209981, norm:0.2919, lr:6.0025e-05 dt: 3332.09ms, tok/sec:157345.14
step 15197, loss: 3.133044, norm:0.2937, lr:6.0024e-05 dt: 3332.10ms, tok/sec:157344.80
step 15198, loss: 3.202111, norm:0.2931, lr:6.0023e-05 dt: 3332.12ms, tok/sec:157343.88
step 15199, loss: 3.223402, norm:0.2824, lr:6.0023e-05 dt: 3332.01ms, tok/sec:157349.03
validation loss: 3.1888
Model and optimizer state saved.
HellaSwag accuracy:-2286417176676826031/-2=1143208588338413056.0000
rank 1 sample 0: Hello, I'm a language model, and this is the question that came under this one question. One of the people who came over in the field was a
rank 1 sample 1: Hello, I'm a language model, which means I've got a good understanding of the basic terms and terminology. You'll even learn how to use the language
rank 1 sample 2: Hello, I'm a language model, but sometimes the code is not.
I'm sorry that your problem has been solved, so don't hesitate - Thanks
rank 1 sample 3: Hello, I'm a language model, and I'm writing this with my mind back to the programming language JREV. Because no programming language is available,
rank 0 sample 0: Hello, I'm a language model, and I love to use languages to get stuffs for me. When I do a good job and have an idea how
rank 0 sample 1: Hello, I'm a language model, so a lot of my first languages never really started on these foundations. Here I'll try to show you what I think
rank 0 sample 2: Hello, I'm a language model, so I learned the ins and outs of the web, and learned about the different types of languages. I'm also passionate
rank 0 sample 3: Hello, I'm a language model, and now I have a few questions: How did you get this word into English? Why has this word come out first
step 15200, loss: 3.162854, norm:0.3055, lr:6.0022e-05 dt: 56371.05ms, tok/sec:9300.66
step 15201, loss: 3.154906, norm:0.2796, lr:6.0021e-05 dt: 3332.24ms, tok/sec:157337.82
step 15202, loss: 3.147431, norm:0.2802, lr:6.0020e-05 dt: 3331.80ms, tok/sec:157358.77
step 15203, loss: 3.192786, norm:0.3135, lr:6.0020e-05 dt: 3332.04ms, tok/sec:157347.29
step 15204, loss: 3.175212, norm:0.2871, lr:6.0019e-05 dt: 3332.06ms, tok/sec:157346.58
step 15205, loss: 3.125229, norm:0.2920, lr:6.0018e-05 dt: 3332.17ms, tok/sec:157341.17
step 15206, loss: 3.164035, norm:0.2943, lr:6.0018e-05 dt: 3331.98ms, tok/sec:157350.22
step 15207, loss: 3.208407, norm:0.2677, lr:6.0017e-05 dt: 3332.06ms, tok/sec:157346.42
step 15208, loss: 3.214911, norm:0.2727, lr:6.0016e-05 dt: 3332.32ms, tok/sec:157334.19
step 15209, loss: 3.193949, norm:0.3448, lr:6.0016e-05 dt: 3331.94ms, tok/sec:157352.07
step 15210, loss: 3.168034, norm:0.2659, lr:6.0015e-05 dt: 3331.90ms, tok/sec:157354.27
step 15211, loss: 3.126297, norm:0.2718, lr:6.0015e-05 dt: 3331.92ms, tok/sec:157353.31
step 15212, loss: 3.182720, norm:0.2667, lr:6.0014e-05 dt: 3331.99ms, tok/sec:157349.64
step 15213, loss: 3.138585, norm:0.2762, lr:6.0013e-05 dt: 3332.25ms, tok/sec:157337.48
step 15214, loss: 3.172682, norm:0.2609, lr:6.0013e-05 dt: 3332.18ms, tok/sec:157340.81
step 15215, loss: 3.095726, norm:0.2899, lr:6.0012e-05 dt: 3331.95ms, tok/sec:157351.51
step 15216, loss: 3.095705, norm:0.2766, lr:6.0012e-05 dt: 3332.00ms, tok/sec:157349.51
step 15217, loss: 3.138261, norm:0.2962, lr:6.0011e-05 dt: 3332.27ms, tok/sec:157336.67
step 15218, loss: 3.110815, norm:0.2520, lr:6.0011e-05 dt: 3331.85ms, tok/sec:157356.38
step 15219, loss: 3.189318, norm:0.3346, lr:6.0010e-05 dt: 3331.96ms, tok/sec:157351.22
step 15220, loss: 3.126847, norm:0.2840, lr:6.0010e-05 dt: 3332.06ms, tok/sec:157346.32
step 15221, loss: 3.118686, norm:0.2769, lr:6.0009e-05 dt: 3332.19ms, tok/sec:157340.33
step 15222, loss: 3.091616, norm:0.2809, lr:6.0009e-05 dt: 3331.95ms, tok/sec:157351.56
step 15223, loss: 3.119230, norm:0.2694, lr:6.0008e-05 dt: 3332.03ms, tok/sec:157347.82
step 15224, loss: 3.095044, norm:0.2706, lr:6.0008e-05 dt: 3332.14ms, tok/sec:157342.85
step 15225, loss: 3.154531, norm:0.2548, lr:6.0007e-05 dt: 3332.19ms, tok/sec:157340.35
step 15226, loss: 3.138507, norm:0.2781, lr:6.0007e-05 dt: 3332.02ms, tok/sec:157348.36
step 15227, loss: 3.167068, norm:0.3006, lr:6.0006e-05 dt: 3331.93ms, tok/sec:157352.49
step 15228, loss: 3.145784, norm:0.2994, lr:6.0006e-05 dt: 3332.07ms, tok/sec:157346.03
step 15229, loss: 3.146670, norm:0.2737, lr:6.0006e-05 dt: 3331.95ms, tok/sec:157351.77
step 15230, loss: 3.112682, norm:0.2830, lr:6.0005e-05 dt: 3331.92ms, tok/sec:157352.94
step 15231, loss: 3.162582, norm:0.2882, lr:6.0005e-05 dt: 3332.04ms, tok/sec:157347.41
step 15232, loss: 3.169566, norm:0.2764, lr:6.0005e-05 dt: 3332.02ms, tok/sec:157348.52
step 15233, loss: 3.211771, norm:0.2866, lr:6.0004e-05 dt: 3332.11ms, tok/sec:157344.22
step 15234, loss: 3.139781, norm:0.2905, lr:6.0004e-05 dt: 3332.12ms, tok/sec:157343.60
step 15235, loss: 3.135950, norm:0.2796, lr:6.0004e-05 dt: 3331.97ms, tok/sec:157350.86
step 15236, loss: 3.183267, norm:0.2733, lr:6.0003e-05 dt: 3332.05ms, tok/sec:157347.11
step 15237, loss: 3.207719, norm:0.2893, lr:6.0003e-05 dt: 3332.10ms, tok/sec:157344.76
step 15238, loss: 3.108376, norm:0.2716, lr:6.0003e-05 dt: 3332.02ms, tok/sec:157348.57
step 15239, loss: 3.109729, norm:0.2788, lr:6.0003e-05 dt: 3334.11ms, tok/sec:157249.78
step 15240, loss: 3.170031, norm:0.2806, lr:6.0002e-05 dt: 3331.91ms, tok/sec:157353.70
step 15241, loss: 3.249657, norm:0.2972, lr:6.0002e-05 dt: 3332.03ms, tok/sec:157348.07
step 15242, loss: 3.221191, norm:0.2766, lr:6.0002e-05 dt: 3331.98ms, tok/sec:157350.42
step 15243, loss: 3.313396, norm:0.2978, lr:6.0002e-05 dt: 3332.08ms, tok/sec:157345.49
step 15244, loss: 3.204990, norm:0.2964, lr:6.0001e-05 dt: 3331.87ms, tok/sec:157355.25
step 15245, loss: 3.247397, norm:0.2763, lr:6.0001e-05 dt: 3332.15ms, tok/sec:157342.13
step 15246, loss: 3.313834, norm:0.2938, lr:6.0001e-05 dt: 3332.08ms, tok/sec:157345.44
step 15247, loss: 3.232864, norm:0.2759, lr:6.0001e-05 dt: 3331.86ms, tok/sec:157355.75
step 15248, loss: 3.293336, norm:0.2948, lr:6.0001e-05 dt: 3332.00ms, tok/sec:157349.49
step 15249, loss: 3.208326, norm:0.2889, lr:6.0001e-05 dt: 3332.25ms, tok/sec:157337.41
HellaSwag accuracy:-2295424375931567023/-2=1147712187965783552.0000
rank 1 sample 0: Hello, I'm a language model, and this is the real question. However it turns out that they are not saying that it is wrong to speak English.
rank 1 sample 1: Hello, I'm a language model, which means I've been learning the language quite a bit here. However, when you type in your own language, you
rank 1 sample 2: Hello, I'm a language model, but some languages like Java and C++ are not. The more languages that are used in Java today, I'll do
rank 1 sample 3: Hello, I'm a language model, and I'm really happy the process started very well. Please be supportive in letting me continue - you're a language model
rank 0 sample 0: Hello, I'm a language model, and I love it! And so here's to me, my daughter, my daughter! Thanks so much for sharing with
rank 0 sample 1: Hello, I'm a language model, so don't worry if you don't know the terminology here, it's the language that will be going to be your
rank 0 sample 2: Hello, I'm a language model, so I hope I teach.
How do children learn to be more intelligent?
As children, we are learning how
rank 0 sample 3: Hello, I'm a language model, and when I do, I get the same problems.
|Look at that line that says, "Don't just
step 15250, loss: 3.230252, norm:0.2770, lr:6.0001e-05 dt: 48368.32ms, tok/sec:10839.49
step 15251, loss: 3.206639, norm:0.2560, lr:6.0000e-05 dt: 3331.97ms, tok/sec:157350.81
step 15252, loss: 3.191398, norm:0.2751, lr:6.0000e-05 dt: 3332.01ms, tok/sec:157348.88
step 15253, loss: 3.163309, norm:0.2822, lr:6.0000e-05 dt: 3331.93ms, tok/sec:157352.63
step 15254, loss: 3.203936, norm:0.2849, lr:6.0000e-05 dt: 3331.85ms, tok/sec:157356.65
step 15255, loss: 3.268697, norm:0.2880, lr:6.0000e-05 dt: 3332.16ms, tok/sec:157341.92
step 15256, loss: 3.163452, norm:0.2847, lr:6.0000e-05 dt: 3331.97ms, tok/sec:157350.88
step 15257, loss: 3.218566, norm:0.2825, lr:6.0000e-05 dt: 3331.77ms, tok/sec:157360.37
validation loss: 3.1863
Model and optimizer state saved.
HellaSwag accuracy:-2295565113419922351/-2=1147782556709961216.0000
step 15258, loss: 3.211083, norm:0.3555, lr:6.0000e-05 dt: 56078.52ms, tok/sec:9349.18


PS:

Read file <anurag_torch_run.err> for stderr output of this job.

