{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-07-31 15:17:45--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt’\n",
      "\n",
      "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.08s   \n",
      "\n",
      "2023-07-31 15:17:45 (13.2 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read it in to inspect it\n",
    "with open('./dataset/input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's look at the first 1000 characters\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = {ch :i for i, ch in enumerate(chars)}\n",
    "itos = {i :ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s : [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join(itos[i] for i in l) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode('hii there')) \n",
    "print(decode(encode('hii there')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we are gonna take arbitary chucks of this data and train it. This chuck is called block size. We need to do this cause its computationally prohibitive to take and train all the data at once. \n",
    "# lets look at the train data. \n",
    "block_size = 8\n",
    "train_data[:block_size+1]\n",
    "\n",
    "# we take block size + 1 cause there are 8 individual examples packed in there. what do you mean by that?\n",
    "# In the context of 18, 47 comes next.\n",
    "# In the context of 18 and 47, 56 comes next. \n",
    "# In the context of 18, 47 and 56, 57 comes next and so on. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]), target is 47\n",
      "when input is tensor([18, 47]), target is 56\n",
      "when input is tensor([18, 47, 56]), target is 57\n",
      "when input is tensor([18, 47, 56, 57]), target is 58\n",
      "when input is tensor([18, 47, 56, 57, 58]), target is 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]), target is 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]), target is 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), target is 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context}, target is {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we train on all the above 8 examples from context 1 to all the way up to block size is because we want the transformer network used to seeing context from as little as one to all the way up to block size. \n",
    "\n",
    "# This is also useful while inferencing, when transformer can start predicting from 1 to block size and after that we start truncating as it can never recieve more than block size context. (not super clear?)\n",
    "\n",
    "# Thus we have looked at the time dimension of the tensors that are going to be fed. \n",
    "# There is one more dimension to care about and that is batch dimension. \n",
    "# When we are processing these chunks, we want to stack multiple chunks in mini batch in a single tensor. This is done purely for computational efficiency reasons. Gpus are good at parallel processing and hence mini batches. Each of these batches are processed independently and they dont talk to each other. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----\n",
      "when input is [24] the target: 43\n",
      "when input is [24, 43] the target: 58\n",
      "when input is [24, 43, 58] the target: 5\n",
      "when input is [24, 43, 58, 5] the target: 57\n",
      "when input is [24, 43, 58, 5, 57] the target: 1\n",
      "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
      "when input is [44] the target: 53\n",
      "when input is [44, 53] the target: 56\n",
      "when input is [44, 53, 56] the target: 1\n",
      "when input is [44, 53, 56, 1] the target: 58\n",
      "when input is [44, 53, 56, 1, 58] the target: 46\n",
      "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52] the target: 58\n",
      "when input is [52, 58] the target: 1\n",
      "when input is [52, 58, 1] the target: 58\n",
      "when input is [52, 58, 1, 58] the target: 46\n",
      "when input is [52, 58, 1, 58, 46] the target: 39\n",
      "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
      "when input is [25] the target: 17\n",
      "when input is [25, 17] the target: 27\n",
      "when input is [25, 17, 27] the target: 10\n",
      "when input is [25, 17, 27, 10] the target: 0\n",
      "when input is [25, 17, 27, 10, 0] the target: 21\n",
      "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
     ]
    }
   ],
   "source": [
    "#This is how one batch is created and looks like\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split== 'train' else val_data\n",
    "\n",
    "    ix = torch.randint(len(data)-block_size, (batch_size,))  # generate batch size number of random numbers between 0 and len(data)-block_size\n",
    "\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "\n",
    "# SO this 4 * 8 array contains a total of 32 examples!!!! \n",
    "# And they are completely independent as far as the transformer is concered.\n",
    "# these examples are \n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/var/folders/lr/1stpmkw94f1497nf96vdnf_m0000gn/T/ipykernel_1532/3520260071.py\u001b[0m(41)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     39 \u001b[0;31m        \u001b[0;31m# T = block size = context lenght = time dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     40 \u001b[0;31m        \u001b[0;31m# idx and targets are both (B,T) tensor of integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 41 \u001b[0;31m        \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_embedding_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B,T,C) # it plucks out that particular row for index 24 for eg i.e similar to  [24, :]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     42 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     43 \u001b[0;31m        \u001b[0;31m# logits are the name given to the representation of the set of predicted possible characters for the next character in the 65x65 table.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "torch.Size([32, 8])\n",
      "tensor([[ 1, 57, 59, 52,  6,  0, 31, 53],\n",
      "        [58, 46, 43,  1, 41, 47, 58, 63],\n",
      "        [59,  1, 46, 39, 57, 58,  1, 39],\n",
      "        [60, 43, 52, 45, 43, 57,  1, 58],\n",
      "        [43, 39, 56, 58, 46,  6,  0, 32],\n",
      "        [53, 50, 43, 56,  6,  1, 58, 46],\n",
      "        [ 1, 41, 39, 58, 41, 46,  1, 43],\n",
      "        [53, 52,  1, 51, 43,  1, 44, 53],\n",
      "        [ 1, 61, 47, 58, 46,  1, 54, 43],\n",
      "        [39, 40, 50, 43,  1, 53, 44,  1],\n",
      "        [39, 58,  1, 61, 43,  1, 40, 53],\n",
      "        [59, 41, 46,  1, 41, 53, 51, 44],\n",
      "        [21, 26, 19,  1, 30, 21, 15, 20],\n",
      "        [43, 58,  1, 57, 53, 59, 52, 42],\n",
      "        [45, 47, 60, 43,  1, 51, 43,  1],\n",
      "        [57,  1, 39, 40, 56, 53, 39, 42],\n",
      "        [46,  1, 50, 53, 57, 58,  1, 51],\n",
      "        [56,  6,  1, 40, 59, 58,  1, 47],\n",
      "        [43, 43, 50, 63,  1, 57, 54, 43],\n",
      "        [50,  5, 57,  6,  1, 41, 56, 47],\n",
      "        [51, 43, 58, 46, 47, 52, 45,  1],\n",
      "        [ 1, 46, 39, 52, 45, 43, 42,  1],\n",
      "        [53, 59,  1, 57, 58, 47, 44, 44],\n",
      "        [58,  0, 24, 53, 53, 49,  5, 42],\n",
      "        [10,  0, 21,  1, 61, 53, 59, 50],\n",
      "        [49, 43,  1, 47, 52, 58, 43, 56],\n",
      "        [41, 53, 51, 43,  6,  1, 21,  1],\n",
      "        [53,  5, 58,  1, 39,  1, 44, 56],\n",
      "        [42,  1, 58, 46, 59, 57,  1, 51],\n",
      "        [ 1, 58, 46, 43, 51, 10,  0, 35],\n",
      "        [52, 12,  0,  0, 16, 33, 23, 17],\n",
      "        [ 1, 47, 57,  1, 61, 53, 56, 58]])\n",
      "tensor([[-4.4243, -4.0743, -5.2462,  ..., -6.6460,  0.6420, -3.1769],\n",
      "        [ 0.1757,  3.0287, -0.8288,  ..., -5.1324, -1.9434, -3.5116],\n",
      "        [-1.5882,  1.7786, -2.2414,  ..., -3.5229, -2.4633, -3.3658],\n",
      "        ...,\n",
      "        [-2.1399,  1.7879, -2.4789,  ..., -3.7586, -1.1538, -3.2055],\n",
      "        [-0.6185,  2.5633, -1.6494,  ..., -4.9363,  0.3981, -4.6458],\n",
      "        [-0.2461,  2.7756, -1.6779,  ..., -3.7978, -0.0755, -4.5221]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch.Size([256, 65])\n",
      "tensor([[[ 0.5978, -0.0514, -0.0646,  ..., -1.4649, -2.0555,  1.8275],\n",
      "         [-0.5201,  0.2831,  1.0847,  ..., -0.0198,  0.7959,  1.6014],\n",
      "         [-0.4002,  0.3302,  1.5454,  ...,  1.3688,  0.4620,  0.2040],\n",
      "         ...,\n",
      "         [ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
      "         [-1.0699, -0.6119, -0.4034,  ...,  1.0078,  0.2930,  0.0943],\n",
      "         [-0.1324, -0.5489,  0.1024,  ..., -0.8599, -1.6050, -0.6985]],\n",
      "\n",
      "        [[ 0.2475, -0.6349, -1.2909,  ...,  1.3064, -0.2256, -1.8305],\n",
      "         [ 1.0901,  0.2170, -2.9996,  ..., -0.5472, -0.8017,  0.7761],\n",
      "         [ 0.3323, -0.0872, -0.7470,  ..., -0.6716, -0.9572, -0.9594],\n",
      "         ...,\n",
      "         [ 1.6515, -0.0424, -0.7355,  ...,  0.8682,  2.0593, -0.8159],\n",
      "         [ 0.2475, -0.6349, -1.2909,  ...,  1.3064, -0.2256, -1.8305],\n",
      "         [-0.8109,  0.2410, -0.1139,  ...,  1.4509,  0.1836,  0.3064]],\n",
      "\n",
      "        [[-0.4002,  0.3302,  1.5454,  ...,  1.3688,  0.4620,  0.2040],\n",
      "         [ 0.5978, -0.0514, -0.0646,  ..., -1.4649, -2.0555,  1.8275],\n",
      "         [ 1.0901,  0.2170, -2.9996,  ..., -0.5472, -0.8017,  0.7761],\n",
      "         ...,\n",
      "         [ 0.2475, -0.6349, -1.2909,  ...,  1.3064, -0.2256, -1.8305],\n",
      "         [ 0.5978, -0.0514, -0.0646,  ..., -1.4649, -2.0555,  1.8275],\n",
      "         [ 1.1513,  1.0539,  3.4105,  ..., -0.5686,  0.9079, -0.1701]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.5978, -0.0514, -0.0646,  ..., -1.4649, -2.0555,  1.8275],\n",
      "         [ 0.2475, -0.6349, -1.2909,  ...,  1.3064, -0.2256, -1.8305],\n",
      "         [ 1.0901,  0.2170, -2.9996,  ..., -0.5472, -0.8017,  0.7761],\n",
      "         ...,\n",
      "         [-1.3441, -0.2827, -0.6887,  ...,  1.4385, -1.3166,  1.2690],\n",
      "         [ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
      "         [ 0.7681, -0.0430,  0.1073,  ..., -0.6499,  0.6144,  0.1669]],\n",
      "\n",
      "        [[-0.2103,  0.4481,  1.2381,  ...,  1.3597, -0.0821,  0.3909],\n",
      "         [-2.0333, -0.5538,  0.1696,  ...,  0.0096,  1.1159, -0.2966],\n",
      "         [ 0.1808, -0.0700, -0.3596,  ...,  1.6097, -0.4032, -0.8345],\n",
      "         ...,\n",
      "         [-1.4731,  0.8040, -0.1425,  ...,  0.7180,  0.3684,  2.2792],\n",
      "         [-1.5766,  0.6679, -1.0362,  ..., -1.2814, -0.4783,  0.2979],\n",
      "         [-0.4892, -2.5589,  1.4134,  ..., -1.4296,  0.2347, -1.2034]],\n",
      "\n",
      "        [[ 0.5978, -0.0514, -0.0646,  ..., -1.4649, -2.0555,  1.8275],\n",
      "         [ 1.6515, -0.0424, -0.7355,  ...,  0.8682,  2.0593, -0.8159],\n",
      "         [-0.5201,  0.2831,  1.0847,  ..., -0.0198,  0.7959,  1.6014],\n",
      "         ...,\n",
      "         [-0.1324, -0.5489,  0.1024,  ..., -0.8599, -1.6050, -0.6985],\n",
      "         [-0.6722,  0.2322, -0.1632,  ...,  0.1390,  0.7560,  0.4296],\n",
      "         [ 0.2475, -0.6349, -1.2909,  ...,  1.3064, -0.2256, -1.8305]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([32, 8, 65])\n",
      "torch.Size([8, 65])\n",
      "tensor([[ 0.5978, -0.0514, -0.0646, -0.4970,  0.4658, -0.2573, -1.0673,  2.0089,\n",
      "         -0.5370,  0.2228,  0.6971, -1.4267,  0.9059,  0.1446,  0.2280,  2.4900,\n",
      "         -1.2237,  1.0107,  0.5560, -1.5935, -1.2706,  0.6903, -0.1961,  0.3449,\n",
      "         -0.3419,  0.4759, -0.7663, -0.4190, -0.4370, -1.0012, -0.4094, -1.6669,\n",
      "         -1.3651, -0.1655,  0.9623,  0.0315, -0.7419, -0.2978,  0.0172, -0.1772,\n",
      "         -0.1334,  0.2940,  1.3850,  0.1209,  2.5418, -0.6405, -1.9740, -0.3296,\n",
      "          0.0080,  0.9262, -1.8846,  0.1670,  0.4586, -1.7662,  0.5860,  1.7510,\n",
      "          0.2807,  0.3110, -0.6538, -0.6576,  0.3184, -0.5496, -1.4649, -2.0555,\n",
      "          1.8275],\n",
      "        [-0.5201,  0.2831,  1.0847,  1.9905,  0.7763, -0.8460,  0.8437,  0.7905,\n",
      "         -0.5287, -0.1187,  0.6618, -0.6682, -1.8731,  0.7459,  2.1471,  1.0535,\n",
      "         -0.7480,  2.0704, -1.1879, -0.7858,  0.1276, -0.9183,  0.5782, -1.7134,\n",
      "         -1.2302, -0.4149, -0.9652, -0.9685, -0.2536, -1.0255, -0.9492, -0.1503,\n",
      "          0.4905, -1.1986,  1.0955, -0.5802,  0.0199, -2.0645, -0.0617, -0.4054,\n",
      "         -0.7169,  0.9026, -0.3288, -0.2391, -1.0618, -0.1223, -1.4403,  0.8433,\n",
      "         -0.7001,  0.9611,  0.8550,  0.4062, -2.2157, -0.3732, -0.6900,  0.4235,\n",
      "          2.6768,  1.0813,  0.6548,  1.9577,  0.1433, -0.0627, -0.0198,  0.7959,\n",
      "          1.6014],\n",
      "        [-0.4002,  0.3302,  1.5454,  1.3778, -1.2796, -0.6325, -1.3849,  0.7045,\n",
      "         -1.6437,  1.0897, -1.6458, -0.1595, -0.2041,  0.7659,  1.6846,  2.2841,\n",
      "          1.8561,  0.8824,  1.0156,  0.1238,  0.4307, -1.0270,  0.1920, -1.3400,\n",
      "          0.1898,  0.8872, -1.7611,  0.0790, -0.3927,  1.3377,  0.7389, -0.2104,\n",
      "         -1.3679,  0.7841, -1.6600,  0.1206, -0.4013, -0.3876, -0.2081, -0.8811,\n",
      "          0.6302, -2.0576, -0.3281,  0.4574,  1.0301, -0.2498, -1.4098,  0.1187,\n",
      "          0.9454,  0.8497,  1.7821,  0.8213, -0.4068,  0.1002,  0.6703,  0.2991,\n",
      "         -1.1020,  1.0575, -1.8988, -0.7949,  0.8804,  0.6107,  1.3688,  0.4620,\n",
      "          0.2040],\n",
      "        [-0.2103,  0.4481,  1.2381,  0.6091,  0.4209, -1.3998, -0.4007, -0.1462,\n",
      "         -1.1387, -0.0134, -1.9390,  0.6582,  0.6734,  0.7523, -0.7533, -0.1611,\n",
      "          0.8354,  1.7862, -0.7115,  0.2381, -1.2085,  0.0717,  0.1532, -2.2039,\n",
      "          0.6705,  0.2791, -0.2735,  0.3476, -0.2101,  2.2659,  1.0809, -0.4287,\n",
      "         -0.2424, -0.2258, -0.3756,  0.3991,  0.7816,  1.2656,  1.3015,  0.1370,\n",
      "          0.9264, -1.1001, -1.0641,  0.6243, -0.2436,  0.1079,  0.4256,  0.1838,\n",
      "         -0.1147,  1.7112,  1.2666,  0.7957,  0.9475,  1.3239,  2.0084,  1.4341,\n",
      "         -0.4606, -0.0487, -1.1202,  1.3071,  0.0508,  0.2770,  1.3597, -0.0821,\n",
      "          0.3909],\n",
      "        [ 0.4160,  0.3362, -0.4512, -0.6996,  0.9208, -0.9963,  1.2962, -2.2434,\n",
      "          0.5272, -0.1585,  0.0933,  0.0697, -1.1470, -1.0414, -0.2572, -0.7223,\n",
      "          0.1643, -1.3590,  0.9622, -0.7641, -1.7653,  0.6884, -0.2245,  0.2468,\n",
      "          0.1747,  0.5243,  0.3091,  1.1661, -2.1821, -1.0422,  1.0207,  3.2082,\n",
      "         -3.7624, -0.5330,  0.6630, -1.5717, -0.5622, -0.2964,  0.5512, -1.2364,\n",
      "          0.9409,  0.7608, -1.3756,  1.2168,  0.0268, -2.1902, -1.1730,  2.5181,\n",
      "          1.6212, -1.8134,  2.0867,  0.1535,  0.1135, -0.1979,  1.6621,  0.6151,\n",
      "          0.6763,  0.6228,  0.0943, -0.3156,  0.7850, -0.8699, -1.6525, -0.8816,\n",
      "         -1.4546],\n",
      "        [ 0.1808, -0.0700, -0.3596, -0.9152,  0.6258,  0.0255,  0.9545,  0.0643,\n",
      "          0.3612,  1.1679, -1.3499, -0.5102,  0.2360, -0.2398, -0.9211,  1.5433,\n",
      "          1.3488, -0.1396,  0.2858,  0.9651, -2.0371,  0.4931,  1.4870,  0.5910,\n",
      "          0.1260, -1.5627, -1.1601, -0.3348,  0.4478, -0.8016,  1.5236,  2.5086,\n",
      "         -0.6631, -0.2513,  1.0101,  0.1215,  0.1584,  1.1340, -1.1539, -0.2984,\n",
      "         -0.5075, -0.9239,  0.5467, -1.4948, -1.2057,  0.5718, -0.5974, -0.6937,\n",
      "          1.6455, -0.8030,  1.3514, -0.2759, -1.5108,  2.1048,  2.7630, -1.7465,\n",
      "          1.4516, -1.5103,  0.8212, -0.2115,  0.7789,  1.5333,  1.6097, -0.4032,\n",
      "         -0.8345],\n",
      "        [-1.0699, -0.6119, -0.4034,  0.3025,  0.6852, -1.0045, -1.0104, -1.0886,\n",
      "          1.3292,  0.5912, -1.1082, -1.2869, -0.8170,  0.9682,  1.6030, -0.0726,\n",
      "         -0.4725, -1.1616,  0.5962,  1.3058, -0.7422, -1.2529,  0.6750,  1.5664,\n",
      "         -0.9238, -0.0956, -1.5452, -0.1801,  3.1838, -0.1277,  0.0910,  0.5422,\n",
      "         -0.6110,  0.5220,  2.1368, -1.4166, -0.8557,  1.0129,  0.6503,  0.2432,\n",
      "          1.2588, -0.0644, -0.9707, -0.4880, -0.2550, -0.4089, -0.7687,  1.0953,\n",
      "          1.5294, -1.2395,  1.0547,  0.5108,  0.3854, -0.8898,  1.3468,  2.3590,\n",
      "          0.1071, -1.2616,  0.7945, -0.7739, -0.1497, -0.6214,  1.0078,  0.2930,\n",
      "          0.0943],\n",
      "        [-0.1324, -0.5489,  0.1024, -0.6916,  0.3507,  1.6147,  1.8203,  0.5122,\n",
      "          1.5810, -2.0063, -1.2925,  0.1268,  1.1099, -0.6592,  0.8084,  1.9072,\n",
      "         -0.3260, -0.3438, -1.4415, -0.1828, -0.8804, -0.6192, -1.4047, -0.8584,\n",
      "         -0.3830, -0.5372, -1.2176, -1.9403, -0.3094,  0.1790,  1.2859,  0.3039,\n",
      "          1.8110,  0.6350, -0.0820, -2.1208,  1.2516, -0.6826,  0.3838,  0.0150,\n",
      "         -0.2801,  1.4896, -0.4646, -1.9210, -0.1062,  1.0614,  0.9308,  3.1170,\n",
      "         -1.5428, -2.2848,  0.5755, -0.8040,  0.8010,  0.0088, -0.4751, -0.9630,\n",
      "         -0.5078,  0.1018,  1.9141, -1.9252, -1.5554, -0.1878, -0.8599, -1.6050,\n",
      "         -0.6985]], grad_fn=<SelectBackward0>)\n",
      "tensor([ 0.5978, -0.0514, -0.0646, -0.4970,  0.4658, -0.2573, -1.0673,  2.0089,\n",
      "        -0.5370,  0.2228,  0.6971, -1.4267,  0.9059,  0.1446,  0.2280,  2.4900,\n",
      "        -1.2237,  1.0107,  0.5560, -1.5935, -1.2706,  0.6903, -0.1961,  0.3449,\n",
      "        -0.3419,  0.4759, -0.7663, -0.4190, -0.4370, -1.0012, -0.4094, -1.6669,\n",
      "        -1.3651, -0.1655,  0.9623,  0.0315, -0.7419, -0.2978,  0.0172, -0.1772,\n",
      "        -0.1334,  0.2940,  1.3850,  0.1209,  2.5418, -0.6405, -1.9740, -0.3296,\n",
      "         0.0080,  0.9262, -1.8846,  0.1670,  0.4586, -1.7662,  0.5860,  1.7510,\n",
      "         0.2807,  0.3110, -0.6538, -0.6576,  0.3184, -0.5496, -1.4649, -2.0555,\n",
      "         1.8275], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__ (self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size\n",
    "                                                  )\n",
    "    \n",
    "        #embedding is a thin wrapper around the tensor which creates vocab_size, vocab_size tensor. \n",
    "\n",
    "        # Creating an embedding layer for 10 unique items, each represented by 5-dimensional embeddings\n",
    "        # embedding_layer = nn.Embedding(num_embeddings=10, embedding_dim=5)\n",
    "\n",
    "        # In the above example, you could indeed use a regular tensor of size (10, 5) to represent embeddings for 10 unique items, and there would not be a functional difference in this specific scenario. However, using the nn.Embedding layer has certain advantages and is more common in practice when working with embeddings for categorical data. Let's explore the reasons:\n",
    "\n",
    "        # Memory Efficiency: When dealing with large datasets or vocabularies, using an nn.Embedding layer can be more memory-efficient. The embedding layer only stores the embeddings for the unique items (based on the number of unique indices), while a regular tensor of size (10, 5) would allocate memory for all elements, including those that might not correspond to any actual item.\n",
    "\n",
    "        # Computational Efficiency: The nn.Embedding layer has optimized implementations for efficient indexing and lookup operations. When you pass a batch of input indices to the embedding layer, it efficiently retrieves the corresponding embeddings. This optimized implementation is especially beneficial when working with large-scale models and datasets.\n",
    "\n",
    "        # Flexibility: The nn.Embedding layer is designed to be integrated seamlessly with other layers in PyTorch's neural network modules (nn.Module). It allows you to easily train and update the embeddings during the learning process, making it convenient for end-to-end training.\n",
    "\n",
    "        # Integration with Embedding Lookup: When using nn.Embedding, you can utilize the torch.nn.functional.embedding function, which provides efficient embedding lookup capabilities. This function is particularly useful when you need to perform lookups across multiple indices simultaneously, such as in recurrent neural networks (RNNs) or transformer models.\n",
    "\n",
    "        # However, if your use case involves a small number of unique items, and memory or computational efficiency is not a concern, you can still represent embeddings using regular tensors. The decision between using a regular tensor and an nn.Embedding layer depends on the specific requirements of your model, the size of your vocabulary, and your computational resources. For larger-scale applications, nn.Embedding is generally preferred due to its efficiency and convenience.\n",
    "\n",
    "\n",
    "         #bigram model is just learning this table. given a word what the probablity of the next word. No context is considered. \n",
    "\n",
    "    def forward(self, idx, targets = None):\n",
    "\n",
    "        # In bigram model the probability of the next word is captured in the table. Hence just simply look up that value with the index of next word.\n",
    "        # C = Channel = embedding size = vocab size\n",
    "        # B = Batch size = no of independent sequences that will be processed in parallel\n",
    "        # T = block size = context lenght = time dimension\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C) # it plucks out that particular row for index 24 for eg i.e similar to  [24, :]\n",
    "\n",
    "        # logits are the name given to the representation of the set of predicted possible characters for the next character in the 65x65 table. \n",
    "        # logits usually refer to the raw output of a neural network's final layer before the application of an activation function to produce probabilities.\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            # cross entropy expects it in different shape. \n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            # cross entropy is just log likelihood loss. \n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            # internal converts logits to probabilities for the next character and applies the loss function. \n",
    "            # Next, the loss is computed as the negative log-likelihood loss. For each token in the batch, the cross-entropy loss measures the difference between the predicted probabilities and the true next word's one-hot encoded representation (the target label).\n",
    "\n",
    "            # The negative log-likelihood loss is mathematically equivalent to multiplying the one-hot encoded target label with the logarithm of the corresponding probability for the true next word.\n",
    "\n",
    "            # By minimizing the cross-entropy loss, the model learns to improve its predictions and generate more accurate sequences of words.\n",
    "\n",
    "            # In summary, the provided code snippet reshapes the logits and targets to match the expected input shapes for the F.cross_entropy function. Inside the function, the logits are converted to probabilities using softmax, and the cross-entropy loss is calculated as the negative log-likelihood between the predicted probabilities and the true one-hot encoded target labels.\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \"\"\"\n",
    "        Job is to given the batch and some T generate the time dimension for the specified set of time (tokens)\n",
    "        \"\"\"\n",
    "        # idx is (B, T) arrary of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            # get the predictions\n",
    "            logits, loss = self.forward(idx, targets=None)\n",
    "\n",
    "            # focus only on the last time step\n",
    "            logits  = logits[:, -1, :] # Becomes (B, C)\n",
    "\n",
    "            #apply softmax to get probabilities \n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "\n",
    "            # smaple from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "\n",
    "            # append sampled index to the running sequence \n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "\n",
    "        return idx\n",
    "\n",
    "    \n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m.forward(idx =xb, targets=yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "    \n",
    "\n",
    "idx = torch.zeros((1, 1), dtype = torch.long)\n",
    "print(decode((m.generate(idx = idx, max_new_tokens=100).tolist()[0])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Pytorch optimizer\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=1e-3)\n",
    "\n",
    "# Typically good lr is 1e-4\n",
    "# Optimizer object will basically take the gradients and update the parameters using the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3796486854553223\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(10000):\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m.forward(idx=xb, targets=yb)\n",
    "    optimizer.zero_grad(set_to_none=True) # we are zeroing out all the gradients from the previous step\n",
    "    loss.backward() # getting the gradients for all the parameters\n",
    "    optimizer.step() # using those gradients to update our parameters\n",
    "\n",
    "\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1, 57, 59, 52,  6,  0, 31, 53])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "llo br. ave aviasurf my, mayo t ivee iuedrd whar ksth y h bora s be hese, woweee; the! KI 'de, ulseecherd d o blllando;\n",
      "\n",
      "Whe, oraingofof win!\n",
      "RIfans picspeserer hee tha,\n",
      "TOFonk? me ain ckntoty dedo bo'llll st ta d:\n",
      "ELIS me hurf lal y, ma dus pe athouo\n",
      "By bre ndy; by s afreanoo adicererupa anse tecorro llaus a!\n",
      "OLeneerithesinthengove fal amas trr\n",
      "TI ar I t, mes, n sar; my w, fredeeyong\n",
      "THek' merer,\n"
     ]
    }
   ],
   "source": [
    "print(decode((m.generate(idx = torch.zeros((1, 1), dtype = torch.long), max_new_tokens=400).tolist()[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Trick in Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [2.],\n",
       "        [3.]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.tril(torch.ones(3, 3)), 1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consider the following toy example:\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 2: using matrix multiply for a weighted aggregation\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1808, -0.0700],\n",
       "         [-0.1789, -0.9852],\n",
       "         [ 0.4469, -0.9597],\n",
       "         [ 1.4014, -0.8953],\n",
       "         [ 1.7626,  0.2725],\n",
       "         [ 0.4127, -0.2376],\n",
       "         [ 0.6486, -0.4774],\n",
       "         [-0.2725,  1.0659]],\n",
       "\n",
       "        [[ 1.3488, -0.1396],\n",
       "         [ 1.6346,  0.8255],\n",
       "         [-0.4025,  1.3186],\n",
       "         [ 1.0845,  1.9096],\n",
       "         [ 1.2105,  0.3470],\n",
       "         [ 0.0504,  0.0121],\n",
       "         [ 0.4982, -0.7895],\n",
       "         [ 2.0218,  1.7191]],\n",
       "\n",
       "        [[-0.6631, -0.2513],\n",
       "         [ 0.3470, -0.1297],\n",
       "         [ 0.5054,  1.0043],\n",
       "         [-0.6484,  0.7059],\n",
       "         [-1.1560, -0.2180],\n",
       "         [-0.6093, -1.7128],\n",
       "         [-1.8150, -1.1410],\n",
       "         [-2.4123, -1.8347]],\n",
       "\n",
       "        [[ 1.6455, -0.8030],\n",
       "         [ 2.9969, -1.0789],\n",
       "         [ 1.4861,  1.0259],\n",
       "         [ 4.2491, -0.7207],\n",
       "         [ 5.7007, -2.2310],\n",
       "         [ 6.5218, -2.4425],\n",
       "         [ 7.3007, -0.9092],\n",
       "         [ 8.9105, -1.3124]]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei / wei.sum(1, keepdim=True)\n",
    "wei @ x[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3488, -0.1396],\n",
       "        [ 1.6346,  0.8255],\n",
       "        [-0.4025,  1.3186],\n",
       "        [ 1.0845,  1.9096],\n",
       "        [ 1.2105,  0.3470],\n",
       "        [ 0.0504,  0.0121],\n",
       "        [ 0.4982, -0.7895],\n",
       "        [ 2.0218,  1.7191]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei @ x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xbow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m wei \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(wei, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     10\u001b[0m xbow3 \u001b[38;5;241m=\u001b[39m wei \u001b[38;5;241m@\u001b[39m x\n\u001b[0;32m---> 11\u001b[0m torch\u001b[38;5;241m.\u001b[39mallclose(\u001b[43mxbow\u001b[49m, xbow3)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xbow' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "# version 3: use Softmax\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T,T)) # dont actually want this to be all uniform. Because different tokens wil find different other tokens interesting and we want that to be data dependent.  \n",
    "# For example I am a vowel then may be I am looking for consonants in my past and may I want to know what those consonants are and I want that information to flow to me. \n",
    "# So I want to now gather information from the past, but I want to do it in a data dependent way. And this is the problem that self attention solves. \n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self-attention!\n",
    "\n",
    "# The way self attention essentially solves this is the following .\n",
    "# Every single node/ token at each position will emit two vectors. It will emit a query and It will emit a key. \n",
    "# The query vector roughly speaking is what am I looking for. And the key vector roughly speaking is what do I contain. \n",
    "# And then the way we get affinities between these tokens now in a sequence is basically we do a dot product between the keys and the queries. \n",
    "# So my query dot products will all the keys from all the other tokens. And that dot product now becomes wei. \n",
    "# Dot product is essentially how similar two vectors are to each other and a magnitude for that strenght. \n",
    "# thus how similar is my query to the keys from other tokens and the magnitude. so I will get to learn more about that token as compared to other tokens in the sequence\n",
    "# Tokens with higher similarity (larger dot product) will have higher attention weights in the final attention distribution.\n",
    "# so its essentially just finding similarity at scale in different contexts and different settings. the final weights effectively serving as an compression algo. \n",
    "\n",
    "# Now there is one more component, value. instead of aggreagting the value of token directly we use different value for that token. \n",
    "# so in a way its like hey here's my query, here's my key and if the query matches your key then this is the value that i will give to you. thus, the original value of token is private to that token. \n",
    "\n",
    "\n",
    "# Also, attention can be seen as a communication mechanism in a directed graph done in a data dependent manner. \n",
    "# What happens is every node has some vector of information and it gets to aggregate information via a weighted sum from all of the nodes that point to it, and this is done in a data dependent manner. \n",
    "# can be applied to any directed graph.\n",
    "# Also these vectors dont have a notion of space. If you want it then you'd need to add it. this is what we did when we calculated the positional embeddings and added it to that vector. \n",
    "\n",
    "# Also the batches dont talk to each other. Hence in this analogy, its similar to 4 seperate pools of 8 nodes. Hence 32 nodes being computed at once. \n",
    "\n",
    "# Now this is called self attention cause, key, query and value are coming from the same source (same x). \n",
    "# In principle, It can be very general - \n",
    "# say queries are produced from x, but keys and values come from a whole seperate source. \n",
    "# thats cross attention. Hence its used when we there is a seperate pool of nodes we'd like to pull information from into our nodes.\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32 # batch, time, channels\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "#Let;s see a single Head perform self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "wei = q @ k.transpose( -2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T)) \n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # prevents communication of past to future\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "         [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "         [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1687, 0.8313, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2477, 0.0514, 0.7008, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4410, 0.0957, 0.3747, 0.0887, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0069, 0.0456, 0.0300, 0.7748, 0.1427, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0660, 0.0892, 0.0413, 0.6316, 0.1649, 0.0069, 0.0000, 0.0000],\n",
       "         [0.0396, 0.2288, 0.0090, 0.2000, 0.2061, 0.1949, 0.1217, 0.0000],\n",
       "         [0.3650, 0.0474, 0.0767, 0.0293, 0.3084, 0.0784, 0.0455, 0.0493]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4820, 0.5180, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1705, 0.4550, 0.3745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0074, 0.7444, 0.0477, 0.2005, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.8359, 0.0416, 0.0525, 0.0580, 0.0119, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1195, 0.2061, 0.1019, 0.1153, 0.1814, 0.2758, 0.0000, 0.0000],\n",
       "         [0.0065, 0.0589, 0.0372, 0.3063, 0.1325, 0.3209, 0.1378, 0.0000],\n",
       "         [0.1416, 0.1519, 0.0384, 0.1643, 0.1207, 0.1254, 0.0169, 0.2408]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.6369, 0.3631, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2586, 0.7376, 0.0038, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4692, 0.3440, 0.1237, 0.0631, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1865, 0.4680, 0.0353, 0.1854, 0.1248, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0828, 0.7479, 0.0017, 0.0735, 0.0712, 0.0228, 0.0000, 0.0000],\n",
       "         [0.0522, 0.0517, 0.0961, 0.0375, 0.1024, 0.5730, 0.0872, 0.0000],\n",
       "         [0.0306, 0.2728, 0.0333, 0.1409, 0.1414, 0.0582, 0.0825, 0.2402]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.5713e-01,  8.8009e-01,  1.6152e-01, -7.8239e-01, -1.4289e-01,\n",
       "           7.4676e-01,  1.0068e-01, -5.2395e-01, -8.8726e-01,  1.9067e-01,\n",
       "           1.7616e-01, -5.9426e-01, -4.8124e-01, -4.8599e-01,  2.8623e-01,\n",
       "           5.7099e-01],\n",
       "         [ 6.7643e-01, -5.4770e-01, -2.4780e-01,  3.1430e-01, -1.2798e-01,\n",
       "          -2.9521e-01, -4.2962e-01, -1.0891e-01, -4.9282e-02,  7.2679e-01,\n",
       "           7.1296e-01, -1.1639e-01,  3.2665e-01,  3.4315e-01, -7.0975e-02,\n",
       "           1.2716e+00],\n",
       "         [ 4.8227e-01, -1.0688e-01, -4.0555e-01,  1.7696e-01,  1.5811e-01,\n",
       "          -1.6967e-01,  1.6217e-02,  2.1509e-02, -2.4903e-01, -3.7725e-01,\n",
       "           2.7867e-01,  1.6295e-01, -2.8951e-01, -6.7610e-02, -1.4162e-01,\n",
       "           1.2194e+00],\n",
       "         [ 1.9708e-01,  2.8561e-01, -1.3028e-01, -2.6552e-01,  6.6781e-02,\n",
       "           1.9535e-01,  2.8074e-02, -2.4511e-01, -4.6466e-01,  6.9287e-02,\n",
       "           1.5284e-01, -2.0324e-01, -2.4789e-01, -1.6213e-01,  1.9474e-01,\n",
       "           7.6778e-01],\n",
       "         [ 2.5104e-01,  7.3456e-01,  5.9385e-01,  2.5159e-01,  2.6064e-01,\n",
       "           7.5820e-01,  5.5947e-01,  3.5387e-01, -5.9338e-01, -1.0807e+00,\n",
       "          -3.1110e-01, -2.7809e-01, -9.0541e-01,  1.3181e-01, -1.3818e-01,\n",
       "           6.3715e-01],\n",
       "         [ 3.4277e-01,  4.9605e-01,  4.7248e-01,  3.0277e-01,  1.8440e-01,\n",
       "           5.8144e-01,  3.8245e-01,  2.9521e-01, -4.8969e-01, -7.7051e-01,\n",
       "          -1.1721e-01, -2.5412e-01, -6.8921e-01,  1.9795e-01, -1.5135e-01,\n",
       "           7.6659e-01],\n",
       "         [ 1.8658e-01, -9.6352e-02, -1.4300e-01,  3.0587e-01,  8.3441e-02,\n",
       "          -6.8647e-03, -2.0472e-01, -1.5350e-01, -7.6250e-02,  3.2689e-01,\n",
       "           3.0896e-01,  7.6626e-02,  9.9243e-02,  1.6560e-01,  1.9745e-01,\n",
       "           7.6248e-01],\n",
       "         [ 1.3013e-01, -3.2832e-02, -4.9645e-01,  2.8652e-01,  2.7042e-01,\n",
       "          -2.6357e-01, -7.3756e-02,  3.7857e-01,  7.4579e-02,  3.3827e-02,\n",
       "           1.4695e-02,  3.1937e-01,  2.9926e-01, -1.6530e-01, -3.8630e-02,\n",
       "           3.3748e-01]],\n",
       "\n",
       "        [[-1.3254e+00,  1.1236e+00,  2.2927e-01, -2.9970e-01, -7.6265e-03,\n",
       "           7.9364e-01,  8.9581e-01,  3.9650e-01, -6.6613e-01, -2.1844e-01,\n",
       "          -1.3539e+00,  4.1245e-01,  9.6011e-01, -1.0805e+00, -3.9751e-01,\n",
       "          -4.4439e-01],\n",
       "         [-3.8338e-01, -1.9659e-01,  8.8455e-02,  1.8560e-01, -8.7010e-02,\n",
       "           1.3239e-01,  3.0841e-01, -2.4350e-01, -1.9396e-01, -1.7634e-02,\n",
       "           4.8438e-01,  5.4210e-01, -2.0407e-02, -4.2467e-01, -2.3463e-01,\n",
       "          -4.6465e-01],\n",
       "         [-1.1100e+00,  3.2334e-01,  4.7054e-01, -6.3595e-02,  2.5443e-01,\n",
       "           1.5352e-01,  2.5186e-01,  2.6286e-01,  2.7916e-01, -3.1660e-03,\n",
       "          -3.2881e-02,  4.8191e-01,  7.4431e-01, -1.9921e-01,  2.7134e-01,\n",
       "          -8.5871e-02],\n",
       "         [-9.7190e-01,  4.6124e-01,  4.2349e-01, -1.7230e-02,  1.5847e-01,\n",
       "           4.1175e-01,  4.0764e-01,  2.4982e-01, -5.0321e-02,  4.1516e-03,\n",
       "          -3.9853e-01,  4.3551e-01,  7.0285e-01, -4.3081e-01,  2.6684e-02,\n",
       "          -2.0169e-01],\n",
       "         [ 3.3586e-01, -8.5915e-02,  9.3660e-01,  7.7311e-01,  1.8037e-01,\n",
       "           8.2853e-01, -6.9183e-02,  2.8814e-01,  1.1734e-01,  6.8448e-01,\n",
       "          -5.8500e-02,  1.2726e-01,  2.9780e-01,  1.9324e-01,  1.5655e-01,\n",
       "          -9.3006e-03],\n",
       "         [ 1.6984e-01,  3.0993e-02,  8.1557e-01,  6.1679e-01,  1.0429e-01,\n",
       "           7.4573e-01,  2.3072e-02,  3.0572e-01,  5.8163e-02,  5.7122e-01,\n",
       "          -4.5275e-02,  1.5051e-01,  3.2901e-01,  5.6984e-02,  1.0311e-01,\n",
       "          -9.9174e-02],\n",
       "         [ 4.6497e-02,  1.5765e-01,  3.9760e-01,  1.7619e-01, -2.1168e-01,\n",
       "           2.3365e-01, -6.2083e-02,  2.1726e-01, -7.8725e-03,  4.5389e-01,\n",
       "           3.4349e-01, -5.5631e-02,  3.3726e-01, -3.7591e-01, -1.0140e-02,\n",
       "          -4.5806e-01],\n",
       "         [-5.3896e-01,  7.5555e-01,  3.3034e-01, -1.5849e-01, -2.6740e-01,\n",
       "           4.3495e-01,  3.7772e-01,  5.5794e-01, -1.8369e-01,  1.5938e-01,\n",
       "          -2.1042e-01,  5.5789e-02,  6.3184e-01, -6.4884e-01, -9.6084e-02,\n",
       "          -5.0751e-01]],\n",
       "\n",
       "        [[ 6.8925e-02,  1.2248e+00, -4.1194e-01, -1.7046e-01, -6.9224e-01,\n",
       "          -2.9201e-01,  1.2704e+00, -6.8596e-01,  4.3798e-01, -2.6366e-01,\n",
       "           1.1528e-01,  1.1676e+00, -7.2138e-01, -1.2308e+00,  8.3821e-01,\n",
       "          -5.5987e-01],\n",
       "         [-4.6375e-01,  6.3807e-01, -1.5842e-01, -1.3309e-01, -5.9402e-01,\n",
       "          -5.0374e-01,  2.3289e-01, -3.2126e-01,  4.5781e-01, -1.8590e-01,\n",
       "           1.9215e-01,  3.7566e-01, -3.5905e-01, -7.7262e-01,  3.5036e-01,\n",
       "           6.9694e-02],\n",
       "         [-6.4044e-01,  1.3831e-01, -6.1007e-02, -1.1112e-01, -4.5228e-01,\n",
       "          -6.2271e-01, -1.7030e-01, -2.4949e-01,  5.0670e-01, -9.6444e-02,\n",
       "           4.8315e-01,  9.4986e-02, -2.9810e-01, -3.6538e-01,  3.9458e-01,\n",
       "           4.1512e-01],\n",
       "         [-6.7193e-01,  1.2516e-01,  7.3386e-02, -1.3198e-01, -1.7880e-01,\n",
       "          -5.6740e-01, -6.8226e-01,  5.0844e-02,  3.3051e-01,  7.8242e-02,\n",
       "           6.8022e-02, -2.4041e-01, -6.6864e-02, -1.8411e-01, -5.3514e-02,\n",
       "           4.5113e-01],\n",
       "         [-1.4270e-02,  1.0195e+00, -3.4792e-01, -1.6421e-01, -5.5846e-01,\n",
       "          -3.2457e-01,  9.9404e-01, -5.6891e-01,  4.0097e-01, -1.8123e-01,\n",
       "           1.1856e-01,  9.8704e-01, -6.4057e-01, -1.0320e+00,  7.3320e-01,\n",
       "          -4.3167e-01],\n",
       "         [-6.3858e-01, -7.6533e-02, -3.6510e-01,  1.7782e-01, -6.5426e-02,\n",
       "          -3.5158e-01,  7.9591e-02,  1.7384e-01,  3.6676e-01, -4.2302e-02,\n",
       "           2.4923e-01,  4.8239e-01, -2.1295e-01, -2.9492e-01,  3.4749e-01,\n",
       "          -1.7111e-01],\n",
       "         [-2.2366e-01, -5.5317e-02, -1.8296e-01,  2.4258e-01,  2.5357e-01,\n",
       "          -1.6154e-01, -2.3908e-01,  3.3243e-01,  1.0304e-01,  2.6067e-01,\n",
       "          -5.0670e-02,  3.6947e-01, -4.9856e-02,  1.1197e-01,  1.1752e-01,\n",
       "          -2.5078e-01],\n",
       "         [-2.4821e-01,  1.4845e-01, -3.5033e-01,  1.7102e-01,  1.6613e-01,\n",
       "          -2.0643e-01,  8.6633e-02,  8.8414e-02,  2.1188e-01,  2.5805e-01,\n",
       "           5.5146e-02,  4.2668e-01, -2.0443e-01, -1.7372e-01,  3.8899e-01,\n",
       "           5.1725e-02]],\n",
       "\n",
       "        [[ 9.7183e-02,  5.7301e-02, -1.0468e-01, -4.6654e-02, -1.4006e-01,\n",
       "          -8.4126e-01, -1.3625e-01, -6.7465e-01, -2.1541e-01,  1.0993e+00,\n",
       "           2.3427e-01,  3.2605e-02, -1.8521e-01,  1.4780e-01, -6.1045e-01,\n",
       "           1.5391e+00],\n",
       "         [ 1.9305e-01, -2.1031e-01, -3.4658e-01,  2.0567e-01, -1.7798e-01,\n",
       "          -7.4604e-01, -6.4427e-01, -6.9183e-01, -2.0558e-01,  7.0413e-01,\n",
       "           2.3632e-01,  9.8800e-04, -1.7015e-01,  1.1203e-01, -7.1064e-01,\n",
       "           1.2431e+00],\n",
       "         [ 2.9114e-01, -4.8343e-01, -5.9254e-01,  4.6477e-01, -2.1832e-01,\n",
       "          -6.4460e-01, -1.1627e+00, -7.0993e-01, -1.9703e-01,  2.9262e-01,\n",
       "           2.3669e-01, -3.1049e-02, -1.5471e-01,  7.7153e-02, -8.1137e-01,\n",
       "           9.3578e-01],\n",
       "         [ 1.7549e-01, -3.4260e-02, -2.0523e-01,  2.7644e-02, -2.1312e-01,\n",
       "          -5.6022e-01, -3.5273e-01, -6.2722e-01, -3.0037e-01,  4.6061e-01,\n",
       "           1.5004e-01,  1.9040e-02, -1.4646e-01,  1.7220e-01, -6.2559e-01,\n",
       "           1.0722e+00],\n",
       "         [ 1.7354e-01, -1.7962e-01, -2.7874e-01, -1.0590e-01, -1.2952e-01,\n",
       "          -3.5086e-01, -5.5830e-01, -3.8638e-01, -2.9719e-01,  3.3368e-02,\n",
       "           1.7392e-01,  5.5898e-02, -7.2007e-02,  1.3182e-02, -6.6710e-01,\n",
       "           5.4229e-01],\n",
       "         [ 2.4678e-01, -4.7274e-01, -5.2827e-01,  3.1212e-01, -1.7528e-01,\n",
       "          -4.8636e-01, -1.1223e+00, -5.4196e-01, -2.0142e-01,  4.0103e-02,\n",
       "           2.2231e-01, -2.9380e-02, -9.4353e-02,  2.6374e-02, -7.8726e-01,\n",
       "           6.2836e-01],\n",
       "         [-3.9784e-01,  2.5915e-01,  5.0358e-01, -4.6864e-01, -2.2024e-02,\n",
       "          -3.2242e-01, -1.2578e-01,  1.0634e-01,  1.3618e-01,  1.7780e-01,\n",
       "           1.0391e-01, -6.2540e-01,  3.8904e-01,  3.3690e-01, -5.5140e-01,\n",
       "           5.2246e-01],\n",
       "         [-3.5927e-01,  3.3935e-02, -2.9863e-02, -1.5019e-01, -6.0351e-03,\n",
       "          -6.5732e-02, -3.9659e-01, -6.0435e-02, -5.7551e-01, -2.9157e-01,\n",
       "           1.4899e-01, -7.5002e-02,  7.3228e-02, -4.7413e-02, -6.4394e-01,\n",
       "           2.8560e-01]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T)) \n",
    "tril"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nano_gpt_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
